Issue Type,Summary,Description,Custom field (Story Points),Version,Contributors,Developer_Rank,Developer_commits,Developer_created_MRs,Developer_modified_files,Developer_commits_reviews,Developer_updated_MRs,Developer_fixed_defects,Developer_ARs,Tester_detected_defects,Tester_ARs,Creator_ARs,Version_encoded
Bug,Reactivating a draining agent leaves the agent in draining state.,"When reactivating an agent that's in the draining state, the master erases it from its draining maps, and erases its estimated drain time._x000D_
_x000D_
However, it doesn't send any message to the agent, so if the agent is still draining and waiting for tasks to terminate, it will stay in that state, ultimately making any tasks that then get launched get DROPPED due to the agent still being in a draining state._x000D_
_x000D_
Seems like we should either:_x000D_
_x000D_
* Disallow the user from reactivating if still in draining, or_x000D_
* Send a message to the agent, and have the agent move itself out of draining.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.0
Improvement,Agent draining logging makes it hard to tell which tasks did not terminate.,"When draining an agent, it's hard to tell which tasks failed to terminate._x000D_
_x000D_
The master prints a count of the tasks remaining (only as VLOG(1) however), but not the IDs:_x000D_
_x000D_
{noformat}_x000D_
I1223 13:19:49.021764 30480 master.cpp:6367] DRAINING Agent c0146010-8af6-4a9d-bcdb-99e30a778663-S6 has 0 pending tasks, 1 tasks, and 0 operations_x000D_
{noformat}_x000D_
_x000D_
The agent does not print how many or which ones._x000D_
_x000D_
It would be helpful to at least see which tasks need to be drained when it begins, and possibly, upon each check, which ones remain.",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.0
Bug,Master's agent draining VLOG prints incorrect task counts.,"This logic is printing the framework counts of these maps rather than the task counts:_x000D_
_x000D_
https://github.com/apache/mesos/blob/4575c9b452c25f64e6c6cc3eddc12ed3b1f8538b/src/master/master.cpp#L6318-L6319_x000D_
_x000D_
{code}_x000D_
  // Check if the agent has any tasks running or operations pending._x000D_
  if (!slave->pendingTasks.empty() ||_x000D_
      !slave->tasks.empty() ||_x000D_
      !slave->operations.empty()) {_x000D_
    VLOG(1)_x000D_
      << ""DRAINING Agent "" << slaveId << "" has ""_x000D_
      << slave->pendingTasks.size() << "" pending tasks, ""_x000D_
      << slave->tasks.size() << "" tasks, and ""_x000D_
      << slave->operations.size() << "" operations"";_x000D_
    return;_x000D_
  }_x000D_
{code}_x000D_
_x000D_
Since these are {{hashmap<FrameworkID, hashmap<TaskID, Task>>}}.",1.0,1.9.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.6282051282051282,0.7515151515151515,0.7515151515151515,1.0
Task,Implement SSL downgrade on the native SSL socket,"The new SSL socket implementation (the non-libevent one) does not currently implement the SSL downgrade hack.  We could probably use {{peek}} to achieve the same result, or modify our socket BIO to look at the first few bytes.",3.0,0,0.0,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.0
Task,Implement chmod() support for stout,"When using executor domain sockets, we need to be able to change permissions on the domain socket to 0600. To do that, we should implement a new function `os::chmod()` in stout.",1.0,0,0.0,0.0,0.3333333333333333,0.5,0.14285714285714285,0.0,0.2,0.5087719298245614,0.4357142857142857,0.16666666666666666,0.1818181818181818,0.17575757575757575,0.0
Task,Let the command executor connect through a domain socket when available,"If the command executor is using the v1 API (--http_command_executors agent flag) and the MESOS_DOMAIN_SOCKET environment variable is set, the command executor should use the domain socket to communicate with the agent or die trying.",3.0,0,0.0,0.0,0.3333333333333333,0.5,0.14285714285714285,0.0,0.2,0.5087719298245614,0.4357142857142857,0.16666666666666666,0.1818181818181818,0.17575757575757575,0.0
Task,Validate task's resources limits and the `share_cgroups` field,"When launching a task, we need to validate:_x000D_
 # Only CPU and memory are supported as resource limits._x000D_
 # Resource limit must be larger than resource request._x000D_
 # `TaskInfo` can only include resource limits when the relevant agent possesses the TASK_RESOURCE_LIMITS capability._x000D_
 # The value of the field `share_cgroups` should be same for all the tasks launched by a single default executor._x000D_
 # It is not allowed to set resource limits for the task which has the field `share_cgroups` set as true._x000D_
_x000D_
We also need to add validation to the agent which will ensure that non-debug 2nd-or-lower-level nested containers cannot be launched via the {{LaunchContainer}} call.",3.0,0,0.5,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.23076923076923075,0.296969696969697,0.296969696969697,0.0
Bug,Libprocess SSL verification can leak memory,"In {{process::network::openssl::verify()}}, when the SSL hostname validation scheme is set to ""openssl"", the function can return without freeing an {{X509}} object, leading to a memory leak.",1.0,1.9.0,0.5,0.0,0.3333333333333333,0.5,0.14285714285714285,0.0,0.2,0.5087719298245614,0.4357142857142857,0.6025641025641025,0.8242424242424242,0.8181818181818181,1.0
Bug,Agent's 'executorTerminated()' can cause double task status update,"When the agent first receives a task status update from an executor, it executes {{Slave::statusUpdate()}}, which adds the task ID to the {{Executor::pendingStatusUpdates}} map, but leaves the ID in {{Executor::launchedTasks}}._x000D_
_x000D_
Meanwhile, the code in {{Slave::executorTerminated()}} is not capable of handling the intermediate task state which exists in between the execution of {{Slave::statusUpdate()}} and {{Slave::_statusUpdate()}}. If {{Slave::executorTerminated()}} executes at that point in time, it's possible that the task will be transitioned to a terminal state twice (for example, it could be transitioned to TASK_FINISHED by the executor, then to TASK_FAILED by the agent if the executor suddenly terminates)._x000D_
_x000D_
If the agent has already received a status update from an executor, that state transition should be honored even if the executor terminates immediately after it's sent. We should ensure that {{Slave::executorTerminated()}} cannot cause a valid update received from an executor to be ignored.",3.0,1.9.0,0.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,1.0
Improvement,Log all reverse DNS lookup failures in 'legacy' TLS (SSL) hostname validation scheme.,"There were being logged at VLOG(2):_x000D_
_x000D_
https://github.com/apache/mesos/blob/1.9.0/3rdparty/libprocess/src/openssl.cpp#L859-L860_x000D_
_x000D_
In the same spirit as MESOS-9340, we'd like to log all networking related errors as warnings and include any relevant information (IP address, etc).",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.0
Task,Implement glue code for the Windows event loop and OpenSSL's basic I/O abstraction,"In order for the Windows event loop to pass data to the OpenSSL library, we will need some glue code in the form of a ""BIO"":_x000D_
https://www.openssl.org/docs/man1.1.1/man7/bio.html_x000D_
_x000D_
This will basically need to wrap the two {{windows::read}} and {{windows::write}} async I/O functions in the appropriate callbacks necessary for OpenSSL.  There are also a few other callbacks necessary.  This page contains the set of functions used to build up a new BIO type:_x000D_
https://www.openssl.org/docs/man1.1.1/man3/BIO_meth_new.html",3.0,0,0.0,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.0
Bug,Very large quota values can crash master.,"We are observing the following crash on the 1.9.1 master:_x000D_
_x000D_
{code}_x000D_
I1008 10:12:15.148486  4687 http.cpp:1115] HTTP POST for /master/api/v1?_ts=1570529541073&UPDATE_QUOTA from 10.0.7.253:35410 with User-Agent='Mozilla/5.0 (Windows NT 6.1; Win64; x64) Ap>_x000D_
I1008 10:12:15.148665  4687 http.cpp:263] Processing call UPDATE_QUOTA_x000D_
I1008 10:12:15.148756  4687 quota_handler.cpp:1136] Authorizing principal 'bootstrapuser' to update quota config for role 's1'_x000D_
I1008 10:12:15.149169  4685 registrar.cpp:487] Applied 1 operations in 56277ns; attempting to update the registry_x000D_
I1008 10:12:15.149338  4681 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 13_x000D_
I1008 10:12:15.149467  4689 replica.cpp:541] Replica received write request for position 13 from __req_res__(29)@10.0.7.253:5050_x000D_
I1008 10:12:15.151820  4683 replica.cpp:695] Replica received learned notice for position 13 from log-network(2)@10.0.7.253:5050_x000D_
I1008 10:12:15.153559  4679 registrar.cpp:544] Successfully updated the registry in 4.348928ms_x000D_
I1008 10:12:15.153592  4678 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 14_x000D_
I1008 10:12:15.153715  4679 hierarchical.cpp:1619] Updated quota for role 's1',  guarantees: {} limits: cpus:2; disk:-9.22337203685478e+15; gpus:3; mem:1000000000000_x000D_
I1008 10:12:15.153796  4677 replica.cpp:541] Replica received write request for position 14 from __req_res__(30)@10.0.7.253:5050_x000D_
I1008 10:12:15.155380  4691 replica.cpp:695] Replica received learned notice for position 14 from log-network(2)@10.0.7.253:5050_x000D_
I1008 10:12:15.249722  4677 authenticator.cpp:324] dstip=10.0.7.253 type=audit timestamp=2019-10-08 10:12:15.249673984+00:00 reason=""Valid authentication token"" uid=""bootstrapuser"" obje>_x000D_
I1008 10:12:15.249956  4682 http.cpp:1115] HTTP GET for /master/state-summary?_ts=1570529541169 from 10.0.7.253:35414 with User-Agent='Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebK>_x000D_
I1008 10:12:15.250633  4691 http.cpp:1132] HTTP GET for /master/state-summary?_ts=1570529541169 from 10.0.7.253:35414: '200 OK' after 1.72621ms_x000D_
I1008 10:12:15.570379  4689 hierarchical.cpp:1908] Before allocation, required quota headroom is {} and available quota headroom is cpus:0.9; disk:75853; mem:5507_x000D_
F1008 10:12:15.570580  4689 resource_quantities.cpp:330] Check failed: scalar >= Value::Scalar() (-9.22337203685478e+15 vs. 0)_x000D_
*** Check failure stack trace: ***_x000D_
    @     0x7fc786f0148d  google::LogMessage::Fail()_x000D_
    @     0x7fc786f036e8  google::LogMessage::SendToLog()_x000D_
    @     0x7fc786f01023  google::LogMessage::Flush()_x000D_
    @     0x7fc786f04029  google::LogMessageFatal::~LogMessageFatal()_x000D_
    @     0x7fc785954dfa  mesos::ResourceQuantities::add()_x000D_
    @     0x7fc785954fb6  mesos::ResourceQuantities::fromScalarResource()_x000D_
    @     0x7fc78595e135  mesos::shrinkResources()_x000D_
    @     0x7fc785a874a9  mesos::internal::master::allocator::internal::HierarchicalAllocatorProcess::__allocate()_x000D_
    @     0x7fc785a88089  mesos::internal::master::allocator::internal::HierarchicalAllocatorProcess::_allocate()_x000D_
    @     0x7fc785a93882  _ZNO6lambda12CallableOnceIFvPN7process11ProcessBaseEEE10CallableFnINS_8internal7PartialIZNS1_8dispatchI7NothingN5mesos8internal6master9allocator8internal28Hier>_x000D_
    @     0x7fc786e49e21  process::ProcessBase::consume()_x000D_
    @     0x7fc786e6141b  process::ProcessManager::resume()_x000D_
    @     0x7fc786e670b6  _ZNSt6thread5_ImplISt12_Bind_simpleIFZN7process14ProcessManager12init_threadsEvEUlvE_vEEE6_M_runEv_x000D_
    @     0x7fc782a28b22  (unknown)_x000D_
    @     0x7fc7821be94a  (unknown)_x000D_
    @     0x7fc781eef07f  clone_x000D_
{code}_x000D_
_x000D_
Note that the value of disk quota limit is *logged* as ""negative""._x000D_
_x000D_
Update: we figured out that in reality the quota limit on that master has been set to an insanely large value._x000D_
_x000D_
The situation is exacerbated by the fact that the crash is not guaranteed to occur immediately, i.e. these values might become persisted in the registry.",3.0,1.9.0,0.5,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.14102564102564102,0.08484848484848485,0.08484848484848485,1.0
Bug,Command executor can miss exit status for short-lived commands due to double-reaping.,"Hi,_x000D_
_x000D_
While testing Mesos to see if we could use it at work, I encountered a random bug which I believe happens when a command exits really quickly, when run via the command executor._x000D_
_x000D_
See the attached test case, but basically all it does is constantly start ""exit 0"" tasks._x000D_
_x000D_
At some point, a task randomly fails with the error ""Failed to get exit status for Command"":_x000D_
_x000D_
 _x000D_
{noformat}_x000D_
'state': 'TASK_FAILED', 'message': 'Failed to get exit status for Command', 'source': 'SOURCE_EXECUTOR',{noformat}_x000D_
  _x000D_
_x000D_
I've had a look at the code, and I found something which could potentially explain it - it's the first time I look at the code so apologies if I'm missing something._x000D_
_x000D_
 We can see the error originates from `reaped`:_x000D_
_x000D_
[https://github.com/apache/mesos/blob/master/src/launcher/executor.cpp#L1017]_x000D_
{noformat}_x000D_
    } else if (status_->isNone()) {_x000D_
      taskState = TASK_FAILED;_x000D_
      message = ""Failed to get exit status for Command"";_x000D_
    } else {{noformat}_x000D_
 _x000D_
_x000D_
Looking at the code, we can see that the `status_` future can be set to `None` in `ReaperProcess::reap`:_x000D_
_x000D_
[https://github.com/apache/mesos/blob/master/3rdparty/libprocess/src/reap.cpp#L69]_x000D_
_x000D_
 _x000D_
_x000D_
 _x000D_
{noformat}_x000D_
Future<Option<int>> ReaperProcess::reap(pid_t pid)_x000D_
{_x000D_
  // Check to see if this pid exists._x000D_
  if (os::exists(pid)) {_x000D_
    Owned<Promise<Option<int>>> promise(new Promise<Option<int>>());_x000D_
    promises.put(pid, promise);_x000D_
    return promise->future();_x000D_
  } else {_x000D_
    return None();_x000D_
  }_x000D_
}{noformat}_x000D_
 _x000D_
_x000D_
 _x000D_
_x000D_
So we could have this if the process has already been reaped (`kill -0` will fail)._x000D_
_x000D_
 _x000D_
_x000D_
Now, looking at the code path which spawns the process:_x000D_
_x000D_
`launchTaskSubprocess`_x000D_
_x000D_
[https://github.com/apache/mesos/blob/master/src/launcher/executor.cpp#L724]_x000D_
_x000D_
 _x000D_
_x000D_
calls `subprocess`:_x000D_
_x000D_
[https://github.com/apache/mesos/blob/master/3rdparty/libprocess/src/subprocess.cpp#L315]_x000D_
_x000D_
 _x000D_
_x000D_
If we look at the bottom of the function we can see the following:_x000D_
_x000D_
[https://github.com/apache/mesos/blob/master/3rdparty/libprocess/src/subprocess.cpp#L462]_x000D_
_x000D_
 _x000D_
_x000D_
 _x000D_
{noformat}_x000D_
  // We need to bind a copy of this Subprocess into the onAny callback_x000D_
  // below to ensure that we don't close the file descriptors before_x000D_
  // the subprocess has terminated (i.e., because the caller doesn't_x000D_
  // keep a copy of this Subprocess around themselves)._x000D_
  process::reap(process.data->pid)_x000D_
    .onAny(lambda::bind(internal::cleanup, lambda::_1, promise, process));  return process;{noformat}_x000D_
 _x000D_
_x000D_
 _x000D_
_x000D_
So at this point we've already called `process::reap`._x000D_
_x000D_
 _x000D_
_x000D_
And after that, the executor also calls `process::reap`:_x000D_
_x000D_
[https://github.com/apache/mesos/blob/master/src/launcher/executor.cpp#L801]_x000D_
_x000D_
 _x000D_
_x000D_
 _x000D_
{noformat}_x000D_
    // Monitor this process._x000D_
    process::reap(pid.get())_x000D_
      .onAny(defer(self(), &Self::reaped, pid.get(), lambda::_1));{noformat}_x000D_
 _x000D_
_x000D_
 _x000D_
_x000D_
But if we look at the implementation of `process::reap`:_x000D_
_x000D_
[https://github.com/apache/mesos/blob/master/3rdparty/libprocess/src/reap.cpp#L152]_x000D_
_x000D_
 _x000D_
_x000D_
 _x000D_
{noformat}_x000D_
Future<Option<int>> reap(pid_t pid)_x000D_
{_x000D_
  // The reaper process is instantiated in `process::initialize`._x000D_
  process::initialize();  return dispatch(_x000D_
      internal::reaper,_x000D_
      &internal::ReaperProcess::reap,_x000D_
      pid);_x000D_
}{noformat}_x000D_
We can see that `ReaperProcess::reap` is going to get called asynchronously._x000D_
_x000D_
 _x000D_
_x000D_
Doesn't this mean that it's possible that the first call to `reap` set up by `subprocess` ([https://github.com/apache/mesos/blob/master/3rdparty/libprocess/src/subprocess.cpp#L462)|https://github.com/apache/mesos/blob/master/3rdparty/libprocess/src/subprocess.cpp#L462]_x000D_
_x000D_
will get executed first, and if the task has already exited by that time, the child will get reaped before the call to `reap` set up by the executor ([https://github.com/apache/mesos/blob/master/src/launcher/executor.cpp#L801]) gets a chance to run?_x000D_
_x000D_
 _x000D_
_x000D_
In that case, when it runs_x000D_
_x000D_
 _x000D_
{noformat}_x000D_
if (os::exists(pid)) {{noformat}_x000D_
would return false, `reap` would set the future to None which would result in this error._x000D_
_x000D_
 ",1.0,0,0.5,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.01282051282051282,0.0,0.0,0.0
Task,Consolidate 'Master::authorizeReserveResources' overloads,"We should remove {{Master::authorizeReserveResources(Resources, Option<Principal>}} in favor of {{Master::authorizeReserveResources(Reserve, Option<Principal>)}}.",1.0,0,0.5,0.0,0.3333333333333333,0.5,0.14285714285714285,0.0,0.2,0.5087719298245614,0.4357142857142857,1.0,0.8121212121212121,0.8121212121212121,0.0
Task,Update 'Master::Http::_reserve' to also require 'source' resources,We need to always pass {{source}} into {{Master::Http::_reserve}}.,2.0,0,0.5,0.0,0.3333333333333333,0.5,0.14285714285714285,0.0,0.2,0.5087719298245614,0.4357142857142857,1.0,0.8121212121212121,0.8121212121212121,0.0
Task,Update validation of 'ReserveResources' for 'source',We need to update {{master::validation::master::call}} for {{source}}. In particular we need to require that {{source}} and {{resources}} have a common ancestor.,1.0,0,0.5,0.0,0.3333333333333333,0.5,0.14285714285714285,0.0,0.2,0.5087719298245614,0.4357142857142857,1.0,0.8121212121212121,0.8121212121212121,0.0
Task,"Provide a function to compute a common ""reservation ancestor"" between two 'Resources'","We need to provide a function to compute a common ""reservation ancestor"" between two resources, {{Try<Resources> getReservationAncestor(const Resources&, const Resources&)}}._x000D_
_x000D_
The common ancestor can be found by repeatedly popping dynamic reservations from the full {{Resources}}._x000D_
_x000D_
We should test the following cases:_x000D_
 * either LHS or RHS empty_x000D_
 * both empty -> empty ancestor_x000D_
 * {{STATIC}} reservations on path_x000D_
 * partially reserved LHS/RHS (partially reserved: not all {{Resource}} have the same reservation).",2.0,0,0.5,0.0,0.3333333333333333,0.5,0.14285714285714285,0.0,0.2,0.5087719298245614,0.4357142857142857,1.0,0.8121212121212121,0.8121212121212121,0.0
Task,Intermediate rejection of Reserve operations with source set,We need to update {{Master::authorizeReserveResources}} to reject any {{Reserve}} operation whenever {{source}} is set until we have a proper implementation in place.,1.0,0,0.5,0.0,0.3333333333333333,0.5,0.14285714285714285,0.0,0.2,0.5087719298245614,0.4357142857142857,1.0,0.8121212121212121,0.8121212121212121,0.0
Bug,Sorter may leak clients allocations.,"In MESOS-9015, we allowed resource quantities to change when updating an existing allocation. When the allocation is updated to empty, however, we forget to remove the client in the map in the `sorter::update()` if the `newAllocation` is `empty()`._x000D_
_x000D_
https://github.com/apache/mesos/blob/master/src/master/allocator/mesos/sorter/drf/sorter.hpp#L382-L384_x000D_
_x000D_
The above case could happen, for example, when a CSI volume with a stale profile is destroyed, it would be better to convert it into an empty resource since the disk space is no longer available. ",2.0,0,0.0,0.0,0.6666666666666666,1.0,0.5714285714285714,0.0,0.0,0.5438596491228069,0.5357142857142857,0.3974358974358974,0.4,0.4,0.0
Bug,"'dist' and 'distcheck' cmake targets are implemented as shell scripts, so fail on Windows/MSVC.","Mesos failed to build due to error MSB6006: ""cmd.exe"" exited with code 1 on Windows using MSVC. It can be first reproduced on {color:#24292e}e0f7e2d{color} reversion on master branch. Could you please take a look at this isssue? Thanks a lot!_x000D_
_x000D_
Reproduce steps:_x000D_
_x000D_
1. git clone -c core.autocrlf=true [https://github.com/apache/mesos] D:\mesos\src_x000D_
 2. Open a VS 2017 x64 command prompt as admin and browse to D:\mesos_x000D_
 3. cd src_x000D_
 4. .\bootstrap.bat_x000D_
 5. cd .._x000D_
 6. mkdir build_x64 && pushd build_x64_x000D_
 7. cmake ..\src -G ""Visual Studio 15 2017 Win64"" -DCMAKE_SYSTEM_VERSION=10.0.17134.0 -DENABLE_LIBEVENT=1 -DHAS_AUTHENTICATION=0 -DPATCHEXE_PATH=""C:\gnuwin32\bin"" -T host=x64_x000D_
 8. msbuild Mesos.sln /p:Configuration=Debug /p:Platform=x64 /maxcpucount:4 /t:Rebuild_x000D_
_x000D_
 _x000D_
_x000D_
ErrorMessage:_x000D_
_x000D_
67>PrepareForBuild:_x000D_
         Creating directory ""x64\Debug\dist\dist.tlog\""._x000D_
       InitializeBuildStatus:_x000D_
         Creating ""x64\Debug\dist\dist.tlog\unsuccessfulbuild"" because ""AlwaysCreate"" was specified._x000D_
_x000D_
67>C:\Program Files (x86)\Microsoft Visual Studio\2017\Enterprise\Common7\IDE\VC\VCTargets\Microsoft.CppCommon.targets(209,5): error MSB6006: ""cmd.exe"" exited with code 1. [D:\Mesos\build_x64\dist.vcxproj]_x000D_
67>Done Building Project ""D:\Mesos\build_x64\dist.vcxproj"" (Rebuild target(s)) -- FAILED._x000D_
_x000D_
 ",1.0,master,0.5,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.038461538461538464,0.01212121212121212,0.01212121212121212,0.0
Bug,agent should not send `TASK_GONE_BY_OPERATOR` if the framework is not partition aware.,"The Mesos agent should not send `TASK_GONE_BY_OPERATOR` if the framework is not partition-aware. We should distinguish the framework capability and send different updates to legacy frameworks._x000D_
_x000D_
The issue is exposed from here:_x000D_
https://github.com/apache/mesos/blob/f0be23765531b05661ed7f1b124faf96744aa80b/src/slave/slave.cpp#L5803_x000D_
_x000D_
An example to follow:_x000D_
https://github.com/apache/mesos/blob/f0be23765531b05661ed7f1b124faf96744aa80b/src/master/master.cpp#L9921",1.0,0,0.5,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6794871794871795,0.5939393939393939,0.5939393939393939,0.0
Bug,Agent could fail to report completed tasks.,"When agent reregisters with a master, we don't report completed executors for active frameworks. We only report completed executors if the framework is also completed on the agent:_x000D_
_x000D_
https://github.com/apache/mesos/blob/1.7.x/src/slave/slave.cpp#L1785-L1832",2.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3974358974358974,0.4,0.4,0.0
Bug,Java bindings sporadically fail to build,"We sporadically (maybe once a month?) observe build failures in the java bindings in our internal CI. They look like this:_x000D_
_x000D_
{noformat}_x000D_
14:32:18 [ERROR] /home/ubuntu/workspace/mesos/Mesos_CI-build/FLAG/SSL/label/mesos-ec2-ubuntu-16.04/mesos/build/src/java/generated/org/apache/mesos/Protos.java:[14594,45] error: cannot access StringBuilder_x000D_
14:32:18 [ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.5.1:compile (default-compile) on project mesos: Compilation failure_x000D_
14:32:18 [ERROR] /home/ubuntu/workspace/mesos/Mesos_CI-build/FLAG/SSL/label/mesos-ec2-ubuntu-16.04/mesos/build/src/java/generated/org/apache/mesos/Protos.java:[14594,45] error: cannot access StringBuilder_x000D_
{noformat}",3.0,0,0.0,0.0,0.3333333333333333,0.5,0.14285714285714285,0.0,0.2,0.5087719298245614,0.4357142857142857,0.16666666666666666,0.1818181818181818,0.17575757575757575,0.0
Documentation,Standalone container documentation,We should add documentation for standalone containers.,3.0,0,0.5,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Bug,53598228fe should be backported to 1.7.x,"Commit 53598228fe on the master branch should be backported to 1.7.x. _x000D_
_x000D_
 ",1.0,0,0.5,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.01282051282051282,0.006060606060606061,0.006060606060606061,0.0
Bug,The agent crashes after the disk du isolator supporting rootfs checks.,"This issue was broken by this patch:_x000D_
https://github.com/apache/mesos/commit/8ba0682521c6051b42f33b3dd96a37f4d46a290d#diff-33089e53bdf9f646cdb9317c212eda02_x000D_
_x000D_
A task can be launched without disk resource. However, after this patch, if the disk resource does not exist, the agent crashes - because the info->paths only add an entry 'path' when there is a quota and the quota comes from the disk resource._x000D_
_x000D_
{noformat}_x000D_
Aug 09 14:54:00 ip-172-12-2-196.us-west-2.compute.internal mesos-agent[15492]: F0809 14:54:00.017730 15498 process.cpp:3057] Aborting libprocess: 'posix-disk-isolator(1)@172.12.2.196:5051' threw exception: _Map_base::at_x000D_
Aug 09 14:54:00 ip-172-12-2-196.us-west-2.compute.internal mesos-agent[15492]: *** Check failure stack trace: ***_x000D_
Aug 09 14:54:00 ip-172-12-2-196.us-west-2.compute.internal mesos-agent[15492]: @     0x7f65f7d585cd  google::LogMessage::Fail()_x000D_
Aug 09 14:54:00 ip-172-12-2-196.us-west-2.compute.internal mesos-agent[15492]: @     0x7f65f7d5a828  google::LogMessage::SendToLog()_x000D_
Aug 09 14:54:00 ip-172-12-2-196.us-west-2.compute.internal mesos-agent[15492]: @     0x7f65f7d58163  google::LogMessage::Flush()_x000D_
Aug 09 14:54:00 ip-172-12-2-196.us-west-2.compute.internal mesos-agent[15492]: @     0x7f65f7d5b169  google::LogMessageFatal::~LogMessageFatal()_x000D_
Aug 09 14:54:00 ip-172-12-2-196.us-west-2.compute.internal mesos-agent[15492]: @     0x7f65f7cb8dbd  process::ProcessManager::resume()_x000D_
Aug 09 14:54:00 ip-172-12-2-196.us-west-2.compute.internal mesos-agent[15492]: @     0x7f65f7cbe926  _ZNSt6thread5_ImplISt12_Bind_simpleIFZN7process14ProcessManager12init_threadsEvEUlvE_vEEE6_M_runEv_x000D_
Aug 09 14:54:00 ip-172-12-2-196.us-west-2.compute.internal mesos-agent[15492]: @     0x7f65f3976070  (unknown)_x000D_
Aug 09 14:54:00 ip-172-12-2-196.us-west-2.compute.internal mesos-agent[15492]: @     0x7f65f3194e25  start_thread_x000D_
Aug 09 14:54:00 ip-172-12-2-196.us-west-2.compute.internal mesos-agent[15492]: @     0x7f65f2ebebad  __clone_x000D_
{noformat}",2.0,0,0.5,0.5444947209653093,0.0,0.0,0.0,0.5,1.0,0.05263157894736842,0.03571428571428571,0.6794871794871795,0.5939393939393939,0.5939393939393939,0.0
Bug,Master does not handle returning unreachable agents as draining/deactivated,"The master has two code paths for handling agent reregistration messages, one culminating in {{Master::___reregisterSlave}} and the other in {{Master::}}{{__reregisterSlave}}. The two paths are not continuations of each other.  Looks like we missed the double-underscore case in the initial implementation.  This is the path that unreachable agents take, when/if they come back to the cluster.  The result is that when unreachable agents are marked for draining, they do not get sent the appropriate message unless they are forced to reregister again (i.e. restarted manually).",3.0,0,0.0,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.0
Improvement,Removal of a role from the suppression list should be equivalent to REVIVE.,"[~timcharper] and [~zen-dog] pointed out that removal of a role from the suppression list (e.g. via UPDATE_FRAMEWORK) does not clear filters. This means that schedulers have to issue a separate explicit REVIVE for the roles they want to remove._x000D_
_x000D_
It seems like these are not the semantics we want, and we should instead be clearing filters upon removing a role from the suppression list.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.0
Bug,DRF sorter may omit clients in sorting after removing an inactive leaf node.,"The sorter assumes inactive leaf nodes are placed in the tail in the children list of a node._x000D_
However, when collapsing a parent node with a single ""."" virtual child node, its position may fail to be updated due to a bug in `Sorter::remove()`:_x000D_
_x000D_
{noformat}_x000D_
CHECK(child->isLeaf());_x000D_
...._x000D_
current->kind = child->kind;_x000D_
..._x000D_
if (current->kind == Node::INTERNAL) {_x000D_
}_x000D_
{noformat}_x000D_
_x000D_
This bug would manifest, if_x000D_
(1) we have a/b and a/._x000D_
(2) deactivate(a),  i.e. a/. becomes inactive_leaf_x000D_
(3) remove(a/b)_x000D_
When these happens, a/. will collapse to `a` as an inactive_leaf, due to the bug above, however, it will not be placed at the end, resulting in all the clients after `a` not included in the sort()._x000D_
_x000D_
Luckily, this should never happen in practice, because only frameworks will get deactivated, and frameworks don’t have sub clients._x000D_
",3.0,0,0.0,0.0,0.6666666666666666,1.0,0.5714285714285714,0.0,0.0,0.5438596491228069,0.5357142857142857,0.3974358974358974,0.4,0.4,0.0
Bug,OperationReconciliationTest.FrameworkReconciliationRaceWithUpdateSlaveMessage is severely flaky,"Flakes are frequently observed in the internal CI._x000D_
_x000D_
Example:_x000D_
{code}_x000D_
[ RUN      ] ContentType/OperationReconciliationTest.FrameworkReconciliationRaceWithUpdateSlaveMessage/1_x000D_
I0806 20:00:24.128456 29945 cluster.cpp:177] Creating default 'local' authorizer_x000D_
I0806 20:00:24.132164 21364 master.cpp:440] Master 7bbcb55d-ce3b-40e6-a605-62ed7d843832 (ip-172-16-10-6.ec2.internal) started on 172.16.10.6:36902_x000D_
I0806 20:00:24.132181 21364 master.cpp:443] Flags at startup: --acls="""" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""1secs"" --allocator=""hierarchical"" --authenticate_agents=""true"" --authenticate_frameworks=""true"" --authenticate_http_frameworks=""true"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""true"" --authentication_v0_timeout=""15secs"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/MpmzC4/credentials"" --filter_gpu_resources=""true"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_operator_event_stream_subscribers=""1000"" --max_unreachable_tasks_per_framework=""1000"" --memory_profiling=""false"" --min_allocatable_resources=""cpus:0.01|mem:32"" --port=""5050"" --publish_per_framework_metrics=""true"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""in_memory"" --registry_fetch_timeout=""1mins"" --registry_gc_interval=""15mins"" --registry_max_agent_age=""2weeks"" --registry_max_agent_count=""102400"" --registry_store_timeout=""100secs"" --registry_strict=""false"" --require_agent_domain=""false"" --role_sorter=""drf"" --root_submissions=""true"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/MpmzC4/master"" --zk_session_timeout=""10secs""_x000D_
I0806 20:00:24.132485 21364 master.cpp:492] Master only allowing authenticated frameworks to register_x000D_
I0806 20:00:24.132494 21364 master.cpp:498] Master only allowing authenticated agents to register_x000D_
I0806 20:00:24.132500 21364 master.cpp:504] Master only allowing authenticated HTTP frameworks to register_x000D_
I0806 20:00:24.132506 21364 credentials.hpp:37] Loading credentials for authentication from '/tmp/MpmzC4/credentials'_x000D_
I0806 20:00:24.132709 21364 master.cpp:548] Using default 'crammd5' authenticator_x000D_
I0806 20:00:24.132845 21364 http.cpp:975] Creating default 'basic' HTTP authenticator for realm 'mesos-master-readonly'_x000D_
I0806 20:00:24.132975 21364 http.cpp:975] Creating default 'basic' HTTP authenticator for realm 'mesos-master-readwrite'_x000D_
I0806 20:00:24.133085 21364 http.cpp:975] Creating default 'basic' HTTP authenticator for realm 'mesos-master-scheduler'_x000D_
I0806 20:00:24.133188 21364 master.cpp:629] Authorization enabled_x000D_
I0806 20:00:24.135308 21363 whitelist_watcher.cpp:77] No whitelist given_x000D_
I0806 20:00:24.139948 21364 master.cpp:2168] Elected as the leading master!_x000D_
I0806 20:00:24.139968 21364 master.cpp:1664] Recovering from registrar_x000D_
I0806 20:00:24.140195 21364 registrar.cpp:339] Recovering registrar_x000D_
I0806 20:00:24.141042 21364 registrar.cpp:383] Successfully fetched the registry (0B) in 0ns_x000D_
I0806 20:00:24.141141 21364 registrar.cpp:487] Applied 1 operations in 25620ns; attempting to update the registry_x000D_
I0806 20:00:24.141793 21364 registrar.cpp:544] Successfully updated the registry in 0ns_x000D_
I0806 20:00:24.141894 21364 registrar.cpp:416] Successfully recovered registrar_x000D_
I0806 20:00:24.142277 21364 master.cpp:1817] Recovered 0 agents from the registry (175B); allowing 10mins for agents to reregister_x000D_
I0806 20:00:24.142611 21366 hierarchical.cpp:241] Initialized hierarchical allocator process_x000D_
I0806 20:00:24.142735 21366 hierarchical.cpp:280] Skipping recovery of hierarchical allocator: nothing to recover_x000D_
W0806 20:00:24.147953 29945 process.cpp:2877] Attempted to spawn already running process files@172.16.10.6:36902_x000D_
I0806 20:00:24.149081 29945 containerizer.cpp:318] Using isolation { environment_secret, posix/cpu, posix/mem, filesystem/posix, network/cni }_x000D_
I0806 20:00:24.153627 29945 linux_launcher.cpp:144] Using /sys/fs/cgroup/freezer as the freezer hierarchy for the Linux launcher_x000D_
I0806 20:00:24.154449 29945 provisioner.cpp:300] Using default backend 'copy'_x000D_
I0806 20:00:24.158208 29945 cluster.cpp:518] Creating default 'local' authorizer_x000D_
I0806 20:00:24.160681 21368 slave.cpp:267] Mesos agent started on (1277)@172.16.10.6:36902_x000D_
I0806 20:00:24.160698 21368 slave.cpp:268] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/MpmzC4/mGaOxR/store/appc"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""false"" --authenticatee=""crammd5"" --authentication_backoff_factor=""1secs"" --authentication_timeout_max=""1mins"" --authentication_timeout_min=""5secs"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_destroy_timeout=""1mins"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/MpmzC4/mGaOxR/credential"" --default_role=""*"" --disallow_sharing_agent_ipc_namespace=""false"" --disallow_sharing_agent_pid_namespace=""false"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_ignore_runtime=""false"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/MpmzC4/mGaOxR/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_reregistration_timeout=""2secs"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/MpmzC4/mGaOxR/fetch"" --fetcher_cache_size=""2GB"" --fetcher_stall_timeout=""1mins"" --frameworks_home=""/tmp/MpmzC4/mGaOxR/frameworks"" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --gc_non_executor_container_sandboxes=""false"" --help=""false"" --hostname_lookup=""true"" --http_command_executor=""false"" --http_credentials=""/tmp/MpmzC4/mGaOxR/http_credentials"" --http_heartbeat_interval=""30secs"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher=""linux"" --launcher_dir=""/home/centos/workspace/mesos/Mesos_CI-build/FLAG/CMake/label/mesos-ec2-centos-7/mesos/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_executors_per_framework=""150"" --memory_profiling=""false"" --network_cni_metrics=""true"" --network_cni_root_dir_persist=""false"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --port=""5051"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --reconfiguration_policy=""equal"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --runtime_dir=""/tmp/ContentType_OperationReconciliationTest_FrameworkReconciliationRaceWithUpdateSlaveMessage_1_rFyIeB"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/ContentType_OperationReconciliationTest_FrameworkReconciliationRaceWithUpdateSlaveMessage_1_C1BDVk"" --zk_session_timeout=""10secs""_x000D_
I0806 20:00:24.161079 21368 credentials.hpp:86] Loading credential for authentication from '/tmp/MpmzC4/mGaOxR/credential'_x000D_
I0806 20:00:24.161200 21368 slave.cpp:300] Agent using credential for: test-principal_x000D_
I0806 20:00:24.161213 21368 credentials.hpp:37] Loading credentials for authentication from '/tmp/MpmzC4/mGaOxR/http_credentials'_x000D_
I0806 20:00:24.161360 21368 http.cpp:975] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-readonly'_x000D_
I0806 20:00:24.161633 21368 disk_profile_adaptor.cpp:78] Creating default disk profile adaptor module_x000D_
I0806 20:00:24.163035 21368 slave.cpp:615] Agent resources: [{""name"":""cpus"",""scalar"":{""value"":2.0},""type"":""SCALAR""},{""name"":""mem"",""scalar"":{""value"":1024.0},""type"":""SCALAR""},{""name"":""disk"",""scalar"":{""value"":1024.0},""type"":""SCALAR""},{""name"":""ports"",""ranges"":{""range"":[{""begin"":31000,""end"":32000}]},""type"":""RANGES""}]_x000D_
I0806 20:00:24.163216 21368 slave.cpp:623] Agent attributes: [  ]_x000D_
I0806 20:00:24.163226 21368 slave.cpp:632] Agent hostname: ip-172-16-10-6.ec2.internal_x000D_
I0806 20:00:24.164947 21368 task_status_update_manager.cpp:181] Pausing sending task status updates_x000D_
I0806 20:00:24.164978 21368 status_update_manager_process.hpp:379] Pausing operation status update manager_x000D_
I0806 20:00:24.166102 21368 state.cpp:67] Recovering state from '/tmp/ContentType_OperationReconciliationTest_FrameworkReconciliationRaceWithUpdateSlaveMessage_1_C1BDVk/meta'_x000D_
I0806 20:00:24.166347 21368 slave.cpp:7444] Finished recovering checkpointed state from '/tmp/ContentType_OperationReconciliationTest_FrameworkReconciliationRaceWithUpdateSlaveMessage_1_C1BDVk/meta', beginning agent recovery_x000D_
I0806 20:00:24.166906 21368 task_status_update_manager.cpp:207] Recovering task status update manager_x000D_
I0806 20:00:24.167408 21368 containerizer.cpp:821] Recovering Mesos containers_x000D_
I0806 20:00:24.167639 21368 linux_launcher.cpp:286] Recovering Linux launcher_x000D_
I0806 20:00:24.168066 21368 containerizer.cpp:1147] Recovering isolators_x000D_
I0806 20:00:24.169267 21368 containerizer.cpp:1186] Recovering provisioner_x000D_
I0806 20:00:24.169919 21368 provisioner.cpp:500] Provisioner recovery complete_x000D_
I0806 20:00:24.171048 21368 composing.cpp:339] Finished recovering all containerizers_x000D_
I0806 20:00:24.171375 21368 slave.cpp:7908] Recovering executors_x000D_
I0806 20:00:24.171452 21368 slave.cpp:8061] Finished recovery_x000D_
I0806 20:00:24.172451 21368 slave.cpp:1351] New master detected at master@172.16.10.6:36902_x000D_
I0806 20:00:24.172538 21368 slave.cpp:1416] Detecting new master_x000D_
I0806 20:00:24.172674 21368 task_status_update_manager.cpp:181] Pausing sending task status updates_x000D_
I0806 20:00:24.172704 21368 status_update_manager_process.hpp:379] Pausing operation status update manager_x000D_
I0806 20:00:24.173789 21368 slave.cpp:1443] Authenticating with master master@172.16.10.6:36902_x000D_
I0806 20:00:24.173863 21368 slave.cpp:1452] Using default CRAM-MD5 authenticatee_x000D_
I0806 20:00:24.174245 21368 authenticatee.cpp:121] Creating new client SASL connection_x000D_
I0806 20:00:24.174455 21368 master.cpp:10578] Authenticating slave(1277)@172.16.10.6:36902_x000D_
I0806 20:00:24.174621 21368 authenticator.cpp:414] Starting authentication session for crammd5-authenticatee(2162)@172.16.10.6:36902_x000D_
I0806 20:00:24.174991 21368 authenticator.cpp:98] Creating new server SASL connection_x000D_
I0806 20:00:24.175177 21368 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5_x000D_
I0806 20:00:24.175195 21368 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'_x000D_
I0806 20:00:24.175279 21368 authenticator.cpp:204] Received SASL authentication start_x000D_
I0806 20:00:24.175318 21368 authenticator.cpp:326] Authentication requires more steps_x000D_
I0806 20:00:24.175391 21368 authenticatee.cpp:259] Received SASL authentication step_x000D_
I0806 20:00:24.175469 21368 authenticator.cpp:232] Received SASL authentication step_x000D_
I0806 20:00:24.175490 21368 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'ip-172-16-10-6.ec2.internal' server FQDN: 'ip-172-16-10-6.ec2.internal' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false _x000D_
I0806 20:00:24.175500 21368 auxprop.cpp:181] Looking up auxiliary property '*userPassword'_x000D_
I0806 20:00:24.175530 21368 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'_x000D_
I0806 20:00:24.175544 21368 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'ip-172-16-10-6.ec2.internal' server FQDN: 'ip-172-16-10-6.ec2.internal' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true _x000D_
I0806 20:00:24.175550 21368 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true_x000D_
I0806 20:00:24.175557 21368 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true_x000D_
I0806 20:00:24.175570 21368 authenticator.cpp:318] Authentication success_x000D_
I0806 20:00:24.175745 21368 authenticatee.cpp:299] Authentication success_x000D_
I0806 20:00:24.175917 21368 master.cpp:10610] Successfully authenticated principal 'test-principal' at slave(1277)@172.16.10.6:36902_x000D_
I0806 20:00:24.175993 21368 authenticator.cpp:432] Authentication session cleanup for crammd5-authenticatee(2162)@172.16.10.6:36902_x000D_
I0806 20:00:24.176301 21368 slave.cpp:1543] Successfully authenticated with master master@172.16.10.6:36902_x000D_
I0806 20:00:24.176601 21368 slave.cpp:1993] Will retry registration in 8.320422ms if necessary_x000D_
I0806 20:00:24.176966 21368 master.cpp:7086] Received register agent message from slave(1277)@172.16.10.6:36902 (ip-172-16-10-6.ec2.internal)_x000D_
I0806 20:00:24.177207 21368 master.cpp:4202] Authorizing agent providing resources 'cpus:2; mem:1024; disk:1024; ports:[31000-32000]' with principal 'test-principal'_x000D_
I0806 20:00:24.178033 21368 master.cpp:7153] Authorized registration of agent at slave(1277)@172.16.10.6:36902 (ip-172-16-10-6.ec2.internal)_x000D_
I0806 20:00:24.178100 21368 master.cpp:7265] Registering agent at slave(1277)@172.16.10.6:36902 (ip-172-16-10-6.ec2.internal) with id 7bbcb55d-ce3b-40e6-a605-62ed7d843832-S0_x000D_
I0806 20:00:24.178675 21368 registrar.cpp:487] Applied 1 operations in 200693ns; attempting to update the registry_x000D_
I0806 20:00:24.179406 21368 registrar.cpp:544] Successfully updated the registry in 0ns_x000D_
I0806 20:00:24.179685 21368 master.cpp:7313] Admitted agent 7bbcb55d-ce3b-40e6-a605-62ed7d843832-S0 at slave(1277)@172.16.10.6:36902 (ip-172-16-10-6.ec2.internal)_x000D_
I0806 20:00:24.180236 21368 master.cpp:7358] Registered agent 7bbcb55d-ce3b-40e6-a605-62ed7d843832-S0 at slave(1277)@172.16.10.6:36902 (ip-172-16-10-6.ec2.internal) with cpus:2; mem:1024; disk:1024; ports:[31000-32000]_x000D_
I0806 20:00:24.180523 21368 hierarchical.cpp:617] Added agent 7bbcb55d-ce3b-40e6-a605-62ed7d843832-S0 (ip-172-16-10-6.ec2.internal) with cpus:2; mem:1024; disk:1024; ports:[31000-32000] (allocated: {})_x000D_
I0806 20:00:24.180778 21368 hierarchical.cpp:1508] Performed allocation for 1 agents in 100435ns_x000D_
I0806 20:00:24.180869 21368 slave.cpp:1576] Registered with master master@172.16.10.6:36902; given agent ID 7bbcb55d-ce3b-40e6-a605-62ed7d843832-S0_x000D_
I0806 20:00:24.181309 21368 slave.cpp:1611] Checkpointing SlaveInfo to '/tmp/ContentType_OperationReconciliationTest_FrameworkReconciliationRaceWithUpdateSlaveMessage_1_C1BDVk/meta/slaves/7bbcb55d-ce3b-40e6-a605-62ed7d843832-S0/slave.info'_x000D_
I0806 20:00:24.182250 21368 slave.cpp:1663] Forwarding agent update {""operations"":{},""resource_providers"":{},""resource_version_uuid"":{""value"":""/JaO7ln0ROi+NZkPMvYECw==""},""slave_id"":{""value"":""7bbcb55d-ce3b-40e6-a605-62ed7d843832-S0""},""update_oversubscribed_resources"":false}_x000D_
I0806 20:00:24.182448 21368 task_status_update_manager.cpp:188] Resuming sending task status updates_x000D_
I0806 20:00:24.182512 21368 status_update_manager_process.hpp:385] Resuming operation status update manager_x000D_
I0806 20:00:24.184386 21363 master.cpp:8457] Ignoring update on agent 7bbcb55d-ce3b-40e6-a605-62ed7d843832-S0 at slave(1277)@172.16.10.6:36902 (ip-172-16-10-6.ec2.internal) as it reports no changes_x000D_
I0806 20:00:24.202025 21368 http_connection.hpp:227] New endpoint detected at http://172.16.10.6:36902/slave(1277)/api/v1/resource_provider_x000D_
I0806 20:00:24.204447 21366 http_connection.hpp:283] Connected with the remote endpoint at http://172.16.10.6:36902/slave(1277)/api/v1/resource_provider_x000D_
I0806 20:00:24.205152 21367 http_connection.hpp:131] Sending 1 call to http://172.16.10.6:36902/slave(1277)/api/v1/resource_provider_x000D_
I0806 20:00:24.206167 21367 process.cpp:3671] Handling HTTP event for process 'slave(1277)' with path: '/slave(1277)/api/v1/resource_provider'_x000D_
I0806 20:00:24.207383 21365 http.cpp:1115] HTTP POST for /slave(1277)/api/v1/resource_provider from 172.16.10.6:48886_x000D_
I0806 20:00:24.207850 21365 manager.cpp:807] Subscribing resource provider {""name"":""test"",""type"":""org.apache.mesos.rp.test""}_x000D_
I0806 20:00:24.210747 21361 slave.cpp:8417] Handling resource provider message 'SUBSCRIBE: {""id"":{""value"":""08838aad-7f0b-495d-8b72-dd2c2678d320""},""name"":""test"",""type"":""org.apache.mesos.rp.test""}'_x000D_
I0806 20:00:24.212172 21362 http_connection.hpp:131] Sending 3 call to http://172.16.10.6:36902/slave(1277)/api/v1/resource_provider_x000D_
I0806 20:00:24.213117 21364 process.cpp:3671] Handling HTTP event for process 'slave(1277)' with path: '/slave(1277)/api/v1/resource_provider'_x000D_
I0806 20:00:24.214156 21363 http.cpp:1115] HTTP POST for /slave(1277)/api/v1/resource_provider from 172.16.10.6:48884_x000D_
I0806 20:00:24.214596 21363 manager.cpp:1039] Received UPDATE_STATE call with resources '[{""disk"":{""source"":{""type"":""RAW""}},""name"":""disk"",""provider_id"":{""value"":""08838aad-7f0b-495d-8b72-dd2c2678d320""},""scalar"":{""value"":200.0},""type"":""SCALAR""}]' and 0 operations from resource provider 08838aad-7f0b-495d-8b72-dd2c2678d320_x000D_
I0806 20:00:24.214985 21363 slave.cpp:8417] Handling resource provider message 'UPDATE_STATE: 08838aad-7f0b-495d-8b72-dd2c2678d320 disk[RAW]:200'_x000D_
I0806 20:00:24.215077 21363 slave.cpp:8537] Forwarding new total resources cpus:2; mem:1024; disk:1024; ports:[31000-32000]; disk[RAW]:200_x000D_
I0806 20:00:24.217061 29945 scheduler.cpp:189] Version: 1.9.0_x000D_
I0806 20:00:24.217216 21363 hierarchical.cpp:753] Grew agent 7bbcb55d-ce3b-40e6-a605-62ed7d843832-S0 by disk[RAW]:200 (total), {  } (used)_x000D_
I0806 20:00:24.217445 21363 hierarchical.cpp:710] Agent 7bbcb55d-ce3b-40e6-a605-62ed7d843832-S0 (ip-172-16-10-6.ec2.internal) updated with total resources cpus:2; mem:1024; disk:1024; ports:[31000-32000]; disk[RAW]:200_x000D_
I0806 20:00:24.217761 21363 scheduler.cpp:342] Using default 'basic' HTTP authenticatee_x000D_
I0806 20:00:24.218135 21363 scheduler.cpp:525] New master detected at master@172.16.10.6:36902_x000D_
I0806 20:00:24.218155 21363 scheduler.cpp:534] Waiting for 0ns before initiating a re-(connection) attempt with the master_x000D_
I0806 20:00:24.222335 21365 scheduler.cpp:416] Connected with the master at http://172.16.10.6:36902/master/api/v1/scheduler_x000D_
I0806 20:00:24.223047 21365 scheduler.cpp:246] Adding authentication headers to SUBSCRIBE call to http://172.16.10.6:36902/master/api/v1/scheduler_x000D_
I0806 20:00:24.223444 21364 scheduler.cpp:600] Sending SUBSCRIBE call to http://172.16.10.6:36902/master/api/v1/scheduler_x000D_
I0806 20:00:24.224323 21363 process.cpp:3671] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler'_x000D_
I0806 20:00:24.225673 21365 http.cpp:1115] HTTP POST for /master/api/v1/scheduler from 172.16.10.6:48890_x000D_
I0806 20:00:24.225988 21365 master.cpp:2668] Received subscription request for HTTP framework 'default'_x000D_
I0806 20:00:24.226054 21365 master.cpp:2240] Authorizing framework principal 'test-principal' to receive offers for roles '{ default-role }'_x000D_
I0806 20:00:24.227097 21365 master.cpp:2740] Subscribing framework 'default' with checkpointing disabled and capabilities [ MULTI_ROLE, RESERVATION_REFINEMENT ]_x000D_
I0806 20:00:24.229744 21365 master.cpp:10808] Adding framework 7bbcb55d-ce3b-40e6-a605-62ed7d843832-0000 (default) with roles {  } suppressed_x000D_
I0806 20:00:24.230711 21367 hierarchical.cpp:368] Added framework 7bbcb55d-ce3b-40e6-a605-62ed7d843832-0000_x000D_
I0806 20:00:24.231781 21367 hierarchical.cpp:1508] Performed allocation for 1 agents in 939160ns_x000D_
I0806 20:00:24.232277 21367 master.cpp:10393] Sending offers [ 7bbcb55d-ce3b-40e6-a605-62ed7d843832-O0 ] to framework 7bbcb55d-ce3b-40e6-a605-62ed7d843832-0000 (default)_x000D_
I0806 20:00:24.233058 21362 scheduler.cpp:847] Enqueuing event SUBSCRIBED received from http://172.16.10.6:36902/master/api/v1/scheduler_x000D_
I0806 20:00:24.233510 21362 scheduler.cpp:847] Enqueuing event HEARTBEAT received from http://172.16.10.6:36902/master/api/v1/scheduler_x000D_
I0806 20:00:24.235025 21366 scheduler.cpp:847] Enqueuing event OFFERS received from http://172.16.10.6:36902/master/api/v1/scheduler_x000D_
I0806 20:00:24.242210 21363 scheduler.cpp:246] Adding authentication headers to ACCEPT call to http://172.16.10.6:36902/master/api/v1/scheduler_x000D_
I0806 20:00:24.242585 21363 scheduler.cpp:600] Sending ACCEPT call to http://172.16.10.6:36902/master/api/v1/scheduler_x000D_
I0806 20:00:24.243816 21368 process.cpp:3671] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler'_x000D_
I0806 20:00:24.244995 21364 http.cpp:1115] HTTP POST for /master/api/v1/scheduler from 172.16.10.6:48888_x000D_
I0806 20:00:24.245823 21364 master.cpp:12685] Removing offer 7bbcb55d-ce3b-40e6-a605-62ed7d843832-O0_x000D_
I0806 20:00:24.246209 21364 master.cpp:4739] Processing ACCEPT call for offers: [ 7bbcb55d-ce3b-40e6-a605-62ed7d843832-O0 ] on agent 7bbcb55d-ce3b-40e6-a605-62ed7d843832-S0 at slave(1277)@172.16.10.6:36902 (ip-172-16-10-6.ec2.internal) for framework 7bbcb55d-ce3b-40e6-a605-62ed7d843832-0000 (default)_x000D_
I0806 20:00:24.246385 21364 master.cpp:3824] Authorizing principal 'test-principal' to reserve resources 'disk(allocated: default-role)(reservations: [(DYNAMIC,default-role,test-principal)])[RAW]:200'_x000D_
I0806 20:00:24.248404 21364 master.cpp:5108] Applying RESERVE operation for resources [{""allocation_info"":{""role"":""default-role""},""disk"":{""source"":{""type"":""RAW""}},""name"":""disk"",""provider_id"":{""value"":""08838aad-7f0b-495d-8b72-dd2c2678d320""},""reservations"":[{""principal"":""test-principal"",""role"":""default-role"",""type"":""DYNAMIC""}],""scalar"":{""value"":200.0},""type"":""SCALAR""}] from framework 7bbcb55d-ce3b-40e6-a605-62ed7d843832-0000 (default) to agent 7bbcb55d-ce3b-40e6-a605-62ed7d843832-S0 at slave(1277)@172.16.10.6:36902 (ip-172-16-10-6.ec2.internal)_x000D_
I0806 20:00:24.248947 21364 master.cpp:12576] Sending operation 'operation' (uuid: aea1e1d0-f192-4940-8afb-1d435a4d144b) to agent 7bbcb55d-ce3b-40e6-a605-62ed7d843832-S0 at slave(1277)@172.16.10.6:36902 (ip-172-16-10-6.ec2.internal)_x000D_
I0806 20:00:24.249485 21362 slave.cpp:4346] Ignoring new checkpointed resources and operations identical to the current version_x000D_
I0806 20:00:24.251276 21364 hierarchical.cpp:956] Updated allocation of framework 7bbcb55d-ce3b-40e6-a605-62ed7d843832-0000 on agent 7bbcb55d-ce3b-40e6-a605-62ed7d843832-S0 from cpus(allocated: default-role):2; mem(allocated: default-role):1024; disk(allocated: default-role):1024; disk(allocated: default-role)[RAW]:200; ports(allocated: default-role):[31000-32000] to cpus(allocated: default-role):2; mem(allocated: default-role):1024; disk(allocated: default-role):1024; ports(allocated: default-role):[31000-32000]; disk(allocated: default-role)(reservations: [(DYNAMIC,default-role,test-principal)])[RAW]:200_x000D_
I0806 20:00:24.251394 21364 hierarchical.cpp:1432] Allocation paused_x000D_
I0806 20:00:24.251924 21364 hierarchical.cpp:1218] Recovered disk(allocated: default-role)(reservations: [(DYNAMIC,default-role,test-principal)])[RAW]:200 (total: cpus:2; mem:1024; disk:1024; ports:[31000-32000]; disk(reservations: [(DYNAMIC,default-role,test-principal)])[RAW]:200, allocated: cpus(allocated: default-role):2; mem(allocated: default-role):1024; disk(allocated: default-role):1024; ports(allocated: default-role):[31000-32000]) on agent 7bbcb55d-ce3b-40e6-a605-62ed7d843832-S0 from framework 7bbcb55d-ce3b-40e6-a605-62ed7d843832-0000_x000D_
I0806 20:00:24.252714 21368 http_connection.hpp:131] Sending 2 call to http://172.16.10.6:36902/slave(1277)/api/v1/resource_provider_x000D_
I0806 20:00:24.253152 21364 hierarchical.cpp:1218] Recovered cpus(allocated: default-role):2; mem(allocated: default-role):1024; disk(allocated: default-role):1024; ports(allocated: default-role):[31000-32000] (total: cpus:2; mem:1024; disk:1024; ports:[31000-32000]; disk(reservations: [(DYNAMIC,default-role,test-principal)])[RAW]:200, allocated: {}) on agent 7bbcb55d-ce3b-40e6-a605-62ed7d843832-S0 from framework 7bbcb55d-ce3b-40e6-a605-62ed7d843832-0000_x000D_
I0806 20:00:24.253226 21364 hierarchical.cpp:1264] Framework 7bbcb55d-ce3b-40e6-a605-62ed7d843832-0000 filtered agent 7bbcb55d-ce3b-40e6-a605-62ed7d843832-S0 for 5secs_x000D_
I0806 20:00:24.253456 21364 hierarchical.cpp:1442] Allocation resumed_x000D_
I0806 20:00:24.254040 21361 process.cpp:3671] Handling HTTP event for process 'slave(1277)' with path: '/slave(1277)/api/v1/resource_provider'_x000D_
I0806 20:00:24.293757 21363 http.cpp:1115] HTTP POST for /slave(1277)/api/v1/resource_provider from 172.16.10.6:48884_x000D_
I0806 20:00:24.294436 21364 slave.cpp:8417] Handling resource provider message 'UPDATE_OPERATION_STATUS: (uuid: aea1e1d0-f192-4940-8afb-1d435a4d144b) for framework 7bbcb55d-ce3b-40e6-a605-62ed7d843832-0000 (latest state: OPERATION_FINISHED, status update state: OPERATION_FINISHED)'_x000D_
I0806 20:00:24.294539 21364 slave.cpp:8870] Updating the state of operation 'operation' (uuid: aea1e1d0-f192-4940-8afb-1d435a4d144b) for framework 7bbcb55d-ce3b-40e6-a605-62ed7d843832-0000 (latest state: OPERATION_FINISHED, status update state: OPERATION_FINISHED)_x000D_
I0806 20:00:24.294571 21364 slave.cpp:8624] Forwarding status update of operation 'operation' (operation_uuid: aea1e1d0-f192-4940-8afb-1d435a4d144b) for framework 7bbcb55d-ce3b-40e6-a605-62ed7d843832-0000_x000D_
I0806 20:00:24.294821 21364 master.cpp:12232] Updating the state of operation 'operation' (uuid: aea1e1d0-f192-4940-8afb-1d435a4d144b) for framework 7bbcb55d-ce3b-40e6-a605-62ed7d843832-0000 (latest state: OPERATION_PENDING, status update state: OPERATION_FINISHED)_x000D_
I0806 20:00:24.294898 21364 master.cpp:9168] Forwarding operation status update OPERATION_FINISHED (Status UUID: INVALID UUID) for operation UUID aea1e1d0-f192-4940-8afb-1d435a4d144b (framework-supplied ID 'operation') of framework '7bbcb55d-ce3b-40e6-a605-62ed7d843832-0000' on agent 7bbcb55d-ce3b-40e6-a605-62ed7d843832-S0_x000D_
I0806 20:00:24.296190 21362 scheduler.cpp:847] Enqueuing event UPDATE_OPERATION_STATUS received from http://172.16.10.6:36902/master/api/v1/scheduler_x000D_
I0806 20:00:24.304412 29945 master.cpp:1135] Master terminating_x000D_
I0806 20:00:24.305038 21366 hierarchical.cpp:775] Removed all filters for agent 7bbcb55d-ce3b-40e6-a605-62ed7d843832-S0_x000D_
I0806 20:00:24.305061 21366 hierarchical.cpp:650] Removed agent 7bbcb55d-ce3b-40e6-a605-62ed7d843832-S0_x000D_
I0806 20:00:24.310272 21363 slave.cpp:6409] Got exited event for master@172.16.10.6:36902_x000D_
W0806 20:00:24.310297 21363 slave.cpp:6414] Master disconnected! Waiting for a new master to be elected_x000D_
E0806 20:00:24.310421 21363 scheduler.cpp:820] End-Of-File received from master. The master closed the event stream_x000D_
I0806 20:00:24.310497 21364 hierarchical.cpp:417] Removed framework 7bbcb55d-ce3b-40e6-a605-62ed7d843832-0000_x000D_
I0806 20:00:24.311053 21363 scheduler.cpp:499] Re-detecting master_x000D_
I0806 20:00:24.311995 21363 scheduler.cpp:450] Ignoring disconnection attempt from stale connection_x000D_
I0806 20:00:24.312081 21363 scheduler.cpp:525] New master detected at master@172.16.10.6:36902_x000D_
I0806 20:00:24.312100 21363 scheduler.cpp:534] Waiting for 0ns before initiating a re-(connection) attempt with the master_x000D_
I0806 20:00:24.312278 21363 scheduler.cpp:450] Ignoring disconnection attempt from stale connection_x000D_
I0806 20:00:24.313170 29945 cluster.cpp:177] Creating default 'local' authorizer_x000D_
I0806 20:00:24.319995 21366 scheduler.cpp:416] Connected with the master at http://172.16.10.6:36902/master/api/v1/scheduler_x000D_
I0806 20:00:24.327302 21362 master.cpp:440] Master aef84e86-60f9-4d30-93f8-98210598b3a3 (ip-172-16-10-6.ec2.internal) started on 172.16.10.6:36902_x000D_
I0806 20:00:24.327320 21362 master.cpp:443] Flags at startup: --acls="""" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""1secs"" --allocator=""hierarchical"" --authenticate_agents=""true"" --authenticate_frameworks=""true"" --authenticate_http_frameworks=""true"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""true"" --authentication_v0_timeout=""15secs"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/MpmzC4/credentials"" --filter_gpu_resources=""true"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_operator_event_stream_subscribers=""1000"" --max_unreachable_tasks_per_framework=""1000"" --memory_profiling=""false"" --min_allocatable_resources=""cpus:0.01|mem:32"" --port=""5050"" --publish_per_framework_metrics=""true"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""in_memory"" --registry_fetch_timeout=""1mins"" --registry_gc_interval=""15mins"" --registry_max_agent_age=""2weeks"" --registry_max_agent_count=""102400"" --registry_store_timeout=""100secs"" --registry_strict=""false"" --require_agent_domain=""false"" --role_sorter=""drf"" --root_submissions=""true"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/MpmzC4/master"" --zk_session_timeout=""10secs""_x000D_
I0806 20:00:24.327594 21362 master.cpp:492] Master only allowing authenticated frameworks to register_x000D_
I0806 20:00:24.327603 21362 master.cpp:498] Master only allowing authenticated agents to register_x000D_
I0806 20:00:24.327608 21362 master.cpp:504] Master only allowing authenticated HTTP frameworks to register_x000D_
I0806 20:00:24.327615 21362 credentials.hpp:37] Loading credentials for authentication from '/tmp/MpmzC4/credentials'_x000D_
I0806 20:00:24.327802 21362 master.cpp:548] Using default 'crammd5' authenticator_x000D_
I0806 20:00:24.327941 21362 http.cpp:975] Creating default 'basic' HTTP authenticator for realm 'mesos-master-readonly'_x000D_
I0806 20:00:24.328074 21362 http.cpp:975] Creating default 'basic' HTTP authenticator for realm 'mesos-master-readwrite'_x000D_
I0806 20:00:24.328189 21362 http.cpp:975] Creating default 'basic' HTTP authenticator for realm 'mesos-master-scheduler'_x000D_
I0806 20:00:24.328295 21362 master.cpp:629] Authorization enabled_x000D_
I0806 20:00:24.330752 21362 hierarchical.cpp:241] Initialized hierarchical allocator process_x000D_
I0806 20:00:24.330838 21362 whitelist_watcher.cpp:77] No whitelist given_x000D_
I0806 20:00:24.332286 21362 master.cpp:2168] Elected as the leading master!_x000D_
I0806 20:00:24.332306 21362 master.cpp:1664] Recovering from registrar_x000D_
I0806 20:00:24.332532 21362 registrar.cpp:339] Recovering registrar_x000D_
I0806 20:00:24.333374 21362 registrar.cpp:383] Successfully fetched the registry (0B) in 0ns_x000D_
I0806 20:00:24.333463 21362 registrar.cpp:487] Applied 1 operations in 27198ns; attempting to update the registry_x000D_
I0806 20:00:24.334455 21362 registrar.cpp:544] Successfully updated the registry in 0ns_x000D_
I0806 20:00:24.334558 21362 registrar.cpp:416] Successfully recovered registrar_x000D_
I0806 20:00:24.335054 21362 master.cpp:1817] Recovered 0 agents from the registry (175B); allowing 10mins for agents to reregister_x000D_
I0806 20:00:24.335175 21362 hierarchical.cpp:280] Skipping recovery of hierarchical allocator: nothing to recover_x000D_
I0806 20:00:24.336079 21361 slave.cpp:1351] New master detected at master@172.16.10.6:36902_x000D_
I0806 20:00:24.336210 21361 slave.cpp:1416] Detecting new master_x000D_
I0806 20:00:24.336489 21361 scheduler.cpp:525] New master detected at master@172.16.10.6:36902_x000D_
I0806 20:00:24.336508 21361 scheduler.cpp:534] Waiting for 0ns before initiating a re-(connection) attempt with the master_x000D_
I0806 20:00:24.336859 21361 task_status_update_manager.cpp:181] Pausing sending task status updates_x000D_
I0806 20:00:24.336889 21361 status_update_manager_process.hpp:379] Pausing operation status update manager_x000D_
I0806 20:00:24.337605 21361 scheduler.cpp:450] Ignoring disconnection attempt from stale connection_x000D_
I0806 20:00:24.337633 21361 scheduler.cpp:450] Ignoring disconnection attempt from stale connection_x000D_
I0806 20:00:24.343678 21363 slave.cpp:1443] Authenticating with master master@172.16.10.6:36902_x000D_
I0806 20:00:24.343744 21363 slave.cpp:1452] Using default CRAM-MD5 authenticatee_x000D_
I0806 20:00:24.344133 21363 authenticatee.cpp:121] Creating new client SASL connection_x000D_
I0806 20:00:24.344328 21363 master.cpp:10578] Authenticating slave(1277)@172.16.10.6:36902_x000D_
I0806 20:00:24.344491 21363 authenticator.cpp:414] Starting authentication session for crammd5-authenticatee(2163)@172.16.10.6:36902_x000D_
I0806 20:00:24.344847 21363 authenticator.cpp:98] Creating new server SASL connection_x000D_
I0806 20:00:24.345027 21363 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5_x000D_
I0806 20:00",2.0,1.9.0,0.5,0.0,0.3333333333333333,0.5,0.14285714285714285,0.0,0.2,0.5087719298245614,0.4357142857142857,0.14102564102564102,0.08484848484848485,0.08484848484848485,1.0
Bug,MasterQuotaTest.RescindOffersEnforcingLimits is flaky,"Showed up on ASF CI:_x000D_
https://builds.apache.org/view/M-R/view/Mesos/job/Mesos-Buildbot/6657/BUILDTOOL=cmake,COMPILER=clang,CONFIGURATION=--verbose%20--disable-libtool-wrappers%20--enable-parallel-test-execution=no,ENVIRONMENT=GLOG_v=1%20MESOS_VERBOSE=1%20MESOS_TEST_AWAIT_TIMEOUT=60secs,OS=centos:7,label_exp=(ubuntu)&&(!ubuntu-us1)&&(!ubuntu-eu2)&&(!ubuntu-4)&&(!H21)&&(!H23)&&(!H26)&&(!H27)/consoleFull_x000D_
_x000D_
{code}_x000D_
3: [ RUN      ] MasterQuotaTest.RescindOffersEnforcingLimits_x000D_
3: I0802 09:57:05.017333 15861 cluster.cpp:177] Creating default 'local' authorizer_x000D_
3: I0802 09:57:05.029503 15877 master.cpp:440] Master 9dd926f8-c8be-42ad-a1c7-ef0d88a99199 (148706d6d9ee) started on 172.17.0.2:41613_x000D_
3: I0802 09:57:05.029911 15877 master.cpp:443] Flags at startup: --acls="""" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""50ms"" --allocator=""hierarchical"" --authenticate_agents=""true"" --authenticate_frameworks=""true"" --authenticate_http_frameworks=""true"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""true"" --authentication_v0_timeout=""15secs"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/450qM2/credentials"" --filter_gpu_resources=""true"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_operator_event_stream_subscribers=""1000"" --max_unreachable_tasks_per_framework=""1000"" --memory_profiling=""false"" --min_allocatable_resources=""cpus:0.01|mem:32"" --port=""5050"" --publish_per_framework_metrics=""true"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""in_memory"" --registry_fetch_timeout=""1mins"" --registry_gc_interval=""15mins"" --registry_max_agent_age=""2weeks"" --registry_max_agent_count=""102400"" --registry_store_timeout=""100secs"" --registry_strict=""false"" --require_agent_domain=""false"" --role_sorter=""drf"" --root_submissions=""true"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/450qM2/master"" --zk_session_timeout=""10secs""_x000D_
3: I0802 09:57:05.030527 15877 master.cpp:492] Master only allowing authenticated frameworks to register_x000D_
3: I0802 09:57:05.030567 15877 master.cpp:498] Master only allowing authenticated agents to register_x000D_
3: I0802 09:57:05.030601 15877 master.cpp:504] Master only allowing authenticated HTTP frameworks to register_x000D_
3: I0802 09:57:05.030634 15877 credentials.hpp:37] Loading credentials for authentication from '/tmp/450qM2/credentials'_x000D_
3: I0802 09:57:05.031009 15877 master.cpp:548] Using default 'crammd5' authenticator_x000D_
3: I0802 09:57:05.031306 15877 http.cpp:975] Creating default 'basic' HTTP authenticator for realm 'mesos-master-readonly'_x000D_
3: I0802 09:57:05.031747 15877 http.cpp:975] Creating default 'basic' HTTP authenticator for realm 'mesos-master-readwrite'_x000D_
3: I0802 09:57:05.032049 15877 http.cpp:975] Creating default 'basic' HTTP authenticator for realm 'mesos-master-scheduler'_x000D_
3: I0802 09:57:05.032627 15877 master.cpp:629] Authorization enabled_x000D_
3: I0802 09:57:05.033092 15880 hierarchical.cpp:241] Initialized hierarchical allocator process_x000D_
3: I0802 09:57:05.033552 15892 whitelist_watcher.cpp:77] No whitelist given_x000D_
3: I0802 09:57:05.052621 15877 master.cpp:2168] Elected as the leading master!_x000D_
3: I0802 09:57:05.052700 15877 master.cpp:1664] Recovering from registrar_x000D_
3: I0802 09:57:05.052943 15873 registrar.cpp:339] Recovering registrar_x000D_
3: I0802 09:57:05.054116 15873 registrar.cpp:383] Successfully fetched the registry (0B) in 1.131008ms_x000D_
3: I0802 09:57:05.054304 15873 registrar.cpp:487] Applied 1 operations in 73189ns; attempting to update the registry_x000D_
3: I0802 09:57:05.055423 15873 registrar.cpp:544] Successfully updated the registry in 1.02912ms_x000D_
3: I0802 09:57:05.055572 15873 registrar.cpp:416] Successfully recovered registrar_x000D_
3: I0802 09:57:05.057384 15876 hierarchical.cpp:280] Skipping recovery of hierarchical allocator: nothing to recover_x000D_
3: I0802 09:57:05.057569 15877 master.cpp:1817] Recovered 0 agents from the registry (143B); allowing 10mins for agents to reregister_x000D_
3: W0802 09:57:05.074198 15861 process.cpp:2877] Attempted to spawn already running process files@172.17.0.2:41613_x000D_
3: I0802 09:57:05.086482 15880 hierarchical.cpp:1508] Performed allocation for 0 agents in 119003ns_x000D_
3: I0802 09:57:05.089071 15861 containerizer.cpp:318] Using isolation { environment_secret, posix/cpu, posix/mem, filesystem/posix, network/cni }_x000D_
3: W0802 09:57:05.090075 15861 backend.cpp:76] Failed to create 'overlay' backend: OverlayBackend requires root privileges_x000D_
3: W0802 09:57:05.090544 15861 backend.cpp:76] Failed to create 'aufs' backend: AufsBackend requires root privileges_x000D_
3: W0802 09:57:05.090595 15861 backend.cpp:76] Failed to create 'bind' backend: BindBackend requires root privileges_x000D_
3: I0802 09:57:05.090658 15861 provisioner.cpp:300] Using default backend 'copy'_x000D_
3: I0802 09:57:05.093798 15861 cluster.cpp:518] Creating default 'local' authorizer_x000D_
3: I0802 09:57:05.099562 15882 slave.cpp:267] Mesos agent started on (17)@172.17.0.2:41613_x000D_
3: I0802 09:57:05.099623 15882 slave.cpp:268] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/450qM2/jqdwI0/store/appc"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""false"" --authenticatee=""crammd5"" --authentication_backoff_factor=""1secs"" --authentication_timeout_max=""1mins"" --authentication_timeout_min=""5secs"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_destroy_timeout=""1mins"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/450qM2/jqdwI0/credential"" --default_role=""*"" --disallow_sharing_agent_ipc_namespace=""false"" --disallow_sharing_agent_pid_namespace=""false"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_ignore_runtime=""false"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/450qM2/jqdwI0/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_reregistration_timeout=""2secs"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/450qM2/jqdwI0/fetch"" --fetcher_cache_size=""2GB"" --fetcher_stall_timeout=""1mins"" --frameworks_home=""/tmp/450qM2/jqdwI0/frameworks"" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --gc_non_executor_container_sandboxes=""false"" --help=""false"" --hostname_lookup=""true"" --http_command_executor=""false"" --http_credentials=""/tmp/450qM2/jqdwI0/http_credentials"" --http_heartbeat_interval=""30secs"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher=""posix"" --launcher_dir=""/tmp/SRC/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_executors_per_framework=""150"" --memory_profiling=""false"" --network_cni_metrics=""true"" --network_cni_root_dir_persist=""false"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --port=""5051"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --reconfiguration_policy=""equal"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:1;mem:1024"" --revocable_cpu_low_priority=""true"" --runtime_dir=""/tmp/MasterQuotaTest_RescindOffersEnforcingLimits_Nys8o1"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/MasterQuotaTest_RescindOffersEnforcingLimits_XCzL51"" --zk_session_timeout=""10secs""_x000D_
3: I0802 09:57:05.100447 15882 credentials.hpp:86] Loading credential for authentication from '/tmp/450qM2/jqdwI0/credential'_x000D_
3: I0802 09:57:05.100728 15882 slave.cpp:300] Agent using credential for: test-principal_x000D_
3: I0802 09:57:05.100751 15882 credentials.hpp:37] Loading credentials for authentication from '/tmp/450qM2/jqdwI0/http_credentials'_x000D_
3: I0802 09:57:05.101040 15882 http.cpp:975] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-readonly'_x000D_
3: I0802 09:57:05.101528 15882 disk_profile_adaptor.cpp:78] Creating default disk profile adaptor module_x000D_
3: I0802 09:57:05.102972 15882 slave.cpp:615] Agent resources: [{""name"":""cpus"",""scalar"":{""value"":1.0},""type"":""SCALAR""},{""name"":""mem"",""scalar"":{""value"":1024.0},""type"":""SCALAR""},{""name"":""disk"",""scalar"":{""value"":3749337.0},""type"":""SCALAR""},{""name"":""ports"",""ranges"":{""range"":[{""begin"":31000,""end"":32000}]},""type"":""RANGES""}]_x000D_
3: I0802 09:57:05.103250 15882 slave.cpp:623] Agent attributes: [  ]_x000D_
3: I0802 09:57:05.103266 15882 slave.cpp:632] Agent hostname: 148706d6d9ee_x000D_
3: I0802 09:57:05.104347 15886 task_status_update_manager.cpp:181] Pausing sending task status updates_x000D_
3: I0802 09:57:05.104373 15878 status_update_manager_process.hpp:379] Pausing operation status update manager_x000D_
3: I0802 09:57:05.107336 15877 state.cpp:67] Recovering state from '/tmp/MasterQuotaTest_RescindOffersEnforcingLimits_XCzL51/meta'_x000D_
3: I0802 09:57:05.120726 15882 slave.cpp:7444] Finished recovering checkpointed state from '/tmp/MasterQuotaTest_RescindOffersEnforcingLimits_XCzL51/meta', beginning agent recovery_x000D_
3: I0802 09:57:05.121412 15892 task_status_update_manager.cpp:207] Recovering task status update manager_x000D_
3: I0802 09:57:05.122179 15882 containerizer.cpp:821] Recovering Mesos containers_x000D_
3: I0802 09:57:05.122614 15882 containerizer.cpp:1147] Recovering isolators_x000D_
3: I0802 09:57:05.126801 15882 containerizer.cpp:1186] Recovering provisioner_x000D_
3: I0802 09:57:05.127704 15876 provisioner.cpp:500] Provisioner recovery complete_x000D_
3: I0802 09:57:05.140862 15882 hierarchical.cpp:1508] Performed allocation for 0 agents in 107533ns_x000D_
3: I0802 09:57:05.141611 15878 composing.cpp:339] Finished recovering all containerizers_x000D_
3: I0802 09:57:05.142130 15878 slave.cpp:7908] Recovering executors_x000D_
3: I0802 09:57:05.142243 15878 slave.cpp:8061] Finished recovery_x000D_
3: I0802 09:57:05.143589 15888 task_status_update_manager.cpp:181] Pausing sending task status updates_x000D_
3: I0802 09:57:05.143630 15886 status_update_manager_process.hpp:379] Pausing operation status update manager_x000D_
3: I0802 09:57:05.143709 15892 slave.cpp:1351] New master detected at master@172.17.0.2:41613_x000D_
3: I0802 09:57:05.143868 15892 slave.cpp:1416] Detecting new master_x000D_
3: W0802 09:57:05.144331 15861 process.cpp:2877] Attempted to spawn already running process version@172.17.0.2:41613_x000D_
3: I0802 09:57:05.146203 15861 sched.cpp:239] Version: 1.9.0_x000D_
3: I0802 09:57:05.147024 15887 sched.cpp:343] New master detected at master@172.17.0.2:41613_x000D_
3: I0802 09:57:05.147194 15887 sched.cpp:408] Authenticating with master master@172.17.0.2:41613_x000D_
3: I0802 09:57:05.147215 15887 sched.cpp:415] Using default CRAM-MD5 authenticatee_x000D_
3: I0802 09:57:05.147907 15890 authenticatee.cpp:121] Creating new client SASL connection_x000D_
3: I0802 09:57:05.148314 15874 master.cpp:10578] Authenticating scheduler-e08ce575-9286-4e6e-9572-c83259dad792@172.17.0.2:41613_x000D_
3: I0802 09:57:05.148574 15874 authenticator.cpp:414] Starting authentication session for crammd5-authenticatee(41)@172.17.0.2:41613_x000D_
3: I0802 09:57:05.149073 15874 authenticator.cpp:98] Creating new server SASL connection_x000D_
3: I0802 09:57:05.149623 15874 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5_x000D_
3: I0802 09:57:05.149660 15874 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'_x000D_
3: I0802 09:57:05.149797 15874 authenticator.cpp:204] Received SASL authentication start_x000D_
3: I0802 09:57:05.149865 15874 authenticator.cpp:326] Authentication requires more steps_x000D_
3: I0802 09:57:05.150003 15874 authenticatee.cpp:259] Received SASL authentication step_x000D_
3: I0802 09:57:05.150120 15874 authenticator.cpp:232] Received SASL authentication step_x000D_
3: I0802 09:57:05.150152 15874 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: '148706d6d9ee' server FQDN: '148706d6d9ee' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false _x000D_
3: I0802 09:57:05.150167 15874 auxprop.cpp:181] Looking up auxiliary property '*userPassword'_x000D_
3: I0802 09:57:05.150226 15874 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'_x000D_
3: I0802 09:57:05.150254 15874 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: '148706d6d9ee' server FQDN: '148706d6d9ee' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true _x000D_
3: I0802 09:57:05.150266 15874 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true_x000D_
3: I0802 09:57:05.150275 15874 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true_x000D_
3: I0802 09:57:05.150293 15874 authenticator.cpp:318] Authentication success_x000D_
3: I0802 09:57:05.150494 15875 authenticatee.cpp:299] Authentication success_x000D_
3: I0802 09:57:05.150573 15874 master.cpp:10610] Successfully authenticated principal 'test-principal' at scheduler-e08ce575-9286-4e6e-9572-c83259dad792@172.17.0.2:41613_x000D_
3: I0802 09:57:05.150702 15874 authenticator.cpp:432] Authentication session cleanup for crammd5-authenticatee(41)@172.17.0.2:41613_x000D_
3: I0802 09:57:05.150995 15875 sched.cpp:520] Successfully authenticated with master master@172.17.0.2:41613_x000D_
3: I0802 09:57:05.151018 15875 sched.cpp:835] Sending SUBSCRIBE call to master@172.17.0.2:41613_x000D_
3: I0802 09:57:05.151211 15875 sched.cpp:870] Will retry registration in 24.561819ms if necessary_x000D_
3: I0802 09:57:05.151538 15875 master.cpp:2908] Received SUBSCRIBE call for framework 'default' at scheduler-e08ce575-9286-4e6e-9572-c83259dad792@172.17.0.2:41613_x000D_
3: I0802 09:57:05.151566 15875 master.cpp:2240] Authorizing framework principal 'test-principal' to receive offers for roles '{ role1 }'_x000D_
3: I0802 09:57:05.152302 15891 master.cpp:2995] Subscribing framework default with checkpointing disabled and capabilities [ MULTI_ROLE, RESERVATION_REFINEMENT ]_x000D_
3: I0802 09:57:05.152519 15887 slave.cpp:1443] Authenticating with master master@172.17.0.2:41613_x000D_
3: I0802 09:57:05.152618 15887 slave.cpp:1452] Using default CRAM-MD5 authenticatee_x000D_
3: I0802 09:57:05.153172 15887 authenticatee.cpp:121] Creating new client SASL connection_x000D_
3: I0802 09:57:05.157215 15891 master.cpp:10808] Adding framework 9dd926f8-c8be-42ad-a1c7-ef0d88a99199-0000 (default) at scheduler-e08ce575-9286-4e6e-9572-c83259dad792@172.17.0.2:41613 with roles {  } suppressed_x000D_
3: I0802 09:57:05.160701 15888 hierarchical.cpp:368] Added framework 9dd926f8-c8be-42ad-a1c7-ef0d88a99199-0000_x000D_
3: I0802 09:57:05.161007 15888 hierarchical.cpp:1508] Performed allocation for 0 agents in 131361ns_x000D_
3: I0802 09:57:05.161232 15888 sched.cpp:751] Framework registered with 9dd926f8-c8be-42ad-a1c7-ef0d88a99199-0000_x000D_
3: I0802 09:57:05.161291 15888 sched.cpp:770] Scheduler::registered took 43881ns_x000D_
3: I0802 09:57:05.161705 15891 master.cpp:10578] Authenticating slave(17)@172.17.0.2:41613_x000D_
3: I0802 09:57:05.161986 15891 authenticator.cpp:414] Starting authentication session for crammd5-authenticatee(42)@172.17.0.2:41613_x000D_
3: I0802 09:57:05.162549 15891 authenticator.cpp:98] Creating new server SASL connection_x000D_
3: I0802 09:57:05.162926 15891 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5_x000D_
3: I0802 09:57:05.162976 15891 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'_x000D_
3: I0802 09:57:05.163103 15891 authenticator.cpp:204] Received SASL authentication start_x000D_
3: I0802 09:57:05.163193 15891 authenticator.cpp:326] Authentication requires more steps_x000D_
3: I0802 09:57:05.163318 15891 authenticatee.cpp:259] Received SASL authentication step_x000D_
3: I0802 09:57:05.163466 15881 authenticator.cpp:232] Received SASL authentication step_x000D_
3: I0802 09:57:05.163518 15881 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: '148706d6d9ee' server FQDN: '148706d6d9ee' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false _x000D_
3: I0802 09:57:05.168320 15881 auxprop.cpp:181] Looking up auxiliary property '*userPassword'_x000D_
3: I0802 09:57:05.168431 15881 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'_x000D_
3: I0802 09:57:05.168474 15881 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: '148706d6d9ee' server FQDN: '148706d6d9ee' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true _x000D_
3: I0802 09:57:05.168493 15881 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true_x000D_
3: I0802 09:57:05.168503 15881 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true_x000D_
3: I0802 09:57:05.168534 15881 authenticator.cpp:318] Authentication success_x000D_
3: I0802 09:57:05.168964 15881 authenticatee.cpp:299] Authentication success_x000D_
3: I0802 09:57:05.169320 15881 master.cpp:10610] Successfully authenticated principal 'test-principal' at slave(17)@172.17.0.2:41613_x000D_
3: I0802 09:57:05.169478 15881 authenticator.cpp:432] Authentication session cleanup for crammd5-authenticatee(42)@172.17.0.2:41613_x000D_
3: I0802 09:57:05.170004 15881 slave.cpp:1543] Successfully authenticated with master master@172.17.0.2:41613_x000D_
3: I0802 09:57:05.170763 15881 slave.cpp:1993] Will retry registration in 4.399581ms if necessary_x000D_
3: I0802 09:57:05.171401 15881 master.cpp:7086] Received register agent message from slave(17)@172.17.0.2:41613 (148706d6d9ee)_x000D_
3: I0802 09:57:05.171723 15881 master.cpp:4202] Authorizing agent providing resources 'cpus:1; mem:1024; disk:3749337; ports:[31000-32000]' with principal 'test-principal'_x000D_
3: I0802 09:57:05.177142 15878 slave.cpp:1993] Will retry registration in 37.129856ms if necessary_x000D_
3: I0802 09:57:05.178035 15878 master.cpp:7079] Ignoring register agent message from slave(17)@172.17.0.2:41613 (148706d6d9ee) as registration is already in progress_x000D_
3: I0802 09:57:05.178143 15878 master.cpp:7153] Authorized registration of agent at slave(17)@172.17.0.2:41613 (148706d6d9ee)_x000D_
3: I0802 09:57:05.178246 15878 master.cpp:7265] Registering agent at slave(17)@172.17.0.2:41613 (148706d6d9ee) with id 9dd926f8-c8be-42ad-a1c7-ef0d88a99199-S0_x000D_
3: I0802 09:57:05.179272 15888 registrar.cpp:487] Applied 1 operations in 362994ns; attempting to update the registry_x000D_
3: I0802 09:57:05.180107 15888 registrar.cpp:544] Successfully updated the registry in 748032ns_x000D_
3: I0802 09:57:05.184391 15879 master.cpp:7313] Admitted agent 9dd926f8-c8be-42ad-a1c7-ef0d88a99199-S0 at slave(17)@172.17.0.2:41613 (148706d6d9ee)_x000D_
3: I0802 09:57:05.185465 15879 master.cpp:7358] Registered agent 9dd926f8-c8be-42ad-a1c7-ef0d88a99199-S0 at slave(17)@172.17.0.2:41613 (148706d6d9ee) with cpus:1; mem:1024; disk:3749337; ports:[31000-32000]_x000D_
3: I0802 09:57:05.186156 15879 hierarchical.cpp:617] Added agent 9dd926f8-c8be-42ad-a1c7-ef0d88a99199-S0 (148706d6d9ee) with cpus:1; mem:1024; disk:3749337; ports:[31000-32000] (allocated: {})_x000D_
3: I0802 09:57:05.187729 15879 hierarchical.cpp:1508] Performed allocation for 1 agents in 1.368422ms_x000D_
3: I0802 09:57:05.187886 15879 slave.cpp:1576] Registered with master master@172.17.0.2:41613; given agent ID 9dd926f8-c8be-42ad-a1c7-ef0d88a99199-S0_x000D_
3: I0802 09:57:05.192725 15876 task_status_update_manager.cpp:188] Resuming sending task status updates_x000D_
3: I0802 09:57:05.192859 15879 slave.cpp:1611] Checkpointing SlaveInfo to '/tmp/MasterQuotaTest_RescindOffersEnforcingLimits_XCzL51/meta/slaves/9dd926f8-c8be-42ad-a1c7-ef0d88a99199-S0/slave.info'_x000D_
3: I0802 09:57:05.193363 15871 master.cpp:10393] Sending offers [ 9dd926f8-c8be-42ad-a1c7-ef0d88a99199-O0 ] to framework 9dd926f8-c8be-42ad-a1c7-ef0d88a99199-0000 (default) at scheduler-e08ce575-9286-4e6e-9572-c83259dad792@172.17.0.2:41613_x000D_
3: I0802 09:57:05.193572 15873 status_update_manager_process.hpp:385] Resuming operation status update manager_x000D_
3: I0802 09:57:05.194280 15873 sched.cpp:934] Scheduler::resourceOffers took 116258ns_x000D_
3: I0802 09:57:05.194945 15873 hierarchical.cpp:1508] Performed allocation for 1 agents in 375907ns_x000D_
3: I0802 09:57:05.195372 15879 slave.cpp:1663] Forwarding agent update {""operations"":{},""resource_providers"":{},""resource_version_uuid"":{""value"":""Chjy8C/USQqbsT32vnA1uw==""},""slave_id"":{""value"":""9dd926f8-c8be-42ad-a1c7-ef0d88a99199-S0""},""update_oversubscribed_resources"":false}_x000D_
3: I0802 09:57:05.196919 15893 master.cpp:8457] Ignoring update on agent 9dd926f8-c8be-42ad-a1c7-ef0d88a99199-S0 at slave(17)@172.17.0.2:41613 (148706d6d9ee) as it reports no changes_x000D_
3: W0802 09:57:05.205972 15861 process.cpp:2877] Attempted to spawn already running process files@172.17.0.2:41613_x000D_
3: I0802 09:57:05.207845 15861 containerizer.cpp:318] Using isolation { environment_secret, posix/cpu, posix/mem, filesystem/posix, network/cni }_x000D_
3: W0802 09:57:05.222391 15861 backend.cpp:76] Failed to create 'overlay' backend: OverlayBackend requires root privileges_x000D_
3: W0802 09:57:05.222438 15861 backend.cpp:76] Failed to create 'aufs' backend: AufsBackend requires root privileges_x000D_
3: W0802 09:57:05.222460 15861 backend.cpp:76] Failed to create 'bind' backend: BindBackend requires root privileges_x000D_
3: I0802 09:57:05.222501 15861 provisioner.cpp:300] Using default backend 'copy'_x000D_
3: I0802 09:57:05.238737 15861 cluster.cpp:518] Creating default 'local' authorizer_x000D_
3: I0802 09:57:05.249315 15876 hierarchical.cpp:1508] Performed allocation for 1 agents in 408138ns_x000D_
3: I0802 09:57:05.276511 15872 slave.cpp:267] Mesos agent started on (18)@172.17.0.2:41613_x000D_
3: I0802 09:57:05.276571 15872 slave.cpp:268] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/450qM2/TDawGY/store/appc"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""false"" --authenticatee=""crammd5"" --authentication_backoff_factor=""1secs"" --authentication_timeout_max=""1mins"" --authentication_timeout_min=""5secs"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_destroy_timeout=""1mins"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/450qM2/TDawGY/credential"" --default_role=""*"" --disallow_sharing_agent_ipc_namespace=""false"" --disallow_sharing_agent_pid_namespace=""false"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_ignore_runtime=""false"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/450qM2/TDawGY/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_reregistration_timeout=""2secs"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/450qM2/TDawGY/fetch"" --fetcher_cache_size=""2GB"" --fetcher_stall_timeout=""1mins"" --frameworks_home=""/tmp/450qM2/TDawGY/frameworks"" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --gc_non_executor_container_sandboxes=""false"" --help=""false"" --hostname_lookup=""true"" --http_command_executor=""false"" --http_credentials=""/tmp/450qM2/TDawGY/http_credentials"" --http_heartbeat_interval=""30secs"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher=""posix"" --launcher_dir=""/tmp/SRC/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_executors_per_framework=""150"" --memory_profiling=""false"" --network_cni_metrics=""true"" --network_cni_root_dir_persist=""false"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --port=""5051"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --reconfiguration_policy=""equal"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:1;mem:1024"" --revocable_cpu_low_priority=""true"" --runtime_dir=""/tmp/MasterQuotaTest_RescindOffersEnforcingLimits_TODS3Y"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/MasterQuotaTest_RescindOffersEnforcingLimits_w13frZ"" --zk_session_timeout=""10secs""_x000D_
3: I0802 09:57:05.277412 15872 credentials.hpp:86] Loading credential for authentication from '/tmp/450qM2/TDawGY/credential'_x000D_
3: I0802 09:57:05.277693 15872 slave.cpp:300] Agent using credential for: test-principal_x000D_
3: I0802 09:57:05.277715 15872 credentials.hpp:37] Loading credentials for authentication from '/tmp/450qM2/TDawGY/http_credentials'_x000D_
3: I0802 09:57:05.277998 15872 http.cpp:975] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-readonly'_x000D_
3: I0802 09:57:05.278468 15872 disk_profile_adaptor.cpp:78] Creating default disk profile adaptor module_x000D_
3: I0802 09:57:05.279918 15872 slave.cpp:615] Agent resources: [{""name"":""cpus"",""scalar"":{""value"":1.0},""type"":""SCALAR""},{""name"":""mem"",""scalar"":{""value"":1024.0},""type"":""SCALAR""},{""name"":""disk"",""scalar"":{""value"":3749337.0},""type"":""SCALAR""},{""name"":""ports"",""ranges"":{""range"":[{""begin"":31000,""end"":32000}]},""type"":""RANGES""}]_x000D_
3: I0802 09:57:05.280186 15872 slave.cpp:623] Agent attributes: [  ]_x000D_
3: I0802 09:57:05.280205 15872 slave.cpp:632] Agent hostname: 148706d6d9ee_x000D_
3: I0802 09:57:05.283205 15872 task_status_update_manager.cpp:181] Pausing sending task status updates_x000D_
3: I0802 09:57:05.283267 15872 status_update_manager_process.hpp:379] Pausing operation status update manager_x000D_
3: I0802 09:57:05.288424 15872 state.cpp:67] Recovering state from '/tmp/MasterQuotaTest_RescindOffersEnforcingLimits_w13frZ/meta'_x000D_
3: I0802 09:57:05.289094 15872 slave.cpp:7444] Finished recovering checkpointed state from '/tmp/MasterQuotaTest_RescindOffersEnforcingLimits_w13frZ/meta', beginning agent recovery_x000D_
3: I0802 09:57:05.289788 15875 task_status_update_manager.cpp:207] Recovering task status update manager_x000D_
3: I0802 09:57:05.290593 15872 containerizer.cpp:821] Recovering Mesos containers_x000D_
3: I0802 09:57:05.290952 15872 containerizer.cpp:1147] Recovering isolators_x000D_
3: I0802 09:57:05.292505 15885 containerizer.cpp:1186] Recovering provisioner_x000D_
3: I0802 09:57:05.293473 15885 provisioner.cpp:500] Provisioner recovery complete_x000D_
3: I0802 09:57:05.294914 15885 composing.cpp:339] Finished recovering all containerizers_x000D_
3: I0802 09:57:05.295361 15885 slave.cpp:7908] Recovering executors_x000D_
3: I0802 09:57:05.295472 15885 slave.cpp:8061] Finished recovery_x000D_
3: W0802 09:57:05.297149 15861 process.cpp:2877] Attempted to spawn already running process version@172.17.0.2:41613_x000D_
3: I0802 09:57:05.297420 15885 slave.cpp:1351] New master detected at master@172.17.0.2:41613_x000D_
3: I0802 09:57:05.297458 15880 task_status_update_manager.cpp:181] Pausing sending task status updates_x000D_
3: I0802 09:57:05.297533 15885 slave.cpp:1416] Detecting new master_x000D_
3: I0802 09:57:05.297538 15880 status_update_manager_process.hpp:379] Pausing operation status update manager_x000D_
3: I0802 09:57:05.305006 15887 hierarchical.cpp:1508] Performed allocation for 1 agents in 317611ns_x000D_
3: I0802 09:57:05.305449 15861 sched.cpp:239] Version: 1.9.0_x000D_
3: I0802 09:57:05.306335 15882 sched.cpp:343] New master detected at master@172.17.0.2:41613_x000D_
3: I0802 09:57:05.306438 15882 sched.cpp:408] Authenticating with master master@172.17.0.2:41613_x000D_
3: I0802 09:57:05.306457 15882 sched.cpp:415] Using default CRAM-MD5 authenticatee_x000D_
3: I0802 09:57:05.307082 15882 authenticatee.cpp:121] Creating new client SASL connection_x000D_
3: I0802 09:57:05.307696 15882 master.cpp:10578] Authenticating scheduler-76d82e9d-f02c-4174-894e-be6b1a164320@172.17.0.2:41613_x000D_
3: I0802 09:57:05.307929 15882 authenticator.cpp:414] Starting authentication session for crammd5-authenticatee(43)@172.17.0.2:41613_x000D_
3: I0802 09:57:05.308429 15876 slave.cpp:1443] Authenticating with master master@172.17.0.2:41613_x000D_
3: I0802 09:57:05.308714 15882 authenticator.cpp:98] Creating new server SASL connection_x000D_
3: I0802 09:57:05.309056 15882 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5_x000D_
3: I0802 09:57:05.309088 15882 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'_x000D_
3: I0802 09:57:05.309224 15882 authenticator.cpp:204] Received SASL authentication start_x000D_
3: I0802 09:57:05.309303 15882 authenticator.cpp:326] Authentication requires more steps_x000D_
3: I0802 09:57:05.309432 15882 authenticatee.cpp:259] Received SASL authentication step_x000D_
3: I0802 09:57:05.309566 15882 authenticator.cpp:232] Received SASL authentication step_x000D_
3: I0802 09:57:05.309602 15882 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: '148706d6d9ee' server FQDN: '148706d6d9ee' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false _x000D_
3: I0802 09:57:05.309623 15882 auxprop.cpp:181] Looking up auxiliary property '*userPassword'_x000D_
3: I0802 09:57:05.309686 15882 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'_x000D_
3: I0802 09:57:05.309722 15882 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: '148706d6d9ee' server FQDN: '148706d6d9ee' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true _x000D_
3: I0802 09:57:05.309739 15882 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true_x000D_
3: I0802 09:57:05.309751 15882 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true_x000D_
3: I0802 09:57:05.309772 15882 authenticator.cpp:318] Authentication success_x000D_
3: I0802 09:57:05.310050 15882 authenticatee.cpp:299] Authentication success_x000D_
3: I0802 09:57:05.310207 15870 master.cpp:10610] Successfully authenticated principal 'test-principal' at scheduler-76d82e9d-f02c-4174-894e-be6b1a164320@172.17.0.2:41613_x000D_
3: I0802 09:57:05.310354 15870 authenticator.cpp:432] Authentication session cleanup for crammd5-authenticatee(43)@172.17.0.2:41613_x000D_
3: I0802 09:57:05.310809 15876 slave.cpp:1452] Using default CRAM-MD5 authenticatee_x000D_
3: I0802 09:57:05.311417 15876 authenticatee.cpp:121] Creating new client SASL connection_x000D_
3: I0802 09:57:05.311868 15876 master.cpp:10578] Authenticating slave(18)@172.17.0.2:41613_x000D_
3: I0802 09:57:05.312104 15876 authenticator.cpp:414] Starting authentication session for crammd5-authenticatee(44)@172.17.0.2:41613_x000D_
3: I0802 09:57:05.316658 15876 authenticator.cpp:98] Creating new server SASL connection_x000D_
3: I0802 09:57:05.317152 15876 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5_x000D_
3: I0802 09:57:05.317185 15876 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'_x000D_
3: I0802 09:57:05.317333 15876 authenticator.cpp:204] Received SASL authentication start_x000D_
3: I0802 09:57:05.317412 15876 authenticator.cpp:326] Authentication requires more steps_x000D_
3: I0802 09:57:05.317543 15876 authenticatee.cpp:259] Received SASL authentication step_x000D_
3: I0802 09:57:05.317685 15876 authenticator.cpp:232] Received SASL authentication step_x000D_
3: I0802 09:57:05.317723 15876 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: '148706d6d9ee' server FQDN: '148706d6d9ee' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false _x000D_
3: I0802 09:57:05.317742 15876 auxprop.cpp:181] Looking up auxiliary property '*userPassword'_x000D_
3: I0802 09:57:05.317811 15876 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'_x000D_
3: I0802 09:57:05.317848 15876 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: '148706d6d9ee' server FQDN: '148706d6d9ee' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true _x000D_
3: I0802 09:57:05.317867 15876 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true_x000D_
3: I0802 09:57:05.317879 15876 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true_x000D_
3: I0802 09:57:05.317901 15876 authenticator.cpp:318] Authentication success_x000D_
3: I0802 09:57:05.318174 15876 authenticatee.cpp:299] Authentication success_x000D_
3: I0802 09:57:05.318508 15876 master.cpp:10610] Successfully authenticated principal 'test-principal",1.0,1.9.0,0.5,0.0,0.6666666666666666,1.0,0.5714285714285714,0.0,0.0,0.5438596491228069,0.5357142857142857,0.14102564102564102,0.08484848484848485,0.08484848484848485,1.0
Bug,SSL socket error logging can be improved.,"While debugging some unrelated linkage problem, I noticed the following error output;_x000D_
{noformat}_x000D_
16:19:25 I0728 16:19:25.323794 26188 libevent_ssl_socket.cpp:1244] Socket error: error:00000005:lib(0):func(0):DH lib_x000D_
{noformat}_x000D_
_x000D_
The error message appears not very helpful and that we can improve on._x000D_
_x000D_
When receiving a libevent openssl-error, we do not check the error code but pass it on to openssl for retrieving an error string -- this is not ideal considering that openssl does signal more; _x000D_
The error code 5, which actually means {{SSL_ERROR_SYSCALL}} does hint that we should now check {{errno}} for more information on the problem.  We should only ever invoke openssl's error string generator when we did receive a {{SSL_ERROR_SSL}}._x000D_
_x000D_
Also see http://openssl.6102.n7.nabble.com/SSL-read-return-1-error-00000005-lib-0-func-0-DH-lib-tp27612p27613.html",1.0,1.9.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3717948717948718,0.24848484848484845,0.24848484848484845,1.0
Bug,jsonify uses non-standard mapping for protobuf map fields.,"Jsonify current treats protobuf as a regular repeated field. For example, for the schema _x000D_
_x000D_
{noformat}_x000D_
message QuotaConfig {_x000D_
  required string role = 1;_x000D_
_x000D_
  map<string, Value.Scalar> guarantees = 2;_x000D_
  map<string, Value.Scalar> limits = 3;_x000D_
}_x000D_
{noformat}_x000D_
_x000D_
it will produce:_x000D_
_x000D_
{noformat}_x000D_
{_x000D_
  ""configs"": [_x000D_
    {_x000D_
      ""role"": ""role1"",_x000D_
      ""guarantees"": [_x000D_
        {_x000D_
          ""key"": ""cpus"",_x000D_
          ""value"": {_x000D_
            ""value"": 1_x000D_
          }_x000D_
        },_x000D_
        {_x000D_
          ""key"": ""mem"",_x000D_
          ""value"": {_x000D_
            ""value"": 512_x000D_
          }_x000D_
        }_x000D_
      ]_x000D_
    }_x000D_
  ]_x000D_
}_x000D_
{noformat}_x000D_
_x000D_
This output cannot be parsed back to proto messages. We need to specialize jsonify for Maps type to get the standard output:_x000D_
_x000D_
{noformat}_x000D_
    {_x000D_
      ""configs"": [_x000D_
        {_x000D_
          ""role"": ""role1"",_x000D_
          ""guarantees"": {_x000D_
            ""cpus"": 1,_x000D_
            ""mem"": 512_x000D_
          }_x000D_
        }_x000D_
      ]_x000D_
    }_x000D_
{noformat}",3.0,0,0.0,0.0,0.6666666666666666,1.0,0.5714285714285714,0.0,0.0,0.5438596491228069,0.5357142857142857,0.3974358974358974,0.4,0.4,0.0
Bug,Mesos failed to build due to fatal error C1083 on Windows using MSVC.,"Mesos failed to build due to fatal error C1083: Cannot open include file: 'slave/volume_gid_manager/state.pb.h': No such file or directory on Windows using MSVC. It can be first reproduced on 6a026e3 reversion on master branch. Could you please take a look at this isssue? Thanks a lot!_x000D_
_x000D_
Reproduce steps:_x000D_
_x000D_
1. git clone -c core.autocrlf=true https://github.com/apache/mesos D:\mesos\src_x000D_
2. Open a VS 2017 x64 command prompt as admin and browse to D:\mesos_x000D_
3. cd src_x000D_
4. .\bootstrap.bat_x000D_
5. cd .._x000D_
6. mkdir build_x64 && pushd build_x64_x000D_
7. cmake ..\src -G ""Visual Studio 15 2017 Win64"" -DCMAKE_SYSTEM_VERSION=10.0.17134.0 -DENABLE_LIBEVENT=1 -DHAS_AUTHENTICATION=0 -DPATCHEXE_PATH=""C:\gnuwin32\bin"" -T host=x64_x000D_
8. msbuild Mesos.sln /p:Configuration=Debug /p:Platform=x64 /maxcpucount:4 /t:Rebuild_x000D_
_x000D_
 _x000D_
_x000D_
ErrorMessage:_x000D_
_x000D_
D:\Mesos\src\include\mesos/docker/spec.hpp(29): fatal error C1083: Cannot open include file: 'mesos/docker/spec.pb.h': No such file or directory_x000D_
_x000D_
D:\Mesos\src\src\slave/volume_gid_manager/state.hpp(21): fatal error C1083: Cannot open include file: 'slave/volume_gid_manager/state.pb.h': No such file or directory_x000D_
_x000D_
D:\Mesos\src\src\slave/volume_gid_manager/state.hpp(21): fatal error C1083: Cannot open include file: 'slave/volume_gid_manager/state.pb.h': No such file or directory_x000D_
_x000D_
 _x000D_
_x000D_
 ",1.0,0,0.5,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.038461538461538464,0.01212121212121212,0.01212121212121212,0.0
Bug,/roles and GET_ROLES does not always expose parent roles.,"If some descendant roles are present in frameworks, then the parent roles will not be exposed in the /roles and GET_ROLES endpoints._x000D_
_x000D_
This is because the tracking is currently based on frameworks being subscribed to the role.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.0
Bug,/roles and GET_ROLES do not expose roles with only static reservations,"If a role is only known to the master because of an agent with static reservations to that role, it will not be shown in the /roles and GET_ROLES APIs._x000D_
_x000D_
This is because the roles are tracked based on frameworks primarily. We'll need to update the tracking to include when there are agents with reservations.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.0
Bug,RoleTest.RolesEndpointContainsConsumedQuota is flaky.,"{noformat}_x000D_
[ RUN      ] RoleTest.RolesEndpointContainsConsumedQuota_x000D_
I0710 07:05:42.670790  9995 cluster.cpp:176] Creating default 'local' authorizer_x000D_
I0710 07:05:42.672238  9999 master.cpp:440] Master 8db40cec-43ef-41a1-89a4-4f7b877d8f13 (ip-172-16-10-69.ec2.internal) started on 172.16.10.69:37082_x000D_
I0710 07:05:42.672256  9999 master.cpp:443] Flags at startup: --acls="""" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""1secs"" --allocator=""hierarchical"" --authenticate_agents=""true"" --authenticate_frameworks=""true"" --authenticate_http_frameworks=""true"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""true"" --authentication_v0_timeout=""15secs"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/1d0m6o/credentials"" --filter_gpu_resources=""true"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_operator_event_stream_subscribers=""1000"" --max_unreachable_tasks_per_framework=""1000"" --memory_profiling=""false"" --min_allocatable_resources=""cpus:0.01|mem:32"" --port=""5050"" --publish_per_framework_metrics=""true"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""in_memory"" --registry_fetch_timeout=""1mins"" --registry_gc_interval=""15mins"" --registry_max_agent_age=""2weeks"" --registry_max_agent_count=""102400"" --registry_store_timeout=""100secs"" --registry_strict=""false"" --require_agent_domain=""false"" --role_sorter=""drf"" --root_submissions=""true"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/1d0m6o/master"" --zk_session_timeout=""10secs""_x000D_
I0710 07:05:42.672351  9999 master.cpp:492] Master only allowing authenticated frameworks to register_x000D_
I0710 07:05:42.672356  9999 master.cpp:498] Master only allowing authenticated agents to register_x000D_
I0710 07:05:42.672360  9999 master.cpp:504] Master only allowing authenticated HTTP frameworks to register_x000D_
I0710 07:05:42.672364  9999 credentials.hpp:37] Loading credentials for authentication from '/tmp/1d0m6o/credentials'_x000D_
I0710 07:05:42.672430  9999 master.cpp:548] Using default 'crammd5' authenticator_x000D_
I0710 07:05:42.672466  9999 http.cpp:975] Creating default 'basic' HTTP authenticator for realm 'mesos-master-readonly'_x000D_
I0710 07:05:42.672508  9999 http.cpp:975] Creating default 'basic' HTTP authenticator for realm 'mesos-master-readwrite'_x000D_
I0710 07:05:42.672538  9999 http.cpp:975] Creating default 'basic' HTTP authenticator for realm 'mesos-master-scheduler'_x000D_
I0710 07:05:42.672569  9999 master.cpp:629] Authorization enabled_x000D_
I0710 07:05:42.672658 10001 hierarchical.cpp:241] Initialized hierarchical allocator process_x000D_
I0710 07:05:42.672685 10001 whitelist_watcher.cpp:77] No whitelist given_x000D_
I0710 07:05:42.673316 10001 master.cpp:2150] Elected as the leading master!_x000D_
I0710 07:05:42.673331 10001 master.cpp:1664] Recovering from registrar_x000D_
I0710 07:05:42.673616 10001 registrar.cpp:339] Recovering registrar_x000D_
I0710 07:05:42.673874 10001 registrar.cpp:383] Successfully fetched the registry (0B) in 239104ns_x000D_
I0710 07:05:42.673923 10001 registrar.cpp:487] Applied 1 operations in 7745ns; attempting to update the registry_x000D_
I0710 07:05:42.674052  9999 registrar.cpp:544] Successfully updated the registry in 108032ns_x000D_
I0710 07:05:42.674082  9999 registrar.cpp:416] Successfully recovered registrar_x000D_
I0710 07:05:42.674152  9999 master.cpp:1799] Recovered 0 agents from the registry (180B); allowing 10mins for agents to reregister_x000D_
I0710 07:05:42.674185  9996 hierarchical.cpp:280] Skipping recovery of hierarchical allocator: nothing to recover_x000D_
W0710 07:05:42.676100  9995 process.cpp:2877] Attempted to spawn already running process files@172.16.10.69:37082_x000D_
I0710 07:05:42.676537  9995 containerizer.cpp:314] Using isolation { environment_secret, posix/cpu, posix/mem, filesystem/posix, network/cni }_x000D_
I0710 07:05:42.678514  9995 linux_launcher.cpp:144] Using /cgroup/freezer as the freezer hierarchy for the Linux launcher_x000D_
I0710 07:05:42.678980  9995 provisioner.cpp:298] Using default backend 'copy'_x000D_
I0710 07:05:42.680043  9995 cluster.cpp:510] Creating default 'local' authorizer_x000D_
I0710 07:05:42.680832  9998 slave.cpp:265] Mesos agent started on (522)@172.16.10.69:37082_x000D_
I0710 07:05:42.680850  9998 slave.cpp:266] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/1d0m6o/qvvVks/store/appc"" --authenticate_http_executors=""true"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""true"" --authenticatee=""crammd5"" --authentication_backoff_factor=""1secs"" --authentication_timeout_max=""1mins"" --authentication_timeout_min=""5secs"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_destroy_timeout=""1mins"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/1d0m6o/qvvVks/credential"" --default_role=""*"" --disallow_sharing_agent_ipc_namespace=""false"" --disallow_sharing_agent_pid_namespace=""false"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_ignore_runtime=""false"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/1d0m6o/qvvVks/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_reregistration_timeout=""2secs"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/1d0m6o/qvvVks/fetch"" --fetcher_cache_size=""2GB"" --fetcher_stall_timeout=""1mins"" --frameworks_home=""/tmp/1d0m6o/qvvVks/frameworks"" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --gc_non_executor_container_sandboxes=""false"" --help=""false"" --hostname_lookup=""true"" --http_command_executor=""false"" --http_credentials=""/tmp/1d0m6o/qvvVks/http_credentials"" --http_heartbeat_interval=""30secs"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --jwt_secret_key=""/tmp/1d0m6o/qvvVks/jwt_secret_key"" --launcher=""linux"" --launcher_dir=""/home/centos/workspace/mesos/Mesos_CI-build/FLAG/SSL/label/mesos-ec2-centos-6/mesos/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_executors_per_framework=""150"" --memory_profiling=""false"" --network_cni_metrics=""true"" --network_cni_root_dir_persist=""false"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --port=""5051"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --reconfiguration_policy=""equal"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus(role):1;mem(role):10;disk:0;ports:[]"" --revocable_cpu_low_priority=""true"" --runtime_dir=""/tmp/RoleTest_RolesEndpointContainsConsumedQuota_1IIJA6"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/RoleTest_RolesEndpointContainsConsumedQuota_xr6xQK"" --zk_session_timeout=""10secs""_x000D_
I0710 07:05:42.681233  9998 credentials.hpp:86] Loading credential for authentication from '/tmp/1d0m6o/qvvVks/credential'_x000D_
I0710 07:05:42.681301  9998 slave.cpp:298] Agent using credential for: test-principal_x000D_
I0710 07:05:42.681313  9998 credentials.hpp:37] Loading credentials for authentication from '/tmp/1d0m6o/qvvVks/http_credentials'_x000D_
I0710 07:05:42.681396  9998 http.cpp:975] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-executor'_x000D_
I0710 07:05:42.681429  9998 http.cpp:996] Creating default 'jwt' HTTP authenticator for realm 'mesos-agent-executor'_x000D_
I0710 07:05:42.681473  9998 http.cpp:975] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-readonly'_x000D_
I0710 07:05:42.681493  9998 http.cpp:996] Creating default 'jwt' HTTP authenticator for realm 'mesos-agent-readonly'_x000D_
I0710 07:05:42.681519  9998 http.cpp:975] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-readwrite'_x000D_
I0710 07:05:42.681535  9998 http.cpp:996] Creating default 'jwt' HTTP authenticator for realm 'mesos-agent-readwrite'_x000D_
W0710 07:05:42.681565  9995 process.cpp:2877] Attempted to spawn already running process version@172.16.10.69:37082_x000D_
I0710 07:05:42.681998  9998 disk_profile_adaptor.cpp:78] Creating default disk profile adaptor module_x000D_
I0710 07:05:42.682571  9998 slave.cpp:613] Agent resources: [{""name"":""cpus"",""reservations"":[{""role"":""role"",""type"":""STATIC""}],""scalar"":{""value"":1.0},""type"":""SCALAR""},{""name"":""mem"",""reservations"":[{""role"":""role"",""type"":""STATIC""}],""scalar"":{""value"":10.0},""type"":""SCALAR""}]_x000D_
I0710 07:05:42.682636  9998 slave.cpp:621] Agent attributes: [  ]_x000D_
I0710 07:05:42.682643  9998 slave.cpp:630] Agent hostname: ip-172-16-10-69.ec2.internal_x000D_
I0710 07:05:42.682718  9996 task_status_update_manager.cpp:181] Pausing sending task status updates_x000D_
I0710 07:05:42.682737  9996 status_update_manager_process.hpp:379] Pausing operation status update manager_x000D_
I0710 07:05:42.682947  9998 state.cpp:67] Recovering state from '/tmp/RoleTest_RolesEndpointContainsConsumedQuota_xr6xQK/meta'_x000D_
I0710 07:05:42.683017  9998 slave.cpp:7246] Finished recovering checkpointed state from '/tmp/RoleTest_RolesEndpointContainsConsumedQuota_xr6xQK/meta', beginning agent recovery_x000D_
I0710 07:05:42.683105  9998 task_status_update_manager.cpp:207] Recovering task status update manager_x000D_
I0710 07:05:42.683320 10002 containerizer.cpp:796] Recovering Mesos containers_x000D_
I0710 07:05:42.683378 10002 linux_launcher.cpp:286] Recovering Linux launcher_x000D_
I0710 07:05:42.683485 10002 containerizer.cpp:1122] Recovering isolators_x000D_
I0710 07:05:42.683931 10000 containerizer.cpp:1161] Recovering provisioner_x000D_
I0710 07:05:42.684257 10001 provisioner.cpp:498] Provisioner recovery complete_x000D_
I0710 07:05:42.684689  9997 composing.cpp:339] Finished recovering all containerizers_x000D_
I0710 07:05:42.684762  9997 slave.cpp:7708] Recovering executors_x000D_
I0710 07:05:42.684785  9997 slave.cpp:7861] Finished recovery_x000D_
I0710 07:05:42.685237  9996 status_update_manager_process.hpp:379] Pausing operation status update manager_x000D_
I0710 07:05:42.685223 10003 task_status_update_manager.cpp:181] Pausing sending task status updates_x000D_
I0710 07:05:42.685498  9995 sched.cpp:239] Version: 1.9.0_x000D_
I0710 07:05:42.685214  9997 slave.cpp:1258] New master detected at master@172.16.10.69:37082_x000D_
I0710 07:05:42.685556  9997 slave.cpp:1323] Detecting new master_x000D_
I0710 07:05:42.685786  9997 sched.cpp:343] New master detected at master@172.16.10.69:37082_x000D_
I0710 07:05:42.686059  9997 sched.cpp:408] Authenticating with master master@172.16.10.69:37082_x000D_
I0710 07:05:42.686069  9997 sched.cpp:415] Using default CRAM-MD5 authenticatee_x000D_
I0710 07:05:42.686162  9997 authenticatee.cpp:121] Creating new client SASL connection_x000D_
I0710 07:05:42.686285  9999 master.cpp:10380] Authenticating scheduler-c1197694-9c80-4659-ba29-891d2bcb6a32@172.16.10.69:37082_x000D_
I0710 07:05:42.686347  9999 authenticator.cpp:414] Starting authentication session for crammd5-authenticatee(1050)@172.16.10.69:37082_x000D_
I0710 07:05:42.686412  9999 authenticator.cpp:98] Creating new server SASL connection_x000D_
I0710 07:05:42.686487  9999 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5_x000D_
I0710 07:05:42.686499  9999 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'_x000D_
I0710 07:05:42.686535  9999 authenticator.cpp:204] Received SASL authentication start_x000D_
I0710 07:05:42.686586  9999 authenticator.cpp:326] Authentication requires more steps_x000D_
I0710 07:05:42.686622  9999 authenticatee.cpp:259] Received SASL authentication step_x000D_
I0710 07:05:42.686669  9999 authenticator.cpp:232] Received SASL authentication step_x000D_
I0710 07:05:42.686686  9999 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'ip-172-16-10-69' server FQDN: 'ip-172-16-10-69' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false _x000D_
I0710 07:05:42.686715  9999 auxprop.cpp:181] Looking up auxiliary property '*userPassword'_x000D_
I0710 07:05:42.686728  9999 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'_x000D_
I0710 07:05:42.686738  9999 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'ip-172-16-10-69' server FQDN: 'ip-172-16-10-69' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true _x000D_
I0710 07:05:42.686743  9999 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true_x000D_
I0710 07:05:42.686749  9999 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true_x000D_
I0710 07:05:42.686763  9999 authenticator.cpp:318] Authentication success_x000D_
I0710 07:05:42.686810  9999 authenticatee.cpp:299] Authentication success_x000D_
I0710 07:05:42.686854  9999 master.cpp:10412] Successfully authenticated principal 'test-principal' at scheduler-c1197694-9c80-4659-ba29-891d2bcb6a32@172.16.10.69:37082_x000D_
I0710 07:05:42.686887 10000 authenticator.cpp:432] Authentication session cleanup for crammd5-authenticatee(1050)@172.16.10.69:37082_x000D_
I0710 07:05:42.687379  9999 sched.cpp:520] Successfully authenticated with master master@172.16.10.69:37082_x000D_
I0710 07:05:42.687400  9999 sched.cpp:835] Sending SUBSCRIBE call to master@172.16.10.69:37082_x000D_
I0710 07:05:42.687448  9999 sched.cpp:870] Will retry registration in 667.029736ms if necessary_x000D_
I0710 07:05:42.687536 10001 master.cpp:2890] Received SUBSCRIBE call for framework 'default' at scheduler-c1197694-9c80-4659-ba29-891d2bcb6a32@172.16.10.69:37082_x000D_
I0710 07:05:42.687556 10001 master.cpp:2222] Authorizing framework principal 'test-principal' to receive offers for roles '{ role }'_x000D_
I0710 07:05:42.687672  9998 master.cpp:2977] Subscribing framework default with checkpointing disabled and capabilities [ MULTI_ROLE, RESERVATION_REFINEMENT ]_x000D_
I0710 07:05:42.688201  9998 master.cpp:10610] Adding framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000 (default) at scheduler-c1197694-9c80-4659-ba29-891d2bcb6a32@172.16.10.69:37082 with roles {  } suppressed_x000D_
I0710 07:05:42.688334 10000 sched.cpp:751] Framework registered with 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000_x000D_
I0710 07:05:42.688364 10000 sched.cpp:770] Scheduler::registered took 14639ns_x000D_
I0710 07:05:42.689081  9998 hierarchical.cpp:368] Added framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000_x000D_
I0710 07:05:42.689308  9998 hierarchical.cpp:1508] Performed allocation for 0 agents in 29346ns_x000D_
I0710 07:05:42.693774  9997 slave.cpp:1350] Authenticating with master master@172.16.10.69:37082_x000D_
I0710 07:05:42.693804  9997 slave.cpp:1359] Using default CRAM-MD5 authenticatee_x000D_
I0710 07:05:42.693876  9997 authenticatee.cpp:121] Creating new client SASL connection_x000D_
I0710 07:05:42.693958  9997 master.cpp:10380] Authenticating slave(522)@172.16.10.69:37082_x000D_
I0710 07:05:42.693998  9997 authenticator.cpp:414] Starting authentication session for crammd5-authenticatee(1051)@172.16.10.69:37082_x000D_
I0710 07:05:42.694056  9997 authenticator.cpp:98] Creating new server SASL connection_x000D_
I0710 07:05:42.694115  9997 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5_x000D_
I0710 07:05:42.694125  9997 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'_x000D_
I0710 07:05:42.694160  9999 authenticator.cpp:204] Received SASL authentication start_x000D_
I0710 07:05:42.694198  9999 authenticator.cpp:326] Authentication requires more steps_x000D_
I0710 07:05:42.694236  9999 authenticatee.cpp:259] Received SASL authentication step_x000D_
I0710 07:05:42.694277  9999 authenticator.cpp:232] Received SASL authentication step_x000D_
I0710 07:05:42.694290  9999 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'ip-172-16-10-69' server FQDN: 'ip-172-16-10-69' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false _x000D_
I0710 07:05:42.694296  9999 auxprop.cpp:181] Looking up auxiliary property '*userPassword'_x000D_
I0710 07:05:42.694304  9999 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'_x000D_
I0710 07:05:42.694312  9999 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'ip-172-16-10-69' server FQDN: 'ip-172-16-10-69' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true _x000D_
I0710 07:05:42.694319  9999 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true_x000D_
I0710 07:05:42.694336  9999 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true_x000D_
I0710 07:05:42.694345  9999 authenticator.cpp:318] Authentication success_x000D_
I0710 07:05:42.694392 10001 authenticatee.cpp:299] Authentication success_x000D_
I0710 07:05:42.694452  9999 master.cpp:10412] Successfully authenticated principal 'test-principal' at slave(522)@172.16.10.69:37082_x000D_
I0710 07:05:42.694469 10001 slave.cpp:1450] Successfully authenticated with master master@172.16.10.69:37082_x000D_
I0710 07:05:42.694512 10003 authenticator.cpp:432] Authentication session cleanup for crammd5-authenticatee(1051)@172.16.10.69:37082_x000D_
I0710 07:05:42.694573 10001 slave.cpp:1900] Will retry registration in 15.73611ms if necessary_x000D_
I0710 07:05:42.694640 10001 master.cpp:6900] Received register agent message from slave(522)@172.16.10.69:37082 (ip-172-16-10-69.ec2.internal)_x000D_
I0710 07:05:42.694720 10001 master.cpp:4099] Authorizing agent providing resources 'cpus(reservations: [(STATIC,role)]):1; mem(reservations: [(STATIC,role)]):10' with principal 'test-principal'_x000D_
I0710 07:05:42.694792 10001 master.cpp:3721] Authorizing principal 'test-principal' to reserve resources 'cpus(reservations: [(STATIC,role)]):1; mem(reservations: [(STATIC,role)]):10'_x000D_
I0710 07:05:42.694949 10001 master.cpp:6967] Authorized registration of agent at slave(522)@172.16.10.69:37082 (ip-172-16-10-69.ec2.internal)_x000D_
I0710 07:05:42.694988 10001 master.cpp:7082] Registering agent at slave(522)@172.16.10.69:37082 (ip-172-16-10-69.ec2.internal) with id 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0_x000D_
I0710 07:05:42.695119 10001 registrar.cpp:487] Applied 1 operations in 52120ns; attempting to update the registry_x000D_
I0710 07:05:42.695276 10001 registrar.cpp:544] Successfully updated the registry in 137216ns_x000D_
I0710 07:05:42.695327 10001 master.cpp:7130] Admitted agent 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0 at slave(522)@172.16.10.69:37082 (ip-172-16-10-69.ec2.internal)_x000D_
I0710 07:05:42.695451 10001 master.cpp:7175] Registered agent 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0 at slave(522)@172.16.10.69:37082 (ip-172-16-10-69.ec2.internal) with cpus(reservations: [(STATIC,role)]):1; mem(reservations: [(STATIC,role)]):10_x000D_
I0710 07:05:42.695500 10000 hierarchical.cpp:617] Added agent 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0 (ip-172-16-10-69.ec2.internal) with cpus(reservations: [(STATIC,role)]):1; mem(reservations: [(STATIC,role)]):10 (allocated: {})_x000D_
I0710 07:05:42.695525 10001 slave.cpp:1483] Registered with master master@172.16.10.69:37082; given agent ID 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0_x000D_
I0710 07:05:42.695639 10000 hierarchical.cpp:1508] Performed allocation for 1 agents in 92565ns_x000D_
I0710 07:05:42.695673 10000 task_status_update_manager.cpp:188] Resuming sending task status updates_x000D_
I0710 07:05:42.695745 10001 slave.cpp:1518] Checkpointing SlaveInfo to '/tmp/RoleTest_RolesEndpointContainsConsumedQuota_xr6xQK/meta/slaves/8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0/slave.info'_x000D_
I0710 07:05:42.695773 10000 master.cpp:10195] Sending offers [ 8db40cec-43ef-41a1-89a4-4f7b877d8f13-O0 ] to framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000 (default) at scheduler-c1197694-9c80-4659-ba29-891d2bcb6a32@172.16.10.69:37082_x000D_
I0710 07:05:42.695832 10000 status_update_manager_process.hpp:385] Resuming operation status update manager_x000D_
I0710 07:05:42.696385 10001 slave.cpp:1570] Forwarding agent update {""operations"":{},""resource_providers"":{},""resource_version_uuid"":{""value"":""RXSv7a6tQBuTqO8uJ7NIJw==""},""slave_id"":{""value"":""8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0""},""update_oversubscribed_resources"":false}_x000D_
I0710 07:05:42.696590 10001 master.cpp:8261] Ignoring update on agent 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0 at slave(522)@172.16.10.69:37082 (ip-172-16-10-69.ec2.internal) as it reports no changes_x000D_
I0710 07:05:42.696941 10000 sched.cpp:934] Scheduler::resourceOffers took 1.002341ms_x000D_
I0710 07:05:42.697464 10000 master.cpp:12470] Removing offer 8db40cec-43ef-41a1-89a4-4f7b877d8f13-O0_x000D_
I0710 07:05:42.697743 10000 master.cpp:4636] Processing ACCEPT call for offers: [ 8db40cec-43ef-41a1-89a4-4f7b877d8f13-O0 ] on agent 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0 at slave(522)@172.16.10.69:37082 (ip-172-16-10-69.ec2.internal) for framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000 (default) at scheduler-c1197694-9c80-4659-ba29-891d2bcb6a32@172.16.10.69:37082_x000D_
I0710 07:05:42.697794 10000 master.cpp:3653] Authorizing framework principal 'test-principal' to launch task 26c92e4e-ef13-4984-9b06-b18f880decf3_x000D_
W0710 07:05:42.698462 10003 validation.cpp:1640] Executor 'dummy' for task '26c92e4e-ef13-4984-9b06-b18f880decf3' uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases._x000D_
W0710 07:05:42.698487 10003 validation.cpp:1652] Executor 'dummy' for task '26c92e4e-ef13-4984-9b06-b18f880decf3' uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases._x000D_
I0710 07:05:42.698549 10003 master.cpp:4171] Adding executor 'dummy' with resources {} of framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000 (default) at scheduler-c1197694-9c80-4659-ba29-891d2bcb6a32@172.16.10.69:37082 on agent 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0 at slave(522)@172.16.10.69:37082 (ip-172-16-10-69.ec2.internal)_x000D_
I0710 07:05:42.698590 10003 master.cpp:4197] Adding task 26c92e4e-ef13-4984-9b06-b18f880decf3 with resources cpus(allocated: role)(reservations: [(STATIC,role)]):1; mem(allocated: role)(reservations: [(STATIC,role)]):10 of framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000 (default) at scheduler-c1197694-9c80-4659-ba29-891d2bcb6a32@172.16.10.69:37082 on agent 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0 at slave(522)@172.16.10.69:37082 (ip-172-16-10-69.ec2.internal)_x000D_
I0710 07:05:42.698686 10003 master.cpp:5615] Launching task 26c92e4e-ef13-4984-9b06-b18f880decf3 of framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000 (default) at scheduler-c1197694-9c80-4659-ba29-891d2bcb6a32@172.16.10.69:37082 with resources [{""allocation_info"":{""role"":""role""},""name"":""cpus"",""reservations"":[{""role"":""role"",""type"":""STATIC""}],""scalar"":{""value"":1.0},""type"":""SCALAR""},{""allocation_info"":{""role"":""role""},""name"":""mem"",""reservations"":[{""role"":""role"",""type"":""STATIC""}],""scalar"":{""value"":10.0},""type"":""SCALAR""}] on agent 8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0 at slave(522)@172.16.10.69:37082 (ip-172-16-10-69.ec2.internal) on  new executor_x000D_
I0710 07:05:42.698873 10000 hierarchical.cpp:1432] Allocation paused_x000D_
I0710 07:05:42.698909 10000 hierarchical.cpp:1442] Allocation resumed_x000D_
I0710 07:05:42.698961 10003 slave.cpp:2037] Got assigned task '26c92e4e-ef13-4984-9b06-b18f880decf3' for framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000_x000D_
I0710 07:05:42.699254 10003 slave.cpp:2411] Authorizing task '26c92e4e-ef13-4984-9b06-b18f880decf3' for framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000_x000D_
I0710 07:05:42.699282 10003 slave.cpp:9198] Authorizing framework principal 'test-principal' to launch task 26c92e4e-ef13-4984-9b06-b18f880decf3_x000D_
I0710 07:05:42.699653 10003 slave.cpp:2854] Launching task '26c92e4e-ef13-4984-9b06-b18f880decf3' for framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000_x000D_
I0710 07:05:42.699702 10003 paths.cpp:801] Creating sandbox '/tmp/RoleTest_RolesEndpointContainsConsumedQuota_xr6xQK/slaves/8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0/frameworks/8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000/executors/dummy/runs/8a7d9b0a-ebcf-4f8a-aa7c-1bbcadb0c166' for user 'root'_x000D_
W0710 07:05:42.701002  9995 process.cpp:2877] Attempted to spawn already running process files@172.16.10.69:37082_x000D_
I0710 07:05:42.701611  9995 containerizer.cpp:314] Using isolation { environment_secret, posix/cpu, posix/mem, filesystem/posix, network/cni }_x000D_
I0710 07:05:42.702396 10003 slave.cpp:9708] Launching executor 'dummy' of framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000 with resources [] in work directory '/tmp/RoleTest_RolesEndpointContainsConsumedQuota_xr6xQK/slaves/8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0/frameworks/8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000/executors/dummy/runs/8a7d9b0a-ebcf-4f8a-aa7c-1bbcadb0c166'_x000D_
I0710 07:05:42.702580 10003 slave.cpp:3080] Queued task '26c92e4e-ef13-4984-9b06-b18f880decf3' for executor 'dummy' of framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000_x000D_
I0710 07:05:42.702809 10003 slave.cpp:3528] Launching container 8a7d9b0a-ebcf-4f8a-aa7c-1bbcadb0c166 for executor 'dummy' of framework 8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000_x000D_
I0710 07:05:42.703053 10003 slave.cpp:991] Successfully attached '/tmp/RoleTest_RolesEndpointContainsConsumedQuota_xr6xQK/slaves/8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0/frameworks/8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000/executors/dummy/runs/8a7d9b0a-ebcf-4f8a-aa7c-1bbcadb0c166' to virtual path '/tmp/RoleTest_RolesEndpointContainsConsumedQuota_xr6xQK/slaves/8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0/frameworks/8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000/executors/dummy/runs/latest'_x000D_
I0710 07:05:42.703063 10000 containerizer.cpp:1357] Starting container 8a7d9b0a-ebcf-4f8a-aa7c-1bbcadb0c166_x000D_
I0710 07:05:42.703073 10003 slave.cpp:991] Successfully attached '/tmp/RoleTest_RolesEndpointContainsConsumedQuota_xr6xQK/slaves/8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0/frameworks/8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000/executors/dummy/runs/8a7d9b0a-ebcf-4f8a-aa7c-1bbcadb0c166' to virtual path '/frameworks/8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000/executors/dummy/runs/latest'_x000D_
I0710 07:05:42.703085 10003 slave.cpp:991] Successfully attached '/tmp/RoleTest_RolesEndpointContainsConsumedQuota_xr6xQK/slaves/8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0/frameworks/8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000/executors/dummy/runs/8a7d9b0a-ebcf-4f8a-aa7c-1bbcadb0c166' to virtual path '/tmp/RoleTest_RolesEndpointContainsConsumedQuota_xr6xQK/slaves/8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0/frameworks/8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000/executors/dummy/runs/8a7d9b0a-ebcf-4f8a-aa7c-1bbcadb0c166'_x000D_
I0710 07:05:42.703593 10000 containerizer.cpp:1529] Checkpointed ContainerConfig at '/tmp/RoleTest_RolesEndpointContainsConsumedQuota_1IIJA6/containers/8a7d9b0a-ebcf-4f8a-aa7c-1bbcadb0c166/config'_x000D_
I0710 07:05:42.703614 10000 containerizer.cpp:3277] Transitioning the state of container 8a7d9b0a-ebcf-4f8a-aa7c-1bbcadb0c166 from PROVISIONING to PREPARING_x000D_
I0710 07:05:42.705749  9998 containerizer.cpp:2055] Launching 'mesos-containerizer' with flags '--help=""false"" --launch_info=""{""command"":{""shell"":true,""value"":""sleep 3600""},""environment"":{""variables"":[{""name"":""LIBPROCESS_PORT"",""type"":""VALUE"",""value"":""0""},{""name"":""MESOS_AGENT_ENDPOINT"",""type"":""VALUE"",""value"":""172.16.10.69:37082""},{""name"":""MESOS_CHECKPOINT"",""type"":""VALUE"",""value"":""0""},{""name"":""MESOS_DIRECTORY"",""type"":""VALUE"",""value"":""/tmp/RoleTest_RolesEndpointContainsConsumedQuota_xr6xQK/slaves/8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0/frameworks/8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000/executors/dummy/runs/8a7d9b0a-ebcf-4f8a-aa7c-1bbcadb0c166""},{""name"":""MESOS_EXECUTOR_AUTHENTICATION_TOKEN"",""type"":""VALUE"",""value"":""eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJjaWQiOiI4YTdkOWIwYS1lYmNmLTRmOGEtYWE3Yy0xYmJjYWRiMGMxNjYiLCJlaWQiOiJkdW1teSIsImZpZCI6IjhkYjQwY2VjLTQzZWYtNDFhMS04OWE0LTRmN2I4NzdkOGYxMy0wMDAwIn0.EfKHriG000A1P2MwWmmXnaLmTpTVdnl5-ZER_MFOkzQ""},{""name"":""MESOS_EXECUTOR_ID"",""type"":""VALUE"",""value"":""dummy""},{""name"":""MESOS_EXECUTOR_SHUTDOWN_GRACE_PERIOD"",""type"":""VALUE"",""value"":""5secs""},{""name"":""MESOS_FRAMEWORK_ID"",""type"":""VALUE"",""value"":""8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000""},{""name"":""MESOS_HTTP_COMMAND_EXECUTOR"",""type"":""VALUE"",""value"":""0""},{""name"":""MESOS_SLAVE_ID"",""type"":""VALUE"",""value"":""8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0""},{""name"":""MESOS_SLAVE_PID"",""type"":""VALUE"",""value"":""slave(522)@172.16.10.69:37082""},{""name"":""MESOS_SANDBOX"",""type"":""VALUE"",""value"":""/tmp/RoleTest_RolesEndpointContainsConsumedQuota_xr6xQK/slaves/8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0/frameworks/8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000/executors/dummy/runs/8a7d9b0a-ebcf-4f8a-aa7c-1bbcadb0c166""}]},""task_environment"":{},""user"":""root"",""working_directory"":""/tmp/RoleTest_RolesEndpointContainsConsumedQuota_xr6xQK/slaves/8db40cec-43ef-41a1-89a4-4f7b877d8f13-S0/frameworks/8db40cec-43ef-41a1-89a4-4f7b877d8f13-0000/executors/dummy/runs/8a7d9b0a-ebcf-4f8a-aa7c-1bbcadb0c166""}"" --pipe_read=""22"" --pipe_write=""24"" --runtime_directory=""/tmp/RoleTest_RolesEndpointContainsConsumedQuota_1IIJA6/containers/8a7d9b0a-ebcf-4f8a-aa7c-1bbcadb0c166"" --unshare_namespace_mnt=""false""'_x000D_
I0710 07:05:42.706059 10002 linux_launcher.cpp:492] Launching container 8a7d9b0a-ebcf-4f8a-aa7c-1bbcadb0c166 and cloning with namespaces _x000D_
I0710 07:05:42.706065  9995 linux_launcher.cpp:144] Using /cgroup/freezer as the freezer hierarchy for the Linux launcher_x000D_
I0710 07:05:42.710623  9995 provisioner.cpp:298] Using default backend 'copy'_x000D_
I0710 07:05:42.712615  9995 cluster.cpp:510] Creating default 'local' authorizer_x000D_
I0710 07:05:42.714180 10001 slave.cpp:265] Mesos agent started on (523)@172.16.10.69:37082_x000D_
I0710 07:05:42.714211 10001 slave.cpp:266] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/1d0m6o/9m8Ujd/store/appc"" --authenticate_http_executors=""true"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""true"" --authenticatee=""crammd5"" --authentication_backoff_factor=""1secs"" --authentication_timeout_max=""1mins"" --authentication_timeout_min=""5secs"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_destroy_timeout=""1mins"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/1d0m6o/9m8Ujd/credential"" --default_role=""*"" --disallow_sharing_agent_ipc_namespace=""false"" --disallow_sharing_agent_pid_namespace=""false"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_ignore_runtime=""false"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/1d0m6o/9m8Ujd/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_reregistration_timeout=""2secs"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/1d0m6o/9m8Ujd/fetch"" --fetcher_cache_size=""2GB"" --fetcher_stall_timeout=""1mins"" --frameworks_home=""/tmp/1d0m6o/9m8Ujd/frameworks"" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --gc_non_executor_container_sandboxes=""false"" --help=""false"" --hostname_lookup=""true"" --http_command_executor=""false"" --http_credentials=""/tmp/1d0m6o/9m8Ujd/http_credentials"" --http_heartbeat_interval=""30secs"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --jwt_secret_key=""/tmp/1d0m6o/9m8Ujd/jwt_secret_key"" --launcher=""linux"" --launcher_dir=""/home/centos/workspace/mesos/Mesos_CI-build/FLAG/SSL/label/mesos-ec2-centos-6/mesos/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_executors_per_framework=""150"" --memory_profiling=""false"" --network_cni_metrics=""true"" --network_cni_root_dir_persist=""false"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --port=""5051"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --reconfiguration_policy=""equal"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:10;mem:100;disk:0;ports:[]"" --revocable_cpu_low_priority=""true"" --runtime_dir=""/tmp/RoleTest_RolesEndpointContainsConsumedQuota_vKyXvR"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/RoleTest_RolesEndpointContainsConsumedQuota_7tv0Hv"" --zk_session_timeout=""10secs""_x000D_
I0710 07:05:42.714774 10001 credentials.hpp:86] Loading credential for authentication from '/tmp/1d0m6o/9m8Ujd/credential'_x000D_
I071",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.0
Task,Expose quota consumption in /roles endpoint.,"As part of exposing quota consumption to users and displaying quota consumption in the ui, we will need to add it to the /roles endpoint (which is currently what the ui uses for the roles table).",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.0
Bug,Make PushGauges support floating point stats.,"Currently, PushGauges are modeled against counters. Thus it does not support floating point stats. This prevents many existing PullGauges to use it. We need to add support for floating point stat.",1.0,0,0.5,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.3974358974358974,0.4,0.4,0.0
Task,Update Docker executor to allow kill policy overrides,"In order for the agent to successfully override the task kill policy of Docker tasks when the agent is being drained, the Docker executor must be able to receive kill policy overrides and must be updated to honor them. Since the Docker executor runs using the executor driver, this is currently not possible. We could, for example, update the executor driver interface, or move the Docker executor off of the executor driver.",3.0,0,0.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Bug,Slow memory growth in master due to deferred deletion of offer filters and timers.,"The allocator does not keep a handle to the offer filter timer, which means it cannot remove the timer overhead (in this case memory) when removing the offer filter earlier (e.g. due to revive):_x000D_
_x000D_
https://github.com/apache/mesos/blob/1.8.0/src/master/allocator/mesos/hierarchical.cpp#L1338-L1352_x000D_
_x000D_
In addition, the offer filter is allocated on the heap but not deleted until the timer fires (which might take forever!):_x000D_
_x000D_
https://github.com/apache/mesos/blob/1.8.0/src/master/allocator/mesos/hierarchical.cpp#L1321_x000D_
https://github.com/apache/mesos/blob/1.8.0/src/master/allocator/mesos/hierarchical.cpp#L1408-L1413_x000D_
https://github.com/apache/mesos/blob/1.8.0/src/master/allocator/mesos/hierarchical.cpp#L2249_x000D_
_x000D_
We'll need to try to backport this to all active release branches.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.0
Bug,`QuotaRoleAllocateNonQuotaResource` is failing.,"{noformat}_x000D_
[ RUN      ] HierarchicalAllocatorTest.QuotaRoleAllocateNonQuotaResource_x000D_
../../src/tests/hierarchical_allocator_tests.cpp:4094: Failure_x000D_
Value of: allocations.get().isPending()_x000D_
  Actual: false_x000D_
Expected: true_x000D_
[  FAILED  ] HierarchicalAllocatorTest.QuotaRoleAllocateNonQuotaResource (12 ms)_x000D_
{noformat}_x000D_
_x000D_
The test is failing because:_x000D_
_x000D_
After agent3 is added, it misses a settle call where the allocation of agent3 is racy._x000D_
In addition, after https://github.com/apache/mesos/commit/7df8cc6b79e294c075de09f1de4b31a2b88423c8_x000D_
we now offer nonquota resources on an agent (even that means ""chopping"") on top of role's satisfied guarantees, the test needs to be updated in accordance with the behavior change.",2.0,0,0.0,0.0,0.6666666666666666,1.0,0.5714285714285714,0.0,0.0,0.5438596491228069,0.5357142857142857,0.3974358974358974,0.4,0.4,0.0
Task,Agent kills all tasks when draining,"The agent's {{DrainSlaveMessage}} handler should kill all tasks when draining is initiated, specifying a kill policy with a grace period equal to the minimum of the task's grace period and the min_grace_period specified in the drain message.",2.0,0,0.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Task,Implement minimal agent-side draining handler,"To unblock other work that can be done in parallel, this ticket captures the implementation of a handler for the {{DrainSlaveMessage}} in the agent which will:_x000D_
* Checkpoint the {{DrainInfo}}_x000D_
* Populate a new data member in the agent with the {{DrainInfo}}_x000D_
",2.0,0,0.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Task,Add minimum master capability for draining and deactivation states,"Since we are adding new fields to the registry to represent agent draining/deactivation, we cannot allow downgrades of masters while such features are in use.  _x000D_
_x000D_
A new minimum capability should be added to the registry with the appropriate documentation:_x000D_
https://github.com/apache/mesos/blob/663bfa68b6ab68f4c28ed6a01ac42ac2ad23ac07/src/master/master.cpp#L1681-L1688_x000D_
http://mesos.apache.org/documentation/latest/downgrades/",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.0
Task,Add draining state information to master state endpoints,"The response for {{GET_STATE}} and {{GET_AGENTS}} should include the new fields indicating deactivation or draining states:_x000D_
{code}_x000D_
message Response {_x000D_
  . . ._x000D_
_x000D_
  message GetAgents {_x000D_
    message Agent {_x000D_
      . . ._x000D_
_x000D_
      optional bool deactivated = 12;_x000D_
      optional DrainInfo drain_info = 13;_x000D_
_x000D_
      . . ._x000D_
    }_x000D_
  }_x000D_
  . . ._x000D_
}_x000D_
{code}_x000D_
_x000D_
The {{/state}} and {{/state-summary}} handlers should also expose this information.",3.0,0,0.5,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.0
Improvement,Track role consumed quota for all roles in the allocator.,We are already tracking role consumed quota for roles with non-default quota in the allocator. We should expand that to track all roles' consumptions which will then be exposed through metrics later.,3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3974358974358974,0.4,0.4,0.0
Improvement,Remove quota role sorter in the allocator.,"Remove the dedicated quota role sorter in favor of using the same sorting between satisfying guarantees and bursting above guarantees up to limits. This is tech debt from when a ""quota role"" was considered different from a ""non-quota"" role. However, they are the same, one just has a default quota._x000D_
_x000D_
The only practical difference between quota role sorter and role sorter now is that quota role sorter ignores the revocable resources both in its total resource pool as well as role allocations. Thus when using DRF, it does not count revocable resources which is arguably the right behavior._x000D_
_x000D_
By removing the quota sorter, we will have all roles sorted together. When using DRF, in the 1st quota guarantee allocation stage, its share calculation will also include revocable resources.",2.0,0,0.0,0.0,0.6666666666666666,1.0,0.5714285714285714,0.0,0.0,0.5438596491228069,0.5357142857142857,0.3974358974358974,0.4,0.4,0.0
Bug,Race between two REMOVE_QUOTA calls crashes the master.,"The existence of the quota in the master is validated here:_x000D_
[https://github.com/apache/mesos/blob/a9a2acabd03181865055b77cf81e7bb310b236d6/src/master/quota_handler.cpp#L700]_x000D_
_x000D_
Then the quota is removed from master in a deferred method call:_x000D_
[https://github.com/apache/mesos/blob/a9a2acabd03181865055b77cf81e7bb310b236d6/src/master/quota_handler.cpp#L744]_x000D_
_x000D_
And then removed from allocator in another deferred call:_x000D_
[https://github.com/apache/mesos/blob/a9a2acabd03181865055b77cf81e7bb310b236d6/src/master/quota_handler.cpp#L753]_x000D_
_x000D_
So, there is a race between two simultaneous REMOVE_QUOTA calls._x000D_
_x000D_
We observe this race on a heavily loaded cluster. Currently we suspect that the client retries the call (due to the call being not processed for a long time),  and this triggers the race.",2.0,"1.5.3,1.6.2,1.7.2,1.8.0,1.9.0",0.5,0.0,0.6666666666666666,1.0,0.5714285714285714,0.0,0.0,0.5438596491228069,0.5357142857142857,0.14102564102564102,0.08484848484848485,0.08484848484848485,0.9817798165137613
Improvement,Centos 6 RPM build is broken on Apache CI,"The centos 6 rpm build on the Apache CI on `build.apache.org` has been broken since April 16, as it fails on the following step:_x000D_
_x000D_
{noformat}_x000D_
RUN  rpm -Uvh --replacepkgs \_x000D_
      http://yum.postgresql.org/9.5/redhat/rhel-6-x86_64/pgdg-centos95-9.5-2.noarch.rpm_x000D_
{noformat}_x000D_
_x000D_
The URL returns a 404 response because the package was removed from the upstream fileserver.",1.0,0,0.0,0.0,0.3333333333333333,0.5,0.14285714285714285,0.0,0.2,0.5087719298245614,0.4357142857142857,0.16666666666666666,0.1818181818181818,0.17575757575757575,0.0
Bug,Random sorter fails to clear removed clients.,"In `RandomSorter::SortInfo::updateRelativeWeights()`, we do not clear the stale `clients` and `weights` vector if the state is dirty. This would result in an allocator crash due to including removed framework and roles in a sorted result e.g. check failure would occur here (https://github.com/apache/mesos/blob/62f0b6973b2268a3305fd631a914433a933c6757/src/master/allocator/mesos/hierarchical.cpp#L1849).",1.0,0,0.0,0.0,0.6666666666666666,1.0,0.5714285714285714,0.0,0.0,0.5438596491228069,0.5357142857142857,0.3974358974358974,0.4,0.4,0.0
Improvement,Randomized the agents in the second allocation stage.,"Agents are currently randomized before the 1st_x000D_
allocation stage (the quota allocation stage) but not in_x000D_
the 2nd stage. One perceived issue is that resources on_x000D_
the agents in the front of the queue are likely to be mostly_x000D_
allocated in the 1st stage, leaving only slices of resources_x000D_
available for the second stage. Thus we may see consistently_x000D_
low quality offers for role/frameworks that get allocated first_x000D_
in the 2nd stage._x000D_
_x000D_
Consider randomizing the agents in the second allocation stage.",1.0,0,0.0,0.0,0.6666666666666666,1.0,0.5714285714285714,0.0,0.0,0.5438596491228069,0.5357142857142857,0.3974358974358974,0.4,0.4,0.0
Bug,/__processes__ endpoint can hang.,"A user reported that the {{/\_\_processes\_\_}} endpoint occasionally hangs._x000D_
_x000D_
Stack traces provided by [~alexr] revealed that all the threads appeared to be idle waiting for events. After investigating the code, the issue was found to be possible when a process gets terminated after the {{/\_\_processes\_\_}} route handler dispatches to it, thus dropping the dispatch and abandoning the future.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.0
Improvement,Log required quota headroom and available quota headroom in the allocator.,This would ease the debugging of allocation issues.,2.0,0,0.0,0.0,0.6666666666666666,1.0,0.5714285714285714,0.0,0.0,0.5438596491228069,0.5357142857142857,0.3974358974358974,0.4,0.4,0.0
Improvement,Take ports out of the GET_ROLES endpoints.,It does not make sense to combine ports across agents.,3.0,0,0.5,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.3974358974358974,0.4,0.4,0.0
Bug,Agent V1 GET_STATE response may report a complete executor's tasks as non-terminal after a graceful agent shutdown,"When the following steps occur:_x000D_
1) A graceful shutdown is initiated on the agent (i.e. SIGUSR1 or /master/machine/down)._x000D_
2) The executor is sent a kill, and the agent counts down on {{executor_shutdown_grace_period}}._x000D_
3) The executor exits, before all terminal status updates reach the agent. This is more likely if {{executor_shutdown_grace_period}} passes._x000D_
_x000D_
This results in a completed executor, with non-terminal tasks (according to status updates)._x000D_
_x000D_
When the agent starts back up, the completed executor will be recovered and shows up correctly  as a completed executor in {{/state}}.  However, if you fetch the V1 {{GET_STATE}} result, there will be an entry in {{launched_tasks}} even though nothing is running._x000D_
{code}_x000D_
get_tasks {_x000D_
  launched_tasks {_x000D_
    name: ""test-task""_x000D_
    task_id {_x000D_
      value: ""dff5a155-47f1-4a71-9b92-30ca059ab456""_x000D_
    }_x000D_
    framework_id {_x000D_
      value: ""4b34a3aa-f651-44a9-9b72-58edeede94ef-0000""_x000D_
    }_x000D_
    executor_id {_x000D_
      value: ""default""_x000D_
    }_x000D_
    agent_id {_x000D_
      value: ""4b34a3aa-f651-44a9-9b72-58edeede94ef-S0""_x000D_
    }_x000D_
    state: TASK_RUNNING_x000D_
    resources { ... }_x000D_
    resources { ... }_x000D_
    resources { ... }_x000D_
    resources { ... }_x000D_
    statuses {_x000D_
      task_id {_x000D_
        value: ""dff5a155-47f1-4a71-9b92-30ca059ab456""_x000D_
      }_x000D_
      state: TASK_RUNNING_x000D_
      agent_id {_x000D_
        value: ""4b34a3aa-f651-44a9-9b72-58edeede94ef-S0""_x000D_
      }_x000D_
      timestamp: 1556674758.2175469_x000D_
      executor_id {_x000D_
        value: ""default""_x000D_
      }_x000D_
      source: SOURCE_EXECUTOR_x000D_
      uuid: ""xPmn\234\236F&\235\\d\364\326\323\222\224""_x000D_
      container_status { ... }_x000D_
    }_x000D_
  }_x000D_
}_x000D_
get_executors {_x000D_
  completed_executors {_x000D_
    executor_info {_x000D_
      executor_id {_x000D_
        value: ""default""_x000D_
      }_x000D_
      command {_x000D_
        value: """"_x000D_
      }_x000D_
      framework_id {_x000D_
        value: ""4b34a3aa-f651-44a9-9b72-58edeede94ef-0000""_x000D_
      }_x000D_
    }_x000D_
  }_x000D_
}_x000D_
get_frameworks {_x000D_
  completed_frameworks {_x000D_
    framework_info {_x000D_
      user: ""user""_x000D_
      name: ""default""_x000D_
      id {_x000D_
        value: ""4b34a3aa-f651-44a9-9b72-58edeede94ef-0000""_x000D_
      }_x000D_
      checkpoint: true_x000D_
      hostname: ""localhost""_x000D_
      principal: ""test-principal""_x000D_
      capabilities {_x000D_
        type: MULTI_ROLE_x000D_
      }_x000D_
      capabilities {_x000D_
        type: RESERVATION_REFINEMENT_x000D_
      }_x000D_
      roles: ""*""_x000D_
    }_x000D_
  }_x000D_
}_x000D_
{code}_x000D_
_x000D_
This happens because we combine executors and completed executors when constructing the response.  The terminal task(s) with non-terminal updates appear under completed executors._x000D_
https://github.com/apache/mesos/blob/89c3dd95a421e14044bc91ceb1998ff4ae3883b4/src/slave/http.cpp#L1734-L1756",2.0,"1.6.0,1.7.0,1.8.0",0.0,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.981651376146789
Bug,Invalid protobuf unions in ExecutorInfo::ContainerInfo will prevent agents from reregistering with 1.8+ masters,"As part of MESOS-6874, the master now validates protobuf unions passed as part of an {{ExecutorInfo::ContainerInfo}}.  This prevents a task from specifying, for example, a {{ContainerInfo::MESOS}}, but filling out the {{docker}} field (which is then ignored by the agent)._x000D_
_x000D_
However, if a task was already launched with an invalid protobuf union, the same validation will happen when the agent tries to reregister with the master.  In this case, if the master is upgraded to validate protobuf unions, the agent reregistration will be rejected._x000D_
_x000D_
{code}_x000D_
master.cpp:7201] Dropping re-registration of agent at slave(1)@172.31.47.126:5051 because it sent an invalid re-registration: Protobuf union `mesos.ContainerInfo` with `Type == MESOS` should not have the field `docker` set._x000D_
{code}_x000D_
_x000D_
This bug was found when upgrading a 1.7.x test cluster to 1.8.0.  When MESOS-6874 was committed, I had assumed the invalid protobufs would be rare.  However, on the test cluster, 13/17 agents had at least one invalid ContainerInfo when reregistering.",3.0,1.8.0,0.5,0.0,0.3333333333333333,0.5,0.14285714285714285,0.0,0.2,0.5087719298245614,0.4357142857142857,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.9908256880733946
Bug,Random sorter generates non-uniform result for hierarchical roles.,"In the presence of hierarchical roles, the random sorter shuffles roles level by level and then pick the active leave nodes using DFS:_x000D_
_x000D_
https://github.com/apache/mesos/blob/7e7cd8de1121589225049ea33df0624b2a1bd754/src/master/allocator/sorter/random/sorter.cpp#L513-L529_x000D_
_x000D_
This makes the result less random because subtrees are always picked together. For example, random sorting result such as `[a/., c/d, a/b, …]` is impossible._x000D_
 ",3.0,0,0.0,0.0,0.6666666666666666,1.0,0.5714285714285714,0.0,0.0,0.5438596491228069,0.5357142857142857,0.3974358974358974,0.4,0.4,0.0
Bug,Heartbeat calls from executor to agent are reported as errors,"These HEARTBEAT calls and events were added in MESOS-7564. _x000D_
_x000D_
HEARTBEAT calls are generated by the executor library, which does not have access to the executor's Framework/Executor IDs.  The library therefore uses some dummy values instead, because HEARTBEAT calls do not really require required fields.  When the agent receives these dummy values, it returns a 400 Bad Request.  It should return 202 Accepted instead.",1.0,"1.7.2,1.8.0",0.0,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.9863302752293577
Improvement,Flatten the weighted shuffling in the random sorter.,"Due to the presence of hierarchical weights, the random sorter currently shuffles level-by-level. We should be able to shuffle all the active leaves only once by calculating (and caching) active leaves' relative weights. This should improve the performance in the presence of hierarchical roles. ",1.0,0,0.0,0.0,0.6666666666666666,1.0,0.5714285714285714,0.0,0.0,0.5438596491228069,0.5357142857142857,0.3974358974358974,0.4,0.4,0.0
Improvement,Apply in place permutation to avoid copying when doing random shuffling.,"This should improve the performance of random sorter. Se [~bmahler]'s comment here:_x000D_
_x000D_
https://github.com/apache/mesos/blob/master/src/master/allocator/sorter/random/utils.hpp#L69-L74",3.0,0,0.0,0.0,0.6666666666666666,1.0,0.5714285714285714,0.0,0.0,0.5438596491228069,0.5357142857142857,0.3974358974358974,0.4,0.4,0.0
Task,Add tests to ensure random sorter performs correct weighted sorting.,"We added tests for the weighted shuffle algorithm, but didn't test that the RandomSorter's sort() function behaves correctly._x000D_
_x000D_
We should also test that hierarchical weights in the random sorter behave correctly.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.0
Bug,Release RPMs are not uploaded to bintray,"While we currently build release RPMs, e.g., [https://builds.apache.org/view/M-R/view/Mesos/job/Packaging/job/CentOS/job/1.7.x/], these artifacts are not uploaded to bintray. Due to that RPM links on the downloads page [http://mesos.apache.org/downloads/] are broken.",3.0,"1.6.2,1.7.2,1.8.0",0.5,0.0,0.3333333333333333,0.5,0.14285714285714285,0.0,0.2,0.5087719298245614,0.4357142857142857,1.0,0.8121212121212121,0.8121212121212121,0.9817737003058103
Bug,Test MasterQuotaTest.AvailableResourcesSingleDisconnectedAgent is flaky,"The test {{MasterQuotaTest.AvailableResourcesSingleDisconnectedAgent}} is flaky, especially under additional system load.",1.0,1.8.0,0.5,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,1.0,0.8121212121212121,0.8121212121212121,0.9908256880733946
Bug,Quota may be under allocated for disk resources.,"Due to a bug in the resources chopping logic:_x000D_
https://github.com/apache/mesos/blob/1915150c6a83cd95197e25a68a6adf9b3ef5fb11/src/master/allocator/mesos/hierarchical.cpp#L1665-L1668_x000D_
_x000D_
{noformat}_x000D_
      if (Resources::shrink(&resource, limitScalar.get())) {_x000D_
        targetScalarQuantites[resource.name()] -= limitScalar.get(); // bug_x000D_
        result += std::move(resource);_x000D_
      }_x000D_
{noformat}_x000D_
_x000D_
When chopping different resources with the same name (e.g. vanilla disk and mount disk), we only include one of the resources. For example, if a role has a quota of 100disk, and an agent has 50 vanilla disk and 50 mount disk, the offer will only contain 50 disk (either vanilla or the mount type). The correct behavior should be that both disks should be offered._x000D_
_x000D_
Since today, only disk resources might have the same name but different meta-data (for unreserved/nonrevocable/nonshared resources -- we only chop this), this bug should only affect disk resources today._x000D_
_x000D_
The correct code should be:_x000D_
_x000D_
{noformat}_x000D_
      if (Resources::shrink(&resource, limitScalar.get())) {_x000D_
        targetScalarQuantites[resource.name()] -= resource.scalar(); // Only subtract the shrunk resource scalar_x000D_
        result += std::move(resource);_x000D_
      }_x000D_
{noformat}_x000D_
",2.0,"1.6.2,1.7.2",0.0,0.0,0.6666666666666666,1.0,0.5714285714285714,0.0,0.0,0.5438596491228069,0.5357142857142857,0.3974358974358974,0.4,0.4,0.9772477064220182
Bug,Quota headroom calculation is off when subroles are involved.,"Quota ""availableHeadroom"" calculation:_x000D_
_x000D_
https://github.com/apache/mesos/blob/6276f7e73b0dbe7df49a7315cd1b83340d66f4ea/src/master/allocator/mesos/hierarchical.cpp#L1751-L1754_x000D_
_x000D_
is off when subroles are involved._x000D_
_x000D_
Specifically, in the formula _x000D_
{noformat}_x000D_
available headroom = total resources - allocated resources - (total reservations - allocated reservations) - unallocated revocable resources_x000D_
{noformat}_x000D_
_x000D_
-The ""allocated resources"" part is hierarchical-aware and aggregate that across all roles, thus allocations to subroles will be counted multiple times (in the case of ""a/b"", once for ""a"" and once for ""a/b"").- Looks like due to the presence of `INTERNAL` node, `roleSorter->allocationScalarQuantities(role)` is *not* hierarchical. Thus this is not an issue._x000D_
_x000D_
(If role `a/b` consumes 1cpu and `a` consumes 1cpu, if we query `roleSorter->allocationScalarQuantities(""a"");` It will return 1cpu, which is correct. In the sorter, there are four nodes, root, `a` (internal, 1cpu), `a/.` (leaf, 1cpu), `a/b` (leaf, 1cpu). Query `a` will return `a/.`)_x000D_
_x000D_
The ""total reservations""  is correct, since today it is ""flat"" (reservations made to ""a/b"" are not counted to ""a""). Thus all reservations are only counted once -- which is the correct semantic here. However, once we fix MESOS-9688 (which likely requires reservation tracking to be hierarchical-aware), we need to ensure that the accounting is still correct._x000D_
_x000D_
-The ""allocated reservations"" is hierarchical-aware, thus overlap accounting would occur.- Similar to the `""allocated resources""` above, this is also not an issue at the moment._x000D_
_x000D_
Basically, when calculating the available headroom, we need to ensure ""single-counting"". Ideally, we only need to look at the root's consumptions.",3.0,0,0.5,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.3974358974358974,0.4,0.4,0.0
Bug,Quota is not enforced properly when subroles have reservations.,"Note: the discussion here concerns quota enforcement for top-level role, setting quota on sublevel role is not supported._x000D_
_x000D_
If a subrole directly makes a reservation, the accounting of `roleConsumedQuota` will be off:_x000D_
_x000D_
https://github.com/apache/mesos/blob/master/src/master/allocator/mesos/hierarchical.cpp#L1703-L1705_x000D_
_x000D_
Specifically, in this formula:_x000D_
`Consumed Quota = reservations + allocation - allocated reservations`_x000D_
_x000D_
The `reservations` part does not account subrole's reservation to its ancestors. If a reservation is made directly for role ""a/b"", its reservation is accounted only for ""a/b"" but not for ""a"". Similarly, if a top role ( ""a"") reservation is refined to a subrole (""a/b""), the current code first subtracts the reservation from ""a"" and then track that under ""a/b""._x000D_
_x000D_
We should make it hierarchical-aware._x000D_
_x000D_
The ""allocation"" and ""allocated reservations"" are both tracked in the sorter where the hierarchical relationship is considered -- allocations are added hierarchically.",3.0,0,0.5,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.3974358974358974,0.4,0.4,0.0
Task,RPM packages should be built with launcher sealing,"We should consider enabling launcher sealing in the Mesos RPM packages. Since this feature is built conditionally, it is hard to write e.g., module code against Mesos packages since required functions might be missing (e.g., [https://github.com/dcos/dcos-mesos-modules/commit/8ce70e6cc789054831daa3058647e326b2b11bc9] cannot be linked against the default RPM package anymore). The RPM's target platform centos7 should include a recent enough kernel for this.",3.0,1.8.0,0.5,0.0,0.3333333333333333,0.5,0.14285714285714285,0.0,0.2,0.5087719298245614,0.4357142857142857,1.0,0.8121212121212121,0.8121212121212121,0.9908256880733946
Improvement,Deprecate v0 quota calls.,"Once we introduce the new quota APIs in MESOS-8068, we should deprecate the `/quota` endpoint. We should mark this as deprecated and hide it in our documentation.",1.0,0,0.5,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.3974358974358974,0.4,0.4,0.0
Improvement,Add authorization support for the new `GET_QUOTA` call.,"The new `GET_QUOTA` call will return QUOTA_CONFIGS:_x000D_
_x000D_
// Used in GET_QUOTA and returned by GET /quota_x000D_
//_x000D_
// Overall cluster quota status, including all roles, their quota configurations and current state (e.g. consumed and effective limits)_x000D_
message QuotaStatus {_x000D_
       repeated QuotaInfo infos [deprecated = true];_x000D_
       repeated QuotaConfig configs; _x000D_
}_x000D_
_x000D_
Currently, the GET_QUOTA authorizable action set both value_x000D_
and quota_info fields. The value field is set due to_x000D_
backward compatibility for the GET_QUOTA_WITH_ROLE action._x000D_
_x000D_
We should make the GET_QUOTA action only set the value_x000D_
field with the role name. Since the quota.QuotaInfo field_x000D_
is being deprecated, it should not be set (the local authorizer_x000D_
only looks at the value field, it is also probably the case_x000D_
for any external authorizer modules).",2.0,0,0.0,0.0,0.6666666666666666,1.0,0.5714285714285714,0.0,0.0,0.5438596491228069,0.5357142857142857,0.3974358974358974,0.4,0.4,0.0
Improvement,Add authorization support for `UPDATE_QUOTA` call.,"For the new `UPDATE_QUOTA` call, we need to add the corresponding authorization support. Unfortunately, there is already an action named `update_quotas`. We can use `update_quota_configs` instead.",3.0,0,0.0,0.0,0.6666666666666666,1.0,0.5714285714285714,0.0,0.0,0.5438596491228069,0.5357142857142857,0.3974358974358974,0.4,0.4,0.0
Bug,OperationReconciliationTest.AgentPendingOperationAfterMasterFailover is flaky again (3x) due to orphan operations,"This test fails consistently when run while the system is stressed:_x000D_
{code}_x000D_
[ RUN      ] ContentType/OperationReconciliationTest.AgentPendingOperationAfterMasterFailover/0_x000D_
F0305 08:10:07.670622  3982 hierarchical.cpp:1259] Check failed: slave.getAllocated().contains(resources) {} does not contain disk(allocated: default-role)[RAW(,,profile)]:200_x000D_
*** Check failure stack trace: ***_x000D_
    @     0x7f1120b0ce5e  google::LogMessage::Fail()_x000D_
    @     0x7f1120b0cdbb  google::LogMessage::SendToLog()_x000D_
    @     0x7f1120b0c7b5  google::LogMessage::Flush()_x000D_
    @     0x7f1120b0f578  google::LogMessageFatal::~LogMessageFatal()_x000D_
    @     0x7f111e536f2a  mesos::internal::master::allocator::internal::HierarchicalAllocatorProcess::recoverResources()_x000D_
    @     0x5580c2651c26  _ZZN7process8dispatchIN5mesos8internal6master9allocator21MesosAllocatorProcessERKNS1_11FrameworkIDERKNS1_7SlaveIDERKNS1_9ResourcesERK6OptionINS1_7FiltersEES8_SB_SE_SJ_EEvRKNS_3PIDIT_EEMSL_FvT0_T1_T2_T3_EOT4_OT5_OT6_OT7_ENKUlOS6_OS9_OSC_OSH_PNS_11ProcessBaseEE_clES13_S14_S15_S16_S18__x000D_
    @     0x5580c26c7e02  _ZN5cpp176invokeIZN7process8dispatchIN5mesos8internal6master9allocator21MesosAllocatorProcessERKNS3_11FrameworkIDERKNS3_7SlaveIDERKNS3_9ResourcesERK6OptionINS3_7FiltersEESA_SD_SG_SL_EEvRKNS1_3PIDIT_EEMSN_FvT0_T1_T2_T3_EOT4_OT5_OT6_OT7_EUlOS8_OSB_OSE_OSJ_PNS1_11ProcessBaseEE_JS8_SB_SE_SJ_S1A_EEEDTclcl7forwardISN_Efp_Espcl7forwardIT0_Efp0_EEEOSN_DpOS1C__x000D_
    @     0x5580c26c5b1e  _ZN6lambda8internal7PartialIZN7process8dispatchIN5mesos8internal6master9allocator21MesosAllocatorProcessERKNS4_11FrameworkIDERKNS4_7SlaveIDERKNS4_9ResourcesERK6OptionINS4_7FiltersEESB_SE_SH_SM_EEvRKNS2_3PIDIT_EEMSO_FvT0_T1_T2_T3_EOT4_OT5_OT6_OT7_EUlOS9_OSC_OSF_OSK_PNS2_11ProcessBaseEE_JS9_SC_SF_SK_St12_PlaceholderILi1EEEE13invoke_expandIS1C_St5tupleIJS9_SC_SF_SK_S1E_EES1H_IJOS1B_EEJLm0ELm1ELm2ELm3ELm4EEEEDTcl6invokecl7forwardISO_Efp_Espcl6expandcl3getIXT2_EEcl7forwardISS_Efp0_EEcl7forwardIST_Efp2_EEEEOSO_OSS_N5cpp1416integer_sequenceImJXspT2_EEEEOST__x000D_
    @     0x5580c26c47ac  _ZNO6lambda8internal7PartialIZN7process8dispatchIN5mesos8internal6master9allocator21MesosAllocatorProcessERKNS4_11FrameworkIDERKNS4_7SlaveIDERKNS4_9ResourcesERK6OptionINS4_7FiltersEESB_SE_SH_SM_EEvRKNS2_3PIDIT_EEMSO_FvT0_T1_T2_T3_EOT4_OT5_OT6_OT7_EUlOS9_OSC_OSF_OSK_PNS2_11ProcessBaseEE_JS9_SC_SF_SK_St12_PlaceholderILi1EEEEclIJS1B_EEEDTcl13invoke_expandcl4movedtdefpT1fEcl4movedtdefpT10bound_argsEcvN5cpp1416integer_sequenceImJLm0ELm1ELm2ELm3ELm4EEEE_Ecl16forward_as_tuplespcl7forwardIT_Efp_EEEEDpOS1K__x000D_
    @     0x5580c26c3ad7  _ZN5cpp176invokeIN6lambda8internal7PartialIZN7process8dispatchIN5mesos8internal6master9allocator21MesosAllocatorProcessERKNS6_11FrameworkIDERKNS6_7SlaveIDERKNS6_9ResourcesERK6OptionINS6_7FiltersEESD_SG_SJ_SO_EEvRKNS4_3PIDIT_EEMSQ_FvT0_T1_T2_T3_EOT4_OT5_OT6_OT7_EUlOSB_OSE_OSH_OSM_PNS4_11ProcessBaseEE_JSB_SE_SH_SM_St12_PlaceholderILi1EEEEEJS1D_EEEDTclcl7forwardISQ_Efp_Espcl7forwardIT0_Efp0_EEEOSQ_DpOS1I__x000D_
    @     0x5580c26c32ad  _ZN6lambda8internal6InvokeIvEclINS0_7PartialIZN7process8dispatchIN5mesos8internal6master9allocator21MesosAllocatorProcessERKNS7_11FrameworkIDERKNS7_7SlaveIDERKNS7_9ResourcesERK6OptionINS7_7FiltersEESE_SH_SK_SP_EEvRKNS5_3PIDIT_EEMSR_FvT0_T1_T2_T3_EOT4_OT5_OT6_OT7_EUlOSC_OSF_OSI_OSN_PNS5_11ProcessBaseEE_JSC_SF_SI_SN_St12_PlaceholderILi1EEEEEJS1E_EEEvOSR_DpOT0__x000D_
    @     0x5580c26c0a5e  _ZNO6lambda12CallableOnceIFvPN7process11ProcessBaseEEE10CallableFnINS_8internal7PartialIZNS1_8dispatchIN5mesos8internal6master9allocator21MesosAllocatorProcessERKNSA_11FrameworkIDERKNSA_7SlaveIDERKNSA_9ResourcesERK6OptionINSA_7FiltersEESH_SK_SN_SS_EEvRKNS1_3PIDIT_EEMSU_FvT0_T1_T2_T3_EOT4_OT5_OT6_OT7_EUlOSF_OSI_OSL_OSQ_S3_E_JSF_SI_SL_SQ_St12_PlaceholderILi1EEEEEEclEOS3__x000D_
    @     0x7f1120a51c60  _ZNO6lambda12CallableOnceIFvPN7process11ProcessBaseEEEclES3__x000D_
    @     0x7f1120a16a4e  process::ProcessBase::consume()_x000D_
    @     0x7f1120a3d9d8  _ZNO7process13DispatchEvent7consumeEPNS_13EventConsumerE_x000D_
    @     0x5580c2284afa  process::ProcessBase::serve()_x000D_
    @     0x7f1120a138db  process::ProcessManager::resume()_x000D_
    @     0x7f1120a0fc28  _ZZN7process14ProcessManager12init_threadsEvENKUlvE_clEv_x000D_
    @     0x7f1120a375d0  _ZNSt12_Bind_simpleIFZN7process14ProcessManager12init_threadsEvEUlvE_vEE9_M_invokeIJEEEvSt12_Index_tupleIJXspT_EEE_x000D_
    @     0x7f1120a36734  _ZNSt12_Bind_simpleIFZN7process14ProcessManager12init_threadsEvEUlvE_vEEclEv_x000D_
    @     0x7f1120a3569c  _ZNSt6thread11_State_implISt12_Bind_simpleIFZN7process14ProcessManager12init_threadsEvEUlvE_vEEE6_M_runEv_x000D_
    @     0x7f111499276f  (unknown)_x000D_
    @     0x7f111507273a  start_thread_x000D_
    @     0x7f11140f8e7f  __GI___clone_x000D_
{code}",3.0,1.8.0,1.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.16666666666666666,0.1818181818181818,0.6242424242424242,0.9908256880733946
Bug,Mesos Master Crashes with Launch Group when using Port Resources,"Original Issue: [https://lists.apache.org/thread.html/979c8799d128ad0c436b53f2788568212f97ccf324933524f1b4d189@%3Cuser.mesos.apache.org%3E]_x000D_
_x000D_
 When the ports resources is removed, Mesos functions normally (I'm able to launch the task as many times as possible, while it always fails continually)._x000D_
_x000D_
Attached is a snippet of the mesos master log from OFFER to crash._x000D_
_x000D_
",3.0,"1.4.3,1.7.1",0.5,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.01282051282051282,0.0,0.0,0.9680733944954129
Improvement,Display quota consumption in the webui.,"Currently, the Roles table in the webui displays allocation and quota guarantees / limits. However, quota ""consumption"" is different from allocation, in that reserved resources are always considered consumed against the quota._x000D_
_x000D_
This discrepancy has led to confusion from users. One exampled occurred when an agent was added with a large reservation exceeding the memory quota guarantee. The user sees memory chopping in offers, and since the scheduler didn't want to use the reservation, it can't launch its tasks._x000D_
_x000D_
If consumption is shown in the UI, we should include a tool tip that indicates how consumed is calculated so that users know how to interpret it.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.0
Bug,Fetcher vulnerability - escaping from sandbox,"I have noticed that there is a possibility to exploit fetcher and  overwrite any file on the agent host._x000D_
_x000D_
scenario to reproduce:_x000D_
_x000D_
1) prepare a file with any content and name a file like ""../../../etc/test"" and archive it. We can use python and zipfile module to achieve that:_x000D_
{code:java}_x000D_
>>> import zipfile_x000D_
>>> zip = zipfile.ZipFile(""exploit.zip"", ""w"")_x000D_
>>> zip.writestr(""../../../../../../../../../../../../etc/mariusz_was_here.txt"", ""some content"")_x000D_
>>> zip.close()_x000D_
_x000D_
{code}_x000D_
2) prepare a service that will use our artifact (exploit.zip)_x000D_
_x000D_
3) run service_x000D_
_x000D_
at the end in /etc we will get our file. As you can imagine there is a lot possibility how we can use it._x000D_
_x000D_
 _x000D_
_x000D_
 ",3.0,1.7.2,0.5,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.01282051282051282,0.0,0.0,0.981834862385321
Bug,Master check failure when marking agent unreachable.,"{code}_x000D_
Mar 11 10:04:33 research docker[4503]: I0311 10:04:33.815433    13 http.cpp:1185] HTTP POST for /master/api/v1/scheduler from 10.142.0.5:55133_x000D_
Mar 11 10:04:33 research docker[4503]: I0311 10:04:33.815588    13 master.cpp:5467] Processing DECLINE call for offers: [ 5e57f633-a69c-4009-b773-990b4b8984ad-O58323 ] for framework 5e57f633-a69c-4009-b7_x000D_
Mar 11 10:04:33 research docker[4503]: I0311 10:04:33.815693    13 master.cpp:10703] Removing offer 5e57f633-a69c-4009-b773-990b4b8984ad-O58323_x000D_
Mar 11 10:04:35 research docker[4503]: I0311 10:04:35.820142    10 master.cpp:8227] Marking agent 5e57f633-a69c-4009-b773-990b4b8984ad-S49 at slave(1)@10.142.0.10:5051 (tf-mesos-agent-t7c8.c.bitcoin-engi_x000D_
Mar 11 10:04:35 research docker[4503]: I0311 10:04:35.820367    10 registrar.cpp:495] Applied 1 operations in 86528ns; attempting to update the registry_x000D_
Mar 11 10:04:35 research docker[4503]: I0311 10:04:35.820572    10 registrar.cpp:552] Successfully updated the registry in 175872ns_x000D_
Mar 11 10:04:35 research docker[4503]: I0311 10:04:35.820642    11 master.cpp:8275] Marked agent 5e57f633-a69c-4009-b773-990b4b8984ad-S49 at slave(1)@10.142.0.10:5051 (tf-mesos-agent-t7c8.c.bitcoin-engin_x000D_
Mar 11 10:04:35 research docker[4503]: I0311 10:04:35.820957     9 hierarchical.cpp:609] Removed agent 5e57f633-a69c-4009-b773-990b4b8984ad-S49_x000D_
Mar 11 10:04:35 research docker[4503]: F0311 10:04:35.851961    11 master.cpp:10018] Check failed: 'framework' Must be non NULL_x000D_
Mar 11 10:04:35 research docker[4503]: *** Check failure stack trace: ***_x000D_
Mar 11 10:04:36 research docker[4503]: @     0x7f96c6044a7d  google::LogMessage::Fail()_x000D_
Mar 11 10:04:36 research docker[4503]: @     0x7f96c6046830  google::LogMessage::SendToLog()_x000D_
Mar 11 10:04:36 research docker[4503]: @     0x7f96c6044663  google::LogMessage::Flush()_x000D_
Mar 11 10:04:36 research docker[4503]: @     0x7f96c6047259  google::LogMessageFatal::~LogMessageFatal()_x000D_
Mar 11 10:04:36 research docker[4503]: @     0x7f96c5258e14  google::CheckNotNull<>()_x000D_
Mar 11 10:04:36 research docker[4503]: @     0x7f96c521dfc8  mesos::internal::master::Master::__removeSlave()_x000D_
Mar 11 10:04:36 research docker[4503]: @     0x7f96c521f1a2  mesos::internal::master::Master::_markUnreachable()_x000D_
Mar 11 10:04:36 research docker[4503]: @     0x7f96c5f98f11  process::ProcessBase::consume()_x000D_
Mar 11 10:04:36 research docker[4503]: @     0x7f96c5fb2a4a  process::ProcessManager::resume()_x000D_
Mar 11 10:04:36 research docker[4503]: @     0x7f96c5fb65d6  _ZNSt6thread5_ImplISt12_Bind_simpleIFZN7process14ProcessManager12init_threadsEvEUlvE_vEEE6_M_runEv_x000D_
Mar 11 10:04:36 research docker[4503]: @     0x7f96c35d4c80  (unknown)_x000D_
Mar 11 10:04:36 research docker[4503]: @     0x7f96c2de76ba  start_thread_x000D_
Mar 11 10:04:36 research docker[4503]: @     0x7f96c2b1d41d  (unknown)_x000D_
Mar 11 10:04:36 research docker[4503]: *** Aborted at 1520762676 (unix time) try ""date -d @1520762676"" if you are using GNU date ***_x000D_
Mar 11 10:04:36 research docker[4503]: PC: @     0x7f96c2a4d196 (unknown)_x000D_
Mar 11 10:04:36 research docker[4503]: *** SIGSEGV (@0x0) received by PID 1 (TID 0x7f96b986d700) from PID 0; stack trace: ***_x000D_
Mar 11 10:04:36 research docker[4503]: @     0x7f96c2df1390 (unknown)_x000D_
Mar 11 10:04:36 research docker[4503]: @     0x7f96c2a4d196 (unknown)_x000D_
Mar 11 10:04:36 research docker[4503]: @     0x7f96c604ce2c google::DumpStackTraceAndExit()_x000D_
Mar 11 10:04:36 research docker[4503]: @     0x7f96c6044a7d google::LogMessage::Fail()_x000D_
Mar 11 10:04:36 research docker[4503]: @     0x7f96c6046830 google::LogMessage::SendToLog()_x000D_
Mar 11 10:04:36 research docker[4503]: @     0x7f96c6044663 google::LogMessage::Flush()_x000D_
Mar 11 10:04:36 research docker[4503]: @     0x7f96c6047259 google::LogMessageFatal::~LogMessageFatal()_x000D_
Mar 11 10:04:36 research docker[4503]: @     0x7f96c5258e14 google::CheckNotNull<>()_x000D_
Mar 11 10:04:36 research docker[4503]: @     0x7f96c521dfc8 mesos::internal::master::Master::__removeSlave()_x000D_
Mar 11 10:04:36 research docker[4503]: @     0x7f96c521f1a2 mesos::internal::master::Master::_markUnreachable()_x000D_
Mar 11 10:04:36 research docker[4503]: @     0x7f96c5f98f11 process::ProcessBase::consume()_x000D_
Mar 11 10:04:36 research docker[4503]: @     0x7f96c5fb2a4a process::ProcessManager::resume()_x000D_
Mar 11 10:04:36 research docker[4503]: @     0x7f96c5fb65d6 _ZNSt6thread5_ImplISt12_Bind_simpleIFZN7process14ProcessManager12init_threadsEvEUlvE_vEEE6_M_runEv_x000D_
Mar 11 10:04:36 research docker[4503]: @     0x7f96c35d4c80 (unknown)_x000D_
Mar 11 10:04:36 research docker[4503]: @     0x7f96c2de76ba start_thread_x000D_
Mar 11 10:04:36 research docker[4503]: @     0x7f96c2b1d41d (unknown)_x000D_
Mar 11 10:04:38 research systemd[1]: mesos-master2.service: main process exited, code=exited, status=139/n/a_x000D_
Mar 11 10:04:38 research docker[18886]: mesos-master_x000D_
Mar 11 10:04:38 research systemd[1]: Unit mesos-master2.service entered failed state._x000D_
{code}_x000D_
_x000D_
Additional case:_x000D_
_x000D_
{noformat}_x000D_
 I0715 02:56:40.071446    13 master.cpp:1295] Agent 9d8dd16c-13f4-4f15-bac8-e5138b2862ee-S0 at slave(1)@10.0.138.150:5051 (10.0.138.150) disconnected_x000D_
 I0715 02:56:40.071503    13 master.cpp:3333] Disconnecting agent 9d8dd16c-13f4-4f15-bac8-e5138b2862ee-S0 at slave(1)@10.0.138.150:5051 (10.0.138.150)_x000D_
 I0715 02:56:40.071527    13 master.cpp:3352] Deactivating agent 9d8dd16c-13f4-4f15-bac8-e5138b2862ee-S0 at slave(1)@10.0.138.150:5051 (10.0.138.150)_x000D_
 I0715 02:56:40.071563    13 master.cpp:1319] Removing framework 9d8dd16c-13f4-4f15-bac8-e5138b2862ee-0000 (toil) from disconnected agent 9d8dd16c-13f4-4f15-bac8-e5138b2862ee-S0 at slave(1)@10.0.138.150:5051 (10.0.138.150) because the framework is not checkpointing_x000D_
 I0715 02:56:40.071579    13 master.cpp:11006] Removing framework 9d8dd16c-13f4-4f15-bac8-e5138b2862ee-0000 (toil) from agent 9d8dd16c-13f4-4f15-bac8-e5138b2862ee-S0 at slave(1)@10.0.138.150:5051 (10.0.138.150)_x000D_
 I0715 02:56:40.071583    12 hierarchical.cpp:829] Agent 9d8dd16c-13f4-4f15-bac8-e5138b2862ee-S0 deactivated_x000D_
 I0715 02:56:40.071619    13 master.cpp:11766] Removing executor 'toil-41' with resources {} of framework 9d8dd16c-13f4-4f15-bac8-e5138b2862ee-0000 on agent 9d8dd16c-13f4-4f15-bac8-e5138b2862ee-S0 at slave(1)@10.0.138.150:5051 (10.0.138.150)_x000D_
 I0715 02:58:08.642220    12 master.cpp:9130] Marking agent 9d8dd16c-13f4-4f15-bac8-e5138b2862ee-S0 (10.0.138.150) unreachable: health check timed out_x000D_
 I0715 02:58:08.642675    11 registrar.cpp:487] Applied 1 operations in 305592ns; attempting to update the registry_x000D_
 I0715 02:58:08.642922    13 registrar.cpp:544] Successfully updated the registry in 187904ns_x000D_
 I0715 02:58:08.643081    17 master.cpp:9173] Marked agent 9d8dd16c-13f4-4f15-bac8-e5138b2862ee-S0 (10.0.138.150) unreachable: health check timed out_x000D_
 F0715 02:58:08.643210    17 master.cpp:11402] Check failed: 'framework' Must be non NULL_x000D_
 *** Check failure stack trace: ***_x000D_
 I0715 02:58:08.643254    12 hierarchical.cpp:680] Removed agent 9d8dd16c-13f4-4f15-bac8-e5138b2862ee-S0_x000D_
     @     0x7ffbcffd090d  google::LogMessage::Fail()_x000D_
     @     0x7ffbcffd2748  google::LogMessage::SendToLog()_x000D_
     @     0x7ffbcffd04f3  google::LogMessage::Flush()_x000D_
     @     0x7ffbcffd31d9  google::LogMessageFatal::~LogMessageFatal()_x000D_
     @     0x7ffbcec65024  google::CheckNotNull<>()_x000D_
     @     0x7ffbcec32658  mesos::internal::master::Master::__removeSlave()_x000D_
     @     0x7ffbcec33b13  mesos::internal::master::Master::_markUnreachable()_x000D_
     @     0x7ffbcec33e55  _ZNO6lambda12CallableOnceIFN7process6FutureIbEEvEE10CallableFnINS_8internal7PartialIZN5mesos8internal6master6Master15markUnreachableERKNS9_9SlaveInfoEbRKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEEEUlbE_JbEEEEclEv_x000D_
     @     0x7ffbce93d5d8  _ZNO6lambda12CallableOnceIFvPN7process11ProcessBaseEEE10CallableFnINS_8internal7PartialIZNS1_8internal8DispatchINS1_6FutureIbEEEclINS0_IFSC_vEEEEESC_RKNS1_4UPIDEOT_EUlSt10unique_ptrINS1_7PromiseIbEESt14default_deleteISO_EEOSG_S3_E_ISR_SG_St12_PlaceholderILi1EEEEEEclEOS3__x000D_
     @     0x7ffbcff18371  process::ProcessBase::consume()_x000D_
     @     0x7ffbcff3a97a  process::ProcessManager::resume()_x000D_
     @     0x7ffbcff3e6a6  _ZNSt6thread5_ImplISt12_Bind_simpleIFZN7process14ProcessManager12init_threadsEvEUlvE_vEEE6_M_runEv_x000D_
     @     0x7ffbcc2cd9e0  (unknown)_x000D_
     @     0x7ffbcbde06db  start_thread_x000D_
     @     0x7ffbcbb0988f  (unknown)_x000D_
{noformat}_x000D_
_x000D_
After an analysis of the more recent logs included in the comments below, the following seems to occur:_x000D_
_x000D_
1) The last of the framework’s tasks is removed:_x000D_
_x000D_
Oct 27 23:21:18 ip-10-0-131-86.ec2.internal docker[1839]: I1027 23:21:18.493418    15 master.cpp:12171] Removing task 2 with resources cpus(allocated: *):1; disk(allocated: *):4024; mem(allocated: *):2048 of framework 522424c1-2fac-42ab-9a70-b424266218a9-0000 on agent 522424c1-2fac-42ab-9a70-b424266218a9-S0 at slave(1)@10.0.143.144:5051 (10.0.143.144)_x000D_
which means the framework’s entry in slave->tasks is erased: https://github.com/apache/mesos/blob/e13929d62663015162db7e66c6600fe414d03ec3/src/master/master.cpp#L13527-L13529_x000D_
_x000D_
2) Later, the agent disconnects and since the framework is not checkpointing, it is removed from the Slave struct:_x000D_
_x000D_
Oct 27 23:23:20 ip-10-0-131-86.ec2.internal docker[1839]: I1027 23:23:20.248260    14 master.cpp:1321] Removing framework 522424c1-2fac-42ab-9a70-b424266218a9-0000 (toil) from disconnected agent 522424c1-2fac-42ab-9a70-b424266218a9-S0 at slave(1)@10.0.143.144:5051 (10.0.143.144) because the framework is not checkpointing_x000D_
Oct 27 23:23:20 ip-10-0-131-86.ec2.internal docker[1839]: I1027 23:23:20.248289    14 master.cpp:11436] Removing framework 522424c1-2fac-42ab-9a70-b424266218a9-0000 (toil) from agent 522424c1-2fac-42ab-9a70-b424266218a9-S0 at slave(1)@10.0.143.144:5051 (10.0.143.144)_x000D_
Oct 27 23:23:20 ip-10-0-131-86.ec2.internal docker[1839]: I1027 23:23:20.248311    14 master.cpp:12211] Removing executor 'toil-440' with resources {} of framework 522424c1-2fac-42ab-9a70-b424266218a9-0000 on agent 522424c1-2fac-42ab-9a70-b424266218a9-S0 at slave(1)@10.0.143.144:5051 (10.0.143.144)_x000D_
We see no logging related to task removal since slave->tasks[framework->id()] was empty this time. however, since we use operator[] to inspect the task map here, we perform an insertion and it has a side effect: https://github.com/apache/mesos/blob/e13929d62663015162db7e66c6600fe414d03ec3/src/master/master.cpp#L11416_x000D_
This means that slave->tasks[framework->id()] now exists but has been initialized to an empty map. ruh roh._x000D_
_x000D_
3) Very soon after, the framework failover timeout elapses and the framework is removed:_x000D_
_x000D_
Oct 27 23:23:22 ip-10-0-131-86.ec2.internal docker[1839]: I1027 23:23:22.890070    11 master.cpp:10224] Framework failover timeout, removing framework 522424c1-2fac-42ab-9a70-b424266218a9-0000 (toil)_x000D_
4) Now when __removeSlave() iterates over the keys of slave->tasks, it finds a key which points to a framework that has already been removed: https://github.com/apache/mesos/blob/e13929d62663015162db7e66c6600fe414d03ec3/src/master/master.cpp#L11796-L11800_x000D_
_x000D_
We need to prevent that unintended map insertion to avoid the crash._x000D_
_x000D_
The resolution of this ticket should also involve the writing of a regression test.",3.0,1.5.0,0.5,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.963302752293578
Bug,mesos/mesos-centos nightly docker image has to include the SHA of the build.,"As a snapshot build, we need to identify the exact HEAD of the branch build. Our current snapshot builds lack this information due to the way the build is setup._x000D_
_x000D_
The current build identifies e.g. when running the agent like this;_x000D_
{noformat}_x000D_
$ docker run -it docker.io/mesos/mesos-centos:master-2019-02-15 mesos-slave --work_dir=/tmp --master=127.0.0.1:5050_x000D_
I0223 02:22:43.317088     1 main.cpp:349] Build: 2019-02-15 22:46:47 by_x000D_
I0223 02:22:43.317643     1 main.cpp:350] Version: 1.7.2_x000D_
I0223 02:22:43.332036     1 systemd.cpp:240] systemd version `219` detected_x000D_
I0223 02:22:43.332067     1 main.cpp:452] Initializing systemd state_x000D_
E0223 02:22:43.332135     1 main.cpp:461] EXIT with status 1: Failed to initialize systemd: Failed to locate systemd runtime directory: /run/systemd/system_x000D_
{noformat}_x000D_
_x000D_
Note we lack a user in the first output line and the GIT sha altogether. Only tagged builds should commonly lack the SHA as it is not needed._x000D_
{noformat}_x000D_
I0215 08:28:20.871155 34809 main.cpp:358] Git SHA: dff75bb705dca473a5c4019d9ed6e2d3530e3865_x000D_
{noformat}_x000D_
_x000D_
",2.0,0,0.0,0.3740573152337858,0.0,0.0,0.0,0.2,0.4,0.43859649122807015,0.2714285714285714,0.3717948717948718,0.24848484848484845,0.24848484848484845,0.0
Task,Clean up `QuotaRequest` and `QuotaInfo`.,"Once quota limits are implementation complete, we should clean up the `QuotaInfo` and `QuotaRequest` proto along with other legacy code and tests. Specifically:_x000D_
_x000D_
- Remove the experimental `limits` field in `QuotaInfo` and `QuotaRequest` (code in https://reviews.apache.org/r/65852/ and https://reviews.apache.org/r/65851/ and https://reviews.apache.org/r/65334/)_x000D_
- Update the related `QuotaRequest` validation code and tests (code in https://reviews.apache.org/r/65784/ and https://reviews.apache.org/r/65785/)_x000D_
- The `principal` field is no longer used. It was used to support the remove_quota acl which was already deprecated. (On second thought, while it is not used by the Mesos local authorizer, an external auth module could potentially depend on this. We need to keep this for backward compatibility.)",2.0,0,0.0,0.0,0.6666666666666666,1.0,0.5714285714285714,0.0,0.0,0.5438596491228069,0.5357142857142857,0.3974358974358974,0.4,0.4,0.0
Task,Provide backward compatibility for old quota configurations.,"Current (old) masters only support quota guarantee which also servers as limits implicitly. When upgrading to new masters where guarantees and limits are decoupled, we need to ensure backward compatibility such that the existing (old) quota configurations are honored and there should be no change to the cluster behavior._x000D_
_x000D_
To this end, new masters should also be able to consume the old quota registry. The old guarantee field will be used to set both guarantee and limits.",2.0,0,0.0,0.0,0.6666666666666666,1.0,0.5714285714285714,0.0,0.0,0.5438596491228069,0.5357142857142857,0.3974358974358974,0.4,0.4,0.0
Task,Deprecate `SET_QUOTA` and `REMOVE_QUOTA` calls in favor of `UPDATE_QUOTA`.,"Once the `UPDATE_QUOTA` call (MESOS-9596) is implemented and wired, we should deprecate the existing calls `REMOVE_QUOTA` and `SET_QUOTA`. In the user-facing documentation, we should hide the old API and showcase the new one.",1.0,0,0.0,0.0,0.6666666666666666,1.0,0.5714285714285714,0.0,0.0,0.5438596491228069,0.5357142857142857,0.3974358974358974,0.4,0.4,0.0
Task,Update `GET_QUOTA` to return both guarantees and limits. ,"We should mark the existing `QuotaInfo` message as deprecated in favor of the new `QuotaConfig`:_x000D_
_x000D_
{noformat}_x000D_
message GetQuota {_x000D_
  required quota.QuotaStatus status = 1;_x000D_
}_x000D_
_x000D_
message QuotaStatus {_x000D_
       repeated QuotaInfo infos [deprecated = true];_x000D_
       repeated QuotaConfig configs; _x000D_
}_x000D_
_x000D_
message QuotaConfig {_x000D_
        required  string role;_x000D_
        map<string, Value.Scalar> guarantees;_x000D_
        map<string, Value.Scalar> limits;_x000D_
}_x000D_
{noformat}_x000D_
_x000D_
We will continue to fill in the QuotaInfo though for backward compatibility. See the design doc: [New API|https://docs.google.com/document/d/13vG5uH4YVwM79ErBPYAZfnqYFOBbUy2Lym0_9iAQ5Uk/edit#heading=h.z2vfcyzabymz]",3.0,0,0.0,0.0,0.6666666666666666,1.0,0.5714285714285714,0.0,0.0,0.5438596491228069,0.5357142857142857,0.3974358974358974,0.4,0.4,0.0
Task,Update GET `/quota` to return both guarantees and limits.,"We should mark the existing `QuotaInfo` message as deprecated in favor of the new `QuotaConfig`:_x000D_
_x000D_
{noformat}_x000D_
message QuotaStatus {_x000D_
       repeated QuotaInfo infos [deprecated = true];_x000D_
       repeated QuotaConfig configs; _x000D_
}_x000D_
_x000D_
message QuotaConfig {_x000D_
        required  string role;_x000D_
        map<string, Value.Scalar> guarantees;_x000D_
        map<string, Value.Scalar> limits;_x000D_
}_x000D_
{noformat}_x000D_
_x000D_
We will continue to fill in the QuotaInfo though for backward compatibility. See the design doc: [New API|https://docs.google.com/document/d/13vG5uH4YVwM79ErBPYAZfnqYFOBbUy2Lym0_9iAQ5Uk/edit#]_x000D_
_x000D_
Note, we only update this v0 endpoint for the GET method. There is no plan to support configuring quota limits from this endpoint. V1 calls should be used.",1.0,0,0.0,0.0,0.6666666666666666,1.0,0.5714285714285714,0.0,0.0,0.5438596491228069,0.5357142857142857,0.3974358974358974,0.4,0.4,0.0
Bug,Test `StorageLocalResourceProviderTest.RetryRpcWithExponentialBackoff` is flaky.,"Observed on ASF CI:_x000D_
{noformat}_x000D_
/tmp/SRC/src/tests/storage_local_resource_provider_tests.cpp:5027_x000D_
Failed to wait 1mins for offers_x000D_
{noformat}_x000D_
Full log:  [^RetryRpcWithExponentialBackoff-badrun.txt] ",3.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.2,0.2631578947368421,0.36428571428571427,0.7307692307692307,0.7515151515151515,0.7575757575757576,0.0
Bug,Mesos package naming appears to be undeterministic.,"Transcribed from slack; https://mesos.slack.com/archives/C7N086PK2/p1550158266006900_x000D_
_x000D_
It appears there are a number of RPM packages called “mesos-1.7.1-2.0.1.el7.x86_64.rpm” in the wild._x000D_
_x000D_
I’ve caught specimens with build dates February 1st, 7th and 13th. While it’s somewhat troubling in itself, none of these packages is the one referred to in Yum repository metadata (repos.mesosphere.com), which is a package built today on the 14th, so I can’t install Mesos right now._x000D_
_x000D_
Could it be that your pipeline is creating a new package with the same verson and release in every nightly build?_x000D_
_x000D_
Repository metadata_x000D_
{noformat}_x000D_
sqlite3 *primary.sqlite ""select name, version, release, strftime('%d-%m-%Y %H:%M', datetime(time_build, 'unixepoch')) build_as_string, rpm_buildhost from packages where name = 'mesos' and version = '1.7.1';""_x000D_
mesos|1.7.1|2.0.1|14-02-2019 12:30|ip-172-16-10-254.ec2.internal_x000D_
Packages downloaded while investigating over the past few days _x000D_
Name : mesos_x000D_
Version : 1.7.1_x000D_
Release : 2.0.1_x000D_
Architecture: x86_64_x000D_
Install Date: (not installed)_x000D_
Group : misc_x000D_
Size : 298787793_x000D_
License : Apache-2.0_x000D_
Signature : RSA/SHA256, Fri 01 Feb 2019 11:38:47 PM UTC, Key ID df7d54cbe56151bf_x000D_
Source RPM : mesos-1.7.1-2.0.1.src.rpm_x000D_
Build Date : Fri 01 Feb 2019 11:15:17 PM UTC_x000D_
Build Host : ip-172-16-10-11.ec2.internal_x000D_
Relocations : / _x000D_
Packager : dev@mesos.apache.org_x000D_
URL : https://mesos.apache.org/_x000D_
Summary : Cluster resource manager with efficient resource isolation_x000D_
Description :_x000D_
[snip]_x000D_
_x000D_
Name : mesos_x000D_
Version : 1.7.1_x000D_
Release : 2.0.1_x000D_
Architecture: x86_64_x000D_
Install Date: (not installed)_x000D_
Group : misc_x000D_
Size : 298791347_x000D_
License : Apache-2.0_x000D_
Signature : RSA/SHA256, Thu 07 Feb 2019 10:33:06 PM UTC, Key ID df7d54cbe56151bf_x000D_
Source RPM : mesos-1.7.1-2.0.1.src.rpm_x000D_
Build Date : Thu 07 Feb 2019 10:31:02 PM UTC_x000D_
Build Host : ip-172-16-10-4.ec2.internal_x000D_
Relocations : / _x000D_
Packager : dev@mesos.apache.org_x000D_
URL : https://mesos.apache.org/_x000D_
Summary : Cluster resource manager with efficient resource isolation_x000D_
Description :_x000D_
[snip]_x000D_
_x000D_
Name : mesos_x000D_
Version : 1.7.1_x000D_
Release : 2.0.1_x000D_
Architecture: x86_64_x000D_
Install Date: (not installed)_x000D_
Group : misc_x000D_
Size : 298789309_x000D_
License : Apache-2.0_x000D_
Signature : RSA/SHA256, Wed Feb 13 04:35:02 2019, Key ID df7d54cbe56151bf_x000D_
Source RPM : mesos-1.7.1-2.0.1.src.rpm_x000D_
Build Date : Wed Feb 13 04:32:41 2019_x000D_
Build Host : ip-172-16-10-83.ec2.internal_x000D_
Relocations : / _x000D_
Packager : dev@mesos.apache.org_x000D_
URL : https://mesos.apache.org/_x000D_
Summary : Cluster resource manager with efficient resource isolation_x000D_
Description :_x000D_
 {noformat}",1.0,1.7.1,0.0,0.3740573152337858,0.0,0.0,0.0,0.2,0.4,0.43859649122807015,0.2714285714285714,0.3717948717948718,0.24848484848484845,0.24848484848484845,0.9817431192660551
Task,Document per framework minimal allocatable resources in framework development guides,With MESOS-9523 we introduced fields into {{FrameworkInfo}} to give frameworks a way to express their resource requirements. We should document this feature in the framework development guide(s).,1.0,0,0.5,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,1.0,0.8121212121212121,0.8121212121212121,0.0
Bug,Operations are leaked in Framework struct when agents are removed,"Currently, when agents are removed from the master, their operations are not removed from the {{Framework}} structs. We should ensure that this occurs in all cases.",2.0,0,0.5,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Bug,Allocator CHECK failure: reservationScalarQuantities.contains(role).,"We recently upgraded our Mesos cluster from version 1.3 to 1.5, and since then have been getting periodic master crashes due to this error:_x000D_
{code:java}_x000D_
Feb 5 15:53:57 ip-10-0-16-140 mesos-master[8414]: F0205 15:53:57.385118 8434 hierarchical.cpp:2630] Check failed: reservationScalarQuantities.contains(role){code}_x000D_
Full stack trace is at the end of this issue description. When the master fails, we automatically restart it and it rejoins the cluster just fine. I did some initial searching and was unable to find any existing bug reports or other people experiencing this issue. We run a cluster of 3 masters, and see crashes on all 3 instances._x000D_
_x000D_
Right before the crash, we saw a {{Removed agent:...}} log line noting that it was agent 9b912afa-1ced-49db-9c85-7bc5a22ef072-S6 that was removed._x000D_
{code:java}_x000D_
294929:Feb 5 15:53:57 ip-10-0-16-140 mesos-master[8414]: I0205 15:53:57.384759 8432 master.cpp:9893] Removed agent 9b912afa-1ced-49db-9c85-7bc5a22ef072-S6 at slave(1)@10.0.18.78:5051 (10.0.18.78): the agent unregistered{code}_x000D_
I saved the full log from the master, so happy to provide more info from it, or anything else about our current environment._x000D_
_x000D_
Full stack trace is below._x000D_
{code:java}_x000D_
Feb 5 15:53:57 ip-10-0-16-140 mesos-master[8414]: @ 0x7f87e9170a7d google::LogMessage::Fail()_x000D_
Feb 5 15:53:57 ip-10-0-16-140 mesos-master[8414]: @ 0x7f87e9172830 google::LogMessage::SendToLog()_x000D_
Feb 5 15:53:57 ip-10-0-16-140 mesos-master[8414]: @ 0x7f87e9170663 google::LogMessage::Flush()_x000D_
Feb 5 15:53:57 ip-10-0-16-140 mesos-master[8414]: @ 0x7f87e9173259 google::LogMessageFatal::~LogMessageFatal()_x000D_
Feb 5 15:53:57 ip-10-0-16-140 mesos-master[8414]: @ 0x7f87e8443cbd mesos::internal::master::allocator::internal::HierarchicalAllocatorProcess::untrackReservations()_x000D_
Feb 5 15:53:57 ip-10-0-16-140 mesos-master[8414]: @ 0x7f87e8448fcd mesos::internal::master::allocator::internal::HierarchicalAllocatorProcess::removeSlave()_x000D_
Feb 5 15:53:57 ip-10-0-16-140 mesos-master[8414]: @ 0x7f87e90c4f11 process::ProcessBase::consume()_x000D_
Feb 5 15:53:57 ip-10-0-16-140 mesos-master[8414]: @ 0x7f87e90dea4a process::ProcessManager::resume()_x000D_
Feb 5 15:53:57 ip-10-0-16-140 mesos-master[8414]: @ 0x7f87e90e25d6 _ZNSt6thread5_ImplISt12_Bind_simpleIFZN7process14ProcessManager12init_threadsEvEUlvE_vEEE6_M_runEv_x000D_
Feb 5 15:53:57 ip-10-0-16-140 mesos-master[8414]: @ 0x7f87e6700c80 (unknown)_x000D_
Feb 5 15:53:57 ip-10-0-16-140 mesos-master[8414]: @ 0x7f87e5f136ba start_thread_x000D_
Feb 5 15:53:57 ip-10-0-16-140 mesos-master[8414]: @ 0x7f87e5c4941d (unknown){code}",3.0,"1.5.0,1.5.1,1.5.2,1.6.0,1.6.1,1.6.2,1.7.0,1.7.1",0.5,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.01282051282051282,0.0,0.0,0.9714105504587156
Improvement,Marking an unreachable agent as gone should transition the tasks to terminal state,"If an unreachable agent is marked as gone, currently master just marks that agent in the registry but doesn't do anything about its tasks. So the tasks are in UNREACHABLE state in the master forever, until the master fails over. This is not great UX. We should transition these to terminal state instead._x000D_
_x000D_
This fix should also include a test to verify.",3.0,0,0.5,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.8846153846153846,0.9878787878787878,0.9878787878787878,0.0
Task,Agent `ReconcileOperations` handler should handle operation affecting default resources,{{Slave::reconcileOperations()}} has to be updated to send {{OPERATION_DROPPED}} for unknown operations that don't have a resource provider ID.,3.0,0,0.5,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.28205128205128205,0.2606060606060606,0.2606060606060606,0.0
Task,Master should clean up operations from downgraded agents,"If a Mesos agent is upgraded to provide reliable feedback for operations on agent default resources and then later downgraded, the master may possess in-memory state related to operations requesting feedback which should be cleaned up. We should update the master to detect downgraded agents and clean up appropriately._x000D_
_x000D_
This ticket does not include sending best-effort feedback to schedulers for operations on downgraded agents._x000D_
_x000D_
The upgrade documentation should also be updated with a note about the impact of downgrades on operation feedback.",3.0,0,0.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Task,Agent capability for operation feedback on default resources,We should add an agent capability to prevent the master from sending operations on agent default resources which request feedback to older agents which are not able to handle them.,3.0,0,0.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Bug,Reviewboard bot fails on verify-reviews.py.,"Seeing this on our Azure based Mesos CI for review requests._x000D_
_x000D_
{noformat}_x000D_
Started by timer_x000D_
[EnvInject] - Loading node environment variables._x000D_
Building remotely on dummy-slave-01 (dummy-slave) in workspace /home/jenkins/workspace/mesos-reviewbot_x000D_
 > git rev-parse --is-inside-work-tree # timeout=10_x000D_
Fetching changes from the remote Git repository_x000D_
 > git config remote.origin.url https://github.com/apache/mesos # timeout=10_x000D_
Pruning obsolete local branches_x000D_
Cleaning workspace_x000D_
 > git rev-parse --verify HEAD # timeout=10_x000D_
Resetting working tree_x000D_
 > git reset --hard # timeout=10_x000D_
 > git clean -fdx # timeout=10_x000D_
Fetching upstream changes from https://github.com/apache/mesos_x000D_
 > git --version # timeout=10_x000D_
 > git fetch --tags --progress https://github.com/apache/mesos +refs/heads/*:refs/remotes/origin/* --prune_x000D_
 > git rev-parse refs/remotes/origin/master^{commit} # timeout=10_x000D_
 > git rev-parse refs/remotes/origin/origin/master^{commit} # timeout=10_x000D_
Checking out Revision 3478e344fb77d931f6122980c6e94cd3913c441d (refs/remotes/origin/master)_x000D_
 > git config core.sparsecheckout # timeout=10_x000D_
 > git checkout -f 3478e344fb77d931f6122980c6e94cd3913c441d_x000D_
Commit message: ""Sent SIGKILL to I/O switchboard server as a safeguard.""_x000D_
 > git rev-list --no-walk 3478e344fb77d931f6122980c6e94cd3913c441d # timeout=10_x000D_
[mesos-reviewbot] $ /usr/bin/env bash /tmp/jenkins5023908134863801311.sh_x000D_
git rev-parse HEAD_x000D_
Traceback (most recent call last):_x000D_
  File ""/home/jenkins/workspace/mesos-reviewbot/mesos/support/verify-reviews.py"", line 101, in <module>_x000D_
    HEAD = shell(""git rev-parse HEAD"")_x000D_
  File ""/home/jenkins/workspace/mesos-reviewbot/mesos/support/verify-reviews.py"", line 97, in shell_x000D_
    out = subprocess.check_output(command, stderr=subprocess.STDOUT, shell=True)_x000D_
  File ""/usr/lib/python3.5/subprocess.py"", line 626, in check_output_x000D_
    **kwargs).stdout_x000D_
  File ""/usr/lib/python3.5/subprocess.py"", line 708, in run_x000D_
    output=stdout, stderr=stderr)_x000D_
subprocess.CalledProcessError: Command 'git rev-parse HEAD' returned non-zero exit status 128_x000D_
Build step 'Execute shell' marked build as failure_x000D_
Finished: FAILURE_x000D_
{noformat}_x000D_
_x000D_
This is happening pretty much exactly since we landed https://github.com/apache/mesos/commit/3badf7179992e61f30f5a79da9d481dd451c7c2f#diff-0bcbb572aad3fe39e0e5c3c8a8c3e515",2.0,0,0.5,0.25037707390648567,0.3333333333333333,0.5,0.14285714285714285,0.5,0.0,0.2631578947368421,0.29285714285714287,0.3717948717948718,0.24848484848484845,0.24848484848484845,0.0
Improvement,"Disallowed nan, inf and so on in `Value::Scalar`.","Mesos does not expect `Value::Scalar` to be nan, inf and etc. We should validate this when parsing.",1.0,0,0.0,0.0,0.6666666666666666,1.0,0.5714285714285714,0.0,0.0,0.5438596491228069,0.5357142857142857,0.3974358974358974,0.4,0.4,0.0
Bug,Master will leak operations when agents are removed,"Usually, offer operations are removed when the framework acknowledges_x000D_
a terminal operation status update._x000D_
_x000D_
However, currently only operations on registered agents can be_x000D_
acknowledged, so operations on agents which don't come back will be permanently leaked.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.16666666666666666,0.1818181818181818,0.17575757575757575,0.0
Improvement,Use ResourceQuantities in the allocator and sorter to improve performance.,"In allocator and sorter, we need to do a lot of quantity calculations. Currently, we use the full {{Resources}} type with utilities like {{createScalarResourceQuantities()}}, even though we only care about quantities. Replace {{Resources}} with {{ResourceQuantities}}._x000D_
_x000D_
See:_x000D_
_x000D_
https://github.com/apache/mesos/blob/386b1fe99bb9d10af2abaca4832bf584b6181799/src/master/allocator/sorter/drf/sorter.hpp#L444-L445_x000D_
https://reviews.apache.org/r/70061/_x000D_
_x000D_
With the addition of ResourceQuantities, callers can now just do {{ResourceQuantities.fromScalarResources(r.scalars())}} instead of using {{Resources::createStrippedScalarQuantity()}}, which should actually be a bit more efficient since we only copy the shared pointers rather than construct new `Resource` objects.",3.0,0,0.0,0.0,0.6666666666666666,1.0,0.5714285714285714,0.0,0.0,0.5438596491228069,0.5357142857142857,0.3974358974358974,0.4,0.4,0.0
Bug,Test `MasterTest.CreateVolumesV1AuthorizationFailure` is flaky.,"{noformat}_x000D_
I1219 22:45:59.578233 26107 slave.cpp:1884] Will retry registration in 2.10132ms if necessary_x000D_
I1219 22:45:59.578615 26107 master.cpp:6125] Received register agent message from slave(463)@172.16.10.13:35739 (ip-172-16-10-13.ec2.internal)_x000D_
I1219 22:45:59.578830 26107 master.cpp:3871] Authorizing agent with principal 'test-principal'_x000D_
I1219 22:45:59.578975 26107 master.cpp:6183] Authorized registration of agent at slave(463)@172.16.10.13:35739 (ip-172-16-10-13.ec2.internal)_x000D_
I1219 22:45:59.579039 26107 master.cpp:6294] Registering agent at slave(463)@172.16.10.13:35739 (ip-172-16-10-13.ec2.internal) with id 85292fcc-b698-4377-9faa-f76b0ccd4ee5-S0_x000D_
I1219 22:45:59.579540 26107 registrar.cpp:495] Applied 1 operations in 143852ns; attempting to update the registry_x000D_
I1219 22:45:59.580102 26109 registrar.cpp:552] Successfully updated the registry in 510208ns_x000D_
I1219 22:45:59.580312 26109 master.cpp:6342] Admitted agent 85292fcc-b698-4377-9faa-f76b0ccd4ee5-S0 at slave(463)@172.16.10.13:35739 (ip-172-16-10-13.ec2.internal)_x000D_
I1219 22:45:59.580968 26111 slave.cpp:1884] Will retry registration in 23.973874ms if necessary_x000D_
I1219 22:45:59.581447 26111 slave.cpp:1486] Registered with master master@172.16.10.13:35739; given agent ID 85292fcc-b698-4377-9faa-f76b0ccd4ee5-S0_x000D_
..._x000D_
I1219 22:45:59.580950 26109 master.cpp:6391] Registered agent 85292fcc-b698-4377-9faa-f76b0ccd4ee5-S0 at slave(463)@172.16.10.13:35739 (ip-172-16-10-13.ec2.internal) with disk(reservations: [(STATIC,role1)]):1024; cpus:2; mem:6796; ports:[31000-32000]_x000D_
I1219 22:45:59.583326 26109 master.cpp:6125] Received register agent message from slave(463)@172.16.10.13:35739 (ip-172-16-10-13.ec2.internal)_x000D_
I1219 22:45:59.583524 26109 master.cpp:3871] Authorizing agent with principal 'test-principal'_x000D_
..._x000D_
W1219 22:45:59.584242 26109 master.cpp:6175] Refusing registration of agent at slave(463)@172.16.10.13:35739 (ip-172-16-10-13.ec2.internal): Authorization failure: Authorizer failure_x000D_
..._x000D_
I1219 22:45:59.586944 26113 http.cpp:1185] HTTP POST for /master/api/v1 from 172.16.10.13:47412_x000D_
I1219 22:45:59.587129 26113 http.cpp:682] Processing call CREATE_VOLUMES_x000D_
/home/centos/workspace/mesos/Mesos_CI-build/FLAG/CMake/label/mesos-ec2-centos-7/mesos/src/tests/master_tests.cpp:9386: Failure_x000D_
Mock function called more times than expected - returning default value._x000D_
    Function call: authorized(@0x7f5066524720 48-byte object <D8-BF 4F-72 50-7F 00-00 00-00 00-00 00-00 00-00 07-00 00-00 00-00 00-00 10-4E 02-48 50-7F 00-00 E0-4C 02-48 50-7F 00-00 06-00 00-00 50-7F 00-00>)_x000D_
          Returns: Abandoned_x000D_
         Expected: to be called once_x000D_
           Actual: called twice - over-saturated and active_x000D_
I1219 22:45:59.587761 26113 master.cpp:3811] Authorizing principal 'test-principal' to create volumes '[{""disk"":{""persistence"":{""id"":""id1"",""principal"":""test-principal""},""volume"":{""container_path"":""path1"",""mode"":""RW""}},""name"":""disk"",""reservations"":[{""role"":""role1"",""type"":""STATIC""}],""scalar"":{""value"":64.0},""type"":""SCALAR""}]'_x000D_
..._x000D_
/home/centos/workspace/mesos/Mesos_CI-build/FLAG/CMake/label/mesos-ec2-centos-7/mesos/src/tests/master_tests.cpp:9398: Failure_x000D_
Failed to wait 15secs for response{noformat}_x000D_
This is because we authorize the retried registration before dropping it._x000D_
_x000D_
Full log: [^mesos-ec2-centos-7-CMake.Mesos.MasterTest.CreateVolumesV1AuthorizationFailure-badrun.txt]",1.0,"1.5.0,1.5.1,1.6.0,1.6.1,1.7.0",0.5,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.7307692307692307,0.7515151515151515,0.7575757575757576,0.9706788990825689
Task,Unblock operation feedback on agent default resources.,"# Remove {{CHECK}} marked with a TODO in {{Master::updateOperationStatus()}}._x000D_
# Update {{Master::acknowledgeOperationStatus()}}, remove the CHECK requiring a resource provider ID._x000D_
# Remove validation in {{Option<Error> validate(mesos::scheduler::Call& call, const Option<Principal>& principal)}}",3.0,0,0.5,0.0,0.3333333333333333,0.5,0.14285714285714285,0.0,0.2,0.5087719298245614,0.4357142857142857,0.28205128205128205,0.2606060606060606,0.2606060606060606,0.0
Task,Master should track operations on agent default resources.,"Make {{Master::updateSlave()}} add operations that the agent sends and the master doesn't know._x000D_
_x000D_
Right now only operations from SLRPs are added to the master's in-memory state.",3.0,0,0.5,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.28205128205128205,0.2606060606060606,0.2606060606060606,0.0
Improvement,Allow for optionally unbundled leveldb from CMake builds.,"Following the example of unbundled libevent and libarchive, we should allow for unbundled leveldb if the user wishes so._x000D_
_x000D_
For leveldb, this task is not as trivial as one would hope due to the fact that we link leveldb statically. This forces us to satisfy leveldb's strong dependencies against gpertools (tcmalloc) as well as snappy._x000D_
Alternatively, we would resort into linking leveldb dynamically, solving these issues._x000D_
",2.0,1.8.0,0.0,0.3740573152337858,0.0,0.0,0.0,0.2,0.4,0.43859649122807015,0.2714285714285714,0.3717948717948718,0.24848484848484845,0.24848484848484845,0.9908256880733946
Task,Update 'mesos task list' to only list running tasks,"Doing a {{mesos task list}} currently returns all tasks that have ever been run (not just running tasks). The default behavior should be to return only the running tasks and offer an option to return all of them. To tell them apart, there should be a state field in the table returned by this command.",2.0,0,0.0,0.25037707390648567,0.3333333333333333,0.5,0.14285714285714285,0.5,0.0,0.2631578947368421,0.29285714285714287,0.10256410256410256,0.1515151515151515,0.1515151515151515,0.0
Task,Use the built CLI binary when running new CLI integration tests in CI,"We currently use the CLI in the virtual environment which is just a Python file being interpreted, we should instead use the binary built by PyInstaller as it is what is gonna be used in production.",3.0,0,0.0,0.25037707390648567,0.3333333333333333,0.5,0.14285714285714285,0.5,0.0,0.2631578947368421,0.29285714285714287,0.10256410256410256,0.1515151515151515,0.1515151515151515,0.0
Bug,Unable to build new Mesos CLI with PyInstaller and Python 3.7.,"Building the new Mesos CLI with Python 3.7 and PyInstaller 3.3.1 (our current dependencu) on your machine currently creates a binary that is not working:_x000D_
_x000D_
{noformat}_x000D_
➜  build (master) ✔ ./src/mesos_x000D_
Fatal Python error: initfsencoding: unable to load the file system codec_x000D_
zipimport.ZipImportError: can't find module 'encodings'_x000D_
_x000D_
Current thread 0x0000000110e145c0 (most recent call first):_x000D_
[1]    83095 abort      ./src/mesos_x000D_
{noformat}_x000D_
_x000D_
As seen in https://github.com/pyinstaller/pyinstaller/issues/3219, this is due to PyInstaller._x000D_
We need to update the PyInstaller dependency to have a version that supports Python 3.7.",1.0,0,0.0,0.25037707390648567,0.3333333333333333,0.5,0.14285714285714285,0.5,0.0,0.2631578947368421,0.29285714285714287,0.10256410256410256,0.1515151515151515,0.1515151515151515,0.0
Bug,Test `SlaveRecoveryTest.AgentReconfigurationWithRunningTask` is flaky.,"The fails with:_x000D_
_x000D_
{noformat}_x000D_
../../src/tests/slave_recovery_tests.cpp:4797_x000D_
Failed to wait 15secs for offers2_x000D_
{noformat}_x000D_
_x000D_
It is flaky because after partially accept the first offer, the scheduler will filter the agent for 5 seconds, and depending on the system load that may last more than 15 seconds:_x000D_
_x000D_
{noformat}_x000D_
I1026 14:32:40.976006 78393344 hierarchical.cpp:2388] Filtered offer with cpus:2 on agent c8805e35-97ee-44d4-9ab9-e7ea8d703c08-S0 for role * of framework c8805e35-97ee-44d4-9ab9-e7ea8d703c08-0000_x000D_
I1026 14:32:40.976065 78393344 hierarchical.cpp:1566] Performed allocation for 1 agents in 197914ns_x000D_
I1026 14:32:41.981853 79466496 hierarchical.cpp:2388] Filtered offer with cpus:2 on agent c8805e35-97ee-44d4-9ab9-e7ea8d703c08-S0 for role * of framework c8805e35-97ee-44d4-9ab9-e7ea8d703c08-0000_x000D_
I1026 14:32:41.981920 79466496 hierarchical.cpp:1566] Performed allocation for 1 agents in 185853ns_x000D_
I1026 14:32:42.983603 78393344 hierarchical.cpp:2388] Filtered offer with cpus:2 on agent c8805e35-97ee-44d4-9ab9-e7ea8d703c08-S0 for role * of framework c8805e35-97ee-44d4-9ab9-e7ea8d703c08-0000_x000D_
I1026 14:32:42.983659 78393344 hierarchical.cpp:1566] Performed allocation for 1 agents in 179397ns_x000D_
I1026 14:32:57.363723 78929920 hierarchical.cpp:2388] Filtered offer with cpus:2 on agent c8805e35-97ee-44d4-9ab9-e7ea8d703c08-S0 for role * of framework c8805e35-97ee-44d4-9ab9-e7ea8d703c08-0000_x000D_
I1026 14:32:57.363788 78929920 hierarchical.cpp:1566] Performed allocation for 1 agents in 466500ns_x000D_
../../src/tests/slave_recovery_tests.cpp:4797: Failure_x000D_
Failed to wait 15secs for offers2_x000D_
{noformat}_x000D_
_x000D_
I think we can either remove the expectation of the second offer (which I think is not essential to the test) or manually control the clock._x000D_
_x000D_
Full log attached._x000D_
",2.0,0,0.5,0.0,0.3333333333333333,0.5,0.14285714285714285,0.0,0.2,0.5087719298245614,0.4357142857142857,0.3974358974358974,0.4,0.4,0.0
Bug,FetcherTest.DuplicateFileURI fails on macos,"I see {{FetcherTest.DuplicateFileURI}} fail pretty reliably on macos, e.g., 10.14._x000D_
{noformat}_x000D_
../../src/tests/fetcher_tests.cpp:173_x000D_
Value of: os::exists(""two"")_x000D_
  Actual: false_x000D_
Expected: true_x000D_
{noformat}",1.0,0,0.5,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,1.0,0.8121212121212121,0.8121212121212121,0.0
Bug,CLI build step is broken with CMake due to missing file.,The file {{mesos.py}} was not added to {{CLI_FILES}} and this is now an issue when building the CLI using CMake.,1.0,0,0.0,0.25037707390648567,0.3333333333333333,0.5,0.14285714285714285,0.5,0.0,0.2631578947368421,0.29285714285714287,0.10256410256410256,0.1515151515151515,0.1515151515151515,0.0
Task,Add test for `mesos task attach` on task launched without a TTY,"As a source, we could use the tests in https://github.com/dcos/dcos-core-cli/blob/b930d2004dceb47090004ab658f35cb608bc70e4/python/lib/dcoscli/tests/integrations/test_task.py",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.10256410256410256,0.1515151515151515,0.1515151515151515,0.0
Task,Add test(s) for `mesos task attach` on task launched with a TTY ,"As a source, we could use the tests in https://github.com/dcos/dcos-core-cli/blob/b930d2004dceb47090004ab658f35cb608bc70e4/python/lib/dcoscli/tests/integrations/test_task.py",3.0,0,0.0,0.25037707390648567,0.3333333333333333,0.5,0.14285714285714285,0.5,0.0,0.2631578947368421,0.29285714285714287,0.10256410256410256,0.1515151515151515,0.1515151515151515,0.0
Task,Add interactive test(s) for `mesos task exec`,"As a source, we could use the tests in https://github.com/dcos/dcos-core-cli/blob/b930d2004dceb47090004ab658f35cb608bc70e4/python/lib/dcoscli/tests/integrations/test_task.py_x000D_
This will require new helper functions to get the input/output of the command.",3.0,0,0.0,0.25037707390648567,0.3333333333333333,0.5,0.14285714285714285,0.5,0.0,0.2631578947368421,0.29285714285714287,0.10256410256410256,0.1515151515151515,0.1515151515151515,0.0
Task,Add non-interactive test(s) for `mesos task exec`,"As a source, we could use the tests in https://github.com/dcos/dcos-core-cli/blob/b930d2004dceb47090004ab658f35cb608bc70e4/python/lib/dcoscli/tests/integrations/test_task.py",3.0,0,0.0,0.25037707390648567,0.3333333333333333,0.5,0.14285714285714285,0.5,0.0,0.2631578947368421,0.29285714285714287,0.10256410256410256,0.1515151515151515,0.1515151515151515,0.0
Task,Document usage and build of new Mesos CLI,"Stating how to compile and use the Mesos CLI + its limitations (only Mesos containerizer, exec DEBUG follows task-user).",3.0,0,0.0,0.25037707390648567,0.3333333333333333,0.5,0.14285714285714285,0.5,0.0,0.2631578947368421,0.29285714285714287,0.10256410256410256,0.1515151515151515,0.1515151515151515,0.0
Improvement,Optimize `Resources::filter` operation.,"`Resources::filter()` is a heavily used function. Currently it is O(n^2) due to the `add()` operation for each `Resource`:_x000D_
_x000D_
{code:java}_x000D_
Resources Resources::filter(_x000D_
    const lambda::function<bool(const Resource&)>& predicate) const_x000D_
{_x000D_
  Resources result;_x000D_
  foreach (_x000D_
      const Resource_Unsafe& resource_,_x000D_
      resourcesNoMutationWithoutExclusiveOwnership) {_x000D_
    if (predicate(resource_->resource)) {_x000D_
      result.add(resource_);_x000D_
    }_x000D_
  }_x000D_
  return result;_x000D_
}_x000D_
{code}_x000D_
_x000D_
`add()` is O( n ). This is not necessary. `filter()` operation should only remove `Resource` entries. We should be able to  `push_back`  the resource to the vector without scanning, making the `filter()` O( n )._x000D_
",1.0,0,0.0,0.0,0.6666666666666666,1.0,0.5714285714285714,0.0,0.0,0.5438596491228069,0.5357142857142857,0.3974358974358974,0.4,0.4,0.0
Bug,Resource fragmentation: frameworks may be starved of port resources in the presence of large number roles with quota.,"In our environment where there are 1.5k frameworks and quota is heavily utilized, we would experience a severe resource fragmentation issue. Specifically, we observed a large number of port-less offers circulating in the cluster. Thus frameworks that need port resources are not able to launch tasks even if their roles have quota (because currently, we can only set quota for scalar resources, not port range resources)._x000D_
_x000D_
While most of the 1.5k frameworks do not suppress today and we believe the situation will significantly improve once they do. Still, I think there are some improvements the Mesos allocator can make to help._x000D_
_x000D_
h3. How resource becomes fragmented_x000D_
The origin of these port-less offers stems from quota chopping. Specifically, when chopping an agent to satisfy a role’s quota, we will also hand out resources that this role does not have quota for (as long as it does not break other role’s quota). These “extra resources” certainly includes ALL the remaining port resources on the agent. After this offer, the agent will be left with no port resources even though it still has CPUs and etc. Later, these resources may be offered to other frameworks but they are useless due to no ports. Now we have some “bad offers” in the cluster._x000D_
_x000D_
h3. How resource fragmentation prolonged_x000D_
A resource offer, once it is declined (e.g. due to no ports), is recovered by the allocator and offered to other frameworks again. Before this happens, it is possible that this offer might be able to merge with either the remaining resources or other declined resources on the same agent. However, it is conceivable that not uncommonly, the declined offer will be hand out again *as-is*.  This is especially probable if the allocator makes offers faster than the framework offer response time. As a result, we will observe the circulation of bad offers across different frameworks. These bad offers will exist for a long time before being consolidated again. For how long? *The longevity of the bad offer will be roughly proportional to the number of active frameworks*. In the worse case, once all the active frameworks have (hopefully long) declined the bad offer, the bad offer will have nowhere to go and finally start to merge with other resources on that agent._x000D_
_x000D_
Note, since the allocator performance has greatly improved in the past several months. The scenario described here could be increasingly common. Also, as we introduce quota limits and hierarchical quota, there will be much more agent chopping, making resource fragmentation even worse._x000D_
_x000D_
h3. Near-term Mitigations_x000D_
As mentioned above, the longevity of a bad offer is proportional to the active frameworks. Thus framework suppression will certainly help. In addition, from the Mesos side, a couple of mitigation measures are worth considering (other than the long-term optimistic allocation strategy):_x000D_
_x000D_
1. Adding a defragment interval once in a while in the allocator. For example, each minute or a dozen allocation cycles or so, we will pause the allocation, rescind all the offers and start allocating again. This essentially eliminates all the circulating bad offers by giving them a chance to be consolidated. Think of this as a periodic “reboot” of the allocator._x000D_
2. Consider chopping non-quota resources as well. Right now, for resources such as ports (or any other resources that the role does not have quota for), all are allocated in a single offer. We could choose to chop these non-quota resources as well. For example, port resources can be distributed proportionally to allocated CPU resources._x000D_
3. Provide support for specifying port quantities. With this, we can utilize the existing quota or `min_allocatable_resources` APIs to guarantee a certain number of port resources.",3.0,0,0.0,0.0,0.6666666666666666,1.0,0.5714285714285714,0.0,0.0,0.5438596491228069,0.5357142857142857,0.3974358974358974,0.4,0.4,0.0
Improvement,Create cgoup recursively to workaround systemd deleting cgroups_root.,"This is my case:_x000D_
_x000D_
My cgroups_root of mesos-slave is some_user/mesos under /sys/fs/cgroup。_x000D_
_x000D_
It happens that this some_user dir may be gone for some unknown reason, in which case I can no longer create any cgroup and any task will fail._x000D_
_x000D_
So I would like to change _x000D_
_x000D_
 _x000D_
{code:java}_x000D_
Try<Nothing> create = cgroups::create(_x000D_
hierarchy,_x000D_
infos[containerId]->cgroup);_x000D_
{code}_x000D_
to_x000D_
{code:java}_x000D_
Try<Nothing> create = cgroups::create(_x000D_
hierarchy,_x000D_
infos[containerId]->cgroup,_x000D_
true);_x000D_
{code}_x000D_
in CgroupsIsolatorProcess::prepare in src/slave/containerizer/mesos/isolators/cgroups/cgroups.cpp._x000D_
_x000D_
However, I'm not sure if there's any potential problem doing so. Any advice?_x000D_
_x000D_
 ",3.0,"1.5.1,1.6.1,1.7.0",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.007142857142857143,0.01282051282051282,0.006060606060606061,0.006060606060606061,0.9725382262996942
Bug,Mesos fails to build on Fedora 28,"Trying to compile a fresh Mesos checkout on a Fedora 28 system with the following configuration flags:_x000D_
{noformat}_x000D_
../configure --enable-debug --enable-optimize --disable-java --disable-python --disable-libtool-wrappers --enable-ssl --enable-libevent --disable-werror_x000D_
{noformat}_x000D_
and the following compiler_x000D_
{noformat}_x000D_
[bevers@core1.hw.ca1 build]$ gcc --version_x000D_
gcc (GCC) 8.1.1 20180712 (Red Hat 8.1.1-5)_x000D_
Copyright (C) 2018 Free Software Foundation, Inc._x000D_
This is free software; see the source for copying conditions.  There is NO_x000D_
warranty; not even for MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE._x000D_
{noformat}_x000D_
fails the build due to two warnings (even though --disable-werror was passed):_x000D_
{noformat}_x000D_
make[4]: Entering directory '/home/bevers/mesos/build/3rdparty/grpc-1.10.0'_x000D_
[C]       Compiling third_party/cares/cares/ares_init.c_x000D_
third_party/cares/cares/ares_init.c: In function ‘ares_dup’:_x000D_
third_party/cares/cares/ares_init.c:301:17: error: argument to ‘sizeof’ in ‘strncpy’ call is the same expression as the source; did you mean to use the size of the destination? [-Werror=sizeof-pointer-memaccess]_x000D_
           sizeof(src->local_dev_name));_x000D_
                 ^_x000D_
third_party/cares/cares/ares_init.c: At top level:_x000D_
cc1: error: unrecognized command line option ‘-Wno-invalid-source-encoding’ [-Werror]_x000D_
cc1: all warnings being treated as errors_x000D_
make[4]: *** [Makefile:2635: /home/bevers/mesos/build/3rdparty/grpc-1.10.0/objs/opt/third_party/cares/cares/ares_init.o] Error 1_x000D_
{noformat}",2.0,0,0.0,0.0,0.3333333333333333,0.5,0.14285714285714285,0.0,0.2,0.5087719298245614,0.4357142857142857,0.16666666666666666,0.1818181818181818,0.17575757575757575,0.0
Improvement,Rejected quotas request error messages should specify which resources were overcommitted.,"If we reject a quota request due to not having enough available resources, we fail with the following error:_x000D_
{noformat}_x000D_
Not enough available cluster capacity to reasonably satisfy quota_x000D_
request; the force flag can be used to override this check_x000D_
{noformat}_x000D_
_x000D_
but we don't print *which* resource was not available. This can be confusing to operators when the quota was attempted to be set for multiple resources at once.",1.0,0,0.5,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.16666666666666666,0.1818181818181818,0.17575757575757575,0.0
Bug,Docker containerizer actor can get backlogged with large number of containers.,"We observed during some scale testing that we do internally._x000D_
_x000D_
When launching 300+ Docker containers on a single agent box, it's possible that the Docker containerizer actor gets backlogged. As a result, API processing like `GET_CONTAINERS` will become unresponsive. It'll also block Mesos containerizer from launching containers if one specified `--containers=docker,mesos` because Docker containerizer launch will be invoked first by the composing containerizer (and queued)._x000D_
_x000D_
Profiling results show that the bottleneck is `os::killtree`, which will be invoked when the Docker commands are discarded (e.g., client disconnect, etc.)._x000D_
_x000D_
For this particular case, killtree is not really necessary because the docker command does not fork additional subprocesses. If we use the argv version of `subprocess` to launch docker commands, we can simply use os::kill instead. We confirmed that, by switching to os::kill, the performance issues goes away, and the agent can easily scale up to 300+ containers.",3.0,"1.4.2,1.5.1,1.6.1,1.7.0",0.5,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.7948717948717948,1.0,1.0,0.9679816513761469
Task,Add an operation status update manager to the agent,Review here: https://reviews.apache.org/r/69505/,3.0,1.8.0,0.5,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.28205128205128205,0.2606060606060606,0.2606060606060606,0.9908256880733946
Bug,v1 JAVA scheduler library can drop TEARDOWN upon destruction.,"Currently the v1 JAVA scheduler library neither ensures {{Call}} s are sent to the master nor waits for responses. This can be problematic if the library is destroyed (or garbage collected) right after sending a {{TEARDOWN}} call: destruction of the underlying {{Mesos}} actor races with sending the call._x000D_
_x000D_
",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.717948717948718,0.8242424242424242,0.7212121212121212,0.0
Task,Get rid of dependency on `net-tools` in network/cni isolator.,"The `network/cni` isolator has a dependency on `net-tools`. The last release of `net-tools` was released in 2001. The tools were deprecated many years ago (see [Debian|https://lists.debian.org/debian-devel/2009/03/msg00780.html], [RH|https://bugzilla.redhat.com/show_bug.cgi?id=687920], and [LWN|https://lwn.net/Articles/710533/]) and no longer installed by default._x000D_
_x000D_
[https://github.com/apache/mesos/blob/983607e/src/slave/containerizer/mesos/isolators/network/cni/cni.cpp#L2248]",3.0,0,0.0,0.00904977375565611,0.0,0.0,0.0,0.0,0.0,0.0,0.014285714285714285,0.0,0.0,0.0,0.0
Bug,Github's mesos/modules does not build.,The examples modules repo at GitHub.com named mesos/modules does currently not build against the latest Apache Mesos. We should update that system. ,2.0,1.8.0,0.0,0.3740573152337858,0.0,0.0,0.0,0.2,0.4,0.43859649122807015,0.2714285714285714,0.3717948717948718,0.24848484848484845,0.24848484848484845,0.9908256880733946
Bug,Linking libevent should be avoided.,"{{libevent}} is a combination of {{libevent_core}} and {{libevent_extra}}. Mesos/libprocess does not make use of any {{libevent_extra}} functionality and additionally it appears that its use is against best practices._x000D_
_x000D_
From http://www.wangafu.net/~nickm/libevent-book/Ref0_meta.html:_x000D_
{noformat}_x000D_
libevent_x000D_
This library exists for historical reasons; it contains the contents of both libevent_core and libevent_extra._x000D_
You shouldn’t use it; it may go away in a future version of Libevent._x000D_
{noformat}_x000D_
_x000D_
We should adapt our linkage accordingly.",1.0,0,0.0,0.3740573152337858,0.0,0.0,0.0,0.2,0.4,0.43859649122807015,0.2714285714285714,0.3717948717948718,0.24848484848484845,0.24848484848484845,0.0
Bug,LongLivedDefaultExecutorRestart is flaky.,"{noformat}_x000D_
03:52:07  [ RUN      ] GarbageCollectorIntegrationTest.LongLivedDefaultExecutorRestart_x000D_
03:52:07  I0907 03:52:07.699676  2350 cluster.cpp:173] Creating default 'local' authorizer_x000D_
03:52:07  I0907 03:52:07.700664  2374 master.cpp:413] Master 8e9d97f6-4dc4-490b-81f6-d2033e2109d3 (ip-172-16-10-27.ec2.internal) started on 172.16.10.27:45074_x000D_
03:52:07  I0907 03:52:07.700690  2374 master.cpp:416] Flags at startup: --acls="""" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""1secs"" --allocator=""hierarchical"" --authenticate_agents=""true"" --authenticate_frameworks=""true"" --authenticate_http_frameworks=""true"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""true"" --authentication_v0_timeout=""15secs"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/cuUPYo/credentials"" --filter_gpu_resources=""true"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_unreachable_tasks_per_framework=""1000"" --memory_profiling=""false"" --min_allocatable_resources=""cpus:0.01|mem:32"" --port=""5050"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""in_memory"" --registry_fetch_timeout=""1mins"" --registry_gc_interval=""15mins"" --registry_max_agent_age=""2weeks"" --registry_max_agent_count=""102400"" --registry_store_timeout=""100secs"" --registry_strict=""false"" --require_agent_domain=""false"" --role_sorter=""drf"" --root_submissions=""true"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/cuUPYo/master"" --zk_session_timeout=""10secs""_x000D_
03:52:07  I0907 03:52:07.700857  2374 master.cpp:465] Master only allowing authenticated frameworks to register_x000D_
03:52:07  I0907 03:52:07.700870  2374 master.cpp:471] Master only allowing authenticated agents to register_x000D_
03:52:07  I0907 03:52:07.700947  2374 master.cpp:477] Master only allowing authenticated HTTP frameworks to register_x000D_
03:52:07  I0907 03:52:07.700958  2374 credentials.hpp:37] Loading credentials for authentication from '/tmp/cuUPYo/credentials'_x000D_
03:52:07  I0907 03:52:07.701068  2374 master.cpp:521] Using default 'crammd5' authenticator_x000D_
03:52:07  I0907 03:52:07.701151  2374 http.cpp:1037] Creating default 'basic' HTTP authenticator for realm 'mesos-master-readonly'_x000D_
03:52:07  I0907 03:52:07.701254  2374 http.cpp:1037] Creating default 'basic' HTTP authenticator for realm 'mesos-master-readwrite'_x000D_
03:52:07  I0907 03:52:07.701352  2374 http.cpp:1037] Creating default 'basic' HTTP authenticator for realm 'mesos-master-scheduler'_x000D_
03:52:07  I0907 03:52:07.701445  2374 master.cpp:602] Authorization enabled_x000D_
03:52:07  I0907 03:52:07.701566  2370 whitelist_watcher.cpp:77] No whitelist given_x000D_
03:52:07  I0907 03:52:07.701695  2376 hierarchical.cpp:182] Initialized hierarchical allocator process_x000D_
03:52:07  I0907 03:52:07.702237  2374 master.cpp:2083] Elected as the leading master!_x000D_
03:52:07  I0907 03:52:07.702255  2374 master.cpp:1638] Recovering from registrar_x000D_
03:52:07  I0907 03:52:07.702293  2375 registrar.cpp:339] Recovering registrar_x000D_
03:52:07  I0907 03:52:07.706190  2375 registrar.cpp:383] Successfully fetched the registry (0B) in 3.884032ms_x000D_
03:52:07  I0907 03:52:07.706233  2375 registrar.cpp:487] Applied 1 operations in 7967ns; attempting to update the registry_x000D_
03:52:07  I0907 03:52:07.706378  2375 registrar.cpp:544] Successfully updated the registry in 126976ns_x000D_
03:52:07  I0907 03:52:07.706413  2375 registrar.cpp:416] Successfully recovered registrar_x000D_
03:52:07  I0907 03:52:07.706507  2375 master.cpp:1752] Recovered 0 agents from the registry (172B); allowing 10mins for agents to reregister_x000D_
03:52:07  I0907 03:52:07.706548  2375 hierarchical.cpp:220] Skipping recovery of hierarchical allocator: nothing to recover_x000D_
03:52:07  W0907 03:52:07.708107  2350 process.cpp:2810] Attempted to spawn already running process files@172.16.10.27:45074_x000D_
03:52:07  I0907 03:52:07.708500  2350 containerizer.cpp:305] Using isolation { environment_secret, posix/cpu, posix/mem, filesystem/posix, network/cni }_x000D_
03:52:07  I0907 03:52:07.710343  2350 linux_launcher.cpp:144] Using /sys/fs/cgroup/freezer as the freezer hierarchy for the Linux launcher_x000D_
03:52:07  I0907 03:52:07.710748  2350 provisioner.cpp:298] Using default backend 'overlay'_x000D_
03:52:07  I0907 03:52:07.711236  2350 cluster.cpp:485] Creating default 'local' authorizer_x000D_
03:52:07  I0907 03:52:07.711629  2376 slave.cpp:267] Mesos agent started on (90)@172.16.10.27:45074_x000D_
03:52:07  I0907 03:52:07.711647  2376 slave.cpp:268] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_thYmJB/store/appc"" --authenticate_http_executors=""true"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""true"" --authenticatee=""crammd5"" --authentication_backoff_factor=""1secs"" --authentication_timeout_max=""1mins"" --authentication_timeout_min=""5secs"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_destroy_timeout=""1mins"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_thYmJB/credential"" --default_role=""*"" --disallow_sharing_agent_pid_namespace=""false"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_thYmJB/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_reregistration_timeout=""2secs"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_thYmJB/fetch"" --fetcher_cache_size=""2GB"" --fetcher_stall_timeout=""1mins"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --gc_non_executor_container_sandboxes=""true"" --help=""false"" --hostname_lookup=""true"" --http_command_executor=""false"" --http_credentials=""/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_thYmJB/http_credentials"" --http_heartbeat_interval=""30secs"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --jwt_secret_key=""/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_thYmJB/jwt_secret_key"" --launcher=""linux"" --launcher_dir=""/home/ubuntu/workspace/mesos/Mesos_CI-build/FLAG/SSL/label/mesos-ec2-ubuntu-16.04/mesos/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_executors_per_framework=""150"" --memory_profiling=""false"" --network_cni_metrics=""true"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --port=""5051"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --reconfiguration_policy=""equal"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --runtime_dir=""/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_thYmJB"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv"" --zk_session_timeout=""10secs""_x000D_
03:52:07  I0907 03:52:07.711815  2376 credentials.hpp:86] Loading credential for authentication from '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_thYmJB/credential'_x000D_
03:52:07  I0907 03:52:07.711861  2376 slave.cpp:300] Agent using credential for: test-principal_x000D_
03:52:07  I0907 03:52:07.711872  2376 credentials.hpp:37] Loading credentials for authentication from '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_thYmJB/http_credentials'_x000D_
03:52:07  I0907 03:52:07.711936  2376 http.cpp:1037] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-executor'_x000D_
03:52:07  I0907 03:52:07.711989  2376 http.cpp:1058] Creating default 'jwt' HTTP authenticator for realm 'mesos-agent-executor'_x000D_
03:52:07  I0907 03:52:07.712060  2376 http.cpp:1037] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-readonly'_x000D_
03:52:07  I0907 03:52:07.712100  2376 http.cpp:1058] Creating default 'jwt' HTTP authenticator for realm 'mesos-agent-readonly'_x000D_
03:52:07  I0907 03:52:07.712137  2376 http.cpp:1037] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-readwrite'_x000D_
03:52:07  I0907 03:52:07.712158  2376 http.cpp:1058] Creating default 'jwt' HTTP authenticator for realm 'mesos-agent-readwrite'_x000D_
03:52:07  I0907 03:52:07.712229  2376 disk_profile_adaptor.cpp:80] Creating default disk profile adaptor module_x000D_
03:52:07  I0907 03:52:07.712790  2376 slave.cpp:615] Agent resources: [{""name"":""cpus"",""scalar"":{""value"":2.0},""type"":""SCALAR""},{""name"":""mem"",""scalar"":{""value"":1024.0},""type"":""SCALAR""},{""name"":""disk"",""scalar"":{""value"":1024.0},""type"":""SCALAR""},{""name"":""ports"",""ranges"":{""range"":[{""begin"":31000,""end"":32000}]},""type"":""RANGES""}]_x000D_
03:52:07  I0907 03:52:07.712839  2376 slave.cpp:623] Agent attributes: [  ]_x000D_
03:52:07  I0907 03:52:07.712848  2376 slave.cpp:632] Agent hostname: ip-172-16-10-27.ec2.internal_x000D_
03:52:07  I0907 03:52:07.712952  2370 task_status_update_manager.cpp:181] Pausing sending task status updates_x000D_
03:52:07  I0907 03:52:07.713099  2350 scheduler.cpp:189] Version: 1.8.0_x000D_
03:52:07  I0907 03:52:07.713196  2376 state.cpp:66] Recovering state from '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/meta'_x000D_
03:52:07  I0907 03:52:07.713269  2373 slave.cpp:6909] Finished recovering checkpointed state from '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/meta', beginning agent recovery_x000D_
03:52:07  I0907 03:52:07.713306  2373 task_status_update_manager.cpp:207] Recovering task status update manager_x000D_
03:52:07  I0907 03:52:07.713399  2373 containerizer.cpp:727] Recovering Mesos containers_x000D_
03:52:07  I0907 03:52:07.713490  2373 linux_launcher.cpp:286] Recovering Linux launcher_x000D_
03:52:07  I0907 03:52:07.713629  2373 containerizer.cpp:1053] Recovering isolators_x000D_
03:52:07  I0907 03:52:07.713811  2374 scheduler.cpp:355] Using default 'basic' HTTP authenticatee_x000D_
03:52:07  I0907 03:52:07.713838  2372 containerizer.cpp:1092] Recovering provisioner_x000D_
03:52:07  I0907 03:52:07.713896  2374 scheduler.cpp:538] New master detected at master@172.16.10.27:45074_x000D_
03:52:07  I0907 03:52:07.713909  2374 scheduler.cpp:547] Waiting for 0ns before initiating a re-(connection) attempt with the master_x000D_
03:52:07  I0907 03:52:07.713941  2372 provisioner.cpp:494] Provisioner recovery complete_x000D_
03:52:07  I0907 03:52:07.714123  2371 composing.cpp:339] Finished recovering all containerizers_x000D_
03:52:07  I0907 03:52:07.714171  2371 slave.cpp:7138] Recovering executors_x000D_
03:52:07  I0907 03:52:07.714198  2371 slave.cpp:7291] Finished recovery_x000D_
03:52:07  I0907 03:52:07.714467  2371 slave.cpp:1254] New master detected at master@172.16.10.27:45074_x000D_
03:52:07  I0907 03:52:07.714491  2371 slave.cpp:1319] Detecting new master_x000D_
03:52:07  I0907 03:52:07.714519  2371 task_status_update_manager.cpp:181] Pausing sending task status updates_x000D_
03:52:07  I0907 03:52:07.714651  2373 scheduler.cpp:429] Connected with the master at http://172.16.10.27:45074/master/api/v1/scheduler_x000D_
03:52:07  I0907 03:52:07.714872  2373 scheduler.cpp:248] Sending SUBSCRIBE call to http://172.16.10.27:45074/master/api/v1/scheduler_x000D_
03:52:07  I0907 03:52:07.715217  2376 process.cpp:3569] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler'_x000D_
03:52:07  I0907 03:52:07.715551  2374 http.cpp:1177] HTTP POST for /master/api/v1/scheduler from 172.16.10.27:41612_x000D_
03:52:07  I0907 03:52:07.715626  2374 master.cpp:2502] Received subscription request for HTTP framework 'default'_x000D_
03:52:07  I0907 03:52:07.715656  2374 master.cpp:2155] Authorizing framework principal 'test-principal' to receive offers for roles '{ * }'_x000D_
03:52:07  I0907 03:52:07.715811  2372 master.cpp:2637] Subscribing framework 'default' with checkpointing enabled and capabilities [ MULTI_ROLE, RESERVATION_REFINEMENT ]_x000D_
03:52:07  I0907 03:52:07.716225  2372 master.cpp:9883] Adding framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 (default) with roles {  } suppressed_x000D_
03:52:07  I0907 03:52:07.716414  2374 hierarchical.cpp:306] Added framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000_x000D_
03:52:07  I0907 03:52:07.716454  2374 hierarchical.cpp:1564] Performed allocation for 0 agents in 6701ns_x000D_
03:52:07  I0907 03:52:07.716715  2375 scheduler.cpp:845] Enqueuing event SUBSCRIBED received from http://172.16.10.27:45074/master/api/v1/scheduler_x000D_
03:52:07  I0907 03:52:07.716856  2373 scheduler.cpp:845] Enqueuing event HEARTBEAT received from http://172.16.10.27:45074/master/api/v1/scheduler_x000D_
03:52:07  I0907 03:52:07.719863  2373 slave.cpp:1346] Authenticating with master master@172.16.10.27:45074_x000D_
03:52:07  I0907 03:52:07.719892  2373 slave.cpp:1355] Using default CRAM-MD5 authenticatee_x000D_
03:52:07  I0907 03:52:07.719959  2375 authenticatee.cpp:121] Creating new client SASL connection_x000D_
03:52:07  I0907 03:52:07.720403  2375 master.cpp:9653] Authenticating slave(90)@172.16.10.27:45074_x000D_
03:52:07  I0907 03:52:07.720453  2375 authenticator.cpp:414] Starting authentication session for crammd5-authenticatee(200)@172.16.10.27:45074_x000D_
03:52:07  I0907 03:52:07.720512  2375 authenticator.cpp:98] Creating new server SASL connection_x000D_
03:52:07  I0907 03:52:07.720927  2375 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5_x000D_
03:52:07  I0907 03:52:07.720947  2375 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'_x000D_
03:52:07  I0907 03:52:07.720979  2375 authenticator.cpp:204] Received SASL authentication start_x000D_
03:52:07  I0907 03:52:07.721015  2375 authenticator.cpp:326] Authentication requires more steps_x000D_
03:52:07  I0907 03:52:07.721052  2375 authenticatee.cpp:259] Received SASL authentication step_x000D_
03:52:07  I0907 03:52:07.721097  2375 authenticator.cpp:232] Received SASL authentication step_x000D_
03:52:07  I0907 03:52:07.721117  2375 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'ip-172-16-10-27.ec2.internal' server FQDN: 'ip-172-16-10-27.ec2.internal' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false _x000D_
03:52:07  I0907 03:52:07.721127  2375 auxprop.cpp:181] Looking up auxiliary property '*userPassword'_x000D_
03:52:07  I0907 03:52:07.721137  2375 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'_x000D_
03:52:07  I0907 03:52:07.721144  2375 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'ip-172-16-10-27.ec2.internal' server FQDN: 'ip-172-16-10-27.ec2.internal' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true _x000D_
03:52:07  I0907 03:52:07.721151  2375 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true_x000D_
03:52:07  I0907 03:52:07.721158  2375 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true_x000D_
03:52:07  I0907 03:52:07.721169  2375 authenticator.cpp:318] Authentication success_x000D_
03:52:07  I0907 03:52:07.721215  2370 authenticatee.cpp:299] Authentication success_x000D_
03:52:07  I0907 03:52:07.721282  2370 slave.cpp:1446] Successfully authenticated with master master@172.16.10.27:45074_x000D_
03:52:07  I0907 03:52:07.721366  2370 slave.cpp:1877] Will retry registration in 14.537761ms if necessary_x000D_
03:52:07  I0907 03:52:07.721418  2375 master.cpp:9685] Successfully authenticated principal 'test-principal' at slave(90)@172.16.10.27:45074_x000D_
03:52:07  I0907 03:52:07.721427  2374 authenticator.cpp:432] Authentication session cleanup for crammd5-authenticatee(200)@172.16.10.27:45074_x000D_
03:52:07  I0907 03:52:07.721469  2375 master.cpp:6605] Received register agent message from slave(90)@172.16.10.27:45074 (ip-172-16-10-27.ec2.internal)_x000D_
03:52:07  I0907 03:52:07.721518  2375 master.cpp:3964] Authorizing agent providing resources 'cpus:2; mem:1024; disk:1024; ports:[31000-32000]' with principal 'test-principal'_x000D_
03:52:07  I0907 03:52:07.721632  2373 master.cpp:6672] Authorized registration of agent at slave(90)@172.16.10.27:45074 (ip-172-16-10-27.ec2.internal)_x000D_
03:52:07  I0907 03:52:07.721666  2373 master.cpp:6787] Registering agent at slave(90)@172.16.10.27:45074 (ip-172-16-10-27.ec2.internal) with id 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0_x000D_
03:52:07  I0907 03:52:07.721783  2373 registrar.cpp:487] Applied 1 operations in 28796ns; attempting to update the registry_x000D_
03:52:07  I0907 03:52:07.721932  2370 registrar.cpp:544] Successfully updated the registry in 124928ns_x000D_
03:52:07  I0907 03:52:07.721992  2370 master.cpp:6835] Admitted agent 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0 at slave(90)@172.16.10.27:45074 (ip-172-16-10-27.ec2.internal)_x000D_
03:52:07  I0907 03:52:07.722132  2370 master.cpp:6880] Registered agent 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0 at slave(90)@172.16.10.27:45074 (ip-172-16-10-27.ec2.internal) with cpus:2; mem:1024; disk:1024; ports:[31000-32000]_x000D_
03:52:07  I0907 03:52:07.722203  2374 hierarchical.cpp:601] Added agent 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0 (ip-172-16-10-27.ec2.internal) with cpus:2; mem:1024; disk:1024; ports:[31000-32000] (allocated: {})_x000D_
03:52:07  I0907 03:52:07.722205  2370 slave.cpp:1479] Registered with master master@172.16.10.27:45074; given agent ID 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0_x000D_
03:52:07  I0907 03:52:07.722396  2370 slave.cpp:1499] Checkpointing SlaveInfo to '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/meta/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/slave.info'_x000D_
03:52:07  I0907 03:52:07.722447  2374 hierarchical.cpp:1564] Performed allocation for 1 agents in 173395ns_x000D_
03:52:07  I0907 03:52:07.722482  2374 task_status_update_manager.cpp:188] Resuming sending task status updates_x000D_
03:52:07  I0907 03:52:07.722573  2374 master.cpp:9468] Sending offers [ 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-O0 ] to framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 (default)_x000D_
03:52:07  I0907 03:52:07.722651  2370 slave.cpp:1548] Forwarding agent update {""operations"":{},""resource_version_uuid"":{""value"":""V1oIFlhHRv2xxYxsF9hCkQ==""},""slave_id"":{""value"":""8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0""},""update_oversubscribed_resources"":false}_x000D_
03:52:07  I0907 03:52:07.722790  2374 master.cpp:7939] Ignoring update on agent 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0 at slave(90)@172.16.10.27:45074 (ip-172-16-10-27.ec2.internal) as it reports no changes_x000D_
03:52:07  I0907 03:52:07.723096  2375 scheduler.cpp:845] Enqueuing event OFFERS received from http://172.16.10.27:45074/master/api/v1/scheduler_x000D_
03:52:07  I0907 03:52:07.723731  2369 scheduler.cpp:248] Sending ACCEPT call to http://172.16.10.27:45074/master/api/v1/scheduler_x000D_
03:52:07  I0907 03:52:07.724097  2371 process.cpp:3569] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler'_x000D_
03:52:07  I0907 03:52:07.724397  2372 http.cpp:1177] HTTP POST for /master/api/v1/scheduler from 172.16.10.27:41610_x000D_
03:52:07  I0907 03:52:07.724557  2372 master.cpp:11462] Removing offer 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-O0_x000D_
03:52:07  I0907 03:52:07.724704  2372 master.cpp:4467] Processing ACCEPT call for offers: [ 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-O0 ] on agent 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0 at slave(90)@172.16.10.27:45074 (ip-172-16-10-27.ec2.internal) for framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 (default)_x000D_
03:52:07  I0907 03:52:07.724750  2372 master.cpp:3541] Authorizing framework principal 'test-principal' to launch task 2e8c13b6-fa45-4e3c-89cd-398a5abc192f_x000D_
03:52:07  I0907 03:52:07.724932  2372 master.cpp:3541] Authorizing framework principal 'test-principal' to launch task c6c81339-65c6-4f86-b0ab-c5be60ea5fbd_x000D_
03:52:07  I0907 03:52:07.725594  2373 master.cpp:12209] Adding task 2e8c13b6-fa45-4e3c-89cd-398a5abc192f with resources cpus(allocated: *):0.1; mem(allocated: *):32; disk(allocated: *):32 on agent 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0 at slave(90)@172.16.10.27:45074 (ip-172-16-10-27.ec2.internal)_x000D_
03:52:07  I0907 03:52:07.725653  2373 master.cpp:5663] Launching task group { 2e8c13b6-fa45-4e3c-89cd-398a5abc192f } of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 (default) with resources cpus(allocated: *):0.1; mem(allocated: *):32; disk(allocated: *):32 on agent 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0 at slave(90)@172.16.10.27:45074 (ip-172-16-10-27.ec2.internal) on  new executor_x000D_
03:52:07  I0907 03:52:07.725885  2374 slave.cpp:2014] Got assigned task group containing tasks [ 2e8c13b6-fa45-4e3c-89cd-398a5abc192f ] for framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000_x000D_
03:52:07  I0907 03:52:07.725953  2374 slave.cpp:8908] Checkpointing FrameworkInfo to '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/meta/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/framework.info'_x000D_
03:52:07  I0907 03:52:07.726119  2373 master.cpp:12209] Adding task c6c81339-65c6-4f86-b0ab-c5be60ea5fbd with resources cpus(allocated: *):0.1; mem(allocated: *):32; disk(allocated: *):32 on agent 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0 at slave(90)@172.16.10.27:45074 (ip-172-16-10-27.ec2.internal)_x000D_
03:52:07  I0907 03:52:07.726174  2373 master.cpp:5663] Launching task group { c6c81339-65c6-4f86-b0ab-c5be60ea5fbd } of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 (default) with resources cpus(allocated: *):0.1; mem(allocated: *):32; disk(allocated: *):32 on agent 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0 at slave(90)@172.16.10.27:45074 (ip-172-16-10-27.ec2.internal) on  existing executor_x000D_
03:52:07  I0907 03:52:07.726406  2372 hierarchical.cpp:1236] Recovered cpus(allocated: *):1.7; mem(allocated: *):928; disk(allocated: *):928; ports(allocated: *):[31000-32000] (total: cpus:2; mem:1024; disk:1024; ports:[31000-32000], allocated: cpus(allocated: *):0.3; mem(allocated: *):96; disk(allocated: *):96) on agent 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0 from framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000_x000D_
03:52:07  I0907 03:52:07.726444  2372 hierarchical.cpp:1282] Framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 filtered agent 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0 for 5secs_x000D_
03:52:07  I0907 03:52:07.726583  2374 slave.cpp:8919] Checkpointing framework pid '@0.0.0.0:0' to '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/meta/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/framework.pid'_x000D_
03:52:07  I0907 03:52:07.727038  2374 slave.cpp:2014] Got assigned task group containing tasks [ c6c81339-65c6-4f86-b0ab-c5be60ea5fbd ] for framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000_x000D_
03:52:07  I0907 03:52:07.727283  2374 slave.cpp:2388] Authorizing task group containing tasks [ 2e8c13b6-fa45-4e3c-89cd-398a5abc192f ] for framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000_x000D_
03:52:07  I0907 03:52:07.727318  2374 slave.cpp:8466] Authorizing framework principal 'test-principal' to launch task 2e8c13b6-fa45-4e3c-89cd-398a5abc192f_x000D_
03:52:07  I0907 03:52:07.727609  2374 slave.cpp:2388] Authorizing task group containing tasks [ c6c81339-65c6-4f86-b0ab-c5be60ea5fbd ] for framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000_x000D_
03:52:07  I0907 03:52:07.727641  2374 slave.cpp:8466] Authorizing framework principal 'test-principal' to launch task c6c81339-65c6-4f86-b0ab-c5be60ea5fbd_x000D_
03:52:07  I0907 03:52:07.728116  2374 slave.cpp:2831] Launching task group containing tasks [ 2e8c13b6-fa45-4e3c-89cd-398a5abc192f ] for framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000_x000D_
03:52:07  I0907 03:52:07.728165  2374 paths.cpp:752] Creating sandbox '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/executors/default/runs/dbe02af2-3122-4f2e-9747-0c4343627c2f' for user 'root'_x000D_
03:52:07  I0907 03:52:07.728693  2374 slave.cpp:9694] Checkpointing ExecutorInfo to '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/meta/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/executors/default/executor.info'_x000D_
03:52:07  I0907 03:52:07.728947  2374 paths.cpp:755] Creating sandbox '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/meta/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/executors/default/runs/dbe02af2-3122-4f2e-9747-0c4343627c2f'_x000D_
03:52:07  I0907 03:52:07.729185  2374 slave.cpp:8994] Launching executor 'default' of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000 with resources [{""allocation_info"":{""role"":""*""},""name"":""cpus"",""scalar"":{""value"":0.1},""type"":""SCALAR""},{""allocation_info"":{""role"":""*""},""name"":""mem"",""scalar"":{""value"":32.0},""type"":""SCALAR""},{""allocation_info"":{""role"":""*""},""name"":""disk"",""scalar"":{""value"":32.0},""type"":""SCALAR""}] in work directory '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/executors/default/runs/dbe02af2-3122-4f2e-9747-0c4343627c2f'_x000D_
03:52:07  I0907 03:52:07.729526  2374 slave.cpp:9725] Checkpointing TaskInfo to '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/meta/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/executors/default/runs/dbe02af2-3122-4f2e-9747-0c4343627c2f/tasks/2e8c13b6-fa45-4e3c-89cd-398a5abc192f/task.info'_x000D_
03:52:07  I0907 03:52:07.731062  2374 slave.cpp:3028] Queued task group containing tasks [ 2e8c13b6-fa45-4e3c-89cd-398a5abc192f ] for executor 'default' of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000_x000D_
03:52:07  I0907 03:52:07.731325  2374 slave.cpp:988] Successfully attached '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/executors/default/runs/dbe02af2-3122-4f2e-9747-0c4343627c2f' to virtual path '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/executors/default/runs/latest'_x000D_
03:52:07  I0907 03:52:07.731355  2374 slave.cpp:988] Successfully attached '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/executors/default/runs/dbe02af2-3122-4f2e-9747-0c4343627c2f' to virtual path '/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/executors/default/runs/latest'_x000D_
03:52:07  I0907 03:52:07.731370  2374 slave.cpp:988] Successfully attached '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/executors/default/runs/dbe02af2-3122-4f2e-9747-0c4343627c2f' to virtual path '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/executors/default/runs/dbe02af2-3122-4f2e-9747-0c4343627c2f'_x000D_
03:52:07  I0907 03:52:07.731529  2374 slave.cpp:3509] Launching container dbe02af2-3122-4f2e-9747-0c4343627c2f for executor 'default' of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000_x000D_
03:52:07  I0907 03:52:07.731864  2374 slave.cpp:2831] Launching task group containing tasks [ c6c81339-65c6-4f86-b0ab-c5be60ea5fbd ] for framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000_x000D_
03:52:07  I0907 03:52:07.731918  2374 slave.cpp:9725] Checkpointing TaskInfo to '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/meta/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/executors/default/runs/dbe02af2-3122-4f2e-9747-0c4343627c2f/tasks/c6c81339-65c6-4f86-b0ab-c5be60ea5fbd/task.info'_x000D_
03:52:07  I0907 03:52:07.733171  2374 slave.cpp:3028] Queued task group containing tasks [ c6c81339-65c6-4f86-b0ab-c5be60ea5fbd ] for executor 'default' of framework 8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000_x000D_
03:52:07  I0907 03:52:07.733458  2373 containerizer.cpp:1280] Starting container dbe02af2-3122-4f2e-9747-0c4343627c2f_x000D_
03:52:07  I0907 03:52:07.733788  2373 containerizer.cpp:1446] Checkpointed ContainerConfig at '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_thYmJB/containers/dbe02af2-3122-4f2e-9747-0c4343627c2f/config'_x000D_
03:52:07  I0907 03:52:07.733808  2373 containerizer.cpp:3118] Transitioning the state of container dbe02af2-3122-4f2e-9747-0c4343627c2f from PROVISIONING to PREPARING_x000D_
03:52:07  I0907 03:52:07.734777  2369 containerizer.cpp:1939] Launching 'mesos-containerizer' with flags '--help=""false"" --launch_info=""{""command"":{""arguments"":[""mesos-default-executor"",""--launcher_dir=/home/ubuntu/workspace/mesos/Mesos_CI-build/FLAG/SSL/label/mesos-ec2-ubuntu-16.04/mesos/build/src""],""shell"":false,""value"":""/home/ubuntu/workspace/mesos/Mesos_CI-build/FLAG/SSL/label/mesos-ec2-ubuntu-16.04/mesos/build/src/mesos-default-executor""},""environment"":{""variables"":[{""name"":""LIBPROCESS_PORT"",""type"":""VALUE"",""value"":""0""},{""name"":""MESOS_AGENT_ENDPOINT"",""type"":""VALUE"",""value"":""172.16.10.27:45074""},{""name"":""MESOS_CHECKPOINT"",""type"":""VALUE"",""value"":""1""},{""name"":""MESOS_DIRECTORY"",""type"":""VALUE"",""value"":""/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/executors/default/runs/dbe02af2-3122-4f2e-9747-0c4343627c2f""},{""name"":""MESOS_EXECUTOR_AUTHENTICATION_TOKEN"",""type"":""VALUE"",""value"":""eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJjaWQiOiJkYmUwMmFmMi0zMTIyLTRmMmUtOTc0Ny0wYzQzNDM2MjdjMmYiLCJlaWQiOiJkZWZhdWx0IiwiZmlkIjoiOGU5ZDk3ZjYtNGRjNC00OTBiLTgxZjYtZDIwMzNlMjEwOWQzLTAwMDAifQ.Ww__Iwo_c3fJl_ruqYdi_EePl81IKoIQJv74nq6pHl8""},{""name"":""MESOS_EXECUTOR_ID"",""type"":""VALUE"",""value"":""default""},{""name"":""MESOS_EXECUTOR_SHUTDOWN_GRACE_PERIOD"",""type"":""VALUE"",""value"":""5secs""},{""name"":""MESOS_FRAMEWORK_ID"",""type"":""VALUE"",""value"":""8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000""},{""name"":""MESOS_HTTP_COMMAND_EXECUTOR"",""type"":""VALUE"",""value"":""0""},{""name"":""MESOS_RECOVERY_TIMEOUT"",""type"":""VALUE"",""value"":""15mins""},{""name"":""MESOS_SLAVE_ID"",""type"":""VALUE"",""value"":""8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0""},{""name"":""MESOS_SLAVE_PID"",""type"":""VALUE"",""value"":""slave(90)@172.16.10.27:45074""},{""name"":""MESOS_SUBSCRIPTION_BACKOFF_MAX"",""type"":""VALUE"",""value"":""2secs""},{""name"":""MESOS_SANDBOX"",""type"":""VALUE"",""value"":""/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/executors/default/runs/dbe02af2-3122-4f2e-9747-0c4343627c2f""}]},""task_environment"":{},""user"":""root"",""working_directory"":""/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/executors/default/runs/dbe02af2-3122-4f2e-9747-0c4343627c2f""}"" --pipe_read=""15"" --pipe_write=""18"" --runtime_directory=""/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_thYmJB/containers/dbe02af2-3122-4f2e-9747-0c4343627c2f"" --unshare_namespace_mnt=""false""'_x000D_
03:52:07  I0907 03:52:07.734957  2376 linux_launcher.cpp:492] Launching container dbe02af2-3122-4f2e-9747-0c4343627c2f and cloning with namespaces _x000D_
03:52:07  I0907 03:52:07.754221  2369 containerizer.cpp:2044] Checkpointing container's forked pid 10086 to '/tmp/GarbageCollectorIntegrationTest_LongLivedDefaultExecutorRestart_X4h6lv/meta/slaves/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-S0/frameworks/8e9d97f6-4dc4-490b-81f6-d2033e2109d3-0000/executors/default/runs/dbe02af2-3122-4f2e-9747-0c4343627c2f/pids/forked.pid'_x000D_
03:52:07  I0907 03:52:07.754623  2369 containerizer.cpp:3118] Trans",2.0,"1.7.0,1.8.0",0.5,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.3717948717948718,0.24848484848484845,0.24848484848484845,0.9862385321100918
Improvement,Avoid double copying of master->framework messages when incrementing metrics.,"When incrementing metrics, we currently do stuff like_x000D_
{code}_x000D_
metrics.incrementEvent(devolve(evolve(message)));_x000D_
{code}_x000D_
which is not efficient. We should update such callsites to avoid gratuitous conversions which could degrade performance when many events are being sent.",1.0,1.7.0,0.5,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.981651376146789
Bug,Mesos v1 scheduler library does not properly handle SUBSCRIBE retries,"After the authentication related refactor done as part of [https://reviews.apache.org/r/62594/], the state of the scheduler is checked in `send` ([https://github.com/apache/mesos/blob/master/src/scheduler/scheduler.cpp#L234)]  but it is changed in `_send` ([https://github.com/apache/mesos/blob/master/src/scheduler/scheduler.cpp#L234).] As a result, we can have 2 SUBSCRIBE calls in flight at the same time on the same connection! This is not good and not spec compliant of a HTTP client that is expecting a streaming response._x000D_
_x000D_
We need to fix the library to either drop the retried SUBSCRIBE call if one is in progress (as it was before the refactor) or close the old connection and start a new connection to send the retried SUBSCRIBE call._x000D_
_x000D_
 _x000D_
_x000D_
 _x000D_
_x000D_
 ",3.0,"1.5.1,1.6.1,1.7.0",0.5,0.3740573152337858,0.0,0.0,0.0,0.2,0.4,0.43859649122807015,0.2714285714285714,0.8846153846153846,0.9878787878787878,0.9878787878787878,0.9725382262996942
Documentation,Add docs for UPDATE_OPERATION_STATUS event,We need to add the {{UPDATE_OPERATION_STATUS}} event to the docs for the v1 scheduler API event stream.,1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Improvement,Extend request batching to '/roles' endpoint,"For consistency and improved performance under load, the `/roles` endpoint should use the same request batching mechanism as `/state`, '/tasks`, ...",3.0,0,0.0,0.0,0.3333333333333333,0.5,0.14285714285714285,0.0,0.2,0.5087719298245614,0.4357142857142857,0.16666666666666666,0.1818181818181818,0.17575757575757575,0.0
Bug,Failed to build Mesos with Python 3.7 and new CLI enabled,"I've tried to build Mesos with the flag 'enable-new-cli' and Python 3.7 and it failed with this error message:_x000D_
{code:java}_x000D_
Traceback (most recent call last):_x000D_
File ""/Users/mesosphere/code/mesos/src/python/cli_new/bin/../tests/main.py"", line 26, in <module>_x000D_
from cli.tests import CLITestCase_x000D_
File ""/Users/mesosphere/code/mesos/src/python/cli_new/lib/cli/__init__.py"", line 24, in <module>_x000D_
from . import util_x000D_
File ""/Users/mesosphere/code/mesos/src/python/cli_new/lib/cli/util.py"", line 29, in <module>_x000D_
from kazoo.client import KazooClient_x000D_
File ""/Users/mesosphere/code/mesos/build/src/.virtualenv/lib/python3.7/site-packages/kazoo/client.py"", line 67, in <module>_x000D_
from kazoo.recipe.partitioner import SetPartitioner_x000D_
File ""/Users/mesosphere/code/mesos/build/src/.virtualenv/lib/python3.7/site-packages/kazoo/recipe/partitioner.py"", line 194_x000D_
self._child_watching(self._allocate_transition, async=True)_x000D_
^_x000D_
SyntaxError: invalid syntax_x000D_
make[3]: *** [check-local] Error 1_x000D_
make[2]: *** [check-am] Error 2_x000D_
make[1]: *** [check] Error 2_x000D_
make: *** [check-recursive] Error 1_x000D_
{code}",2.0,0,0.5,0.25037707390648567,0.3333333333333333,0.5,0.14285714285714285,0.5,0.0,0.2631578947368421,0.29285714285714287,0.01282051282051282,0.006060606060606061,0.006060606060606061,0.0
Bug,./support/python3/mesos-gtest-runner.py --help crashes,"{noformat}_x000D_
$ ./support/python3/mesos-gtest-runner.py --help_x000D_
Traceback (most recent call last):_x000D_
  File ""./support/python3/mesos-gtest-runner.py"", line 196, in <module>_x000D_
    EXECUTABLE, OPTIONS = parse_arguments()_x000D_
  File ""./support/python3/mesos-gtest-runner.py"", line 108, in parse_arguments_x000D_
    .format(default_=DEFAULT_NUM_JOBS))_x000D_
  File ""/usr/lib64/python3.5/argparse.py"", line 1335, in add_argument_x000D_
    raise ValueError('%r is not callable' % (type_func,))_x000D_
ValueError: 'int' is not callable_x000D_
{noformat}",1.0,0,0.0,0.25037707390648567,0.3333333333333333,0.5,0.14285714285714285,0.5,0.0,0.2631578947368421,0.29285714285714287,0.10256410256410256,0.1515151515151515,0.1515151515151515,0.0
Bug,Mesos master segfaults when responding to /state requests.,"{noformat}_x000D_
 *** SIGSEGV (@0x8) received by PID 66991 (TID 0x7f36792b7700) from PID 8; stack trace: ***_x000D_
 @     0x7f367e7226d0 (unknown)_x000D_
 @     0x7f3681266913 _ZZNK5mesos8internal6master19FullFrameworkWriterclEPN4JSON12ObjectWriterEENKUlPNS3_11ArrayWriterEE1_clES7__x000D_
 @     0x7f3681266af0 _ZNSt17_Function_handlerIFvPN9rapidjson6WriterINS0_19GenericStringBufferINS0_4UTF8IcEENS0_12CrtAllocatorEEES4_S4_S5_Lj0EEEEZN4JSON8internal7jsonifyIZNK5mesos8internal6master19FullFrameworkWriterclEPNSA_12ObjectWriterEEUlPNSA_11ArrayWriterEE1_vEESt8functionIS9_ERKT_NSB_6PreferEEUlS8_E_E9_M_invokeERKSt9_Any_dataS8__x000D_
 @     0x7f36812882d0 mesos::internal::master::FullFrameworkWriter::operator()()_x000D_
 @     0x7f36812889d0 _ZNSt17_Function_handlerIFvPN9rapidjson6WriterINS0_19GenericStringBufferINS0_4UTF8IcEENS0_12CrtAllocatorEEES4_S4_S5_Lj0EEEEZN4JSON8internal7jsonifyIN5mesos8internal6master19FullFrameworkWriterEvEESt8functionIS9_ERKT_NSB_6PreferEEUlS8_E_E9_M_invokeERKSt9_Any_dataS8__x000D_
 @     0x7f368121aef0 _ZNSt17_Function_handlerIFvPN9rapidjson6WriterINS0_19GenericStringBufferINS0_4UTF8IcEENS0_12CrtAllocatorEEES4_S4_S5_Lj0EEEEZN4JSON8internal7jsonifyIZZZN5mesos8internal6master6Master4Http25processStateRequestsBatchEvENKUlRKN7process4http7RequestERKNSI_5OwnedINSD_15ObjectApproversEEEE_clESM_SR_ENKUlPNSA_12ObjectWriterEE_clESU_EUlPNSA_11ArrayWriterEE3_vEESt8functionIS9_ERKT_NSB_6PreferEEUlS8_E_E9_M_invokeERKSt9_Any_dataS8__x000D_
 @     0x7f3681241be3 _ZZZN5mesos8internal6master6Master4Http25processStateRequestsBatchEvENKUlRKN7process4http7RequestERKNS4_5OwnedINS_15ObjectApproversEEEE_clES8_SD_ENKUlPN4JSON12ObjectWriterEE_clESH__x000D_
 @     0x7f3681242760 _ZNSt17_Function_handlerIFvPN9rapidjson6WriterINS0_19GenericStringBufferINS0_4UTF8IcEENS0_12CrtAllocatorEEES4_S4_S5_Lj0EEEEZN4JSON8internal7jsonifyIZZN5mesos8internal6master6Master4Http25processStateRequestsBatchEvENKUlRKN7process4http7RequestERKNSI_5OwnedINSD_15ObjectApproversEEEE_clESM_SR_EUlPNSA_12ObjectWriterEE_vEESt8functionIS9_ERKT_NSB_6PreferEEUlS8_E_E9_M_invokeERKSt9_Any_dataS8__x000D_
 @     0x7f36810a41bb _ZNO4JSON5ProxycvSsEv_x000D_
 @     0x7f368215f60e process::http::OK::OK()_x000D_
 @     0x7f3681219061 _ZN7process20AsyncExecutorProcess7executeIZN5mesos8internal6master6Master4Http25processStateRequestsBatchEvEUlRKNS_4http7RequestERKNS_5OwnedINS2_15ObjectApproversEEEE_S8_SD_Li0EEENSt9result_ofIFT_T0_T1_EE4typeERKSI_SJ_SK__x000D_
 @     0x7f36812212c0 _ZZN7process8dispatchINS_4http8ResponseENS_20AsyncExecutorProcessERKZN5mesos8internal6master6Master4Http25processStateRequestsBatchEvEUlRKNS1_7RequestERKNS_5OwnedINS4_15ObjectApproversEEEE_S9_SE_SJ_RS9_RSE_EENS_6FutureIT_EERKNS_3PIDIT0_EEMSQ_FSN_T1_T2_T3_EOT4_OT5_OT6_ENKUlSt10unique_ptrINS_7PromiseIS2_EESt14default_deleteIS17_EEOSH_OS9_OSE_PNS_11ProcessBaseEE_clES1A_S1B_S1C_S1D_S1F__x000D_
 @     0x7f36812215ac _ZNO6lambda12CallableOnceIFvPN7process11ProcessBaseEEE10CallableFnINS_8internal7PartialIZNS1_8dispatchINS1_4http8ResponseENS1_20AsyncExecutorProcessERKZN5mesos8internal6master6Master4Http25processStateRequestsBatchEvEUlRKNSA_7RequestERKNS1_5OwnedINSD_15ObjectApproversEEEE_SI_SN_SS_RSI_RSN_EENS1_6FutureIT_EERKNS1_3PIDIT0_EEMSZ_FSW_T1_T2_T3_EOT4_OT5_OT6_EUlSt10unique_ptrINS1_7PromiseISB_EESt14default_deleteIS1G_EEOSQ_OSI_OSN_S3_E_IS1J_SQ_SI_SN_St12_PlaceholderILi1EEEEEEclEOS3__x000D_
 @     0x7f36821f3541 process::ProcessBase::consume()_x000D_
 @     0x7f3682209fbc process::ProcessManager::resume()_x000D_
 @     0x7f368220fa76 _ZNSt6thread5_ImplISt12_Bind_simpleIFZN7process14ProcessManager12init_threadsEvEUlvE_vEEE6_M_runEv_x000D_
 @     0x7f367eefc2b0 (unknown)_x000D_
 @     0x7f367e71ae25 start_thread_x000D_
 @     0x7f367e444bad __clone_x000D_
{noformat}",3.0,1.7.0,0.5,0.0,0.3333333333333333,0.5,0.14285714285714285,0.0,0.2,0.5087719298245614,0.4357142857142857,0.717948717948718,0.8242424242424242,0.7212121212121212,0.981651376146789
Bug,Zookeeper doesn't compile with newer gcc due to format error,RR: https://reviews.apache.org/r/68370/,2.0,0,0.0,0.3453996983408748,0.0,0.0,0.0,0.0,0.2,0.38596491228070173,0.35714285714285715,0.28205128205128205,0.29090909090909095,0.28484848484848485,0.0
Bug,Failed to compile gRPC when the build path contains symlinks.,"{noformat}_x000D_
make[4]: *** No rule to make target '/home/kapil/mesos/master/build/3rdparty/grpc-1.10.0/libs/opt/libgrpc++_unsecure.a'.  Stop.{noformat}_x000D_
Apparently, in GRPC makefile, it uses realpath (no symlinks) when computing the build directory, whereas, Mesos builds use `abspath` (doesn't resolve symlinks). So there is a target mismatch if any directory in your Mesos path is a symlink.",1.0,0,0.5,0.3453996983408748,0.0,0.0,0.0,0.0,0.2,0.38596491228070173,0.35714285714285715,0.28205128205128205,0.29090909090909095,0.7575757575757576,0.0
Bug,MasterTest.TaskStateMetrics is flaky,"Observed on Ubuntu 16.04, cmake build:_x000D_
{code}_x000D_
Mock function called more times than expected - returning directly._x000D_
    Function call: offers(0x7fffcf5518d0, @0x7f64d805d440 48-byte object <C0-D3 39-0C 65-7F 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 01-00 00-00 04-00 00-00 D0-A1 08-D8 64-7F 00-00>)_x000D_
         Expected: to be called once_x000D_
           Actual: called twice - over-saturated and active_x000D_
{code}_x000D_
_x000D_
Full log attached.",1.0,1.7.0,0.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.981651376146789
Bug,Agent and scheduler driver authentication retry backoff time could overflow.,"In the agent we have the following retry backoff calculation logic:_x000D_
_x000D_
https://github.com/apache/mesos/blob/874c752316b14055c0a5a7b67f97ccf912abcc3c/src/slave/slave.cpp#L1401-L1418_x000D_
_x000D_
{code:c++}_x000D_
    Duration backoff =_x000D_
      flags.authentication_backoff_factor * std::pow(2, failedAuthentications);_x000D_
{code}_x000D_
_x000D_
Since the `Duration` uses `int64_t` to hold nanosecond, if we set the `authentication_backoff_factor` to 1 second, we will overflow after 34 failed authentications (from second to nanosecond we lose 30 bits and 2^34 in the `pow()`)._x000D_
_x000D_
The effect is we do not backoff at all, we will just retry immediately after the 5s timeout:_x000D_
https://github.com/apache/mesos/blob/874c752316b14055c0a5a7b67f97ccf912abcc3c/src/master/master.cpp#L9615-L9619_x000D_
_x000D_
The scheduler driver also has the same issue._x000D_
_x000D_
We should also audit all the other backoff logic.",3.0,"1.5.1,1.6.1",0.0,0.0,0.6666666666666666,1.0,0.5714285714285714,0.0,0.0,0.5438596491228069,0.5357142857142857,0.3974358974358974,0.4,0.4,0.9679816513761467
Bug,Agent has a fragile burn-in 5s authentication timeout.,Agent has a fragile burn-in 5s authentication timeout. This also includes the trip to master and queueing delay on the master message queue. We should either increase this or make it configurable.,2.0,"1.4.1,1.5.1,1.6.1",0.0,0.0,0.6666666666666666,1.0,0.5714285714285714,0.0,0.0,0.5438596491228069,0.5357142857142857,0.3974358974358974,0.4,0.4,0.9633944954128442
Bug,Master authentication handling leads to request amplification.,"In `Master::authenticate(const UPID& from, const UPID& pid)`, we have:_x000D_
_x000D_
https://github.com/apache/mesos/blob/master/src/master/master.cpp#L9594_x000D_
_x000D_
{code:c++}_x000D_
  if (authenticating.contains(pid)) {_x000D_
    LOG(INFO) << ""Queuing up authentication request from "" << pid_x000D_
              << "" because authentication is still in progress"";_x000D_
_x000D_
    // Try to cancel the in progress authentication by discarding the_x000D_
    // future._x000D_
    authenticating[pid].discard();_x000D_
_x000D_
    // Retry after the current authenticator session finishes._x000D_
    authenticating[pid]_x000D_
      .onAny(defer(self(), &Self::authenticate, from, pid));_x000D_
_x000D_
    return;_x000D_
  }_x000D_
{code}_x000D_
_x000D_
Let's say the master is processing authentication request R1 (whose associating future is held in `authenticating`). Now it receives another request R2 from the same client (due to e.g. re-try), according to the above code logic, we will (1) discard R1; (2) enqueue R2 as a callback which will be triggered when R1 is discarded, and we will redo `:: authenticate` with R2._x000D_
_x000D_
Here the master assumes that R2 is the most current request. This is true in the above example. However, this assumption could easily break when auth requests come faster than they are discarded. If we have 3 requests (R1, R2, R3) in the event queue, then we could trigger `::authenticate` SIX times in total, once for R1, twice for R2 and three times for R3. This grows in quadratic to the number of enqueued requests and the master will be overwhelmed._x000D_
_x000D_
This issue couples with MESOS-9146 and MESOS-9145 makes the master authentication fragile and can easily be overwhlemed.",3.0,"1.4.1,1.5.1,1.6.1",0.5,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.3974358974358974,0.4,0.4,0.9633944954128442
Bug,MasterQuotaTest.RemoveSingleQuota is flaky.,"{noformat}_x000D_
../../src/tests/master_quota_tests.cpp:493_x000D_
Value of: metrics.at<JSON::Number>(metricKey).isNone()_x000D_
  Actual: false_x000D_
Expected: true_x000D_
{noformat}",2.0,0,0.5,0.0,0.6666666666666666,1.0,0.5714285714285714,0.0,0.0,0.5438596491228069,0.5357142857142857,0.717948717948718,0.8242424242424242,0.7212121212121212,0.0
Improvement,Port mapper CNI plugin should use '-n' option with 'iptables --list',"Without the {{-n}} option, [this iptables command|https://github.com/apache/mesos/blob/9457dce1d99b5616d1b5eeb9a344733f6320d7b5/src/slave/containerizer/mesos/isolators/network/cni/plugins/port_mapper/port_mapper.cpp#L313] could result in a large number of reverse hostname lookups, which could take a while._x000D_
_x000D_
We should use the -n option there to avoid this issue.",1.0,1.6.1,0.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.9725688073394495
Bug,"Port mapper CNI plugin might fail with ""Resource temporarily unavailable""","https://github.com/apache/mesos/blob/master/src/slave/containerizer/mesos/isolators/network/cni/plugins/port_mapper/port_mapper.cpp#L345_x000D_
_x000D_
Looks like we're missing a `-w` for the iptable command. This will lead to issues like_x000D_
{noformat}_x000D_
The CNI plugin '/opt/mesosphere/active/mesos/libexec/mesos/mesos-cni-port-mapper' failed to attach container a710dc89-7b22-493b-b8bb-fb80a99d5321 to CNI network 'mesos-bridge': stdout='{""cniVersion"":""0.3.0"",""code"":103,""msg"":""Failed to add DNAT rule with tag: Resource temporarily unavailable""}_x000D_
{noformat}_x000D_
_x000D_
This becomes more likely if there are many concurrent launches of Mesos contianers that uses port mapper on the box.",1.0,"1.4.1,1.5.1,1.6.1",0.5,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.7948717948717948,1.0,1.0,0.9633944954128442
Bug,Agent reconfiguration can cause master to unsuppress on scheduler's behalf.,"When agent reconfiguration was enabled in Mesos, the allocator was also updated to remove all offer filters associated with an agent when that agent's attributes change. In addition, whenever filters for an agent are removed, the framework is unsuppressed for any roles that had filters on the agent._x000D_
_x000D_
While this ensures that schedulers will have an opportunity to use resources on an agent after reconfiguration, modifying the scheduler's suppression may put the scheduler in an inconsistent state, where it believes it is suppressed in a particular role when it is not.",3.0,"1.5.3,1.6.2,1.7.2",0.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.9726911314984709
Improvement,Optimize range addition operation.,MESOS-9086 made range subtraction operation faster than addition which shows room for improvement for addition.,3.0,0,0.0,0.0,0.6666666666666666,1.0,0.5714285714285714,0.0,0.0,0.5438596491228069,0.5357142857142857,0.3974358974358974,0.4,0.4,0.0
Bug,mesos-style reports violations on a clean checkout,"When running {{support/mesos-style.py}} on a clean checkout of e.g., {{e879e920c35}} Python style violations are reported,_x000D_
_x000D_
{noformat}_x000D_
Checking 46 Python files_x000D_
Using config file /home/bbannier/src/mesos/support/pylint.config_x000D_
************* Module cli.plugins.base_x000D_
E:119, 0: Bad option value 'R0204' (bad-option-value)_x000D_
************* Module settings_x000D_
E: 32, 4: No name 'VERSION' in module 'version' (no-name-in-module)_x000D_
Using config file /home/bbannier/src/mesos/support/pylint.config_x000D_
************* Module mesos.http_x000D_
E: 25, 0: Unable to import 'urlparse' (import-error)_x000D_
E: 87,30: Undefined variable 'xrange' (undefined-variable)_x000D_
Using config file /home/bbannier/src/mesos/support/pylint.config_x000D_
************* Module apply-reviews_x000D_
R: 99, 0: Either all return statements in a function should return an expression, or none of them should. (inconsistent-return-statements)_x000D_
C:124, 9: Do not use `len(SEQUENCE)` to determine if a sequence is empty (len-as-condition)_x000D_
R:302, 4: Unnecessary ""else"" after ""return"" (no-else-return)_x000D_
************* Module generate-endpoint-help_x000D_
R:215, 4: Unnecessary ""else"" after ""return"" (no-else-return)_x000D_
************* Module verify-reviews_x000D_
C:261, 7: Do not use `len(SEQUENCE)` to determine if a sequence is empty (len-as-condition)_x000D_
Total errors found: 9_x000D_
{noformat}_x000D_
_x000D_
I would expect a clean checkout to not report any violations.",2.0,0,0.5,0.25037707390648567,0.3333333333333333,0.5,0.14285714285714285,0.5,0.0,0.2631578947368421,0.29285714285714287,1.0,0.8121212121212121,0.8121212121212121,0.0
Improvement,Add move support to the Resources / Resource_ wrappers.,"Currently, the Resources / Resource_ wrappers do not have move support. Since copying resources are rather expensive (especially when there's large ports ranges or metadata like reservations / labels), we should add move support to reduce copying automatically where possible:_x000D_
_x000D_
* Constructors for both Resources and Resource__x000D_
* +, +=, addition operations: with addition we sometimes need to add the resource to the vector in which case we can move if taking an rvalue",3.0,0,0.5,0.0,0.6666666666666666,1.0,0.5714285714285714,0.0,0.0,0.5438596491228069,0.5357142857142857,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.0
Improvement,Refactor capability related logic in the allocator.,"- Add a function for returning the subset of frameworks that are capable of receiving offers from the agent. This moves the capability checking out of the core allocation logic and means the loops can just iterate over a smaller set of framework candidates rather than having to write 'continue' cases. This covers the GPU_RESOURCES and REGION_AWARE capabilities._x000D_
_x000D_
- Similarly, add a function that allows framework capability based filtering of resources. This pulls out the filtering logic from the core allocation logic and instead the core allocation logic can just all out to the capability filtering function. This covers the SHARED_RESOURCES, REVOCABLE_RESOURCES and RESERVATION_REFINEMENT capabilities. Note that in order to implement this one, we must refactor the shared resources logic in order to have the resource generation occur regardless of the framework capability (followed by getting filtered out via this new function if the framework is not capable).",3.0,0,0.0,0.0,0.6666666666666666,1.0,0.5714285714285714,0.0,0.0,0.5438596491228069,0.5357142857142857,0.3974358974358974,0.4,0.4,0.0
Task,Add allocator quota tests regarding reserve/unreserve already allocated resources.,"Add allocator quota tests regarding reserve/unreserve already allocated resources:_x000D_
_x000D_
- Reserve already allocated resources should not affect quota headroom;_x000D_
- The same applies to unreserve allocated resources.",3.0,0,0.0,0.0,0.6666666666666666,1.0,0.5714285714285714,0.0,0.0,0.5438596491228069,0.5357142857142857,0.3974358974358974,0.4,0.4,0.0
Bug,On macOS libprocess_tests fail to link when compiling with gRPC,"Seems like this was introduces with commit {{a211b4cadf289168464fc50987255d883c226e89}}. Linking {{libprocess-tests}} on macOS with enabled gRPC fails with_x000D_
{noformat}_x000D_
Undefined symbols for architecture x86_64:_x000D_
  ""grpc::TimePoint<std::__1::chrono::time_point<std::__1::chrono::system_clock, std::__1::chrono::duration<long long, std::__1::ratio<1l, 1000000000l> > > >::you_need_a_specialization_of_TimePoint()"", referenced from:_x000D_
      process::Future<Try<tests::Pong, process::grpc::StatusError> > process::grpc::client::Runtime::call<std::__1::unique_ptr<grpc::ClientAsyncResponseReader<tests::Pong>, std::__1::default_delete<grpc::ClientAsyncResponseReader<tests::Pong> > > (tests::PingPong::Stub::*)(grpc::ClientContext*, tests::Ping const&, grpc::CompletionQueue*), tests::Ping, tests::Pong, 0>(process::grpc::client::Connection const&, std::__1::unique_ptr<grpc::ClientAsyncResponseReader<tests::Pong>, std::__1::default_delete<grpc::ClientAsyncResponseReader<tests::Pong> > > (tests::PingPong::Stub::*&&)(grpc::ClientContext*, tests::Ping const&, grpc::CompletionQueue*), tests::Ping&&, process::grpc::client::CallOptions const&)::'lambda'(tests::Ping const&, bool, grpc::CompletionQueue*)::operator()(tests::Ping const&, bool, grpc::CompletionQueue*) const in libprocess_tests-grpc_tests.o_x000D_
ld: symbol(s) not found for architecture x86_64_x000D_
clang-6.0: error: linker command failed with exit code 1 (use -v to see invocation)_x000D_
{noformat}",2.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.2631578947368421,0.36428571428571427,0.28205128205128205,0.296969696969697,0.296969696969697,0.0
Improvement,`createStrippedScalarQuantity()` should clear all metadata fields.,"Currently `createStrippedScalarQuantity()` strips resource meta-data and transforms dynamic reservations into a static reservation. However, no current code depends on the reservations in resources returned by this helper function. This leads to boilerplate code around call sites and performance overhead.",3.0,0,0.0,0.0,0.6666666666666666,1.0,0.5714285714285714,0.0,0.0,0.5438596491228069,0.5357142857142857,0.3974358974358974,0.4,0.4,0.0
Improvement,Optimize range subtraction operation.,"Based on the profiling result of MESOS-8989, the range subtraction operation is about 2~3 times more expensive than that of addition. It's not obvious that this has to be the case._x000D_
_x000D_
The current range subtraction implementation relies on boost IntervalSet, the construction cost of the IntervalSet could be the culprit:_x000D_
https://github.com/apache/mesos/blob/9147283171d761a4d38710f24ba654f8a96e325c/src/common/values.cpp#L378-L387_x000D_
_x000D_
I think we could do better by writing a one-pass (with sorting) algorithm like that of addition.",3.0,0,0.0,0.0,0.6666666666666666,1.0,0.5714285714285714,0.0,0.0,0.5438596491228069,0.5357142857142857,0.3974358974358974,0.4,0.4,0.0
Bug,Virtualenv management in support directory is buggy.,"When switching back and forth from Python 2 to 3, the virtualenv does not get correctly reinstalled.",2.0,1.7.0,0.0,0.25037707390648567,0.3333333333333333,0.5,0.14285714285714285,0.5,0.0,0.2631578947368421,0.29285714285714287,0.10256410256410256,0.1515151515151515,0.1515151515151515,0.981651376146789
Bug,Pylint is too noisy when using mesos-style.py,"{code}_x000D_
apache-mesos (MESOS-9073) $ git commit -m ""Test.""_x000D_
No C++ files to lint_x000D_
No JavaScript files to lint_x000D_
Checking 1 Python file_x000D_
Using config file /Users/Armand/Code/apache-mesos/support/pylint.config_x000D_
_x000D_
-------------------------------------------------------------------_x000D_
Your code has been rated at 10.00/10 (previous run: 9.22/10, +0.78)_x000D_
_x000D_
Total errors found: 0_x000D_
[MESOS-9073 a3509d402] Test._x000D_
 1 file changed, 1 insertion(+), 1 deletion(-)_x000D_
{code}_x000D_
The score needs to be removed.",1.0,0,0.0,0.25037707390648567,0.3333333333333333,0.5,0.14285714285714285,0.5,0.0,0.2631578947368421,0.29285714285714287,0.10256410256410256,0.1515151515151515,0.1515151515151515,0.0
Bug,mesos-style.py messaging is poor,"After running into all sorts of issues connected to the {{-c}} option, at some point I tried the {{-k}} option and got the following output;_x000D_
{noformat}_x000D_
➜ apache-mesos (MESOS-9056) ✔ python3 support/apply-reviews.py -k -r 66683_x000D_
2018-10-05 12:14:19 URL:https://reviews.apache.org/r/66683/diff/raw/ [1863/1863] -> ""66683.patch"" [1]_x000D_
No C++ files to lint_x000D_
No JavaScript files to lint_x000D_
Checking 2 Python files_x000D_
Total errors found: 0_x000D_
[MESOS-9056 5f8f48b7a] Updated address field of new CLI config to accept URLs._x000D_
Author: Armand Grillet <agrillet@mesosphere.io>_x000D_
2 files changed, 12 insertions(+), 2 deletions(-)_x000D_
{noformat}_x000D_
It appears that the messaging here is worth reconsidering;_x000D_
 - ""No C++ files to lint"", ""No JavaScript files to lint"", ""Total errors found: 0"" seems to have little value for the normal user - how about we make that a user activated verbose output?",1.0,"1.7.0,1.8.0",0.5,0.25037707390648567,0.3333333333333333,0.5,0.14285714285714285,0.5,0.0,0.2631578947368421,0.29285714285714287,0.3717948717948718,0.24848484848484845,0.24848484848484845,0.9862385321100918
Bug,Agent GC could unmount a dangling persistent volume multiple times.,"When the agent GC an executor dir and the sandbox of one of its run that contains a dangling persistent volume, the agent might try to unmount the persistent volume twice, which leads to an {{EINVAL}} when trying to unmount the target for the second time._x000D_
_x000D_
Here is the log from a failure run of {{GarbageCollectorIntegrationTest.ROOT_DanglingMount}}:_x000D_
{noformat}_x000D_
W0702 23:35:31.669946 25401 gc.cpp:241] Unmounting dangling mount point '/tmp/GarbageCollectorIntegrationTest_ROOT_DanglingMount_zkItvU/slaves/f4dc0941-e3b0-4f2c-b7f9-025a1af264c8-S0/frameworks/f4dc0941-e3b0-4f2c-b7f9-025a1af264c8-0000/executors/test-task123/runs/3fcde2c8-b461-4f22-afec-daa269291c95/dangling' of persistent volume '/tmp/GarbageCollectorIntegrationTest_ROOT_DanglingMount_zkItvU/volumes/roles/default-role/persistence-id' inside garbage collected path '/tmp/GarbageCollectorIntegrationTest_ROOT_DanglingMount_zkItvU/slaves/f4dc0941-e3b0-4f2c-b7f9-025a1af264c8-S0/frameworks/f4dc0941-e3b0-4f2c-b7f9-025a1af264c8-0000/executors/test-task123'_x000D_
W0702 23:35:31.683878 25401 gc.cpp:241] Unmounting dangling mount point '/tmp/GarbageCollectorIntegrationTest_ROOT_DanglingMount_zkItvU/slaves/f4dc0941-e3b0-4f2c-b7f9-025a1af264c8-S0/frameworks/f4dc0941-e3b0-4f2c-b7f9-025a1af264c8-0000/executors/test-task123/runs/3fcde2c8-b461-4f22-afec-daa269291c95/dangling' of persistent volume '/tmp/GarbageCollectorIntegrationTest_ROOT_DanglingMount_zkItvU/volumes/roles/default-role/persistence-id' inside garbage collected path '/tmp/GarbageCollectorIntegrationTest_ROOT_DanglingMount_zkItvU/slaves/f4dc0941-e3b0-4f2c-b7f9-025a1af264c8-S0/frameworks/f4dc0941-e3b0-4f2c-b7f9-025a1af264c8-0000'_x000D_
W0702 23:35:31.683912 25401 gc.cpp:248] Skipping deletion of '/tmp/GarbageCollectorIntegrationTest_ROOT_DanglingMount_zkItvU/slaves/f4dc0941-e3b0-4f2c-b7f9-025a1af264c8-S0/frameworks/f4dc0941-e3b0-4f2c-b7f9-025a1af264c8-0000' because unmount failed on '/tmp/GarbageCollectorIntegrationTest_ROOT_DanglingMount_zkItvU/slaves/f4dc0941-e3b0-4f2c-b7f9-025a1af264c8-S0/frameworks/f4dc0941-e3b0-4f2c-b7f9-025a1af264c8-0000/executors/test-task123/runs/3fcde2c8-b461-4f22-afec-daa269291c95/dangling': Failed to unmount '/tmp/GarbageCollectorIntegrationTest_ROOT_DanglingMount_zkItvU/slaves/f4dc0941-e3b0-4f2c-b7f9-025a1af264c8-S0/frameworks/f4dc0941-e3b0-4f2c-b7f9-025a1af264c8-0000/executors/test-task123/runs/3fcde2c8-b461-4f22-afec-daa269291c95/dangling': Invalid argument_x000D_
{noformat}",2.0,"1.4.2,1.5.2,1.6.1,1.7.0",0.5,0.0,0.0,0.0,0.0,0.0,0.4,0.07017543859649122,0.12142857142857143,0.7307692307692307,0.7515151515151515,0.7575757575757576,0.9680045871559633
Bug,Archiver utility extracts links within subdirectories incorrectly,"The fix for MESOS-9008 did not correctly fix extraction of links with the archiver utility._x000D_
_x000D_
Two problems:_x000D_
1) Links that were originally relative within the archiver are transformed to point to absolute locations after extraction.  The link targets should remain relative._x000D_
2) Links within subdirectories (i.e. path/to/link -> path/to/target) will lose part of the target path upon extraction (i.e. path/to/link -> target)",2.0,0,0.0,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.0
Bug,DefaultExecutorTest.SigkillExecutor is flaky,"Failed on windows CI but does not look like windows specific:_x000D_
_x000D_
{noformat}_x000D_
[ RUN      ] MesosContainerizer/DefaultExecutorTest.SigkillExecutor/0_x000D_
d:\dcos\mesos\mesos\src\tests\default_executor_tests.cpp(1621): error: Mock function called more times than expected - returning directly._x000D_
    Function call: offers(00000036FB4FB628, @0000015400EA7C10 48-byte object <D0-86 EB-06 F7-7F 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 01-00 00-00 04-00 00-00 A0-82 EA-00 54-01 00-00>)_x000D_
         Expected: to be called once_x000D_
           Actual: called twice - over-saturated and active_x000D_
[  FAILED  ] MesosContainerizer/DefaultExecutorTest.SigkillExecutor/0, where GetParam() = ""mesos"" (6214 ms)_x000D_
{noformat}_x000D_
_x000D_
{noformat}_x000D_
  Future<v1::scheduler::Event::Offers> offers;_x000D_
  EXPECT_CALL(*scheduler, offers(_, _))_x000D_
    .WillOnce(FutureArg<1>(&offers));_x000D_
{noformat}_x000D_
_x000D_
Subsequent offers calls should be ignored._x000D_
",1.0,0,0.0,0.0,0.6666666666666666,1.0,0.5714285714285714,0.0,0.0,0.5438596491228069,0.5357142857142857,0.3974358974358974,0.4,0.4,0.0
Bug,MasterAPITest.SubscribersReceiveHealthUpdates is flaky,"This test fails flaky on CI. Log attached._x000D_
",1.0,0,0.0,0.0,0.6666666666666666,1.0,0.5714285714285714,0.0,0.0,0.5438596491228069,0.5357142857142857,0.3974358974358974,0.4,0.4,0.0
Bug,Fetcher fails to extract some archives containing hardlinks,"We recently switched the infrastructure to e.g., extract archives to libarchive which seems to have narrower support for e.g., hardlinks in archives, see e.g., [https://github.com/libarchive/libarchive/wiki/Hardlinks] upstream (likely outdated)._x000D_
_x000D_
In a particular case, we tried to extract https://artifacts.elastic.co/downloads/kibana/kibana-5.6.5-linux-x86_64.tar.gz which the fetcher successfully extracted in e.g., 1.6.0, but which now leads to failures like_x000D_
{noformat}_x000D_
W0619 08:53:38.000000  4610 fetcher.cpp:913] Begin fetcher log (stderr in sandbox) for container d39bc16c-e6c6-440f-a82c-ad26332a1b36 from running command: /opt/mesosphere/packages/mesos--95f27ab971fb1d03bc1277f6a16e63a9815e1a61/libexec/mesos/mesos-fetcher_x000D_
I0619 08:53:34.620458  7571 fetcher.cpp:560] Fetcher Info: {""cache_directory"":""\/tmp\/mesos\/fetch\/nobody"",""items"":[{""action"":""DOWNLOAD_AND_CACHE"",""cache_filename"":""c29-kibana-5.6__64.tar.gz"",""uri"":{""cache"":true,""executable"":false,""extract"":true,""value"":""https:\/\/artifacts.elastic.co\/downloads\/kibana\/kibana-5.6.5-linux-x86_64.tar.gz""}},{""action"":""DOWNLOAD_AND_CACHE"",""cache_filename"":""c30-x-pack-5.6.5.zip"",""uri"":{""cache"":true,""executable"":false,""extract"":false,""value"":""https:\/\/artifacts.elastic.co\/downloads\/packs\/x-pack\/x-pack-5.6.5.zip""}}],""sandbox_directory"":""\/var\/lib\/mesos\/slave\/slaves\/7f2e186c-c748-493d-82e4-644b5b23c9bb-S6\/frameworks\/7f2e186c-c748-493d-82e4-644b5b23c9bb-0001\/executors\/kibana.3f6c7799-739e-11e8-a678-168288e5aa33\/runs\/d39bc16c-e6c6-440f-a82c-ad26332a1b36"",""stall_timeout"":{""nanoseconds"":60000000000},""user"":""nobody""}_x000D_
I0619 08:53:34.623996  7571 fetcher.cpp:457] Fetching URI 'https://artifacts.elastic.co/downloads/kibana/kibana-5.6.5-linux-x86_64.tar.gz'_x000D_
I0619 08:53:34.624017  7571 fetcher.cpp:431] Downloading into cache_x000D_
I0619 08:53:34.624027  7571 fetcher.cpp:225] Fetching URI 'https://artifacts.elastic.co/downloads/kibana/kibana-5.6.5-linux-x86_64.tar.gz'_x000D_
I0619 08:53:34.624043  7571 fetcher.cpp:175] Downloading resource from 'https://artifacts.elastic.co/downloads/kibana/kibana-5.6.5-linux-x86_64.tar.gz' to '/tmp/mesos/fetch/nobody/c29-kibana-5.6__64.tar.gz'_x000D_
I0619 08:53:38.634416  7571 fetcher.cpp:350] Fetching from cache_x000D_
E0619 08:53:38.686941  7571 fetcher.cpp:613] EXIT with status 1: Failed to fetch 'https://artifacts.elastic.co/downloads/kibana/kibana-5.6.5-linux-x86_64.tar.gz': Failed to extract archive '/tmp/mesos/fetch/nobody/c29-kibana-5.6__64.tar.gz' to '/var/lib/mesos/slave/slaves/7f2e186c-c748-493d-82e4-644b5b23c9bb-S6/frameworks/7f2e186c-c748-493d-82e4-644b5b23c9bb-0001/executors/kibana.3f6c7799-739e-11e8-a678-168288e5aa33/runs/d39bc16c-e6c6-440f-a82c-ad26332a1b36': Failed to write archive header: Hard-link target 'kibana-5.6.5-linux-x86_64/node_modules/svgo/node_modules/js-yaml/bin/js-yaml.js' does not exist._x000D_
_x000D_
End fetcher log for container d39bc16c-e6c6-440f-a82c-ad26332a1b36_x000D_
E0619 08:53:38.000000  4610 fetcher.cpp:571] Failed to run mesos-fetcher: Failed to fetch all URIs for container 'd39bc16c-e6c6-440f-a82c-ad26332a1b36': exited with status 1_x000D_
E0619 08:53:38.000000  4608 slave.cpp:6180] Container 'd39bc16c-e6c6-440f-a82c-ad26332a1b36' for executor 'kibana.3f6c7799-739e-11e8-a678-168288e5aa33' of framework 7f2e186c-c748-493d-82e4-644b5b23c9bb-0001 failed to start: Failed to fetch all URIs for container 'd39bc16c-e6c6-440f-a82c-ad26332a1b36': exited with status 1_x000D_
{noformat}",1.0,1.7.0,0.5,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,1.0,0.8121212121212121,0.8121212121212121,0.981651376146789
Bug,Operator API event stream can miss task status updates.,"As of now, the master only sends TaskUpdated messages to subscribers when the latest known task state on the agent changed:_x000D_
_x000D_
{noformat}_x000D_
  // src/master/master.cpp_x000D_
  if (!protobuf::isTerminalState(task->state())) {_x000D_
    if (status.state() != task->state()) {_x000D_
      sendSubscribersUpdate = true;_x000D_
    }_x000D_
_x000D_
    task->set_state(latestState.getOrElse(status.state()));_x000D_
  }_x000D_
{noformat}_x000D_
_x000D_
The latest state is set like this:_x000D_
_x000D_
{noformat}_x000D_
// src/messages/messages.proto_x000D_
message StatusUpdate {_x000D_
  [...]_x000D_
  // This corresponds to the latest state of the task according to the_x000D_
  // agent. Note that this state might be different than the state in_x000D_
  // 'status' because task status update manager queues updates. In_x000D_
  // other words, 'status' corresponds to the update at top of the_x000D_
  // queue and 'latest_state' corresponds to the update at bottom of_x000D_
  // the queue._x000D_
  optional TaskState latest_state = 7;_x000D_
}_x000D_
{noformat}_x000D_
_x000D_
However, the `TaskStatus` message included in an `TaskUpdated` event is the event at the bottom of the queue when the update was sent._x000D_
_x000D_
So we can easily get in a situation where e.g. the first TaskUpdated has .status.state == TASK_STARTING and .state == TASK_RUNNING, and the second update with .status.state == TASK_RUNNNING and .state == TASK_RUNNING would not get delivered because the latest known state did not change._x000D_
_x000D_
This implies that schedulers can not reliably wait for the status information corresponding to specific task state, since there is no guarantee that subscribers get notified during the time when this status update will be included in the status field.",3.0,0,0.0,0.0,0.3333333333333333,0.5,0.14285714285714285,0.0,0.2,0.5087719298245614,0.4357142857142857,0.16666666666666666,0.1818181818181818,0.17575757575757575,0.0
Improvement,Add default bodies for libprocess HTTP error responses.,"By default on error libprocess would only return a response_x000D_
with the correct status code and no response body._x000D_
_x000D_
However, most browsers do not visually indicate the response_x000D_
status code, so if any error occurs anyone using a browser will only_x000D_
see a blank page, making it hard to figure out what happened.",3.0,0,0.0,0.0,0.3333333333333333,0.5,0.14285714285714285,0.0,0.2,0.5087719298245614,0.4357142857142857,0.16666666666666666,0.1818181818181818,0.17575757575757575,0.0
Improvement,Allow for unbundled libevent in CMake builds to work around 2.1.x SSL issues.,"On macOS and Ubuntu 17, libevent-openssl >= 2.1.x is broken in conjunction with libprocess._x000D_
_x000D_
We tried to pinpoint the issue but so far with no success. For enabling CMake SSL builds on those systems , we have to support prior libevent versions. Allowing building against a preinstalled libevent version paves that way.",2.0,0,0.0,0.3740573152337858,0.0,0.0,0.0,0.2,0.4,0.43859649122807015,0.2714285714285714,0.3717948717948718,0.24848484848484845,0.24848484848484845,0.0
Improvement,Add a better benchmark for range type resources.,"While investigating the performance issue with the `available()` function in the allocator, the arithmetic of range type resources, namely ports, appear to be most expensive. And the subtraction proves to be significantly more expensive than the addition whose cause is not immediately clear to me._x000D_
_x000D_
While we do have a benchmark `[Resources_Scalar_Arithmetic_BENCHMARK_Test|https://github.com/apache/mesos/blob/1b851877ff22cf2da193bb649dc838bb50c26b34/src/tests/resources_tests.cpp#L3610-L3626]` that contains a range parametrized test, its result is somewhat misleading. It evaluates the addition performance by repeatedly summing up the same port resources which is fine. However, when evaluating the subtraction, it took the result from the addition and repeatedly subtracting the same port resources. The result is that after the first subtraction, the subtrahend becomes empty and all the subsequent subtractions are shorthanded, producing uninstructive result.",2.0,0,0.0,0.0,0.6666666666666666,1.0,0.5714285714285714,0.0,0.0,0.5438596491228069,0.5357142857142857,0.3974358974358974,0.4,0.4,0.0
Bug,Posting to the operator api with 'accept recordio' header can crash the agent,"It's possible to crash the mesos agent by posting a reasonable request to the operator API._x000D_
h3. Background:_x000D_
_x000D_
Sending a request to the v1 api endpoint with an unsupported 'accept' header:_x000D_
{code:java}_x000D_
curl -X POST http://10.0.3.27:5051/api/v1 \_x000D_
  -H 'accept: application/atom+xml' \_x000D_
  -H 'content-type: application/json' \_x000D_
  -d '{""type"":""GET_CONTAINERS"",""get_containers"":{""show_nested"": true,""show_standalone"": true}}'{code}_x000D_
Results in the following friendly error message:_x000D_
{code:java}_x000D_
Expecting 'Accept' to allow application/json or application/x-protobuf or application/recordio{code}_x000D_
h3. Reproducible crash:_x000D_
_x000D_
However, sending the same request with 'application/recordio' 'accept' header:_x000D_
{code:java}_x000D_
curl -X POST \_x000D_
http://10.0.3.27:5051/api/v1 \_x000D_
  -H 'accept: application/recordio' \_x000D_
  -H 'content-type: application/json' \_x000D_
  -d '{""type"":""GET_CONTAINERS"",""get_containers"":{""show_nested"": true,""show_standalone"": true}}'{code}_x000D_
causes the agent to crash (no response is received)._x000D_
_x000D_
Crash log is shown below, full log from the agent is attached here:_x000D_
{code:java}_x000D_
Jun 07 22:30:32 ip-10-0-3-27.us-west-2.compute.internal mesos-agent[3718]: I0607 22:30:32.397320 3743 logfmt.cpp:178] type=audit timestamp=2018-06-07 22:30:32.397243904+00:00 reason=""Error in token 'Missing 'Authorization' header from HTTP request'. Allowing anonymous connection"" object=""/slave(1)/api/v1"" agent=""Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36"" authorizer=""mesos-agent"" action=""POST"" result=allow srcip=10.0.6.99 dstport=5051 srcport=42084 dstip=10.0.3.27_x000D_
Jun 07 22:30:32 ip-10-0-3-27.us-west-2.compute.internal mesos-agent[3718]: W0607 22:30:32.397434 3743 authenticator.cpp:289] Error in token on request from '10.0.6.99:42084': Missing 'Authorization' header from HTTP request_x000D_
Jun 07 22:30:32 ip-10-0-3-27.us-west-2.compute.internal mesos-agent[3718]: W0607 22:30:32.397466 3743 authenticator.cpp:291] Falling back to anonymous connection using user 'dcos_anonymous'_x000D_
Jun 07 22:30:32 ip-10-0-3-27.us-west-2.compute.internal mesos-agent[3718]: I0607 22:30:32.397629 3748 http.cpp:1099] HTTP POST for /slave(1)/api/v1 from 10.0.6.99:42084 with User-Agent='Mozilla/5.0 (Macintosh; Intel Mac OS X 10_13_5) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/66.0.3359.181 Safari/537.36'_x000D_
Jun 07 22:30:32 ip-10-0-3-27.us-west-2.compute.internal mesos-agent[3718]: I0607 22:30:32.397784 3748 http.cpp:2030] Processing GET_CONTAINERS call_x000D_
Jun 07 22:30:32 ip-10-0-3-27.us-west-2.compute.internal mesos-agent[3718]: F0607 22:30:32.398736 3747 http.cpp:121] Serializing a RecordIO stream is not supported_x000D_
Jun 07 22:30:32 ip-10-0-3-27.us-west-2.compute.internal mesos-agent[3718]: *** Check failure stack trace: ***_x000D_
Jun 07 22:30:32 ip-10-0-3-27.us-west-2.compute.internal mesos-agent[3718]: @ 0x7f619478636d google::LogMessage::Fail()_x000D_
Jun 07 22:30:32 ip-10-0-3-27.us-west-2.compute.internal mesos-agent[3718]: @ 0x7f619478819d google::LogMessage::SendToLog()_x000D_
Jun 07 22:30:32 ip-10-0-3-27.us-west-2.compute.internal mesos-agent[3718]: @ 0x7f6194785f5c google::LogMessage::Flush()_x000D_
Jun 07 22:30:32 ip-10-0-3-27.us-west-2.compute.internal mesos-agent[3718]: @ 0x7f6194788a99 google::LogMessageFatal::~LogMessageFatal()_x000D_
Jun 07 22:30:32 ip-10-0-3-27.us-west-2.compute.internal mesos-agent[3718]: @ 0x7f61935e2b9d mesos::internal::serialize()_x000D_
Jun 07 22:30:32 ip-10-0-3-27.us-west-2.compute.internal mesos-agent[3718]: @ 0x7f6193a4c0ef _ZNO6lambda12CallableOnceIFN7process6FutureINS1_4http8ResponseEEERKN4JSON5ArrayEEE10CallableFnIZNK5mesos8internal5slave4Http13getContainersERKNSD_5agent4CallENSD_11ContentTypeERK6OptionINS3_14authentication9PrincipalEEEUlRKNS2_IS7_EEE0_EclES9__x000D_
Jun 07 22:30:32 ip-10-0-3-27.us-west-2.compute.internal mesos-agent[3718]: @ 0x7f6193a81d61 process::internal::thenf<>()_x000D_
Jun 07 22:30:32 ip-10-0-3-27.us-west-2.compute.internal mesos-agent[3718]: @ 0x7f6193a59b15 _ZNO6lambda12CallableOnceIFvRKN7process6FutureIN4JSON5ArrayEEEEE10CallableFnINS_8internal7PartialIPFvONS0_IFNS2_INS1_4http8ResponseEEERKS4_EEESt10unique_ptrINS1_7PromiseISE_EESt14default_deleteISN_EES7_EJSJ_SQ_St12_PlaceholderILi1EEEEEEclES7__x000D_
Jun 07 22:30:32 ip-10-0-3-27.us-west-2.compute.internal mesos-agent[3718]: @ 0x7f6193a6e4e9 process::internal::run<>()_x000D_
Jun 07 22:30:32 ip-10-0-3-27.us-west-2.compute.internal mesos-agent[3718]: @ 0x7f6193a7fa28 process::Future<>::_set<>()_x000D_
Jun 07 22:30:32 ip-10-0-3-27.us-west-2.compute.internal mesos-agent[3718]: @ 0x7f6193a7f9fe process::Future<>::_set<>()_x000D_
Jun 07 22:30:32 ip-10-0-3-27.us-west-2.compute.internal mesos-agent[3718]: @ 0x7f6193a7f9fe process::Future<>::_set<>()_x000D_
Jun 07 22:30:32 ip-10-0-3-27.us-west-2.compute.internal mesos-agent[3718]: @ 0x7f6193a7f9fe process::Future<>::_set<>()_x000D_
Jun 07 22:30:32 ip-10-0-3-27.us-west-2.compute.internal mesos-agent[3718]: @ 0x7f6193a7f9fe process::Future<>::_set<>()_x000D_
Jun 07 22:30:32 ip-10-0-3-27.us-west-2.compute.internal mesos-agent[3718]: @ 0x7f6193a84e00 process::Future<>::onReady()_x000D_
Jun 07 22:30:32 ip-10-0-3-27.us-west-2.compute.internal mesos-agent[3718]: @ 0x7f6193a8509e process::Promise<>::associate()_x000D_
Jun 07 22:30:32 ip-10-0-3-27.us-west-2.compute.internal mesos-agent[3718]: @ 0x7f6193a856ac process::internal::thenf<>()_x000D_
Jun 07 22:30:32 ip-10-0-3-27.us-west-2.compute.internal mesos-agent[3718]: @ 0x7f6193a59935 _ZNO6lambda12CallableOnceIFvRKN7process6FutureISt5tupleIINS2_ISt4listINS2_IN5mesos15ContainerStatusEEESaIS7_EEEENS2_IS4_INS2_INS5_18ResourceStatisticsEEESaISC_EEEEEEEEEE10CallableFnINS_8internal7PartialIPFvONS0_IFNS2_IN4JSON5ArrayEEERKSG_EEESt10unique_ptrINS1_7PromiseISQ_EESt14default_deleteISZ_EESJ_EISV_S12_St12_PlaceholderILi1EEEEEEclESJ__x000D_
Jun 07 22:30:32 ip-10-0-3-27.us-west-2.compute.internal mesos-agent[3718]: @ 0x7f6193a81359 process::internal::run<>()_x000D_
Jun 07 22:30:32 ip-10-0-3-27.us-west-2.compute.internal mesos-agent[3718]: @ 0x7f6193a83f12 _ZN7process6FutureISt5tupleIJNS0_ISt4listINS0_IN5mesos15ContainerStatusEEESaIS5_EEEENS0_IS2_INS0_INS3_18ResourceStatisticsEEESaISA_EEEEEEE4_setIRKSE_EEbOT__x000D_
Jun 07 22:30:32 ip-10-0-3-27.us-west-2.compute.internal mesos-agent[3718]: @ 0x7f6193a85f10 _ZNK7process6FutureISt5tupleIJNS0_ISt4listINS0_IN5mesos15ContainerStatusEEESaIS5_EEEENS0_IS2_INS0_INS3_18ResourceStatisticsEEESaISA_EEEEEEE7onReadyEON6lambda12CallableOnceIFvRKSE_EEE_x000D_
Jun 07 22:30:32 ip-10-0-3-27.us-west-2.compute.internal mesos-agent[3718]: @ 0x7f6193a861ae process::Promise<>::associate()_x000D_
Jun 07 22:30:32 ip-10-0-3-27.us-west-2.compute.internal mesos-agent[3718]: @ 0x7f6193a866ac process::internal::thenf<>()_x000D_
Jun 07 22:30:32 ip-10-0-3-27.us-west-2.compute.internal mesos-agent[3718]: @ 0x7f6193a59875 _ZNO6lambda12CallableOnceIFvRKN7process6FutureISt4listINS2_I7NothingEESaIS5_EEEEEE10CallableFnINS_8internal7PartialIPFvONS0_IFNS2_ISt5tupleIINS2_IS3_INS2_IN5mesos15ContainerStatusEEESaISJ_EEEENS2_IS3_INS2_INSH_18ResourceStatisticsEEESaISO_EEEEEEEERKS7_EEESt10unique_ptrINS1_7PromiseISS_EESt14default_deleteIS11_EESA_EISX_S14_St12_PlaceholderILi1EEEEEEclESA__x000D_
Jun 07 22:30:32 ip-10-0-3-27.us-west-2.compute.internal mesos-agent[3718]: @ 0x7f61935c1a19 process::internal::run<>()_x000D_
Jun 07 22:30:32 ip-10-0-3-27.us-west-2.compute.internal mesos-agent[3718]: @ 0x7f61935cf25f process::Future<>::_set<>()_x000D_
Jun 07 22:30:32 ip-10-0-3-27.us-west-2.compute.internal mesos-agent[3718]: @ 0x7f61935cf44b process::internal::AwaitProcess<>::waited()_x000D_
Jun 07 22:30:32 ip-10-0-3-27.us-west-2.compute.internal mesos-agent[3718]: @ 0x7f61946d79d1 process::ProcessBase::consume()_x000D_
Jun 07 22:30:32 ip-10-0-3-27.us-west-2.compute.internal mesos-agent[3718]: @ 0x7f61946e8dcc process::ProcessManager::resume()_x000D_
Jun 07 22:30:32 ip-10-0-3-27.us-west-2.compute.internal mesos-agent[3718]: @ 0x7f61946ee7a6 _ZNSt6thread5_ImplISt12_Bind_simpleIFZN7process14ProcessManager12init_threadsEvEUlvE_vEEE6_M_runEv_x000D_
Jun 07 22:30:32 ip-10-0-3-27.us-west-2.compute.internal mesos-agent[3718]: @ 0x7f61918d8d73 (unknown)_x000D_
Jun 07 22:30:32 ip-10-0-3-27.us-west-2.compute.internal mesos-agent[3718]: @ 0x7f61913d952c (unknown)_x000D_
Jun 07 22:30:34 ip-10-0-3-27.us-west-2.compute.internal systemd[1]: dcos-mesos-slave.service: Main process exited, code=killed, status=6/ABRT_x000D_
Jun 07 22:30:34 ip-10-0-3-27.us-west-2.compute.internal systemd[1]: dcos-mesos-slave.service: Unit entered failed state._x000D_
Jun 07 22:30:34 ip-10-0-3-27.us-west-2.compute.internal systemd[1]: dcos-mesos-slave.service: Failed with result 'signal'._x000D_
Jun 07 22:30:39 ip-10-0-3-27.us-west-2.compute.internal systemd[1]: dcos-mesos-slave.service: Service hold-off time over, scheduling restart._x000D_
Jun 07 22:30:39 ip-10-0-3-27.us-west-2.compute.internal systemd[1]: Stopped Mesos Agent: distributed systems kernel agent.{code}",2.0,"1.4.1,1.5.1",0.5,0.0,0.3333333333333333,0.5,0.14285714285714285,0.0,0.2,0.5087719298245614,0.4357142857142857,0.01282051282051282,0.0,0.0,0.9588073394495413
Bug,Executor crash trying to print container ID.,"As observed in an internal cluster:_x000D_
_x000D_
{noformat}_x000D_
mesos-default-executor: /pkg/src/mesos/3rdparty/stout/include/stout/option.hpp:112: T& Option<T>::get() & [with T = mesos::ContainerID]: Assertion `isSome()' failed._x000D_
*** Aborted at 1527514147 (unix time) try ""date -d @1527514147"" if you are using GNU date ***_x000D_
PC: @     0x7f9fe3b5c1f7 (unknown)_x000D_
*** SIGABRT (@0x6300000005) received by PID 5 (TID 0x7f9fdfe8e700) from PID 5; stack trace: ***_x000D_
    @     0x7f9fe3ef95e0 (unknown)_x000D_
    @     0x7f9fe3b5c1f7 (unknown)_x000D_
    @     0x7f9fe3b5d8e8 (unknown)_x000D_
    @     0x7f9fe3b55266 (unknown)_x000D_
    @     0x7f9fe3b55312 (unknown)_x000D_
    @     0x7f9fe581b9b0 _ZNR6OptionIN5mesos11ContainerIDEE3getEv.part.134_x000D_
    @     0x7f9fe58a19f5 _ZZN5mesos8internal6checks14CheckerProcess18nestedCommandCheckEvENKUlRKN7process4http8ResponseEE0_clES7__x000D_
    @     0x7f9fe66a8edc process::ProcessManager::resume()_x000D_
    @     0x7f9fe66ae856 _ZNSt6thread5_ImplISt12_Bind_simpleIFZN7process14ProcessManager12init_threadsEvEUlvE_vEEE6_M_runEv_x000D_
    @     0x7f9fe46d32b0 (unknown)_x000D_
    @     0x7f9fe3ef1e25 (unknown)_x000D_
    @     0x7f9fe3c1f34d (unknown)_x000D_
{noformat}_x000D_
_x000D_
The issue is caused by not this block in CheckerProcess not checking that previousCheckContainerId is still some after it had yielded control:_x000D_
{noformat}_x000D_
// checker_process.cpp:649_x000D_
LOG(WARNING) << ""Connection to remove the nested container '""_x000D_
                              << previousCheckContainerId.get() << ""' used for the ""_x000D_
                              << name << "" for task '"" << taskId << ""' failed: ""_x000D_
                              << failure;_x000D_
{noformat}",3.0,"1.4.1,1.5.2,1.6.0,1.7.0",0.0,0.0,0.3333333333333333,0.5,0.14285714285714285,0.0,0.2,0.5087719298245614,0.4357142857142857,0.16666666666666666,0.1818181818181818,0.17575757575757575,0.9679587155963302
Task,Output of tasks gets corrupted if task defines the same environment variables as the executor container,"The issue is easily reproducible if one launches a task group and the taks nested container defines the same set of environment variables as the executor. In those circumstances, the following [snippet is activated|https://github.com/apache/mesos/blob/285d82080748cd69044c226950274c7046048c4b/src/slave/containerizer/mesos/launch.cpp#L1057]:_x000D_
_x000D_
{code}_x000D_
      if (environment.contains(name) && environment[name] != value) {_x000D_
        cout << ""Overwriting environment variable '"" << name << ""'"" << endl;_x000D_
      }_x000D_
{code}_x000D_
_x000D_
But this is not the only time that this file writes into {{cout}}._x000D_
_x000D_
This may be a bad idea because applications which consume the standard output of a task may end up being corrupted by the container manager output. In these cases, writing to {{cerr}} should be the right approach. ",1.0,1.6.0,0.0,0.004524886877828055,0.0,0.0,0.0,0.0,0.2,0.12280701754385964,0.24285714285714285,0.10256410256410256,0.11515151515151516,0.11515151515151516,0.9724770642201834
Bug,python3/post-reviews.py errors due to TypeError.,"{code:java}_x000D_
$ ./support/python3/post-reviews.py _x000D_
Running 'rbt post' across all of ..._x000D_
7a662ee75d706d637f9cc2bcbdd0567c32ccef56 - (HEAD -> alexr/subscribers-health, private/ci/alexr/default) Sent task (health) check updates over the operator streaming API. (2 minutes ago)_x000D_
Creating diff of:_x000D_
7a662ee75d706d637f9cc2bcbdd0567c32ccef56 - (HEAD -> alexr/subscribers-health, private/ci/alexr/default) Sent task (health) check updates over the operator streaming API._x000D_
Press enter to continue or 'Ctrl-C' to skip._x000D_
_x000D_
Traceback (most recent call last):_x000D_
File ""./support/python3/post-reviews.py"", line 432, in <module>_x000D_
main()_x000D_
File ""./support/python3/post-reviews.py"", line 365, in main_x000D_
sys.stdout.buffer.write(output)_x000D_
TypeError: a bytes-like object is required, not 'str'{code}_x000D_
The review still get posted.",1.0,0,0.0,0.25037707390648567,0.3333333333333333,0.5,0.14285714285714285,0.5,0.0,0.2631578947368421,0.29285714285714287,0.10256410256410256,0.1515151515151515,0.1515151515151515,0.0
Task,Master streaming API does not send (health) check updates for tasks.,"Currently, Master API subscribers get task status updates when task state changes (the actual logic is [slightly more complex|https://github.com/apache/mesos/blob/d7d7cfbc3e5609fc9a4e8de8203a6ecb11afeac7/src/master/master.cpp#L10794-L10841]). We use task status updates to deliver health and check information to schedulers, in which case task state does not change. Hence these updates are filtered out and the subscribers do not get any task health updates._x000D_
_x000D_
Here is a test that confirms the described behaviour: https://gist.github.com/rukletsov/c079d95479fb134d137ea3ae8b7ae874",2.0,"1.4.1,1.5.0,1.6.0",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.717948717948718,0.8242424242424242,0.7212121212121212,0.9633333333333333
Task,Per Framework Offer metrics with a specific resource type,"MESOS-8845 counts the number of offers sent to a framework but it would be useful to also know how many of these offers contained ""cpus"", how many contained ""mem"" etc. This is so that we can diagnose/debug offer starvation situations like MESOS-8935 better.",2.0,0,0.5,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.8846153846153846,0.9878787878787878,0.9878787878787878,0.0
Bug,"Quota limit ""chopping"" can lead to cpu-only and memory-only offers.","When we allocate resources to a role, we'll ""chop"" the available resources of the agent up to the quota limit for the role (per MESOS-7099). This prevents the role from exceeding its quota limit._x000D_
_x000D_
This has the unintended consequence of creating cpu-only and memory-only offers._x000D_
_x000D_
Consider agents with 10 cpus and 100 GB mem and roles with quota guarantee/limit of 5 cpus, 10 GB mem. The following allocations will occur:_x000D_
_x000D_
agent 1:_x000D_
 r1 -> 5 cpus 10GB mem_x000D_
 r2 -> 5 cpus 10GB mem_x000D_
 r3 -> 0 cpus 10GB mem (quota allocates even if it can make progress towards a single resource and MESOS-1688 allows this)_x000D_
 r4 -> 0 cpus 10GB mem_x000D_
 ..._x000D_
 r10 -> 0 cpus 10GB mem_x000D_
_x000D_
agent 2:_x000D_
 r3 -> 5 cpus 0GB mem (r3 is already at its 10GB mem limit)_x000D_
 r4 -> 5 cpus 0GB mem_x000D_
 r11 -> 0 cpus 10GB mem_x000D_
 ..._x000D_
 r20 -> 0 cpus 10GB mem_x000D_
_x000D_
Here, roles 3-20 receive memory only and cpu only offers. This gets further exacerbated if DRF chooses the same ordering between roles across cycles. ",3.0,"1.4.2,1.5.0,1.5.1,1.6.0",0.5,0.0,0.6666666666666666,1.0,0.5714285714285714,0.0,0.0,0.5438596491228069,0.5357142857142857,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.9633715596330276
Bug,Quota guarantee metric does not handle removal correctly.,"The quota guarantee metric is not removed when the quota gets removed:_x000D_
https://github.com/apache/mesos/blob/1.6.0/src/master/allocator/mesos/metrics.cpp#L165-L174_x000D_
_x000D_
The consequence of this is that the metric will hold the initial value that gets set and all subsequent removes / sets will not be exposed via the metric.",2.0,0,0.5,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.0
Bug,Autotools don't work with newer OpenJDK versions,"There are three distinct issues with modern Java and Linux versions:_x000D_
_x000D_
1. Mesos configure script expects `libjvm.so` at `$JAVA_HOME/jre/lib/<arch>/server/libjvm.so`, but in the newer openjdk versions, `libjvm.so` is found at `$JAVA_HOME/lib/server/libjvm.so`._x000D_
_x000D_
2. On some distros (e.g., Ubuntu 18.04), JAVA_HOME env var might be missing. In such cases, the configure is able to compute it by looking at `java` and `javac` paths and succeeds. However, some maven plugins require JAVA_HOME to be set and could fail if it's not found._x000D_
{code:java}_x000D_
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-javadoc-plugin:2.8.1:jar (build-and-attach-javadocs) on project mesos: MavenReportException: Error while creating archive: Unable to find javadoc command: The environment variable JAVA_HOME is not correctly set. -> [Help 1]_x000D_
{code}_x000D_
Because configure scripts generate an automake variable `JAVA_HOME`, we can simply invoke maven in the following way to fix this issue:_x000D_
{code:java}_x000D_
JAVA_HOME=$JAVA_HOME mvn ...{code}_x000D_
 These two behaviors were observed with OpenJDK 1.11 on Ubuntu 18.04 but I suspect that the behavior is present on other distros/OpenJDK versions._x000D_
_x000D_
3. `javah` has been removed as of OpenJDK 1.10. Instead `javac -h` is to be used as a replacement. See [http://openjdk.java.net/jeps/313] for more details.",3.0,0,0.0,0.3453996983408748,0.0,0.0,0.0,0.0,0.2,0.38596491228070173,0.35714285714285715,0.28205128205128205,0.29090909090909095,0.28484848484848485,0.0
Improvement,Add framework metrics benchmark test.,Add framework metrics benchmark test.,3.0,0,0.5,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6794871794871795,0.5939393939393939,0.5939393939393939,0.0
Bug,Docker image fetcher fails with HTTP/2.,"{noformat}_x000D_
[ RUN      ] ImageAlpine/ProvisionerDockerTest.ROOT_INTERNET_CURL_SimpleCommand/2_x000D_
..._x000D_
I0510 20:52:00.209815 25010 registry_puller.cpp:287] Pulling image 'quay.io/coreos/alpine-sh' from 'docker-manifest://quay.iocoreos/alpine-sh?latest#https' to '/tmp/ImageAlpine_ProvisionerDockerTest_ROOT_INTERNET_CURL_SimpleCommand_2_wF7EfM/store/docker/staging/qit1Jn'_x000D_
E0510 20:52:00.756072 25003 slave.cpp:6176] Container '5eb869c5-555c-4dc9-a6ce-ddc2e7dbd01a' for executor 'ad9aa898-026e-47d8-bac6-0ff993ec5904' of framework 7dbe7cd6-8ffe-4bcf-986a-17ba677b5a69-0000 failed to start: Failed to decode HTTP responses: Decoding failed_x000D_
HTTP/2 200_x000D_
server: nginx/1.13.12_x000D_
date: Fri, 11 May 2018 03:52:00 GMT_x000D_
content-type: application/vnd.docker.distribution.manifest.v1+prettyjws_x000D_
content-length: 4486_x000D_
docker-content-digest: sha256:61bd5317a92c3213cfe70e2b629098c51c50728ef48ff984ce929983889ed663_x000D_
x-frame-options: DENY_x000D_
strict-transport-security: max-age=63072000; preload_x000D_
..._x000D_
{noformat}_x000D_
_x000D_
Note that curl is saying the HTTP version is ""HTTP/2"". This happens on modern curl that automatically negotiates HTTP/2, but the docker fetcher isn't prepared to parse that._x000D_
_x000D_
{noformat}_x000D_
$ curl -i --raw -L -s -S -o -  'http://quay.io/coreos/alpine-sh?latest#https'_x000D_
HTTP/1.1 301 Moved Permanently_x000D_
Content-Type: text/html_x000D_
Date: Fri, 11 May 2018 04:07:44 GMT_x000D_
Location: https://quay.io/coreos/alpine-sh?latest_x000D_
Server: nginx/1.13.12_x000D_
Content-Length: 186_x000D_
Connection: keep-alive_x000D_
_x000D_
HTTP/2 301_x000D_
server: nginx/1.13.12_x000D_
date: Fri, 11 May 2018 04:07:45 GMT_x000D_
content-type: text/html; charset=utf-8_x000D_
content-length: 287_x000D_
location: https://quay.io/coreos/alpine-sh/?latest_x000D_
x-frame-options: DENY_x000D_
strict-transport-security: max-age=63072000; preload_x000D_
{noformat}",3.0,"1.5.1,1.6.1,1.7.0,1.8.0",0.5,0.3740573152337858,0.0,0.0,0.0,0.2,0.4,0.43859649122807015,0.2714285714285714,0.11538461538461538,0.05454545454545455,0.05454545454545455,0.9771100917431192
Improvement,Add minimum capabilities in the master.,"See Epic MESOS-8878 for motivation._x000D_
_x000D_
Add a new `minimum_capabilities` field in the master registry._x000D_
Upon recovery, the master should check the minimum capabilities record against its own capability and refuse to boot if it lacks any minimum capability._x000D_
_x000D_
We probably want to backport this change to previous versions.",3.0,0,0.0,0.0,0.6666666666666666,1.0,0.5714285714285714,0.0,0.0,0.5438596491228069,0.5357142857142857,0.3974358974358974,0.4,0.4,0.0
Bug,Agent may fail to recover if the agent dies before image store cache checkpointed.,"{noformat}_x000D_
E0502 13:51:45.398555 10100 slave.cpp:7305] EXIT with status 1: Failed to perform recovery: Collect failed: Collect failed: Collect failed: Unexpected empty images file '/var/lib/mesos/slave/store/docker/storedImages'_x000D_
{noformat}_x000D_
_x000D_
This may happen if the agent dies after the file is created but before the contents are persisted on disk.",3.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.017543859649122806,0.007142857142857143,0.6794871794871795,0.5939393939393939,0.5939393939393939,0.0
Bug,Master does not correctly reconcile dropped operations after agent failover,"When an operation does not reach the agent before an agent failover, the master currently does not detect the dropped operation when the agent reregisters and sends the list of its operation in an {{UpdateSlaveMessage}}.",2.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.8121212121212121,0.8121212121212121,0.0
Bug,CMake builds are missing byproduct declaration for jemalloc.,"The {{jemalloc}} dependency is missing a byproduct declaration in the CMake configuration. As a result, building Mesos with enabled {{jemalloc}} using CMake and Ninja will fail.",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.2631578947368421,0.36428571428571427,0.28205128205128205,0.296969696969697,0.296969696969697,0.0
Improvement,Introduce a push-based gauge.,"Currently, we only have pull-based gauges which have significant performance downsides._x000D_
_x000D_
A push-based gauge differs from a pull-based gauge in that the client is responsible for pushing the latest value into the gauge whenever it changes. This can be challenging in some cases as it requires the client to have a good handle on when the gauge value changes (rather than just computing the current value when asked)._x000D_
_x000D_
It is highly recommended to use push-based gauges if possible as they provide significant performance benefits over pull-based gauges. Pull-based gauge suffer from delays getting processed on the event queue of a Process, as well as incur computation cost on the Process each time the metrics are collected. Push-based gauges, on the other hand, incur no cost to the owning Process when metrics are collected, and instead incur a trivial cost when the Process pushes new values in.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.0
Task,Per Framework Offer metrics,"Metrics regarding number of offers (sent, accepted, declined, rescinded) on a per framework basis.",2.0,0,0.5,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.8846153846153846,0.9878787878787878,0.9878787878787878,0.0
Task,Per Framework task state metrics,"Gauge metrics about current number of tasks in active states (RUNNING, STAGING etc).",3.0,0,0.5,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.8846153846153846,0.9878787878787878,0.9878787878787878,0.0
Task,Per Framework Operation metrics,Metris for number of operations sent via ACCEPT calls by framework.,2.0,0,0.5,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.8846153846153846,0.9878787878787878,0.9878787878787878,0.0
Task,Per Framework EVENT metrics,Metrics for number of events sent by the master to the framework.,2.0,0,0.5,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.8846153846153846,0.9878787878787878,0.9878787878787878,0.0
Task,Per Framework CALL metrics,Metrics about number of different kinds of calls sent by a framework to master.,2.0,0,0.5,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.8846153846153846,0.9878787878787878,0.9878787878787878,0.0
Bug,Flaky `MasterAllocatorTest/0.SingleFramework`," _x000D_
{code:java}_x000D_
[ RUN ] MasterAllocatorTest/0.SingleFramework_x000D_
F0426 08:31:29.775804 9701 hierarchical.cpp:586] Check failed: slaves.contains(slaveId)_x000D_
*** Check failure stack trace: ***_x000D_
@ 0x7f365e108fb8 google::LogMessage::Fail()_x000D_
@ 0x7f365e108f15 google::LogMessage::SendToLog()_x000D_
@ 0x7f365e10890f google::LogMessage::Flush()_x000D_
@ 0x7f365e10b6d2 google::LogMessageFatal::~LogMessageFatal()_x000D_
@ 0x7f365c63b8d7 mesos::internal::master::allocator::internal::HierarchicalAllocatorProcess::removeSlave()_x000D_
@ 0x55728a500ac7 _ZZN7process8dispatchIN5mesos8internal6master9allocator21MesosAllocatorProcessERKNS1_7SlaveIDES8_EEvRKNS_3PIDIT_EEMSA_FvT0_EOT1_ENKUlOS6_PNS_11ProcessBaseEE_clESJ_SL__x000D_
@ 0x55728a589908 _ZN5cpp176invokeIZN7process8dispatchIN5mesos8internal6master9allocator21MesosAllocatorProcessERKNS3_7SlaveIDESA_EEvRKNS1_3PIDIT_EEMSC_FvT0_EOT1_EUlOS8_PNS1_11ProcessBaseEE_JS8_SN_EEEDTclcl7forwardISC_Efp_Espcl7forwardIT0_Efp0_EEEOSC_DpOSP__x000D_
@ 0x55728a586a0f _ZN6lambda8internal7PartialIZN7process8dispatchIN5mesos8internal6master9allocator21MesosAllocatorProcessERKNS4_7SlaveIDESB_EEvRKNS2_3PIDIT_EEMSD_FvT0_EOT1_EUlOS9_PNS2_11ProcessBaseEE_JS9_St12_PlaceholderILi1EEEE13invoke_expandISP_St5tupleIJS9_SR_EESU_IJOSO_EEJLm0ELm1EEEEDTcl6invokecl7forwardISD_Efp_Espcl6expandcl3getIXT2_EEcl7forwardISH_Efp0_EEcl7forwardISK_Efp2_EEEEOSD_OSH_N5cpp1416integer_sequenceImJXspT2_EEEESL__x000D_
@ 0x55728a5852b0 _ZNO6lambda8internal7PartialIZN7process8dispatchIN5mesos8internal6master9allocator21MesosAllocatorProcessERKNS4_7SlaveIDESB_EEvRKNS2_3PIDIT_EEMSD_FvT0_EOT1_EUlOS9_PNS2_11ProcessBaseEE_JS9_St12_PlaceholderILi1EEEEclIJSO_EEEDTcl13invoke_expandcl4movedtdefpT1fEcl4movedtdefpT10bound_argsEcvN5cpp1416integer_sequenceImJLm0ELm1EEEE_Ecl16forward_as_tuplespcl7forwardIT_Efp_EEEEDpOSX__x000D_
@ 0x55728a584209 _ZN5cpp176invokeIN6lambda8internal7PartialIZN7process8dispatchIN5mesos8internal6master9allocator21MesosAllocatorProcessERKNS6_7SlaveIDESD_EEvRKNS4_3PIDIT_EEMSF_FvT0_EOT1_EUlOSB_PNS4_11ProcessBaseEE_JSB_St12_PlaceholderILi1EEEEEJSQ_EEEDTclcl7forwardISF_Efp_Espcl7forwardIT0_Efp0_EEEOSF_DpOSV__x000D_
@ 0x55728a583995 _ZN6lambda8internal6InvokeIvEclINS0_7PartialIZN7process8dispatchIN5mesos8internal6master9allocator21MesosAllocatorProcessERKNS7_7SlaveIDESE_EEvRKNS5_3PIDIT_EEMSG_FvT0_EOT1_EUlOSC_PNS5_11ProcessBaseEE_JSC_St12_PlaceholderILi1EEEEEJSR_EEEvOSG_DpOT0__x000D_
@ 0x55728a581522 _ZNO6lambda12CallableOnceIFvPN7process11ProcessBaseEEE10CallableFnINS_8internal7PartialIZNS1_8dispatchIN5mesos8internal6master9allocator21MesosAllocatorProcessERKNSA_7SlaveIDESH_EEvRKNS1_3PIDIT_EEMSJ_FvT0_EOT1_EUlOSF_S3_E_JSF_St12_PlaceholderILi1EEEEEEclEOS3__x000D_
@ 0x7f365e0484c0 _ZNO6lambda12CallableOnceIFvPN7process11ProcessBaseEEEclES3__x000D_
@ 0x7f365e025760 process::ProcessBase::consume()_x000D_
@ 0x7f365e033abc _ZNO7process13DispatchEvent7consumeEPNS_13EventConsumerE_x000D_
@ 0x55728a1cb6ea process::ProcessBase::serve()_x000D_
@ 0x7f365e0225ed process::ProcessManager::resume()_x000D_
@ 0x7f365e01e94c _ZZN7process14ProcessManager12init_threadsEvENKUlvE_clEv_x000D_
@ 0x7f365e031080 _ZNSt12_Bind_simpleIFZN7process14ProcessManager12init_threadsEvEUlvE_vEE9_M_invokeIJEEEvSt12_Index_tupleIJXspT_EEE_x000D_
@ 0x7f365e030a34 _ZNSt12_Bind_simpleIFZN7process14ProcessManager12init_threadsEvEUlvE_vEEclEv_x000D_
@ 0x7f365e030338 _ZNSt6thread11_State_implISt12_Bind_simpleIFZN7process14ProcessManager12init_threadsEvEUlvE_vEEE6_M_runEv_x000D_
@ 0x7f365478976f (unknown)_x000D_
@ 0x7f3654e6973a start_thread_x000D_
@ 0x7f3653eefe7f __GI___clone{code}_x000D_
 ",2.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1923076923076923,0.2424242424242424,0.2424242424242424,0.0
Bug,VolumeSandboxPathIsolatorTest.SharedParentTypeVolume fails on macOS,"This test fails on macOS with:_x000D_
{noformat}_x000D_
[ RUN      ] VolumeSandboxPathIsolatorTest.SharedParentTypeVolume_x000D_
I0423 10:55:19.624977 2767623040 containerizer.cpp:296] Using isolation { environment_secret, filesystem/posix, volume/sandbox_path }_x000D_
I0423 10:55:19.625176 2767623040 provisioner.cpp:299] Using default backend 'copy'_x000D_
../../src/tests/containerizer/volume_sandbox_path_isolator_tests.cpp:130: Failure_x000D_
create: Unknown or unsupported isolator 'volume/sandbox_path'_x000D_
[  FAILED  ] VolumeSandboxPathIsolatorTest.SharedParentTypeVolume (3 ms)_x000D_
{noformat}_x000D_
_x000D_
Likely a regression introduced in commit {{189efed864ca2455674b0790d6be4a73c820afd6}} which removed {{volume/sandbox_path}} for POSIX.",2.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.2631578947368421,0.36428571428571427,0.28205128205128205,0.296969696969697,0.296969696969697,0.0
Improvement,Add jemalloc as optional third-party memory allocator,"As seen MESOS-7876, using jemalloc over the default memory allocator can have performance benefits._x000D_
_x000D_
 _x000D_
_x000D_
Additionally, this is also supports the use case of MESOS-7944 by providing an out-of-the-box option to enable memory profiling. (which is also the ticket referenced in the mailing list discussion about this)",3.0,0,0.0,0.0,0.3333333333333333,0.5,0.14285714285714285,0.0,0.2,0.5087719298245614,0.4357142857142857,0.16666666666666666,0.1818181818181818,0.17575757575757575,0.0
Improvement,/roles and webui roles table should display distinct offered and allocated resources.,"The role endpoints currently show accumulated values for resources (allocated), containing offered resources. For gaining an overview showing our allocated resources separately from the offered resources could improve the signal quality, depending on the use case._x000D_
_x000D_
This also affects the UI display, for example the ""Roles"" tab._x000D_
",3.0,1.5.0,0.5,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.3717948717948718,0.24848484848484845,0.24848484848484845,0.963302752293578
Bug,Transition pending operations to OPERATION_UNREACHABLE when an agent is removed.,"Pending operations on an agent should be transitioned to `OPERATION_UNREACHABLE` when an agent is marked unreachable. We should also make sure that we pro-actively send operation status updates for these operations when the agent becomes unreachable._x000D_
_x000D_
We should also make sure that we send new operation updates if/when the agent reconnects - perhaps this is already accomplished with the existing operation update logic in the agent?",3.0,"1.5.0,1.6.0",0.5,0.0,0.3333333333333333,0.5,0.14285714285714285,0.0,0.2,0.5087719298245614,0.4357142857142857,0.28205128205128205,0.2606060606060606,0.2606060606060606,0.9678899082568808
Bug,Transition operations to OPERATION_GONE_BY_OPERATOR when marking an agent gone.,"The master should transition operations to the state {{OPERATION_GONE_BY_OPERATOR}} when an agent is marked gone, sending an operation status update to the frameworks that created them._x000D_
_x000D_
We should also remove them from {{Master::frameworks}}.",3.0,"1.5.0,1.6.0",0.5,0.0,0.3333333333333333,0.5,0.14285714285714285,0.0,0.2,0.5087719298245614,0.4357142857142857,0.28205128205128205,0.2606060606060606,0.2606060606060606,0.9678899082568808
Task,Make resource provider aware of workloads.,"Since the {{NodePublishVolume}} CSI call is supposed to be called for each workload, SLRP it self should be aware of workloads. Potentially, we could have the following event in the resource provider API:_x000D_
{noformat}_x000D_
// Received when the master or agent wants to update the resource usage of_x000D_
// this resource provider for each workload (e.g., framework or container)._x000D_
message ApplyResourceUsage {_x000D_
  required UUID uuid = 1;_x000D_
_x000D_
  // A map from a workload identifier (e.g., FrameworkID or ContainerID) to_x000D_
  // the resources used by the workload._x000D_
  map<string, Resources> resources = 2;_x000D_
}_x000D_
{noformat}_x000D_
For SLRP or any local resource provider, a workload is a container, and SLRP can implement {{ApplyResourceUsage}} by checking if a resource is used by a new workload, and call {{NodeUnpublishVolume}} and {{NodePublishVolume}} accordingly._x000D_
_x000D_
For ERP, a workload can be a framework, so the resource provider can checkpoint which framework is using what resources and provide such information to the allocator after a failover._x000D_
_x000D_
Note that the {{ApplyResourceUsage}} call should report *all* resources being used on an agent, so it can handle resources without identifiers (such as cpus, mem) correctly.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.7307692307692307,0.7515151515151515,0.7575757575757576,0.0
Task,Create ACL for grow and shrink volume,"As follow up work of MESOS-4965, we should make sure new operations are properly protected in ACL and authorizer.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.07017543859649122,0.12142857142857143,0.038461538461538464,0.07272727272727272,0.07272727272727272,0.0
Bug,`Add` to sequence will not run if it races with sequence destruction,"Adding item to sequence is realized by dispatching `add()` to the sequence actor. However, this could race with the sequence actor destruction.:_x000D_
_x000D_
After the dispatch but before the dispatched `add()` message gets processed by the sequence actor, if the sequence gets destroyed, a terminate message will be injected to the *head* of the message queue. This would result in the destruction of the sequence without the `add()` call ever gets processed. User would end up with a pending future and the future's `onDiscarded' would not be triggered during the sequence destruction._x000D_
_x000D_
The solution is to set the `inject` flag to `false` so that the terminating message is enqueued to the end of the sequence actor message queue. All `add()` messages that happen before the destruction will be processed before the terminating message.",1.0,0,0.0,0.0,0.6666666666666666,1.0,0.5714285714285714,0.0,0.0,0.5438596491228069,0.5357142857142857,0.3974358974358974,0.4,0.4,0.0
Improvement,Don't print full usage for invocation errors,"The current usage string for mesos-master comes in at 399 lines, and for mesos-agent at 685 lines._x000D_
_x000D_
 _x000D_
_x000D_
Printing such a wall of text will overflow most terminal windows, making it necessary to scroll up to see the actual error when invoking mesos with an incorrect command line.",2.0,0,0.0,0.0,0.3333333333333333,0.5,0.14285714285714285,0.0,0.2,0.5087719298245614,0.4357142857142857,0.16666666666666666,0.1818181818181818,0.17575757575757575,0.0
Bug,Mesos configured with `--enable-grpc` doesn't compile on non-Linux builds,"Commit {{59cca968e04dee069e0df2663733b6d6f55af0da}} added {{examples/test_csi_plugin.cpp}} to non-Linux builds that are configured using the {{--enable-grpc}} flag. As {{examples/test_csi_plugin.cpp}} includes {{fs/linux.hpp}}, it can only compile on Linux and needs to be disabled for non-Linux builds.",2.0,1.6.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.2631578947368421,0.36428571428571427,0.28205128205128205,0.296969696969697,0.296969696969697,0.9724770642201834
Bug,SlaveTest.ChangeDomain is disabled.,This test has been disabled in https://github.com/apache/mesos/commit/c0468b240842d4aaf04249cb0a58c59c43d1850d. We should either fix or remove it.,1.0,0,0.5,0.0,0.3333333333333333,0.5,0.14285714285714285,0.0,0.2,0.5087719298245614,0.4357142857142857,0.717948717948718,0.8242424242424242,0.7212121212121212,0.0
Bug,Test `MasterAllocatorTest/*.TaskFinished` is flaky.,"Occasionally the test would crash with the following logs:_x000D_
{noformat}_x000D_
I0225 06:34:21.732908  1835 slave.cpp:3467] Launching container 46824279-01b3-4dcb-9be0-696cdacefe2f for executor 'default' of framework f1caa559-f62f-40f6-9786-100401bc9062-0000_x000D_
I0225 06:34:21.733026  1835 slave.cpp:5705] Forwarding the update TASK_LOST (Status UUID: 29c6ae17-0e46-4dea-be36-e9e64e1d95c5) for task 1 of framework f1caa559-f62f-40f6-9786-100401bc9062-0000 to master@172.16.10.21:58470_x000D_
I0225 06:34:21.733083  1835 slave.cpp:5598] Task status update manager successfully handled status update TASK_LOST (Status UUID: 29c6ae17-0e46-4dea-be36-e9e64e1d95c5) for task 1 of framework f1caa559-f62f-40f6-9786-100401bc9062-0000_x000D_
I0225 06:34:21.733202  1835 master.cpp:7894] Status update TASK_LOST (Status UUID: 29c6ae17-0e46-4dea-be36-e9e64e1d95c5) for task 1 of framework f1caa559-f62f-40f6-9786-100401bc9062-0000 from agent f1caa559-f62f-40f6-9786-100401bc9062-S0 at sla_x000D_
ve(128)@172.16.10.21:58470 (ip-172-16-10-21.ec2.internal)_x000D_
I0225 06:34:21.733232  1835 master.cpp:7950] Forwarding status update TASK_LOST (Status UUID: 29c6ae17-0e46-4dea-be36-e9e64e1d95c5) for task 1 of framework f1caa559-f62f-40f6-9786-100401bc9062-0000_x000D_
I0225 06:34:21.733273  1835 master.cpp:10258] Updating the state of task 1 of framework f1caa559-f62f-40f6-9786-100401bc9062-0000 (latest state: TASK_LOST, status update state: TASK_LOST)_x000D_
*** Aborted at 1519540461 (unix time) try ""date -d @1519540461"" if you are using GNU date ***_x000D_
I0225 06:34:21.733470  1838 sched.cpp:1027] Scheduler::statusUpdate took 8126ns_x000D_
I0225 06:34:21.733700  1838 hierarchical.cpp:1192] Recovered cpus(allocated: *):1; mem(allocated: *):256 (total: cpus:3; mem:1024; disk:35056; ports:[31000-32000], allocated: cpus(allocated: *):1; mem(allocated: *):256) on agent f1caa559-f62f-4_x000D_
0f6-9786-100401bc9062-S0 from framework f1caa559-f62f-40f6-9786-100401bc9062-0000_x000D_
WPC: @               0x20 (unknown)_x000D_
0225 06:34:21.734422  1836 process.cpp:2805] Attempted to spawn already running process version@172.16.10.21:58470_x000D_
I0225 06:34:21.734592  1836 exec.cpp:162] Version: 1.6.0_x000D_
I0225 06:34:21.734761  1837 exec.cpp:212] Executor started at: executor(32)@172.16.10.21:58470 with pid 1814_x000D_
I0225 06:34:21.734835  1837 slave.cpp:4747] Got registration for executor 'default' of framework f1caa559-f62f-40f6-9786-100401bc9062-0000 from executor(32)@172.16.10.21:58470_x000D_
I0225 06:34:21.734966  1840 exec.cpp:236] Executor registered on agent f1caa559-f62f-40f6-9786-100401bc9062-S0_x000D_
I0225 06:34:21.734990  1840 exec.cpp:248] Executor::registered took 9639ns_x000D_
I0225 06:34:21.740001  1836 slave.cpp:3199] Sending queued task '0' to executor 'default' of framework f1caa559-f62f-40f6-9786-100401bc9062-0000 at executor(32)@172.16.10.21:58470_x000D_
I0225 06:34:21.740236  1840 exec.cpp:330] Executor asked to run task '0'_x000D_
I0225 06:34:21.740284  1840 exec.cpp:339] Executor::launchTask took 28783ns_x000D_
I0225 06:34:21.740320  1840 exec.cpp:581] Executor sending status update TASK_RUNNING (Status UUID: 7cf0069b-a6dc-4d7e-9e16-7ade7a451334) for task 0 of framework f1caa559-f62f-40f6-9786-100401bc9062-0000_x000D_
I0225 06:34:21.740419  1840 slave.cpp:5213] Handling status update TASK_RUNNING (Status UUID: 7cf0069b-a6dc-4d7e-9e16-7ade7a451334) for task 0 of framework f1caa559-f62f-40f6-9786-100401bc9062-0000 from executor(32)@172.16.10.21:58470_x000D_
I0225 06:34:21.740563  1840 task_status_update_manager.cpp:328] Received task status update TASK_RUNNING (Status UUID: 7cf0069b-a6dc-4d7e-9e16-7ade7a451334) for task 0 of framework f1caa559-f62f-40f6-9786-100401bc9062-0000_x000D_
I0225 06:34:21.740583  1840 task_status_update_manager.cpp:507] Creating StatusUpdate stream for task 0 of framework f1caa559-f62f-40f6-9786-100401bc9062-0000_x000D_
I0225 06:34:21.740676  1840 task_status_update_manager.cpp:383] Forwarding task status update TASK_RUNNING (Status UUID: 7cf0069b-a6dc-4d7e-9e16-7ade7a451334) for task 0 of framework f1caa559-f62f-40f6-9786-100401bc9062-0000 to the agent_x000D_
I0225 06:34:21.740733  1840 slave.cpp:5705] Forwarding the update TASK_RUNNING (Status UUID: 7cf0069b-a6dc-4d7e-9e16-7ade7a451334) for task 0 of framework f1caa559-f62f-40f6-9786-100401bc9062-0000 to master@172.16.10.21:58470_x000D_
I0225 06:34:21.740799  1840 slave.cpp:5598] Task status update manager successfully handled status update TASK_RUNNING (Status UUID: 7cf0069b-a6dc-4d7e-9e16-7ade7a451334) for task 0 of framework f1caa559-f62f-40f6-9786-100401bc9062-0000_x000D_
I0225 06:34:21.740828  1840 slave.cpp:5614] Sending acknowledgement for status update TASK_RUNNING (Status UUID: 7cf0069b-a6dc-4d7e-9e16-7ade7a451334) for task 0 of framework f1caa559-f62f-40f6-9786-100401bc9062-0000 to executor(32)@172.16.10.21:58470_x000D_
I0225 06:34:21.740886  1840 exec.cpp:398] Executor received status update acknowledgement 7cf0069b-a6dc-4d7e-9e16-7ade7a451334 for task 0 of framework f1caa559-f62f-40f6-9786-100401bc9062-0000_x000D_
{noformat}_x000D_
_x000D_
This has been observed multiple times and every time the test crashed right after_x000D_
{noformat}_x000D_
I0225 06:34:21.733273  1835 master.cpp:10258] Updating the state of task 1 of framework f1caa559-f62f-40f6-9786-100401bc9062-0000 (latest state: TASK_LOST, status update state: TASK_LOST)_x000D_
{noformat}_x000D_
_x000D_
Attached logs of 4 crash instances.",2.0,1.5.0,0.5,0.3740573152337858,0.0,0.0,0.0,0.2,0.4,0.43859649122807015,0.2714285714285714,0.7307692307692307,0.7515151515151515,0.7575757575757576,0.963302752293578
Bug,Quota headroom tracking may be incorrect in the presence of hierarchical reservation.,"When calculating the global quota headroom, we subtract all unallocated reservations by doing_x000D_
_x000D_
{code}_x000D_
for each role with reservation_x000D_
	availableHeadroom -= role total reservation - role allocated reservation;_x000D_
{code}_x000D_
_x000D_
We only traverse roles with reservation. In the presence of hierarchal reservation, this is problematic. Consider a child role (e.g. ""a/b"") with no reservations, it can still get reserved resources if its ancestor has reservations (e.g. ""a"" has reservations). However, allocated reserved resources of role “a/b” will be ignored given the above code._x000D_
_x000D_
The consequence is that availableHeadroom will be underestimated because allocated reservations are underestimated. This would lead to excessive resources set aside for quota headroom.",2.0,1.5.0,0.0,0.0,0.6666666666666666,1.0,0.5714285714285714,0.0,0.0,0.5438596491228069,0.5357142857142857,0.3974358974358974,0.4,0.4,0.963302752293578
Bug,Mesos master stack overflow in libprocess socket send loop.,"Mesos master crashes under load. Attached are some infos from the `lldb`:_x000D_
{code:java}_x000D_
Process 41933 resuming_x000D_
Process 41933 stopped_x000D_
* thread #10, stop reason = EXC_BAD_ACCESS (code=2, address=0x7000089ecff8)_x000D_
frame #0: 0x000000010c30ddb6 libmesos-1.6.0.dylib`::_Some() at some.hpp:35_x000D_
32 template <typename T>_x000D_
33 struct _Some_x000D_
34 {_x000D_
-> 35 _Some(T _t) : t(std::move(_t)) {}_x000D_
36_x000D_
37 T t;_x000D_
38 };_x000D_
Target 0: (mesos-master) stopped._x000D_
(lldb)_x000D_
{code}_x000D_
To quote [~abudnik]_x000D_
{quote}it’s the stack overflow bug in libprocess due to the way `internal::send()` and `internal::_send()` are implemented in `process.cpp`_x000D_
{quote}",1.0,"1.3.2,1.4.1,1.5.0,1.6.0",0.5,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.038461538461538464,0.01212121212121212,0.01212121212121212,0.9587844036697248
Bug,Avoid failure for invalid profile in `UriDiskProfileAdaptor`,We should be defensive and not fail the profile module when the user provides an invalid profile in the profile matrix.,1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.7307692307692307,0.7515151515151515,0.7575757575757576,0.0
Improvement,Add infra to test a hung Docker daemon,"We should add infrastructure to our tests which enables us to test the behavior of the Docker executor and containerizer in the presence of a hung Docker daemon._x000D_
_x000D_
One possible first-order solution is to build a simple binary which never returns. We could initialize the agent/executor with this binary instead of the Docker CLI in order to simulate a Docker daemon which hangs on every call.",3.0,1.5.0,0.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.963302752293578
Improvement,Improve discard handling of 'Docker::inspect()',"In the call path of {{Docker::inspect()}}, each continuation currently checks if {{promise->future().hasDiscard()}}, where the {{promise}} is associated with the output of the {{docker inspect}} call. However, if the call to {{docker inspect}} becomes hung indefinitely, then continuations are never invoked, and a subsequent discard of the returned {{Future}} will have no effect. We should add proper {{onDiscard}} handling to that {{Future}} so that appropriate cleanup is performed in such cases.",3.0,1.5.0,0.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.963302752293578
Improvement,Improve discard handling for 'Docker::stop' and 'Docker::pull'.,"The functions in the Docker library which issue Docker CLI commands should be updated so that when the {{Future}} they return is discarded, any subprocesses which have been spawned will be cleaned up.",3.0,1.5.0,0.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.963302752293578
Task,Allow newline characters when decoding base64 strings in stout.,"Current implementation of `stout::base64::decode` errors out on encountering a newline character (""\n"" or ""\r\n"") which is correct wrt [RFC4668#section-3.3|https://tools.ietf.org/html/rfc4648#section-3.3]. However, most implementations insert a newline to delimit encoded string and ignore (instead of erroring out) the newline character while decoding the string. Since stout facilities are used by third-party modules to encode/decode base64 data, it is desirable to allow decoding of newline-delimited data.",2.0,0,0.0,0.3453996983408748,0.0,0.0,0.0,0.0,0.2,0.38596491228070173,0.35714285714285715,0.28205128205128205,0.29090909090909095,0.28484848484848485,0.0
Task,Enhance V1 scheduler send API to receive sync response from Master,Current scheduler HTTP API doesn't provide a way for the scheduler to get a synchronous response back from the Master. A synchronous API means the scheduler wouldn't have to wait on the event stream to check the status of operations that require master-only validation/approval/rejection.,3.0,0,0.0,0.3453996983408748,0.0,0.0,0.0,0.0,0.2,0.38596491228070173,0.35714285714285715,0.28205128205128205,0.29090909090909095,0.28484848484848485,0.0
Bug,CGROUPS_ROOT_PidNamespaceForward and CGROUPS_ROOT_PidNamespaceBackward tests fail,"{code:java}_x000D_
W0208 04:41:06.970381 348 containerizer.cpp:2335] Attempted to destroy unknown container 001fdfaf-7dab-45b9-ab5c-baa3527d50fb_x000D_
../../src/tests/slave_recovery_tests.cpp:5189: Failure_x000D_
termination.get() is NONE_x000D_
{code}_x000D_
{code:java}_x000D_
W0208 04:41:10.058873 348 containerizer.cpp:2335] Attempted to destroy unknown container e51afc11-cffe-4861-ba13-b124116522b0_x000D_
../../src/tests/slave_recovery_tests.cpp:5294: Failure_x000D_
termination.get() is NONE_x000D_
{code}",2.0,"1.5.0,1.6.0",0.5,0.0,0.6666666666666666,1.0,0.5714285714285714,0.0,0.0,0.5438596491228069,0.5357142857142857,0.1923076923076923,0.2424242424242424,0.2424242424242424,0.9678899082568808
Task,Port libprocess HTTPTest.QueryEncodeDecode,"The test cases:_x000D_
 * -HTTPTest, EndpointsHelp-_x000D_
 * -HTTPTest, EndpointsHelpRemoval-_x000D_
 * -HTTPTest, NestedGet-_x000D_
 * HTTPTest, QueryEncodeDecode_x000D_
_x000D_
Are disabled in the Windows build because they fail.",1.0,0,0.0,0.0,0.6666666666666666,1.0,0.2857142857142857,0.0,0.0,0.0,0.007142857142857143,0.0,0.0,0.0,0.0
Bug,Bug in `Master::detected()` leads to coredump in `MasterZooKeeperTest.MasterInfoAddress`.,"{code:java}_x000D_
15:55:17 Assertion failed: (isSome()), function get, file ../../3rdparty/stout/include/stout/option.hpp, line 119._x000D_
15:55:17 *** Aborted at 1518018924 (unix time) try ""date -d @1518018924"" if you are using GNU date ***_x000D_
15:55:17 PC: @     0x7fff4f8f2e3e __pthread_kill_x000D_
15:55:17 *** SIGABRT (@0x7fff4f8f2e3e) received by PID 39896 (TID 0x700000427000) stack trace: ***_x000D_
15:55:17     @     0x7fff4fa24f5a _sigtramp_x000D_
15:55:17 I0207 07:55:24.945252 4890624 group.cpp:511] ZooKeeper session expired_x000D_
15:55:17     @     0x700000425500 (unknown)_x000D_
15:55:17 2018-02-07 07:55:24,945:39896(0x700000633000):ZOO_INFO@log_env@794: Client environment:user.dir=/private/var/folders/6w/rw03zh013y38ys6cyn8qppf80000gn/T/1mHCvU_x000D_
15:55:17     @     0x7fff4f84f312 abort_x000D_
15:55:17 2018-02-07 07:55:24,945:39896(0x700000633000):ZOO_INFO@zookeeper_init@827: Initiating client connection, host=127.0.0.1:52197 sessionTimeout=10000 watcher=0x10d916590 sessionId=0 sessionPasswd=<null> context=0x7fe1bda706a0 flags=0_x000D_
15:55:17     @     0x7fff4f817368 __assert_rtn_x000D_
15:55:17     @        0x10b9cff97 _ZNR6OptionIN5mesos10MasterInfoEE3getEv_x000D_
15:55:17     @        0x10bbb04b5 Option<>::operator->()_x000D_
15:55:17     @        0x10bd4514a mesos::internal::master::Master::detected()_x000D_
15:55:17     @        0x10bf54558 _ZZN7process8dispatchIN5mesos8internal6master6MasterERKNS_6FutureI6OptionINS1_10MasterInfoEEEESB_EEvRKNS_3PIDIT_EEMSD_FvT0_EOT1_ENKUlOS9_PNS_11ProcessBaseEE_clESM_SO__x000D_
15:55:17     @        0x10bf54310 _ZN5cpp176invokeIZN7process8dispatchIN5mesos8internal6master6MasterERKNS1_6FutureI6OptionINS3_10MasterInfoEEEESD_EEvRKNS1_3PIDIT_EEMSF_FvT0_EOT1_EUlOSB_PNS1_11ProcessBaseEE_JSB_SQ_EEEDTclclsr3stdE7forwardISF_Efp_Espclsr3stdE7forwardIT0_Efp0_EEEOSF_DpOSS__x000D_
15:55:17     @        0x10bf542bb _ZN6lambda8internal7PartialIZN7process8dispatchIN5mesos8internal6master6MasterERKNS2_6FutureI6OptionINS4_10MasterInfoEEEESE_EEvRKNS2_3PIDIT_EEMSG_FvT0_EOT1_EUlOSC_PNS2_11ProcessBaseEE_JSC_NSt3__112placeholders4__phILi1EEEEE13invoke_expandISS_NST_5tupleIJSC_SW_EEENSZ_IJOSR_EEEJLm0ELm1EEEEDTclsr5cpp17E6invokeclsr3stdE7forwardISG_Efp_Espcl6expandclsr3stdE3getIXT2_EEclsr3stdE7forwardISK_Efp0_EEclsr3stdE7forwardISN_Efp2_EEEEOSG_OSK_N5cpp1416integer_sequenceImJXspT2_EEEESO__x000D_
15:55:17     @        0x10bf541f3 _ZNO6lambda8internal7PartialIZN7process8dispatchIN5mesos8internal6master6MasterERKNS2_6FutureI6OptionINS4_10MasterInfoEEEESE_EEvRKNS2_3PIDIT_EEMSG_FvT0_EOT1_EUlOSC_PNS2_11ProcessBaseEE_JSC_NSt3__112placeholders4__phILi1EEEEEclIJSR_EEEDTcl13invoke_expandclL_ZNST_4moveIRSS_EEONST_16remove_referenceISG_E4typeEOSG_EdtdefpT1fEclL_ZNSZ_IRNST_5tupleIJSC_SW_EEEEES14_S15_EdtdefpT10bound_argsEcvN5cpp1416integer_sequenceImJLm0ELm1EEEE_Eclsr3stdE16forward_as_tuplespclsr3stdE7forwardIT_Efp_EEEEDpOS1C__x000D_
15:55:17     @        0x10bf540bd _ZN5cpp176invokeIN6lambda8internal7PartialIZN7process8dispatchIN5mesos8internal6master6MasterERKNS4_6FutureI6OptionINS6_10MasterInfoEEEESG_EEvRKNS4_3PIDIT_EEMSI_FvT0_EOT1_EUlOSE_PNS4_11ProcessBaseEE_JSE_NSt3__112placeholders4__phILi1EEEEEEJST_EEEDTclclsr3stdE7forwardISI_Efp_Espclsr3stdE7forwardIT0_Efp0_EEEOSI_DpOS10__x000D_
15:55:17     @        0x10bf54081 _ZN6lambda8internal6InvokeIvEclINS0_7PartialIZN7process8dispatchIN5mesos8internal6master6MasterERKNS5_6FutureI6OptionINS7_10MasterInfoEEEESH_EEvRKNS5_3PIDIT_EEMSJ_FvT0_EOT1_EUlOSF_PNS5_11ProcessBaseEE_JSF_NSt3__112placeholders4__phILi1EEEEEEJSU_EEEvOSJ_DpOT0__x000D_
15:55:17     @        0x10bf53e06 _ZNO6lambda12CallableOnceIFvPN7process11ProcessBaseEEE10CallableFnINS_8internal7PartialIZNS1_8dispatchIN5mesos8internal6master6MasterERKNS1_6FutureI6OptionINSA_10MasterInfoEEEESK_EEvRKNS1_3PIDIT_EEMSM_FvT0_EOT1_EUlOSI_S3_E_JSI_NSt3__112placeholders4__phILi1EEEEEEEclEOS3__x000D_
15:55:17     @        0x10ebf464f _ZNO6lambda12CallableOnceIFvPN7process11ProcessBaseEEEclES3__x000D_
15:55:17     @        0x10ebf44c4 process::ProcessBase::consume()_x000D_
15:55:17     @        0x10ec6f4d9 _ZNO7process13DispatchEvent7consumeEPNS_13EventConsumerE_x000D_
15:55:17     @        0x10b0b2389 process::ProcessBase::serve()_x000D_
15:55:17     @        0x10ebecccc process::ProcessManager::resume()_x000D_
15:55:17     @        0x10ecbd335 process::ProcessManager::init_threads()::$_2::operator()()_x000D_
15:55:17     @        0x10ecbcee6 _ZNSt3__114__thread_proxyINS_5tupleIJNS_10unique_ptrINS_15__thread_structENS_14default_deleteIS3_EEEEZN7process14ProcessManager12init_threadsEvE3$_2EEEEEPvSB__x000D_
15:55:17     @     0x7fff4fa2e6c1 _pthread_body_x000D_
15:55:17     @     0x7fff4fa2e56d _pthread_start_x000D_
15:55:17     @     0x7fff4fa2dc5d thread_start_x000D_
{code}_x000D_
This failure is most likely caused by calling [leader->has_domain()|https://github.com/apache/mesos/blob/994213739b1afc473bbd9d15ded7c3fd26eaa924/src/master/master.cpp#L2159] on empty `leader`, from logs:_x000D_
{code:java}_x000D_
15:55:17 I0207 07:55:24.944833 5427200 detector.cpp:152] Detected a new leader: None_x000D_
{code}",2.0,1.5.0,0.5,0.0,0.3333333333333333,0.5,0.14285714285714285,0.0,0.2,0.5087719298245614,0.4357142857142857,0.1923076923076923,0.2424242424242424,0.2424242424242424,0.963302752293578
Bug,PythonFramework test fails with cache write failure.,"After some recent changes, the  {{ExamplesTest.PythonFramework}} fails on centos and ubuntu rather frequently (but not always)._x000D_
_x000D_
The symptom always is like this (taken from an ASF CI run): _x000D_
{noformat}_x000D_
[...]_x000D_
I0203 03:21:06.871362 11001 leveldb.cpp:347] Persisting action (16 bytes) to leveldb took 73.84466ms_x000D_
I0203 03:21:06.871433 11001 replica.cpp:712] Persisted action TRUNCATE at position 8_x000D_
I0203 03:21:06.871841 10984 replica.cpp:695] Replica received learned notice for position 8 from log-network(1)@172.17.0.4:43102_x000D_
I0203 03:21:06.908581 11004 hierarchical.cpp:2429] Filtered offer with ports:[31000-32000]; mem:9984; disk:367463 on agent 0bd8b628-491d-46a1-a358-6cc902ee2578-S1 for role * of framework 0bd8b628-491d-46a1-a358-6cc902ee2578-0000_x000D_
I0203 03:21:06.908924 11004 hierarchical.cpp:2429] Filtered offer with cpus:1; mem:10112; disk:367463; ports:[31000-32000] on agent 0bd8b628-491d-46a1-a358-6cc902ee2578-S2 for role * of framework 0bd8b628-491d-46a1-a358-6cc902ee2578-0000_x000D_
I0203 03:21:06.909207 11004 hierarchical.cpp:2429] Filtered offer with ports:[31000-32000]; mem:9984; disk:367463 on agent 0bd8b628-491d-46a1-a358-6cc902ee2578-S0 for role * of framework 0bd8b628-491d-46a1-a358-6cc902ee2578-0000_x000D_
I0203 03:21:06.909306 11004 hierarchical.cpp:1517] Performed allocation for 3 agents in 1.276217ms_x000D_
I0203 03:21:06.945303 10984 leveldb.cpp:347] Persisting action (18 bytes) to leveldb took 73.445285ms_x000D_
I0203 03:21:06.945451 10984 leveldb.cpp:423] Deleting ~2 keys from leveldb took 81868ns_x000D_
I0203 03:21:06.945477 10984 replica.cpp:712] Persisted action TRUNCATE at position 8_x000D_
Traceback (most recent call last):_x000D_
File ""/mesos/mesos-1.6.0/_build/../src/examples/python/test_executor.py"", line 25, in <module>_x000D_
from mesos.executor import MesosExecutorDriver_x000D_
File ""build/bdist.linux-x86_64/egg/mesos/executor/__init__.py"", line 17, in <module>_x000D_
File ""build/bdist.linux-x86_64/egg/mesos/executor/_executor.py"", line 7, in <module>_x000D_
File ""build/bdist.linux-x86_64/egg/mesos/executor/_executor.py"", line 4, in __bootstrap___x000D_
File ""/mesos/mesos-1.6.0/_build/3rdparty/setuptools-20.9.0/pkg_resources/__init__.py"", line 1172, in resource_filename_x000D_
self, resource_name_x000D_
File ""/mesos/mesos-1.6.0/_build/3rdparty/setuptools-20.9.0/pkg_resources/__init__.py"", line 1716, in get_resource_filename_x000D_
self._extract_resource(manager, self._eager_to_zip(name))_x000D_
File ""/mesos/mesos-1.6.0/_build/3rdparty/setuptools-20.9.0/pkg_resources/__init__.py"", line 1746, in _extract_resource_x000D_
self.egg_name, self._parts(zip_path)_x000D_
File ""/mesos/mesos-1.6.0/_build/3rdparty/setuptools-20.9.0/pkg_resources/__init__.py"", line 1239, in get_cache_path_x000D_
self.extraction_error()_x000D_
File ""/mesos/mesos-1.6.0/_build/3rdparty/setuptools-20.9.0/pkg_resources/__init__.py"", line 1219, in extraction_error_x000D_
raise err_x000D_
pkg_resources.ExtractionError: Can't extract file(s) to egg cache_x000D_
_x000D_
The following error occurred while trying to extract file(s) to the Python egg_x000D_
cache:_x000D_
_x000D_
[Errno 17] File exists: '/home/mesos/.python-eggs/mesos.executor-1.6.0-py2.7-linux-x86_64.egg-tmp'_x000D_
_x000D_
The Python egg cache directory is currently set to:_x000D_
_x000D_
/home/mesos/.python-eggs_x000D_
_x000D_
Perhaps your account does not have write access to this directory? You can_x000D_
change the cache directory by setting the PYTHON_EGG_CACHE environment_x000D_
variable to point to an accessible directory.{noformat}_x000D_
 ",2.0,1.6.0,0.0,0.3740573152337858,0.0,0.0,0.0,0.2,0.4,0.43859649122807015,0.2714285714285714,0.3717948717948718,0.24848484848484845,0.24848484848484845,0.9724770642201834
Bug,"Noisy ""transport endpoint is not connected"" logs on closing sockets.","When within libprocess a socket is closing, we try to shut it down. That shutdown fails as the socket is not connected. This is intended behavior. The error code returned {{ENOTCONN}} tells us that there is nothing to see here for such common scenario._x000D_
_x000D_
The problem appears to be the logging of such event - that might appear as not useful - no matter which log-level is used._x000D_
_x000D_
{noformat}_x000D_
E1214 08:15:18.017247 20752 process.cpp:2401] Failed to shutdown socket with fd 288: Transport endpoint is not connected_x000D_
{noformat}_x000D_
_x000D_
We should try to prevent this specific, non actionable logging entirely while making sure we do not hinder debugging scenarios._x000D_
",3.0,"1.4.1,1.5.0,1.6.0",0.0,0.3740573152337858,0.0,0.0,0.0,0.2,0.4,0.43859649122807015,0.2714285714285714,0.3717948717948718,0.24848484848484845,0.24848484848484845,0.9633333333333333
Bug,Fetcher doesn't log it's stdout/stderr properly to the log file,"The fetcher doesn't log it's stdout or stderr to the task's output files as it does on Linux. This makes it extraordinarily difficult to diagnose fetcher failures (bad URI, or permissions problems, or whatever)._x000D_
_x000D_
It does not appear to be a glog issue. I added output to the fetcher via cout and cerr, and that output didn't show up in the log files either. So it appears to be a logging capture issue._x000D_
_x000D_
Note that the container launcher, launched from src/slave/containerizer/mesos/launcher.cpp, does appear to log properly. However, when launching the fetcher itself from src/slave/containerizer/fetcher.cpp (FetcherProcess::run), logging does not happen properly.",2.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.8,0.19298245614035087,0.09999999999999999,0.01282051282051282,0.0,0.0,0.0
Bug,Missing map header when compiling against unbundled protobuf,"When compiling mesos against the system-default version of protobuf on Ubuntu 17.04, the build fails due to a missing include._x000D_
_x000D_
 _x000D_
_x000D_
Explanation for the error by [~kaysoky]:_x000D_
Note that the reason why this doesn't compile in protobuf 3.0.x is due to how the c++ files are generated.  In protobuf 3.0.x (and 3.1.x and 3.2.x) generated code only includes the protobuf map headers if there is a map present in the .proto file:[https://github.com/google/protobuf/blob/3.0.x/src/google/protobuf/compiler/cpp/cpp_file.cc#L817-L827]_x000D_
_x000D_
From 3.3.x onwards, all generated files include {{google/protobuf/generated_message_table_driven.h}}, which in turn includes the map headers:[https://github.com/google/protobuf/blob/3.3.x/src/google/protobuf/compiler/cpp/cpp_file.cc#L1006]",1.0,0,0.0,0.0,0.3333333333333333,0.5,0.14285714285714285,0.0,0.2,0.5087719298245614,0.4357142857142857,0.16666666666666666,0.1818181818181818,0.17575757575757575,0.0
Task,Webui should display role limits.,"With the addition of quota limits (see MESOS-8068), the UI should be updated to display the per role limit information. Specifically, the 'Roles' tab needs to be updated.",1.0,0,0.5,0.0,0.6666666666666666,1.0,0.5714285714285714,0.0,0.0,0.5438596491228069,0.5357142857142857,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.0
Bug,MasterTest.RegistryGcByCount is flaky,"Observed this while testing Mesos 1.5.0-rc1 in ASF CI._x000D_
_x000D_
 _x000D_
_x000D_
{code}_x000D_
_x000D_
3: [ RUN      ] MasterTest.RegistryGcByCount_x000D_
_x000D_
..............snip..........................._x000D_
_x000D_
3: I0123 19:22:05.929347 15994 slave.cpp:1201] Detecting new master_x000D_
_x000D_
3: I0123 19:22:05.931701 15988 slave.cpp:1228] Authenticating with master master@172.17.0.2:45634_x000D_
_x000D_
3: I0123 19:22:05.931838 15988 slave.cpp:1237] Using default CRAM-MD5 authenticatee_x000D_
_x000D_
3: I0123 19:22:05.932153 15999 authenticatee.cpp:121] Creating new client SASL connection_x000D_
_x000D_
3: I0123 19:22:05.932580 15992 master.cpp:8958] Authenticating slave(442)@172.17.0.2:45634_x000D_
_x000D_
3: I0123 19:22:05.932822 15990 authenticator.cpp:414] Starting authentication session for crammd5-authenticatee(870)@172.17.0.2:45634_x000D_
_x000D_
3: I0123 19:22:05.933163 15989 authenticator.cpp:98] Creating new server SASL connection_x000D_
_x000D_
3: I0123 19:22:05.933465 16001 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5_x000D_
_x000D_
3: I0123 19:22:05.933495 16001 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'_x000D_
_x000D_
3: I0123 19:22:05.933631 15987 authenticator.cpp:204] Received SASL authentication start_x000D_
_x000D_
3: I0123 19:22:05.933712 15987 authenticator.cpp:326] Authentication requires more steps_x000D_
_x000D_
3: I0123 19:22:05.933851 15987 authenticatee.cpp:259] Received SASL authentication step_x000D_
_x000D_
3: I0123 19:22:05.934006 15987 authenticator.cpp:232] Received SASL authentication step_x000D_
_x000D_
3: I0123 19:22:05.934041 15987 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: '455912973e2c' server FQDN: '455912973e2c' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false _x000D_
_x000D_
3: I0123 19:22:05.934095 15987 auxprop.cpp:181] Looking up auxiliary property '*userPassword'_x000D_
_x000D_
3: I0123 19:22:05.934147 15987 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'_x000D_
_x000D_
3: I0123 19:22:05.934279 15987 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: '455912973e2c' server FQDN: '455912973e2c' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true _x000D_
_x000D_
3: I0123 19:22:05.934298 15987 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true_x000D_
_x000D_
3: I0123 19:22:05.934307 15987 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true_x000D_
_x000D_
3: I0123 19:22:05.934324 15987 authenticator.cpp:318] Authentication success_x000D_
_x000D_
3: I0123 19:22:05.934463 15995 authenticatee.cpp:299] Authentication success_x000D_
_x000D_
3: I0123 19:22:05.934563 16002 master.cpp:8988] Successfully authenticated principal 'test-principal' at slave(442)@172.17.0.2:45634_x000D_
_x000D_
3: I0123 19:22:05.934708 15993 authenticator.cpp:432] Authentication session cleanup for crammd5-authenticatee(870)@172.17.0.2:45634_x000D_
_x000D_
3: I0123 19:22:05.934891 15995 slave.cpp:1320] Successfully authenticated with master master@172.17.0.2:45634_x000D_
_x000D_
3: I0123 19:22:05.935261 15995 slave.cpp:1764] Will retry registration in 2.234083ms if necessary_x000D_
_x000D_
3: I0123 19:22:05.935436 15999 master.cpp:6061] Received register agent message from slave(442)@172.17.0.2:45634 (455912973e2c)_x000D_
_x000D_
3: I0123 19:22:05.935662 15999 master.cpp:3867] Authorizing agent with principal 'test-principal'_x000D_
_x000D_
3: I0123 19:22:05.936161 15992 master.cpp:6123] Authorized registration of agent at slave(442)@172.17.0.2:45634 (455912973e2c)_x000D_
_x000D_
3: I0123 19:22:05.936261 15992 master.cpp:6234] Registering agent at slave(442)@172.17.0.2:45634 (455912973e2c) with id eef8ea11-9247-44f3-84cf-340b24df3a52-S0_x000D_
_x000D_
3: I0123 19:22:05.936993 15989 registrar.cpp:495] Applied 1 operations in 227911ns; attempting to update the registry_x000D_
_x000D_
3: I0123 19:22:05.937814 15989 registrar.cpp:552] Successfully updated the registry in 743168ns_x000D_
_x000D_
3: I0123 19:22:05.938057 15991 master.cpp:6282] Admitted agent eef8ea11-9247-44f3-84cf-340b24df3a52-S0 at slave(442)@172.17.0.2:45634 (455912973e2c)_x000D_
_x000D_
3: I0123 19:22:05.938891 15991 master.cpp:6331] Registered agent eef8ea11-9247-44f3-84cf-340b24df3a52-S0 at slave(442)@172.17.0.2:45634 (455912973e2c) with cpus:2; mem:1024; disk:1024; ports:[31000-32000]_x000D_
_x000D_
3: I0123 19:22:05.939159 16002 slave.cpp:1764] Will retry registration in 26.332876ms if necessary_x000D_
_x000D_
3: I0123 19:22:05.939349 15994 master.cpp:6061] Received register agent message from slave(442)@172.17.0.2:45634 (455912973e2c)_x000D_
_x000D_
3: I0123 19:22:05.939347 15998 hierarchical.cpp:574] Added agent eef8ea11-9247-44f3-84cf-340b24df3a52-S0 (455912973e2c) with cpus:2; mem:1024; disk:1024; ports:[31000-32000] (allocated: {})_x000D_
_x000D_
3: I0123 19:22:05.939574 15994 master.cpp:3867] Authorizing agent with principal 'test-principal'_x000D_
_x000D_
3: I0123 19:22:05.939704 16002 slave.cpp:1366] Registered with master master@172.17.0.2:45634; given agent ID eef8ea11-9247-44f3-84cf-340b24df3a52-S0_x000D_
_x000D_
3: I0123 19:22:05.939894 15999 task_status_update_manager.cpp:188] Resuming sending task status updates_x000D_
_x000D_
3: I0123 19:22:05.940163 15998 hierarchical.cpp:1517] Performed allocation for 1 agents in 231470ns_x000D_
_x000D_
3: I0123 19:22:05.940194 16001 master.cpp:6123] Authorized registration of agent at slave(442)@172.17.0.2:45634 (455912973e2c)_x000D_
_x000D_
3: I0123 19:22:05.940263 16001 master.cpp:6213] Agent eef8ea11-9247-44f3-84cf-340b24df3a52-S0 at slave(442)@172.17.0.2:45634 (455912973e2c) already registered, resending acknowledgement_x000D_
_x000D_
3: I0123 19:22:05.942983 15994 process.cpp:3515] Handling HTTP event for process 'master' with path: '/master/api/v1'_x000D_
_x000D_
3: I0123 19:22:05.944905 15995 http.cpp:1185] HTTP POST for /master/api/v1 from 172.17.0.2:33442_x000D_
_x000D_
3: I0123 19:22:05.945107 15995 http.cpp:682] Processing call MARK_AGENT_GONE_x000D_
_x000D_
3: I0123 19:22:05.945749 16001 http.cpp:5363] Marking agent 'eef8ea11-9247-44f3-84cf-340b24df3a52-S0' as gone_x000D_
_x000D_
3: I0123 19:22:05.946480 15997 registrar.cpp:495] Applied 1 operations in 186752ns; attempting to update the registry_x000D_
_x000D_
3: I0123 19:22:05.947284 15997 registrar.cpp:552] Successfully updated the registry in 730112ns_x000D_
_x000D_
3: I0123 19:22:05.948225 15988 hierarchical.cpp:609] Removed agent eef8ea11-9247-44f3-84cf-340b24df3a52-S0_x000D_
_x000D_
3: I0123 19:22:05.952500 16002 slave.cpp:1386] Checkpointing SlaveInfo to '/tmp/MasterTest_RegistryGcByCount_HbzHl2/meta/slaves/eef8ea11-9247-44f3-84cf-340b24df3a52-S0/slave.info'_x000D_
_x000D_
3: I0123 19:22:05.953299 16002 slave.cpp:1433] Forwarding agent update \{""operations"":{},""resource_version_uuid"":\{""value"":""nekTyNfGT1S5DNQZxKJ72A==""},""slave_id"":\{""value"":""eef8ea11-9247-44f3-84cf-340b24df3a52-S0""},""update_oversubscribed_resources"":true}_x000D_
_x000D_
3: W0123 19:22:05.953675 16002 slave.cpp:1415] Already registered with master master@172.17.0.2:45634_x000D_
_x000D_
3: I0123 19:22:05.953790 16002 slave.cpp:1433] Forwarding agent update \{""operations"":{},""resource_version_uuid"":\{""value"":""nekTyNfGT1S5DNQZxKJ72A==""},""slave_id"":\{""value"":""eef8ea11-9247-44f3-84cf-340b24df3a52-S0""},""update_oversubscribed_resources"":true}_x000D_
_x000D_
3: I0123 19:22:05.954031 16002 slave.cpp:964] Agent asked to shut down by master@172.17.0.2:45634 because 'Agent has been marked gone'_x000D_
_x000D_
3: I0123 19:22:05.954082 16002 slave.cpp:931] Agent terminating_x000D_
_x000D_
3: W0123 19:22:05.954145 15993 master.cpp:7235] Ignoring update on removed agent eef8ea11-9247-44f3-84cf-340b24df3a52-S0_x000D_
_x000D_
3: W0123 19:22:05.954636 15993 master.cpp:7235] Ignoring update on removed agent eef8ea11-9247-44f3-84cf-340b24df3a52-S0_x000D_
_x000D_
3: W0123 19:22:05.955550 15986 process.cpp:2756] Attempted to spawn already running process files@172.17.0.2:45634_x000D_
_x000D_
3: I0123 19:22:05.956634 15986 containerizer.cpp:304] Using isolation \{ environment_secret, posix/cpu, posix/mem, filesystem/posix, network/cni }_x000D_
_x000D_
3: W0123 19:22:05.957228 15986 backend.cpp:76] Failed to create 'aufs' backend: AufsBackend requires root privileges_x000D_
_x000D_
3: W0123 19:22:05.957363 15986 backend.cpp:76] Failed to create 'bind' backend: BindBackend requires root privileges_x000D_
_x000D_
3: I0123 19:22:05.957401 15986 provisioner.cpp:299] Using default backend 'copy'_x000D_
_x000D_
3: I0123 19:22:05.959393 15986 cluster.cpp:460] Creating default 'local' authorizer_x000D_
_x000D_
3: I0123 19:22:05.961545 15998 slave.cpp:262] Mesos agent started on (443)@172.17.0.2:45634_x000D_
_x000D_
3: I0123 19:22:05.961560 15998 slave.cpp:263] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/MasterTest_RegistryGcByCount_2Nh5JR/store/appc"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""false"" --authenticatee=""crammd5"" --authentication_backoff_factor=""1secs"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/MasterTest_RegistryGcByCount_2Nh5JR/credential"" --default_role=""*"" --disallow_sharing_agent_pid_namespace=""false"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/MasterTest_RegistryGcByCount_2Nh5JR/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_reregistration_timeout=""2secs"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/MasterTest_RegistryGcByCount_2Nh5JR/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_command_executor=""false"" --http_credentials=""/tmp/MasterTest_RegistryGcByCount_2Nh5JR/http_credentials"" --http_heartbeat_interval=""30secs"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher=""posix"" --launcher_dir=""/mesos/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_executors_per_framework=""150"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --port=""5051"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --reconfiguration_policy=""equal"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --runtime_dir=""/tmp/MasterTest_RegistryGcByCount_2Nh5JR"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/MasterTest_RegistryGcByCount_FyzIlU"" --zk_session_timeout=""10secs""_x000D_
_x000D_
3: I0123 19:22:05.961917 15998 credentials.hpp:86] Loading credential for authentication from '/tmp/MasterTest_RegistryGcByCount_2Nh5JR/credential'_x000D_
_x000D_
3: I0123 19:22:05.962070 15998 slave.cpp:295] Agent using credential for: test-principal_x000D_
_x000D_
3: I0123 19:22:05.962090 15998 credentials.hpp:37] Loading credentials for authentication from '/tmp/MasterTest_RegistryGcByCount_2Nh5JR/http_credentials'_x000D_
_x000D_
3: I0123 19:22:05.962347 15998 http.cpp:1045] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-readonly'_x000D_
_x000D_
3: I0123 19:22:05.964292 15988 process.cpp:3515] Handling HTTP event for process 'master' with path: '/master/api/v1'_x000D_
_x000D_
3: I0123 19:22:05.964375 15998 slave.cpp:612] Agent resources: [\{""name"":""cpus"",""scalar"":{""value"":2.0},""type"":""SCALAR""},\{""name"":""mem"",""scalar"":{""value"":1024.0},""type"":""SCALAR""},\{""name"":""disk"",""scalar"":{""value"":1024.0},""type"":""SCALAR""},\{""name"":""ports"",""ranges"":{""range"":[{""begin"":31000,""end"":32000}]},""type"":""RANGES""}]_x000D_
_x000D_
3: I0123 19:22:05.964591 15998 slave.cpp:620] Agent attributes: [  ]_x000D_
_x000D_
3: I0123 19:22:05.964601 15998 slave.cpp:629] Agent hostname: 455912973e2c_x000D_
_x000D_
3: I0123 19:22:05.964753 15995 task_status_update_manager.cpp:181] Pausing sending task status updates_x000D_
_x000D_
3: I0123 19:22:05.966159 16002 http.cpp:1185] HTTP POST for /master/api/v1 from 172.17.0.2:33443_x000D_
_x000D_
3: I0123 19:22:05.966327 16002 http.cpp:682] Processing call MARK_AGENT_GONE_x000D_
_x000D_
3: I0123 19:22:05.966898 15990 http.cpp:5363] Marking agent 'eef8ea11-9247-44f3-84cf-340b24df3a52-S0' as gone_x000D_
_x000D_
3: W0123 19:22:05.966939 15990 http.cpp:5366] Not marking agent 'eef8ea11-9247-44f3-84cf-340b24df3a52-S0' as gone because it has already transitioned to gone_x000D_
_x000D_
3: I0123 19:22:05.966969 15992 state.cpp:66] Recovering state from '/tmp/MasterTest_RegistryGcByCount_FyzIlU/meta'_x000D_
_x000D_
3: I0123 19:22:05.967445 15995 task_status_update_manager.cpp:207] Recovering task status update manager_x000D_
_x000D_
3: I0123 19:22:05.967747 15991 containerizer.cpp:674] Recovering containerizer_x000D_
_x000D_
3: I0123 19:22:05.969804 15999 provisioner.cpp:493] Provisioner recovery complete_x000D_
_x000D_
3: I0123 19:22:05.970371 16002 slave.cpp:6822] Finished recovery_x000D_
_x000D_
3: I0123 19:22:05.971616 16000 task_status_update_manager.cpp:181] Pausing sending task status updates_x000D_
_x000D_
3: I0123 19:22:05.971608 15987 slave.cpp:1146] New master detected at master@172.17.0.2:45634_x000D_
_x000D_
3: I0123 19:22:05.971737 15987 slave.cpp:1201] Detecting new master_x000D_
_x000D_
3: I0123 19:22:05.971755 15990 hierarchical.cpp:1517] Performed allocation for 0 agents in 134021ns_x000D_
_x000D_
3: I0123 19:22:05.972729 15999 slave.cpp:6360] Current disk usage 16.56%. Max allowed age: 5.140805523372986days_x000D_
_x000D_
3: I0123 19:22:05.972985 15988 master.cpp:1878] Skipping periodic registry garbage collection: no agents qualify for removal_x000D_
_x000D_
3: I0123 19:22:05.974480 16001 slave.cpp:1228] Authenticating with master master@172.17.0.2:45634_x000D_
_x000D_
3: I0123 19:22:05.974581 16001 slave.cpp:1237] Using default CRAM-MD5 authenticatee_x000D_
_x000D_
3: I0123 19:22:05.975023 16002 authenticatee.cpp:121] Creating new client SASL connection_x000D_
_x000D_
3: I0123 19:22:05.975343 15998 master.cpp:8958] Authenticating slave(443)@172.17.0.2:45634_x000D_
_x000D_
3: I0123 19:22:05.975476 16000 authenticator.cpp:414] Starting authentication session for crammd5-authenticatee(871)@172.17.0.2:45634_x000D_
_x000D_
3: I0123 19:22:05.975735 15991 authenticator.cpp:98] Creating new server SASL connection_x000D_
_x000D_
3: I0123 19:22:05.976027 15995 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5_x000D_
_x000D_
3: I0123 19:22:05.976143 15995 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'_x000D_
_x000D_
3: I0123 19:22:05.976356 15989 authenticator.cpp:204] Received SASL authentication start_x000D_
_x000D_
3: I0123 19:22:05.976420 15989 authenticator.cpp:326] Authentication requires more steps_x000D_
_x000D_
3: I0123 19:22:05.976552 15990 authenticatee.cpp:259] Received SASL authentication step_x000D_
_x000D_
3: I0123 19:22:05.976698 15987 authenticator.cpp:232] Received SASL authentication step_x000D_
_x000D_
3: I0123 19:22:05.976750 15987 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: '455912973e2c' server FQDN: '455912973e2c' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false _x000D_
_x000D_
3: I0123 19:22:05.976773 15987 auxprop.cpp:181] Looking up auxiliary property '*userPassword'_x000D_
_x000D_
3: I0123 19:22:05.976821 15987 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'_x000D_
_x000D_
3: I0123 19:22:05.976898 15987 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: '455912973e2c' server FQDN: '455912973e2c' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true _x000D_
_x000D_
3: I0123 19:22:05.976924 15987 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true_x000D_
_x000D_
3: I0123 19:22:05.976935 15987 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true_x000D_
_x000D_
3: I0123 19:22:05.976953 15987 authenticator.cpp:318] Authentication success_x000D_
_x000D_
3: I0123 19:22:05.977094 15996 authenticatee.cpp:299] Authentication success_x000D_
_x000D_
3: I0123 19:22:05.977233 15994 master.cpp:8988] Successfully authenticated principal 'test-principal' at slave(443)@172.17.0.2:45634_x000D_
_x000D_
3: I0123 19:22:05.977321 15987 authenticator.cpp:432] Authentication session cleanup for crammd5-authenticatee(871)@172.17.0.2:45634_x000D_
_x000D_
3: I0123 19:22:05.977475 15996 slave.cpp:1320] Successfully authenticated with master master@172.17.0.2:45634_x000D_
_x000D_
3: I0123 19:22:05.977953 15996 slave.cpp:1764] Will retry registration in 6.841446ms if necessary_x000D_
_x000D_
3: I0123 19:22:05.978238 15992 master.cpp:6061] Received register agent message from slave(443)@172.17.0.2:45634 (455912973e2c)_x000D_
_x000D_
3: I0123 19:22:05.978591 15992 master.cpp:3867] Authorizing agent with principal 'test-principal'_x000D_
_x000D_
3: I0123 19:22:05.979161 16000 master.cpp:6123] Authorized registration of agent at slave(443)@172.17.0.2:45634 (455912973e2c)_x000D_
_x000D_
3: I0123 19:22:05.979320 16000 master.cpp:6234] Registering agent at slave(443)@172.17.0.2:45634 (455912973e2c) with id eef8ea11-9247-44f3-84cf-340b24df3a52-S1_x000D_
_x000D_
3: I0123 19:22:05.980505 15991 registrar.cpp:495] Applied 1 operations in 455955ns; attempting to update the registry_x000D_
_x000D_
3: I0123 19:22:05.981642 15991 registrar.cpp:552] Successfully updated the registry in 0ns_x000D_
_x000D_
3: I0123 19:22:05.981912 15988 master.cpp:6282] Admitted agent eef8ea11-9247-44f3-84cf-340b24df3a52-S1 at slave(443)@172.17.0.2:45634 (455912973e2c)_x000D_
_x000D_
3: I0123 19:22:05.982857 15988 master.cpp:6331] Registered agent eef8ea11-9247-44f3-84cf-340b24df3a52-S1 at slave(443)@172.17.0.2:45634 (455912973e2c) with cpus:2; mem:1024; disk:1024; ports:[31000-32000]_x000D_
_x000D_
3: I0123 19:22:05.982964 16001 slave.cpp:1366] Registered with master master@172.17.0.2:45634; given agent ID eef8ea11-9247-44f3-84cf-340b24df3a52-S1_x000D_
_x000D_
3: I0123 19:22:05.983130 15996 task_status_update_manager.cpp:188] Resuming sending task status updates_x000D_
_x000D_
3: I0123 19:22:05.983392 16001 slave.cpp:1386] Checkpointing SlaveInfo to '/tmp/MasterTest_RegistryGcByCount_FyzIlU/meta/slaves/eef8ea11-9247-44f3-84cf-340b24df3a52-S1/slave.info'_x000D_
_x000D_
3: I0123 19:22:05.983423 15994 hierarchical.cpp:574] Added agent eef8ea11-9247-44f3-84cf-340b24df3a52-S1 (455912973e2c) with cpus:2; mem:1024; disk:1024; ports:[31000-32000] (allocated: {})_x000D_
_x000D_
3: I0123 19:22:05.983815 15994 hierarchical.cpp:1517] Performed allocation for 1 agents in 171516ns_x000D_
_x000D_
3: I0123 19:22:05.984135 16001 slave.cpp:1433] Forwarding agent update \{""operations"":{},""resource_version_uuid"":\{""value"":""1HNo1ICkRY24eDUqFmb6+Q==""},""slave_id"":\{""value"":""eef8ea11-9247-44f3-84cf-340b24df3a52-S1""},""update_oversubscribed_resources"":true}_x000D_
_x000D_
3: I0123 19:22:05.984762 15997 master.cpp:7265] Received update of agent eef8ea11-9247-44f3-84cf-340b24df3a52-S1 at slave(443)@172.17.0.2:45634 (455912973e2c) with total oversubscribed resources {}_x000D_
_x000D_
3: I0123 19:22:05.985123 15997 master.cpp:7359] Ignoring update on agent eef8ea11-9247-44f3-84cf-340b24df3a52-S1 at slave(443)@172.17.0.2:45634 (455912973e2c) as it reports no changes_x000D_
_x000D_
3: W0123 19:22:05.985782 15986 process.cpp:2756] Attempted to spawn already running process version@172.17.0.2:45634_x000D_
_x000D_
3: I0123 19:22:05.986744 15986 sched.cpp:232] Version: 1.5.0_x000D_
_x000D_
3: I0123 19:22:05.987470 15990 sched.cpp:336] New master detected at master@172.17.0.2:45634_x000D_
_x000D_
3: I0123 19:22:05.987567 15990 sched.cpp:396] Authenticating with master master@172.17.0.2:45634_x000D_
_x000D_
3: I0123 19:22:05.987582 15990 sched.cpp:403] Using default CRAM-MD5 authenticatee_x000D_
_x000D_
3: I0123 19:22:05.987869 15999 authenticatee.cpp:121] Creating new client SASL connection_x000D_
_x000D_
3: I0123 19:22:05.988121 15991 master.cpp:8958] Authenticating scheduler-be5d74bf-c4bf-4504-9e67-6a4f8df93425@172.17.0.2:45634_x000D_
_x000D_
3: I0123 19:22:05.988296 15987 authenticator.cpp:414] Starting authentication session for crammd5-authenticatee(872)@172.17.0.2:45634_x000D_
_x000D_
3: I0123 19:22:05.988575 15988 authenticator.cpp:98] Creating new server SASL connection_x000D_
_x000D_
3: I0123 19:22:05.988821 15996 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5_x000D_
_x000D_
3: I0123 19:22:05.988852 15996 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'_x000D_
_x000D_
3: I0123 19:22:05.988967 15994 authenticator.cpp:204] Received SASL authentication start_x000D_
_x000D_
3: I0123 19:22:05.989023 15994 authenticator.cpp:326] Authentication requires more steps_x000D_
_x000D_
3: I0123 19:22:05.989145 15993 authenticatee.cpp:259] Received SASL authentication step_x000D_
_x000D_
3: I0123 19:22:05.989271 16001 authenticator.cpp:232] Received SASL authentication step_x000D_
_x000D_
3: I0123 19:22:05.989316 16001 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: '455912973e2c' server FQDN: '455912973e2c' SASL_AUXPROP_VER:_x000D_
_x000D_
IFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false _x000D_
_x000D_
3: I0123 19:22:05.989334 16001 auxprop.cpp:181] Looking up auxiliary property '*userPassword'_x000D_
_x000D_
3: I0123 19:22:05.989377 16001 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'_x000D_
_x000D_
3: I0123 19:22:05.989400 16001 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: '455912973e2c' server FQDN: '455912973e2c' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true _x000D_
_x000D_
3: I0123 19:22:05.989415 16001 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true_x000D_
_x000D_
3: I0123 19:22:05.989423 16001 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true_x000D_
_x000D_
3: I0123 19:22:05.989441 16001 authenticator.cpp:318] Authentication success_x000D_
_x000D_
3: I0123 19:22:05.989531 15997 authenticatee.cpp:299] Authentication success_x000D_
_x000D_
3: I0123 19:22:05.989719 15992 master.cpp:8988] Successfully authenticated principal 'test-principal' at scheduler-be5d74bf-c4bf-4504-9e67-6a4f8df93425@172.17.0.2:45634_x000D_
_x000D_
3: I0123 19:22:05.989751 16000 authenticator.cpp:432] Authentication session cleanup for crammd5-authenticatee(872)@172.17.0.2:45634_x000D_
_x000D_
3: I0123 19:22:05.989894 15998 sched.cpp:502] Successfully authenticated with master master@172.17.0.2:45634_x000D_
_x000D_
3: I0123 19:22:05.989914 15998 sched.cpp:824] Sending SUBSCRIBE call to master@172.17.0.2:45634_x000D_
_x000D_
3: I0123 19:22:05.990039 15998 sched.cpp:857] Will retry registration in 1.379182754secs if necessary_x000D_
_x000D_
3: I0123 19:22:05.990229 15991 master.cpp:2958] Received SUBSCRIBE call for framework 'default' at scheduler-be5d74bf-c4bf-4504-9e67-6a4f8df93425@172.17.0.2:45634_x000D_
_x000D_
3: I0123 19:22:05.990311 15991 master.cpp:2275] Authorizing framework principal 'test-principal' to receive offers for roles '\{ * }'_x000D_
_x000D_
3: I0123 19:22:05.990876 15987 master.cpp:3038] Subscribing framework default with checkpointing disabled and capabilities [ MULTI_ROLE, RESERVATION_REFINEMENT, PARTITION_AWARE ]_x000D_
_x000D_
3: I0123 19:22:05.991102 15987 master.cpp:9179] Adding framework eef8ea11-9247-44f3-84cf-340b24df3a52-0000 (default) at scheduler-be5d74bf-c4bf-4504-9e67-6a4f8df93425@172.17.0.2:45634 with roles {  } suppressed_x000D_
_x000D_
3: I0123 19:22:05.991621 15994 sched.cpp:751] Framework registered with eef8ea11-9247-44f3-84cf-340b24df3a52-0000_x000D_
_x000D_
3: I0123 19:22:05.991731 15988 hierarchical.cpp:297] Added framework eef8ea11-9247-44f3-84cf-340b24df3a52-0000_x000D_
_x000D_
3: I0123 19:22:05.991816 15994 sched.cpp:765] Scheduler::registered took 25842ns_x000D_
_x000D_
3: I0123 19:22:05.993170 15988 hierarchical.cpp:1517] Performed allocation for 1 agents in 1.236264ms_x000D_
_x000D_
3: I0123 19:22:05.993611 15997 master.cpp:8788] Sending 1 offers to framework eef8ea11-9247-44f3-84cf-340b24df3a52-0000 (default) at scheduler-be5d74bf-c4bf-4504-9e67-6a4f8df93425@172.17.0.2:45634_x000D_
_x000D_
3: _x000D_
_x000D_
3: GMOCK WARNING:_x000D_
_x000D_
3: Uninteresting mock function call - returning directly._x000D_
_x000D_
3:     Function call: resourceOffers(0x7ffcbfa4e140, @0x7f82fc814860 \{ 160-byte object <10-22 C5-0C 83-7F 00-00 00-00 00-00 00-00 00-00 5F-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 04-00 00-00 04-00 00-00 80-C0 03-C8 82-7F 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 ... 10-38 04-C8 82-7F 00-00 E0-C4 03-C8 82-7F 00-00 10-B3 00-C8 82-7F 00-00 60-A7 03-C8 82-7F 00-00 10-C3 00-C8 82-7F 00-00 00-00 00-00 00-00 00-00 10-DA 00-C8 82-7F 00-00 00-00 00-00 00-00 00-00> })_x000D_
_x000D_
3: NOTE: You can safely ignore the above warning unless this call should not happen.  Do not suppress it by blindly adding an EXPECT_CALL() if you don't mean to enforce the call.  See https://github.com/google/googletest/blob/master/googlemock/docs/CookBook.md#knowing-when-to-expect for details._x000D_
_x000D_
3: I0123 19:22:05.994109 16001 sched.cpp:921] Scheduler::resourceOffers took 77564ns_x000D_
_x000D_
3: I0123 19:22:05.994665 15989 master.cpp:8425] Performing explicit task state reconciliation for 2 tasks of framework eef8ea11-9247-44f3-84cf-340b24df3a52-0000 (default) at scheduler-be5d74bf-c4bf-4504-9e67-6a4f8df93425@172.17.0.2:45634_x000D_
_x000D_
3: I0123 19:22:05.994779 15989 master.cpp:8573] Sending explicit reconciliation state TASK_GONE_BY_OPERATOR for task 7a4ff1bf-488b-4152-a7e4-cf0876008c4d of framework eef8ea11-9247-44f3-84cf-340b24df3a52-0000 (default) at scheduler-be5d74bf-c4bf-4504-9e67-6a4f8df93425@172.17.0.2:45634_x000D_
_x000D_
3: I0123 19:22:05.994936 15989 master.cpp:8573] Sending explicit reconciliation state TASK_GONE_BY_OPERATOR for task 9d4e66ef-86c7-428a-b110-ba949c0c19b8 of framework eef8ea11-9247-44f3-84cf-340b24df3a52-0000 (default) at scheduler-be5d74bf-c4bf-4504-9e67-6a4f8df93425@172.17.0.2:45634_x000D_
_x000D_
3: I0123 19:22:05.995215 15989 sched.cpp:1029] Scheduler::statusUpdate took 36257ns_x000D_
_x000D_
3: I0123 19:22:05.995381 15989 sched.cpp:1029] Scheduler::statusUpdate took 36862ns_x000D_
_x000D_
3: /mesos/src/tests/master_tests.cpp:8606: Failure_x000D_
_x000D_
3:       Expected: TASK_UNKNOWN_x000D_
_x000D_
3: To be equal to: reconcileUpdate1->state()_x000D_
_x000D_
3:       Which is: TASK_GONE_BY_OPERATOR_x000D_
_x000D_
3: I0123 19:22:05.995779 16000 master.cpp:1420] Framework eef8ea11-9247-44f3-84cf-340b24df3a52-0000 (default) at scheduler-be5d74bf-c4bf-4504-9e67-6a4f8df93425@172.17.0.2:45634 disconnected_x000D_
_x000D_
3: I0123 19:22:05.995802 16000 master.cpp:3328] Deactivating framework eef8ea11-9247-44f3-84cf-340b24df3a52-0000 (default) at scheduler-be5d74bf-c4bf-4504-9e67-6a4f8df93425@172.17.0.2:45634_x000D_
_x000D_
3: I0123 19:22:05.996014 15991 hierarchical.cpp:405] Deactivated framework eef8ea11-9247-44f3-84cf-340b24df3a52-0000_x000D_
_x000D_
3: I0123 19:22:05.996299 15994 slave.cpp:931] Agent terminating_x000D_
_x000D_
3: I0123 19:22:05.996402 16000 master.cpp:10703] Removing offer eef8ea11-9247-44f3-84cf-340b24df3a52-O0_x000D_
_x000D_
3: I0123 19:22:05.996474 16000 master.cpp:3305] Disconnecting framework eef8ea11-9247-44f3-84cf-340b24df3a52-0000 (default) at scheduler-be5d74bf-c4bf-4504-9e67-6a4f8df93425@172.17.0.2:45634_x000D_
_x000D_
3: I0123 19:22:05.996522 16000 master.cpp:1435] Giving framework eef8ea11-9247-44f3-84cf-340b24df3a52-0000 (default) at scheduler-be5d74bf-c4bf-4504-9e67-6a4f8df93425@172.17.0.2:45634 0ns to failover_x000D_
_x000D_
3: I0123 19:22:05.996711 16000 master.cpp:1306] Agent eef8ea11-9247-44f3-84cf-340b24df3a52-S1 at slave(443)@172.17.0.2:45634 (455912973e2c) disconnected_x000D_
_x000D_
3: I0123 19:22:05.996732 16000 master.cpp:3365] Disconnecting agent eef8ea11-9247-44f3-84cf-340b24df3a52-S1 at slave(443)@172.17.0.2:45634 (455912973e2c)_x000D_
_x000D_
3: I0123 19:22:05.996789 16000 master.cpp:3384] Deactivating agent eef8ea11-9247-44f3-84cf-340b24df3a52-S1 at slave(443)@172.17.0.2:45634 (455912973e2c)_x000D_
_x000D_
3: I0123 19:22:05.997368 15991 hierarchical.cpp:1192] Recovered cpus(allocated: *):2; mem(allocated: *):1024; disk(allocated: *):1024; ports(allocated: *):[31000-32000] (total: cpus:2; mem:1024; disk:1024; ports:[31000-32000], allocated: {}) on agent eef8ea11-9247-44f3-84cf-340b24df3a52-S1 from framework eef8ea11-9247-44f3-84cf-340b24df3a52-0000_x000D_
_x000D_
3: I0123 19:22:05.997454 15991 hierarchical.cpp:766] Agent eef8ea11-9247-44f3-84cf-340b24df3a52-S1 deactivated_x000D_
_x000D_
3: I0123 19:22:05.998085 15992 master.cpp:8603] Framework failover timeout, removing framework eef8ea11-9247-44f3-84cf-340b24df3a52-0000 (default) at scheduler-be5d74bf-c4bf-4504-9e67-6a4f8df93425@172.17.0.2:45634_x000D_
_x000D_
3: I0123 19:22:05.998239 15992 master.cpp:9480] Removing framework eef8ea11-9247-44f3-84cf-340b24df3a52-0000 (default) at scheduler-be5d74bf-c4bf-4504-9e67-6a4f8df93425@172.17.0.2:45634_x000D_
_x000D_
3: I0123 19:22:05.998915 15995 hierarchical.cpp:344] Removed framework eef8ea11-9247-44f3-84cf-340b24df3a52-0000_x000D_
_x000D_
3: I0123 19:22:06.013475 15986 master.cpp:1148] Master terminating_x000D_
_x000D_
3: I0123 19:22:06.014463 15994 hierarchical.cpp:609] Removed agent eef8ea11-9247-44f3-84cf-340b24df3a52-S1_x000D_
_x000D_
3: [  FAILED  ] MasterTest.RegistryGcByCount (172 ms)_x000D_
_x000D_
{code}",3.0,1.5.0,0.5,0.0,0.3333333333333333,0.5,0.14285714285714285,0.0,0.2,0.5087719298245614,0.4357142857142857,0.8846153846153846,0.9878787878787878,0.9878787878787878,0.963302752293578
Bug,ExampleTests PythonFramework fails with sigabort.,"Starting the {{PythonFramework}} manually results in a sigabort:_x000D_
_x000D_
{noformat}_x000D_
$ ./src/examples/python/test-framework local_x000D_
[..]_x000D_
I0124 15:22:46.637238 65925120 master.cpp:563] Using default 'crammd5' authenticator_x000D_
W0124 15:22:46.637269 65925120 authenticator.cpp:513] No credentials provided, authentication requests will be refused_x000D_
I0124 15:22:46.637284 65925120 authenticator.cpp:520] Initializing server SASL_x000D_
I0124 15:22:46.659503 2385417024 resolver.cpp:69] Creating default secret resolver_x000D_
I0124 15:22:46.659624 2385417024 containerizer.cpp:304] Using isolation { environment_secret, filesystem/posix, posix/mem, posix/cpu }_x000D_
I0124 15:22:46.659951 2385417024 provisioner.cpp:299] Using default backend 'copy'_x000D_
I0124 15:22:46.661628 67534848 slave.cpp:262] Mesos agent started on (1)@192.168.178.20:49682_x000D_
I0124 15:22:46.661669 67534848 slave.cpp:263] Flags at startup: --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/var/folders/_t/rdp354gx7j5fjww270kbk6_r0000gn/T/mesos/store/appc"" --authenticate_http_executors=""false"" --authenticate_http_readonly=""false"" --authenticate_http_readwrite=""false"" --authenticatee=""crammd5"" --authentication_backoff_factor=""1secs"" --authorizer=""local"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/var/folders/_t/rdp354gx7j5fjww270kbk6_r0000gn/T/mesos/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_reregistration_timeout=""2secs"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/var/folders/_t/rdp354gx7j5fjww270kbk6_r0000gn/T/mesos/work/agents/0/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_command_executor=""false"" --http_heartbeat_interval=""30secs"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher=""posix"" --launcher_dir=""/usr/local/libexec/mesos"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_executors_per_framework=""150"" --oversubscribed_resources_interval=""15secs"" --port=""5051"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --reconfiguration_policy=""equal"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""1secs"" --runtime_dir=""/var/folders/_t/rdp354gx7j5fjww270kbk6_r0000gn/T/mesos/work/agents/0/run"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --version=""false"" --work_dir=""/var/folders/_t/rdp354gx7j5fjww270kbk6_r0000gn/T/mesos/work/agents/0/work"" --zk_session_timeout=""10secs""_x000D_
python(1780,0x700004068000) malloc: *** error for object 0x106ac07c8: pointer being freed was not allocated_x000D_
*** set a breakpoint in malloc_error_break to debug_x000D_
{noformat}_x000D_
_x000D_
_x000D_
When running the {{PythonFramework}} via lldb, I get the following stacktrace:_x000D_
_x000D_
{noformat}_x000D_
* thread #7, stop reason = signal SIGABRT_x000D_
  * frame #0: 0x00007fff55321e3e libsystem_kernel.dylib`__pthread_kill + 10_x000D_
    frame #1: 0x00007fff55460150 libsystem_pthread.dylib`pthread_kill + 333_x000D_
    frame #2: 0x00007fff5527e312 libsystem_c.dylib`abort + 127_x000D_
    frame #3: 0x00007fff5537b866 libsystem_malloc.dylib`free + 521_x000D_
    frame #4: 0x000000010d24daac _scheduler.so`google::protobuf::internal::ArenaStringPtr::DestroyNoArena(this=0x000070000ac355b0, default_value="""") at arenastring.h:264_x000D_
    frame #5: 0x000000010d2fe1aa _scheduler.so`mesos::Resource::SharedDtor(this=0x000070000ac35580) at mesos.pb.cc:31016_x000D_
    frame #6: 0x000000010d2fe063 _scheduler.so`mesos::Resource::~Resource(this=0x000070000ac35580) at mesos.pb.cc:31011_x000D_
    frame #7: 0x000000010d2fe485 _scheduler.so`mesos::Resource::~Resource(this=0x000070000ac35580) at mesos.pb.cc:31009_x000D_
    frame #8: 0x000000010b0257c7 _scheduler.so`mesos::Resources::parse(name=""cpus"", value=""8"", role=""*"") at resources.cpp:702_x000D_
    frame #9: 0x000000010c7ae4c9 _scheduler.so`mesos::internal::slave::Containerizer::resources(flags=0x000000010202bac0) at containerizer.cpp:118_x000D_
    frame #10: 0x000000010c3a93e1 _scheduler.so`mesos::internal::slave::Slave::initialize(this=0x000000010202ba00) at slave.cpp:472_x000D_
    frame #11: 0x000000010c3d7cb2 _scheduler.so`virtual thunk to mesos::internal::slave::Slave::initialize(this=0x000000010202ba00) at slave.cpp:0_x000D_
    frame #12: 0x000000010e459c39 _scheduler.so`process::ProcessManager::resume(this=0x00000001005790f0, process=0x000000010202ba00) at process.cpp:2819_x000D_
    frame #13: 0x000000010e57ef75 _scheduler.so`process::ProcessManager::init_threads(this=0x00000001001a9fa8)::$_2::operator()() const at process.cpp:2443_x000D_
    frame #14: 0x000000010e57eb30 _scheduler.so`void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, process::ProcessManager::init_threads()::$_2> >(void*) [inlined] decltype(__f=0x00000001001a9fa8)::$_2>(fp)(std::__1::forward<>(fp0))) std::__1::__invoke<process::ProcessManager::init_threads()::$_2>(process::ProcessManager::init_threads()::$_2&&) at type_traits:4291_x000D_
    frame #15: 0x000000010e57eb1f _scheduler.so`void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, process::ProcessManager::init_threads()::$_2> >(void*) [inlined] void std::__1::__thread_execute<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, process::ProcessManager::init_threads()::$_2>(__t=0x00000001001a9fa0)::$_2>&, std::__1::__tuple_indices<>) at thread:336_x000D_
    frame #16: 0x000000010e57eafb _scheduler.so`void* std::__1::__thread_proxy<std::__1::tuple<std::__1::unique_ptr<std::__1::__thread_struct, std::__1::default_delete<std::__1::__thread_struct> >, process::ProcessManager::init_threads()::$_2> >(__vp=0x00000001001a9fa0) at thread:346_x000D_
    frame #17: 0x00007fff5545d6c1 libsystem_pthread.dylib`_pthread_body + 340_x000D_
    frame #18: 0x00007fff5545d56d libsystem_pthread.dylib`_pthread_start + 377_x000D_
    frame #19: 0x00007fff5545cc5d libsystem_pthread.dylib`thread_start + 13_x000D_
{noformat}_x000D_
_x000D_
Given that this example works (most of the time) on other macOS systems, I am assuming this is a problem of my system.",3.0,1.5.0,0.0,0.3740573152337858,0.0,0.0,0.0,0.2,0.4,0.43859649122807015,0.2714285714285714,0.3717948717948718,0.24848484848484845,0.24848484848484845,0.963302752293578
Bug,Signed/Unsigned comparisons in tests,"Many tests in mesos currently have comparisons between signed and unsigned integers, eg_x000D_
{noformat}_x000D_
    ASSERT_EQ(4, v1Response->read_file().size());_x000D_
{noformat}_x000D_
or comparisons between values of different enums, e.g. TaskState and v1::TaskState:_x000D_
{noformat}_x000D_
  ASSERT_EQ(TASK_STARTING, startingUpdate->status().state());_x000D_
{noformat}_x000D_
Usually, the compiler would catch these and emit a warning, but these are currently silenced because gtest headers are included using the {{-isystem}} command line flag.",1.0,0,0.0,0.0,0.3333333333333333,0.5,0.14285714285714285,0.0,0.2,0.5087719298245614,0.4357142857142857,0.16666666666666666,0.1818181818181818,0.17575757575757575,0.0
Bug,Make clean fails without Python artifacts.,"Make clean may fail if there are no Python artifacts created by previous builds.  _x000D_
{noformat}_x000D_
$ make clean{noformat}_x000D_
{noformat}_x000D_
[...]_x000D_
rm -rf java/target_x000D_
rm -f examples/java/*.class_x000D_
rm -f java/jni/org_apache_mesos*.h_x000D_
find python \( -name ""build"" -o -name ""dist"" -o -name ""*.pyc""	\_x000D_
  -o -name ""*.egg-info"" \) -exec rm -rf '{}' \+_x000D_
find: ‘python’: No such file or directory_x000D_
make[1]: *** [clean-python] Error 1_x000D_
make[1]: Leaving directory `/home/centos/workspace/mesos/build/src'_x000D_
make: *** [clean-recursive] Error 1{noformat}_x000D_
 _x000D_
_x000D_
Triggered by [https://github.com/apache/mesos/blob/62d392704c499e06da0323e50dfd016cdac06f33/src/Makefile.am#L2218-L2219]",1.0,1.5.0,0.0,0.3740573152337858,0.0,0.0,0.0,0.2,0.4,0.43859649122807015,0.2714285714285714,0.3717948717948718,0.24848484848484845,0.24848484848484845,0.963302752293578
Bug,Mesos master might drop some events in the operator API stream,"Inside `Master::updateTask`, we call `Subscribers::send` which asynchronously calls `Subscribers::Subscriber::send` on each subscriber._x000D_
_x000D_
But the problem is that inside `Subscribers:Subscriber::send` we are looking up the state of the master (e.g., getting Task* and Framework*) which might have changed between `Subscribers::send ` and `Subscribers::Subscriber::send`._x000D_
_x000D_
 _x000D_
_x000D_
For example, if a terminal task received an acknowledgement the task might be removed from master's state, causing us to drop the TASK_UPDATED event._x000D_
_x000D_
 _x000D_
_x000D_
We noticed this in an internal cluster, where a TASK_KILLED update was sent to one subscriber but not the other._x000D_
_x000D_
 _x000D_
_x000D_
 _x000D_
_x000D_
 ",3.0,0,0.5,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.8846153846153846,0.9878787878787878,0.9878787878787878,0.0
Bug,Test MasterAllocatorTest/1.SingleFramework is flaky,"Observed in our internal CI on a ubuntu-16 setup in a plain autotools build,_x000D_
{noformat}_x000D_
../../src/tests/master_allocator_tests.cpp:175_x000D_
Mock function called more times than expected - taking default action specified at:_x000D_
../../src/tests/allocator.hpp:273:_x000D_
    Function call: addSlave(@0x7fe8dc03d0e8 1eb6ab2c-293d-4b99-b76b-87bd939a1a19-S1, @0x7fe8dc03d108 hostname: ""ip-172-16-10-65.ec2.internal""_x000D_
resources {_x000D_
  name: ""cpus""_x000D_
  type: SCALAR_x000D_
  scalar {_x000D_
    value: 2_x000D_
  }_x000D_
}_x000D_
resources {_x000D_
  name: ""mem""_x000D_
  type: SCALAR_x000D_
  scalar {_x000D_
    value: 1024_x000D_
  }_x000D_
}_x000D_
resources {_x000D_
  name: ""ports""_x000D_
  type: RANGES_x000D_
  ranges {_x000D_
    range {_x000D_
      begin: 31000_x000D_
      end: 32000_x000D_
    }_x000D_
  }_x000D_
}_x000D_
id {_x000D_
  value: ""1eb6ab2c-293d-4b99-b76b-87bd939a1a19-S1""_x000D_
}_x000D_
checkpoint: true_x000D_
port: 40262_x000D_
, @0x7fe8ffa276c0 { 32-byte object <48-94 7D-0E E9-7F 00-00 00-00 00-00 00-00 00-00 01-00 00-00 00-00 00-00 01-00 00-00 00-00 00-00>, 32-byte object <48-94 7D-0E E9-7F 00-00 00-00 00-00 00-00 00-00 01-00 00-00 00-00 00-00 02-00 00-00 00-00 00-00>, 32-byte object <48-94 7D-0E E9-7F 00-00 00-00 00-00 00-00 00-00 01-00 00-00 00-00 00-00 03-00 00-00 73-79 73-74> }, @0x7fe8ffa27720 48-byte object <01-00 00-00 E8-7F 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 08-7A A2-FF E8-7F 00-00 A0-32 24-7D 62-55 00-00 DE-3C 11-0A E9-7F 00-00>, @0x7fe8dc03d4c8 { cpus:2, mem:1024, ports:[31000-32000] }, @0x7fe8dc03d460 {})_x000D_
         Expected: to be called once_x000D_
           Actual: called twice - over-saturated and active_x000D_
Stacktrace_x000D_
../../src/tests/master_allocator_tests.cpp:175_x000D_
Mock function called more times than expected - taking default action specified at:_x000D_
../../src/tests/allocator.hpp:273:_x000D_
    Function call: addSlave(@0x7fe8dc03d0e8 1eb6ab2c-293d-4b99-b76b-87bd939a1a19-S1, @0x7fe8dc03d108 hostname: ""ip-172-16-10-65.ec2.internal""_x000D_
resources {_x000D_
  name: ""cpus""_x000D_
  type: SCALAR_x000D_
  scalar {_x000D_
    value: 2_x000D_
  }_x000D_
}_x000D_
resources {_x000D_
  name: ""mem""_x000D_
  type: SCALAR_x000D_
  scalar {_x000D_
    value: 1024_x000D_
  }_x000D_
}_x000D_
resources {_x000D_
  name: ""ports""_x000D_
  type: RANGES_x000D_
  ranges {_x000D_
    range {_x000D_
      begin: 31000_x000D_
      end: 32000_x000D_
    }_x000D_
  }_x000D_
}_x000D_
id {_x000D_
  value: ""1eb6ab2c-293d-4b99-b76b-87bd939a1a19-S1""_x000D_
}_x000D_
checkpoint: true_x000D_
port: 40262_x000D_
, @0x7fe8ffa276c0 { 32-byte object <48-94 7D-0E E9-7F 00-00 00-00 00-00 00-00 00-00 01-00 00-00 00-00 00-00 01-00 00-00 00-00 00-00>, 32-byte object <48-94 7D-0E E9-7F 00-00 00-00 00-00 00-00 00-00 01-00 00-00 00-00 00-00 02-00 00-00 00-00 00-00>, 32-byte object <48-94 7D-0E E9-7F 00-00 00-00 00-00 00-00 00-00 01-00 00-00 00-00 00-00 03-00 00-00 73-79 73-74> }, @0x7fe8ffa27720 48-byte object <01-00 00-00 E8-7F 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 08-7A A2-FF E8-7F 00-00 A0-32 24-7D 62-55 00-00 DE-3C 11-0A E9-7F 00-00>, @0x7fe8dc03d4c8 { cpus:2, mem:1024, ports:[31000-32000] }, @0x7fe8dc03d460 {})_x000D_
         Expected: to be called once_x000D_
           Actual: called twice - over-saturated and active_x000D_
{noformat}",3.0,1.5.0,0.5,0.3740573152337858,0.0,0.0,0.0,0.2,0.4,0.43859649122807015,0.2714285714285714,1.0,0.8121212121212121,0.8121212121212121,0.963302752293578
Improvement,Avoid unnecessary copying of protobuf in the v1 API.,"Now that we have move support for protobufs, we can avoid the unnecessary copying of protobuf in the v1 API to improve the performance.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.0
Improvement,Add a download link for master and agent logs in WebUI,"Just like task sandboxes, it would be great for us to provide a download link for mesos and agent logs in the WebUI. Right now the the log link opens up the pailer, which is not really convenient to do `grep` and such while debugging.",3.0,0,0.5,0.25037707390648567,0.3333333333333333,0.5,0.14285714285714285,0.5,0.0,0.2631578947368421,0.29285714285714287,0.8846153846153846,0.9878787878787878,0.9878787878787878,0.0
Bug,ExecutorAuthorizationTest.RunTaskGroup segfaults.,"{noformat}_x000D_
14:32:50 *** Aborted at 1516199570 (unix time) try ""date -d @1516199570"" if you are using GNU date ***_x000D_
14:32:50 PC: @     0x7f36ef13f8b0 std::_Hashtable<>::count()_x000D_
14:32:50 *** SIGSEGV (@0x107c7f88978) received by PID 19547 (TID 0x7f36e2722700) from PID 18446744072769538424; stack trace: ***_x000D_
14:32:50     @     0x7f36dcc763fd (unknown)_x000D_
14:32:50     @     0x7f36dcc7b419 (unknown)_x000D_
14:32:50     @     0x7f36dcc6f918 (unknown)_x000D_
14:32:50     @     0x7f36eb99e330 (unknown)_x000D_
14:32:50     @     0x7f36ef13f8b0 std::_Hashtable<>::count()_x000D_
14:32:50     @     0x7f36ef12bd22 _ZZN7process11ProcessBase8_consumeERKNS0_12HttpEndpointERKSsRKNS_5OwnedINS_4http7RequestEEEENKUlRK6OptionINS7_14authentication20AuthenticationResultEEE0_clESH__x000D_
14:32:50     @     0x7f36ef12c834 _ZNO6lambda12CallableOnceIFN7process6FutureINS1_4http8ResponseEEEvEE10CallableFnINS_8internal7PartialIZNS1_11ProcessBase8_consumeERKNSB_12HttpEndpointERKSsRKNS1_5OwnedINS3_7RequestEEEEUlRK6OptionINS3_14authentication20AuthenticationResultEEE0_JSP_EEEEclEv_x000D_
14:32:50     @     0x7f36ee1c1e8a _ZNO6lambda12CallableOnceIFvPN7process11ProcessBaseEEE10CallableFnINS_8internal7PartialIZNS1_8internal8DispatchINS1_6FutureINS1_4http8ResponseEEEEclINS0_IFSE_vEEEEESE_RKNS1_4UPIDEOT_EUlSt10unique_ptrINS1_7PromiseISD_EESt14default_deleteISQ_EEOSI_S3_E_JST_SI_St12_PlaceholderILi1EEEEEEclEOS3__x000D_
14:32:50     @     0x7f36ef118711 process::ProcessBase::consume()_x000D_
14:32:50     @     0x7f36ef1309a2 process::ProcessManager::resume()_x000D_
14:32:50     @     0x7f36ef134216 _ZNSt6thread5_ImplISt12_Bind_simpleIFZN7process14ProcessManager12init_threadsEvEUlvE_vEEE6_M_runEv_x000D_
14:32:50     @     0x7f36ec15a5b0 (unknown)_x000D_
14:32:50     @     0x7f36eb996184 start_thread_x000D_
14:32:50     @     0x7f36eb6c2ffd (unknown)_x000D_
{noformat}_x000D_
Full log attached.",3.0,1.5.0,0.5,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.717948717948718,0.8242424242424242,0.7212121212121212,0.963302752293578
Task,Test that `UPDATE_STATE` of a resource provider doesn't have unwanted side-effects in master or agent,"While we test the correct behavior of {{UPDATE_STATE}} sent by resource providers when an operation state changes or after (re-)registration, this call might also get sent independent from any such event, e.g., if resources are added to a running resource provider. Correct behavior of master and agent need to be tested. Outstanding offers should be rescinded and internal states updated.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.2631578947368421,0.36428571428571427,0.28205128205128205,0.296969696969697,0.296969696969697,0.0
Task,Source tree contains generated endpoint documentation,"Even though we generate documentation automatically in CI, the source tree still contains checked in, generated endpoint documentation in {{docs/endpoints}}._x000D_
_x000D_
We should remove these source files from the tree. We need to make sure to_x000D_
_x000D_
* not break automatic website generation with {{support/mesos-website/build.sh}},_x000D_
* not break the local website generation workflow with {{site/mesos-website-dev.sh}}, and_x000D_
* not break local website generation workflow with {{rake}} via {{site/Rakefile}}.",1.0,0,0.5,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,1.0,0.8121212121212121,0.8121212121212121,0.0
Task,Test that operations are correctly reported following a master failover,"As the master keeps track of operations running on a resource provider, it needs to be updated on these operations when agents reregister after a master failover. E.g., an operation that has finished during the failover should be reported as finished by the master after the agent on which the resource provider is running has reregistered.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.2631578947368421,0.36428571428571427,0.28205128205128205,0.296969696969697,0.296969696969697,0.0
Bug,RP manager incorrectly setting framework ID leads to CHECK failure,"The resource provider manager [unconditionally sets the framework ID|https://github.com/apache/mesos/blob/3290b401d20f2db2933294470ea8a2356a47c305/src/resource_provider/manager.cpp#L637] when forwarding operation status updates to the agent. This is incorrect, for example, when the resource provider [generates OPERATION_DROPPED updates during reconciliation|https://github.com/apache/mesos/blob/3290b401d20f2db2933294470ea8a2356a47c305/src/resource_provider/storage/provider.cpp#L1653-L1657], and leads to protobuf errors in this case since the framework ID's required {{value}} field is left unset.",1.0,0,0.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Bug,"Mesos can get ""stuck"" when a Process throws an exception.","When a {{Process}} throws an exception, we log it, terminate the throwing {{Process}}, and continue to run. However, currently there exists no known user-level code that I'm aware of that handles the unexpected termination due to an uncaught exception._x000D_
_x000D_
Generally, this means that when an exception is thrown (e.g. a bad call to {{std::map::at}}), the {{Process}} terminates with a log message but things get ""stuck"" and the user has to debug what is wrong / kill the process._x000D_
_x000D_
Libprocess would likely need to provide some primitives to better support handling unexpected termination of a {{Process}} in order for us to provide a strategy where we continue running._x000D_
_x000D_
In the short term, it would be prudent to abort libprocess if any {{Process}} throws an exception so that users can observe the issue and we can get it fixed.",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.0
Bug,Reconfiguration policy fails to handle mount disk resources.,"We deployed {{--reconfiguration_policy=""additive""}} on a number of Mesos agents that had mount disk resources configured, and it looks like the agent confused the size of the mount disk with the size of the work directory resource:_x000D_
_x000D_
_x000D_
{noformat}_x000D_
E0106 01:54:15.000123 1310889 slave.cpp:6733] EXIT with status 1: Failed to perform recovery: Configuration change not permitted under 'additive' policy: Value of scalar resource 'disk' decreased from 1830000 to 868000_x000D_
{noformat}_x000D_
_x000D_
The {{--resources}} flag is_x000D_
{noformat}_x000D_
--resources=""[_x000D_
  {_x000D_
    ""name"": ""disk"",_x000D_
    ""type"": ""SCALAR"",_x000D_
    ""scalar"": {_x000D_
      ""value"": 868000_x000D_
    }_x000D_
  }_x000D_
  ,_x000D_
  {_x000D_
    ""name"": ""disk"",_x000D_
    ""type"": ""SCALAR"",_x000D_
    ""scalar"": {_x000D_
      ""value"": 1830000_x000D_
    },_x000D_
    ""disk"": {_x000D_
      ""source"": {_x000D_
        ""type"": ""MOUNT"",_x000D_
        ""mount"": {_x000D_
          ""root"" : ""/srv/mesos/volumes/a""_x000D_
        }_x000D_
      }_x000D_
    }_x000D_
  }_x000D_
  ,_x000D_
  {_x000D_
    ""name"": ""disk"",_x000D_
    ""type"": ""SCALAR"",_x000D_
    ""scalar"": {_x000D_
      ""value"": 1830000_x000D_
    },_x000D_
    ""disk"": {_x000D_
      ""source"": {_x000D_
        ""type"": ""MOUNT"",_x000D_
        ""mount"": {_x000D_
          ""root"" : ""/srv/mesos/volumes/b""_x000D_
        }_x000D_
      }_x000D_
    }_x000D_
  }_x000D_
  ,_x000D_
  {_x000D_
    ""name"": ""disk"",_x000D_
    ""type"": ""SCALAR"",_x000D_
    ""scalar"": {_x000D_
      ""value"": 1830000_x000D_
    },_x000D_
    ""disk"": {_x000D_
      ""source"": {_x000D_
        ""type"": ""MOUNT"",_x000D_
        ""mount"": {_x000D_
          ""root"" : ""/srv/mesos/volumes/c""_x000D_
        }_x000D_
      }_x000D_
    }_x000D_
  }_x000D_
  ,_x000D_
  {_x000D_
    ""name"": ""disk"",_x000D_
    ""type"": ""SCALAR"",_x000D_
    ""scalar"": {_x000D_
      ""value"": 1830000_x000D_
    },_x000D_
    ""disk"": {_x000D_
      ""source"": {_x000D_
        ""type"": ""MOUNT"",_x000D_
        ""mount"": {_x000D_
          ""root"" : ""/srv/mesos/volumes/d""_x000D_
        }_x000D_
      }_x000D_
    }_x000D_
  }_x000D_
  ,_x000D_
  {_x000D_
    ""name"": ""disk"",_x000D_
    ""type"": ""SCALAR"",_x000D_
    ""scalar"": {_x000D_
      ""value"": 1830000_x000D_
    },_x000D_
    ""disk"": {_x000D_
      ""source"": {_x000D_
        ""type"": ""MOUNT"",_x000D_
        ""mount"": {_x000D_
          ""root"" : ""/srv/mesos/volumes/e""_x000D_
        }_x000D_
      }_x000D_
    }_x000D_
  }_x000D_
  ,_x000D_
  {_x000D_
    ""name"": ""disk"",_x000D_
    ""type"": ""SCALAR"",_x000D_
    ""scalar"": {_x000D_
      ""value"": 1830000_x000D_
    },_x000D_
    ""disk"": {_x000D_
      ""source"": {_x000D_
        ""type"": ""MOUNT"",_x000D_
        ""mount"": {_x000D_
          ""root"" : ""/srv/mesos/volumes/f""_x000D_
        }_x000D_
      }_x000D_
    }_x000D_
  }_x000D_
  ,_x000D_
  {_x000D_
    ""name"": ""disk"",_x000D_
    ""type"": ""SCALAR"",_x000D_
    ""scalar"": {_x000D_
      ""value"": 1830000_x000D_
    },_x000D_
    ""disk"": {_x000D_
      ""source"": {_x000D_
        ""type"": ""MOUNT"",_x000D_
        ""mount"": {_x000D_
          ""root"" : ""/srv/mesos/volumes/g""_x000D_
        }_x000D_
      }_x000D_
    }_x000D_
  }_x000D_
  ,_x000D_
  {_x000D_
    ""name"": ""disk"",_x000D_
    ""type"": ""SCALAR"",_x000D_
    ""scalar"": {_x000D_
      ""value"": 1830000_x000D_
    },_x000D_
    ""disk"": {_x000D_
      ""source"": {_x000D_
        ""type"": ""MOUNT"",_x000D_
        ""mount"": {_x000D_
          ""root"" : ""/srv/mesos/volumes/h""_x000D_
        }_x000D_
      }_x000D_
    }_x000D_
  }_x000D_
]_x000D_
{noformat}",3.0,0,0.5,0.0,0.3333333333333333,0.5,0.14285714285714285,0.0,0.2,0.5087719298245614,0.4357142857142857,0.11538461538461538,0.05454545454545455,0.05454545454545455,0.0
Improvement,"Notion of ""transitioning"" agents in the master is now inaccurate.","While [~xujyan] and I were discussing https://reviews.apache.org/r/57535/ we found a recent change that made the concept of ""in transition"" agents confusing. See my comment here: https://reviews.apache.org/r/52083/#review170066_x000D_
_x000D_
Given the new semantics described in the summary of https://reviews.apache.org/r/52083, the need for a separate method {{transitioning}} no longer exists because now it just wraps around a single variable {{unrecovered}} and gives it an alias which is less intuitive (because when reading the word transitioning one would think it has a more general meaning).",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.0
Task,Use protobuf reflection to simplify upgrading of resources.,"This is the {{upgradeResources}} half of the protobuf-reflection-based upgrade/downgrade of resources: https://issues.apache.org/jira/browse/MESOS-8221_x000D_
_x000D_
We will also add {{state::read}} to complement {{state::checkpoint}} which will be used to read protobufs from disk rather than {{protobuf::read}}.",3.0,0,0.0,0.6726998491704375,0.0,0.0,0.0,0.1,0.6000000000000001,0.2807017543859649,0.29285714285714287,0.21794871794871795,0.26666666666666666,0.26666666666666666,0.0
Task,Test reconciliation after operation is dropped en route to agent,"Since new code paths were added to handle operations on resources in 1.5, we should test that such operations are reconciled correctly after an operation is dropped on the way from the master to the agent.",3.0,0,0.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Improvement,Create AuthN support for prune images API,We want to make sure there is a way to configure AuthZ for new API added in MESOS-8360.,3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.4,0.07017543859649122,0.12142857142857143,0.038461538461538464,0.07272727272727272,0.07272727272727272,0.0
Improvement,Example frameworks to support launching mesos-local.,"The scheduler driver and library support implicit launching of mesos-local for a convenient test setup. Some of our example frameworks account for this in supporting implicit ACL rendering and more. _x000D_
_x000D_
We should unify the experience by documenting this behaviour and adding it to all example frameworks.",3.0,1.5.0,0.0,0.3740573152337858,0.0,0.0,0.0,0.2,0.4,0.43859649122807015,0.2714285714285714,0.3717948717948718,0.24848484848484845,0.24848484848484845,0.963302752293578
Improvement,Example frameworks have an inconsistent UX.,"Our example frameworks are a bit inconsistent when it comes to specifying things like the framework principal / secret etc.. _x000D_
Many of these examples have great value in testing a Mesos cluster. Unifying the parameterizing would improve the user experience when testing Mesos._x000D_
_x000D_
{{MESOS_AUTHENTICATE_FRAMEWORKS}} is being used by many examples for enabling / disabling authentication. {{load_generator_framework}} as one example however uses {{MESOS_AUTHENTICATE}} for that purpose. The credentials themselves are most commonly expected in environment variables {{DEFAULT_PRINCIPAL}} and {{DEFAULT_SECRET}} while in some cases we chose to use {{MESOS_PRINCIPAL}}, {{MESOS_SECRET}} instead.",3.0,1.5.0,0.0,0.3740573152337858,0.0,0.0,0.0,0.2,0.4,0.43859649122807015,0.2714285714285714,0.3717948717948718,0.24848484848484845,0.24848484848484845,0.963302752293578
Bug,Resources may get over allocated to some roles while fail to meet the quota of other roles.,"In the quota role allocation stage, if a role gets some resources on an agent to meet its quota, it will also get all other resources on the same agent that it does not have quota for. This may starve roles behind it that have quotas set for those resources._x000D_
_x000D_
To fix that, we need to track quota headroom in the quota role allocation stage. In that stage, if a role has no quota set for a scalar resource, it will get that resource only when two conditions are both met:_x000D_
_x000D_
- It got some other resources on the same agent to meet its quota; And_x000D_
_x000D_
- After allocating those resources, quota headroom is still above the required amount.",3.0,0,0.0,0.0,0.6666666666666666,1.0,0.5714285714285714,0.0,0.0,0.5438596491228069,0.5357142857142857,0.3974358974358974,0.4,0.4,0.0
Bug,Resubscription of a resource provider will crash the agent if its HTTP connection isn't closed,"A resource provider might resubscribe while its old HTTP connection wasn't properly closed. In that case an agent will crashm with, e.g., the following log:_x000D_
{noformat}_x000D_
I1219 13:33:51.937295 128610304 manager.cpp:570] Subscribing resource provider {""id"":{""value"":""8e71beef-796e-4bde-9257-952ed0f230a5""},""name"":""test"",""type"":""org.apache.mesos.rp.test""}_x000D_
I1219 13:33:51.937443 128610304 manager.cpp:134] Terminating resource provider 8e71beef-796e-4bde-9257-952ed0f230a5_x000D_
I1219 13:33:51.937760 128610304 manager.cpp:134] Terminating resource provider 8e71beef-796e-4bde-9257-952ed0f230a5_x000D_
E1219 13:33:51.937851 129683456 http_connection.hpp:445] End-Of-File received_x000D_
I1219 13:33:51.937865 131293184 slave.cpp:7105] Handling resource provider message 'DISCONNECT: resource provider 8e71beef-796e-4bde-9257-952ed0f230a5'_x000D_
I1219 13:33:51.937968 131293184 slave.cpp:7347] Forwarding new total resources cpus:2; mem:1024; disk:1024; ports:[31000-32000]_x000D_
F1219 13:33:51.938052 132366336 manager.cpp:606] Check failed: resourceProviders.subscribed.contains(resourceProviderId) _x000D_
*** Check failure stack trace: ***_x000D_
E1219 13:33:51.938583 130756608 http_connection.hpp:445] End-Of-File received_x000D_
I1219 13:33:51.938987 129683456 hierarchical.cpp:669] Agent 0019c3fa-28c5-43a9-88d0-709eee271c62-S0 (172.18.8.13) updated with total resources cpus:2; mem:1024; disk:1024; ports:[31000-32000]_x000D_
    @        0x1125380ef  google::LogMessageFatal::~LogMessageFatal()_x000D_
    @        0x112534ae9  google::LogMessageFatal::~LogMessageFatal()_x000D_
I1219 13:33:51.939131 129683456 hierarchical.cpp:1517] Performed allocation for 1 agents in 61830ns_x000D_
I1219 13:33:51.945793 2646795072 slave.cpp:927] Agent terminating_x000D_
I1219 13:33:51.945955 129146880 master.cpp:1305] Agent 0019c3fa-28c5-43a9-88d0-709eee271c62-S0 at slave(1)@172.18.8.13:64430 (172.18.8.13) disconnected_x000D_
I1219 13:33:51.945979 129146880 master.cpp:3364] Disconnecting agent 0019c3fa-28c5-43a9-88d0-709eee271c62-S0 at slave(1)@172.18.8.13:64430 (172.18.8.13)_x000D_
I1219 13:33:51.946022 129146880 master.cpp:3383] Deactivating agent 0019c3fa-28c5-43a9-88d0-709eee271c62-S0 at slave(1)@172.18.8.13:64430 (172.18.8.13)_x000D_
I1219 13:33:51.946081 131293184 hierarchical.cpp:766] Agent 0019c3fa-28c5-43a9-88d0-709eee271c62-S0 deactivated_x000D_
    @        0x115f2761d  mesos::internal::ResourceProviderManagerProcess::subscribe()::$_2::operator()()_x000D_
    @        0x115f2977d  _ZN5cpp176invokeIZN5mesos8internal30ResourceProviderManagerProcess9subscribeERKNS2_14HttpConnectionERKNS1_17resource_provider14Call_SubscribeEE3$_2JN7process6FutureI7NothingEEEEEDTclclsr3stdE7forwardIT_Efp_Espclsr3stdE7forwardIT0_Efp0_EEEOSG_DpOSH__x000D_
    @        0x115f29740  _ZN6lambda8internal7PartialIZN5mesos8internal30ResourceProviderManagerProcess9subscribeERKNS3_14HttpConnectionERKNS2_17resource_provider14Call_SubscribeEE3$_2JN7process6FutureI7NothingEEEE13invoke_expandISC_NSt3__15tupleIJSG_EEENSK_IJEEEJLm0EEEEDTclsr5cpp17E6invokeclsr3stdE7forwardIT_Efp_Espcl6expandclsr3stdE3getIXT2_EEclsr3stdE7forwardIT0_Efp0_EEclsr3stdE7forwardIT1_Efp2_EEEEOSN_OSO_N5cpp1416integer_sequenceImJXspT2_EEEEOSP__x000D_
    @        0x115f296bb  _ZNO6lambda8internal7PartialIZN5mesos8internal30ResourceProviderManagerProcess9subscribeERKNS3_14HttpConnectionERKNS2_17resource_provider14Call_SubscribeEE3$_2JN7process6FutureI7NothingEEEEclIJEEEDTcl13invoke_expandclL_ZNSt3__14moveIRSC_EEONSJ_16remove_referenceIT_E4typeEOSN_EdtdefpT1fEclL_ZNSK_IRNSJ_5tupleIJSG_EEEEESQ_SR_EdtdefpT10bound_argsEcvN5cpp1416integer_sequenceImJLm0EEEE_Eclsr3stdE16forward_as_tuplespclsr3stdE7forwardIT_Efp_EEEEDpOSY__x000D_
    @        0x115f2965d  _ZN5cpp176invokeIN6lambda8internal7PartialIZN5mesos8internal30ResourceProviderManagerProcess9subscribeERKNS5_14HttpConnectionERKNS4_17resource_provider14Call_SubscribeEE3$_2JN7process6FutureI7NothingEEEEEJEEEDTclclsr3stdE7forwardIT_Efp_Espclsr3stdE7forwardIT0_Efp0_EEEOSK_DpOSL__x000D_
    @        0x115f29631  _ZN6lambda8internal6InvokeIvEclINS0_7PartialIZN5mesos8internal30ResourceProviderManagerProcess9subscribeERKNS6_14HttpConnectionERKNS5_17resource_provider14Call_SubscribeEE3$_2JN7process6FutureI7NothingEEEEEJEEEvOT_DpOT0__x000D_
    @        0x115f29526  _ZNO6lambda12CallableOnceIFvvEE10CallableFnINS_8internal7PartialIZN5mesos8internal30ResourceProviderManagerProcess9subscribeERKNS7_14HttpConnectionERKNS6_17resource_provider14Call_SubscribeEE3$_2JN7process6FutureI7NothingEEEEEEclEv_x000D_
    @        0x10b6ca690  _ZNO6lambda12CallableOnceIFvvEEclEv_x000D_
    @        0x10be09295  _ZZN7process8internal8DispatchIvEclIN6lambda12CallableOnceIFvvEEEEEvRKNS_4UPIDEOT_ENKUlOS7_PNS_11ProcessBaseEE_clESD_SF__x000D_
    @        0x10be09180  _ZN5cpp176invokeIZN7process8internal8DispatchIvEclIN6lambda12CallableOnceIFvvEEEEEvRKNS1_4UPIDEOT_EUlOS9_PNS1_11ProcessBaseEE_JS9_SH_EEEDTclclsr3stdE7forwardISD_Efp_Espclsr3stdE7forwardIT0_Efp0_EEESE_DpOSJ__x000D_
    @        0x10be0912b  _ZN6lambda8internal7PartialIZN7process8internal8DispatchIvEclINS_12CallableOnceIFvvEEEEEvRKNS2_4UPIDEOT_EUlOS9_PNS2_11ProcessBaseEE_JS9_NSt3__112placeholders4__phILi1EEEEE13invoke_expandISI_NSJ_5tupleIJS9_SM_EEENSP_IJOSH_EEEJLm0ELm1EEEEDTclsr5cpp17E6invokeclsr3stdE7forwardISD_Efp_Espcl6expandclsr3stdE3getIXT2_EEclsr3stdE7forwardIT0_Efp0_EEclsr3stdE7forwardIT1_Efp2_EEEESE_OST_N5cpp1416integer_sequenceImJXspT2_EEEEOSU__x000D_
    @        0x10be0905f  _ZNO6lambda8internal7PartialIZN7process8internal8DispatchIvEclINS_12CallableOnceIFvvEEEEEvRKNS2_4UPIDEOT_EUlOS9_PNS2_11ProcessBaseEE_JS9_NSt3__112placeholders4__phILi1EEEEEclIJSH_EEEDTcl13invoke_expandclL_ZNSJ_4moveIRSI_EEONSJ_16remove_referenceISD_E4typeESE_EdtdefpT1fEclL_ZNSP_IRNSJ_5tupleIJS9_SM_EEEEESU_SE_EdtdefpT10bound_argsEcvN5cpp1416integer_sequenceImJLm0ELm1EEEE_Eclsr3stdE16forward_as_tuplespclsr3stdE7forwardIT_Efp_EEEEDpOS11__x000D_
    @        0x10be08f4d  _ZN5cpp176invokeIN6lambda8internal7PartialIZN7process8internal8DispatchIvEclINS1_12CallableOnceIFvvEEEEEvRKNS4_4UPIDEOT_EUlOSB_PNS4_11ProcessBaseEE_JSB_NSt3__112placeholders4__phILi1EEEEEEJSJ_EEEDTclclsr3stdE7forwardISF_Efp_Espclsr3stdE7forwardIT0_Efp0_EEESG_DpOSQ__x000D_
    @        0x10be08f11  _ZN6lambda8internal6InvokeIvEclINS0_7PartialIZN7process8internal8DispatchIvEclINS_12CallableOnceIFvvEEEEEvRKNS5_4UPIDEOT_EUlOSC_PNS5_11ProcessBaseEE_JSC_NSt3__112placeholders4__phILi1EEEEEEJSK_EEEvSH_DpOT0__x000D_
    @        0x10be08d36  _ZNO6lambda12CallableOnceIFvPN7process11ProcessBaseEEE10CallableFnINS_8internal7PartialIZNS1_8internal8DispatchIvEclINS0_IFvvEEEEEvRKNS1_4UPIDEOT_EUlOSE_S3_E_JSE_NSt3__112placeholders4__phILi1EEEEEEEclEOS3__x000D_
    @        0x11fd64bc9  _ZNO6lambda12CallableOnceIFvPN7process11ProcessBaseEEEclES3__x000D_
    @        0x11fd64a69  process::ProcessBase::consume()_x000D_
    @        0x11fe20ac4  _ZNO7process13DispatchEvent7consumeEPNS_13EventConsumerE_x000D_
    @        0x113c77819  process::ProcessBase::serve()_x000D_
    @        0x11fd5b8c9  process::ProcessManager::resume()_x000D_
    @        0x11fe8260b  process::ProcessManager::init_threads()::$_1::operator()()_x000D_
    @        0x11fe82190  _ZNSt3__114__thread_proxyINS_5tupleIJNS_10unique_ptrINS_15__thread_structENS_14default_deleteIS3_EEEEZN7process14ProcessManager12init_threadsEvE3$_1EEEEEPvSB__x000D_
    @     0x7fff64da56c1  _pthread_body_x000D_
    @     0x7fff64da556d  _pthread_start_x000D_
    @     0x7fff64da4c5d  thread_start_x000D_
Abort trap: 6_x000D_
{noformat}_x000D_
_x000D_
This is due to a race condition in {{resource_provider/manager.cpp}} when handling closed HTTP connections of resource providers. If a resource provider resubscribes and its old HTTP connection is still open, the resource provider manager will close it. This is unexpected and will trigger closing the new HTTP connection which results in a failed {{CHECK}}.",2.0,1.5.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.2631578947368421,0.36428571428571427,0.28205128205128205,0.296969696969697,0.296969696969697,0.963302752293578
Improvement,Improve JSON v1 operator API performance.,"According to some user reports, a simple comparison of the v1 operator API (using the ""GET_TASKS"" call) and the v0 /tasks HTTP endpoint shows that the v1 API suffers from an inefficient implementation:_x000D_
_x000D_
{noformat: title=Curl Timing}_x000D_
Operator HTTP API (GET_TASKS): 0.02s user 0.08s system 1% cpu 9.883 total_x000D_
Old /tasks API: /tasks: 0.00s user 0.00s system 1% cpu 0.222 total_x000D_
{noformat}_x000D_
_x000D_
Looking over the implementation, it suffers from the same issues we originally had with the JSON endpoints:_x000D_
_x000D_
* Excessive copying up the ""tree"" of state building calls._x000D_
* Building up the state object as opposed to directly serializing it.",2.0,0,0.5,0.0,0.6666666666666666,1.0,0.5714285714285714,0.0,0.0,0.5438596491228069,0.5357142857142857,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.0
Bug,Agent can become stuck in (re-)registering state during upgrades,"Currently, an agent will not be erased from the set of currently (re-)registering agents if_x000D_
_x000D_
 - it tries to (re-)register with a malformed version string_x000D_
 - it tries to (re-)register with a version smaller than the minimum supported version_x000D_
 - it tries to (re-)register with a domain when the master has no domain configured_x000D_
 - the operator marks the slave as gone while the (re-)registration is ongoing_x000D_
_x000D_
Afterwards, all further (re-)registration attempts with the same agent id will be discarded, because the master still  thinks that the original (re-)registration is ongoing._x000D_
_x000D_
Since most realistic way to encounter this issue would be during cluster upgrades, and it will fix itself with a master restart, it is unlikely to be reported externally._x000D_
_x000D_
Review: https://reviews.apache.org/r/64506",3.0,0,0.0,0.0,0.3333333333333333,0.5,0.14285714285714285,0.0,0.2,0.5087719298245614,0.4357142857142857,0.16666666666666666,0.1818181818181818,0.17575757575757575,0.0
Bug,Quota headroom may be insufficiently held when role has more reservation than quota.,"If a role has more reservation than its quota, the current quota headroom calculation is insufficient in guaranteeing quota allocation._x000D_
_x000D_
Consider, role `A` with 100 (units of resource, same below) reservation and 10 quota and role `B` with no reservation and 90 quota. Let's say there is no allocation yet. The existing allocator would calculate that the required headroom is 100. And since unallocated quota role reserved resource is also 100, no additional resources would be held back for the headroom._x000D_
_x000D_
While role `A` would have no problem getting its quota satisfied. Role `B` may have difficulty getting any resources because the ""headroom"" can only be allocated to `A`._x000D_
_x000D_
The solution is to calculate per-role headroom before aggregating the quantity. And unallocated reservations should not count towards quota headroom. In the above case. The headroom for role `A` should be zero, the headroom for role `B` should be 90. Thus the aggregated headroom will be `90`.",3.0,0,0.0,0.0,0.6666666666666666,1.0,0.5714285714285714,0.0,0.0,0.5438596491228069,0.5357142857142857,0.3974358974358974,0.4,0.4,0.0
Bug,MasterTest.RegistryUpdateAfterReconfiguration is flaky,"Observed here: https://jenkins.mesosphere.com/service/jenkins/job/mesos/job/Mesos_CI-build/2399/FLAG=CMake,label=mesos-ec2-debian-8/testReport/junit/mesos-ec2-debian-8-CMake.Mesos/MasterTest/RegistryUpdateAfterReconfiguration/_x000D_
_x000D_
The test here failed because the registry contained 2 slaves, when it should have only one._x000D_
_x000D_
Looking through the log, everything seems normal (in particular, only 1 slave id appears throughout this test). The only thing out of the ordinary seems to be the agent sending two `RegisterSlaveMessage`s and two `ReregisterSlaveMessage`s, but looking at the code for generating the random backoff factor in the slave that seems to be more or less normal, and shouldn't break the test.",1.0,0,0.0,0.0,0.3333333333333333,0.5,0.14285714285714285,0.0,0.2,0.5087719298245614,0.4357142857142857,0.16666666666666666,0.1818181818181818,0.17575757575757575,0.0
Improvement,Improve logs displayed after a slave failed recovery.,We have to add some steps to clean the Docker daemon state used by the Docker containerizer.,2.0,0,0.0,0.25037707390648567,0.3333333333333333,0.5,0.14285714285714285,0.5,0.0,0.2631578947368421,0.29285714285714287,0.10256410256410256,0.1515151515151515,0.1515151515151515,0.0
Bug,Mesos containerizer does not properly handle old running containers,"We were testing an upgrade scenario recently and encountered the following assertion failure:_x000D_
{code}_x000D_
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: I1212 16:45:42.693977 20810 http.cpp:3116] Processing LAUNCH_NESTED_CONTAINER_SESSION call for container 'a89b211a-4549-462d-9cc7-0ea2bac2f729.1c262420-7525-4fee-99c1-aff4f66996bd.check-a41362ae-13c6-4750-990e-a1a0b2792b5f'_x000D_
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: I1212 16:45:42.695179 20807 containerizer.cpp:1169] Trying to chown '/var/lib/mesos/slave/slaves/aaf0a62f-a6eb-4c1d-80db-5fdd26fe8008-S12/frameworks/dcf5f8b5-86a8-44df-ac03-b39404239ad8-0377/executors/kafka__68baefd4-aa8c-4b97-a23e-eb6a73fa91f6/runs/a89b211a-4549-462d-9cc7-0ea2bac2f729/containers/1c262420-7525-4fee-99c1-aff4f66996bd/containers/check-a41362ae-13c6-4750-990e-a1a0b2792b5f' to user 'nobody'_x000D_
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: W1212 16:45:42.695309 20807 containerizer.cpp:1198] Cannot determine executor_info for root container 'a89b211a-4549-462d-9cc7-0ea2bac2f729' which has no config recovered._x000D_
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: I1212 16:45:42.695327 20807 containerizer.cpp:1203] Starting container a89b211a-4549-462d-9cc7-0ea2bac2f729.1c262420-7525-4fee-99c1-aff4f66996bd.check-a41362ae-13c6-4750-990e-a1a0b2792b5f_x000D_
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: I1212 16:45:42.695829 20807 containerizer.cpp:2932] Transitioning the state of container a89b211a-4549-462d-9cc7-0ea2bac2f729.1c262420-7525-4fee-99c1-aff4f66996bd.check-a41362ae-13c6-4750-990e-a1a0b2792b5f from PROVISIONING to PREPARING_x000D_
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: I1212 16:45:42.700569 20811 systemd.cpp:98] Assigned child process '20941' to 'mesos_executors.slice'_x000D_
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: I1212 16:45:42.702945 20811 systemd.cpp:98] Assigned child process '20942' to 'mesos_executors.slice'_x000D_
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: I1212 16:45:42.706069 20806 switchboard.cpp:575] Created I/O switchboard server (pid: 20943) listening on socket file '/tmp/mesos-io-switchboard-74af71bb-2385-4dde-9762-94d0196124d3' for container a89b211a-4549-462d-9cc7-0ea2bac2f729.1c262420-7525-4fee-99c1-aff4f66996bd.check-a41362ae-13c6-4750-990e-a1a0b2792b5f_x000D_
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: mesos-agent: /pkg/src/mesos/3rdparty/stout/include/stout/option.hpp:115: T& Option<T>::get() & [with T = mesos::slave::ContainerConfig]: Assertion `isSome()' failed._x000D_
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: *** Aborted at 1513097142 (unix time) try ""date -d @1513097142"" if you are using GNU date ***_x000D_
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: PC: @     0x7f472f2851f7 __GI_raise_x000D_
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: *** SIGABRT (@0x5134) received by PID 20788 (TID 0x7f472a2bf700) from PID 20788; stack trace: ***_x000D_
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: @     0x7f472f6225e0 (unknown)_x000D_
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: @     0x7f472f2851f7 __GI_raise_x000D_
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: @     0x7f472f2868e8 __GI_abort_x000D_
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: @     0x7f472f27e266 __assert_fail_base_x000D_
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: @     0x7f472f27e312 __GI___assert_fail_x000D_
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: @     0x7f4731c481e3 _ZNR6OptionIN5mesos5slave15ContainerConfigEE3getEv.part.170_x000D_
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: @     0x7f4731c61c2d mesos::internal::slave::MesosContainerizerProcess::_launch()_x000D_
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: @     0x7f4731c7f403 _ZN5cpp176invokeIZN7process8dispatchIN5mesos8internal5slave13Containerizer12LaunchResultENS5_25MesosContainerizerProcessERKNS3_11ContainerIDERK6OptionINS3_5slave11ContainerIOEERKSt3mapISsSsSt4lessISsESaISt4pairIKSsSsEEERKSC_ISsESB_SH_SR_SU_EENS1_6FutureIT_EERKNS1_3PIDIT0_EEMSZ_FSX_T1_T2_T3_T4_EOT5_OT6_OT7_OT8_EUlSt10unique_ptrINS1_7PromiseIS7_EESt14default_deleteIS1J_EEOS9_OSF_OSP_OSS_PNS1_11ProcessBaseEE_IS1M_S9_SF_SP_SS_S1S_EEEDTclcl7forwardISW_Efp_Espcl7forwardIT0_Efp0_EEEOSW_DpOS1U__x000D_
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: @     0x7f4731c7f4f1 _ZNO6lambda12CallableOnceIFvPN7process11ProcessBaseEEE10CallableFnINS_8internal7PartialIZNS1_8dispatchIN5mesos8internal5slave13Containerizer12LaunchResultENSC_25MesosContainerizerProcessERKNSA_11ContainerIDERK6OptionINSA_5slave11ContainerIOEERKSt3mapISsSsSt4lessISsESaISt4pairIKSsSsEEERKSJ_ISsESI_SO_SY_S11_EENS1_6FutureIT_EERKNS1_3PIDIT0_EEMS16_FS14_T1_T2_T3_T4_EOT5_OT6_OT7_OT8_EUlSt10unique_ptrINS1_7PromiseISE_EESt14default_deleteIS1Q_EEOSG_OSM_OSW_OSZ_S3_E_IS1T_SG_SM_SW_SZ_St12_PlaceholderILi1EEEEEEclEOS3__x000D_
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: @     0x7f47325dbb31 process::ProcessBase::consume()_x000D_
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: @     0x7f47325ea882 process::ProcessManager::resume()_x000D_
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: @     0x7f47325efcf6 _ZNSt6thread5_ImplISt12_Bind_simpleIFZN7process14ProcessManager12init_threadsEvEUlvE_vEEE6_M_runEv_x000D_
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: @     0x7f472fafa230 (unknown)_x000D_
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: @     0x7f472f61ae25 start_thread_x000D_
Dec 12 16:45:42 agent.hostname mesos-agent[20788]: @     0x7f472f34834d __clone_x000D_
Dec 12 16:45:42 agent.hostname systemd[1]: dcos-mesos-slave.service: main process exited, code=killed, status=6/ABRT_x000D_
Dec 12 16:45:42 agent.hostname systemd[1]: Unit dcos-mesos-slave.service entered failed state._x000D_
Dec 12 16:45:42 agent.hostname systemd[1]: dcos-mesos-slave.service failed._x000D_
{code}_x000D_
_x000D_
Looking into {{Slave::_launch}}, indeed we find an unguarded access to the parent container's {{ContainerConfig}} [here|https://github.com/apache/mesos/blob/c320ab3b2dc4a16de7e060b9e15e9865a73389b0/src/slave/containerizer/mesos/containerizer.cpp#L1716]._x000D_
_x000D_
We recently [added checkpointing|https://github.com/apache/mesos/commit/03a2a4dfa47b1d47c5eb23e81f5ef8213e46d545] of {{ContainerConfig}} to the Mesos containerizer. It seems that we are not appropriately handling upgrades, when there may be old containers running for which we do not expect to recover a {{ContainerConfig}}.",2.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.4,0.07017543859649122,0.12142857142857143,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Documentation,Document container image garbage collection.,Document container image garbage collection.,2.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.4,0.07017543859649122,0.12142857142857143,0.6794871794871795,0.5939393939393939,0.5939393939393939,0.0
Task,Introduce a UUID message type,"Currently when UUID need to be part of a protobuf message, we use a byte array field for that. This has some drawbacks, especially when it comes to outputting the UUID in logs: To stringify the UUID field, we first have to create a stout UUID, then call {{.toString()}} of that one. It would help to have a UUID type in {{mesos.proto}} and provide a stringification function for it in {{type_utils.hpp}}.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.2631578947368421,0.36428571428571427,0.28205128205128205,0.296969696969697,0.296969696969697,0.0
Documentation,Update CHANGELOG to call out agent reconfiguration feature,This is a feature worth calling out in the 1.5.0 CHANGELOG.,1.0,0,0.5,0.0,0.3333333333333333,0.5,0.14285714285714285,0.0,0.2,0.5087719298245614,0.4357142857142857,0.8846153846153846,0.9878787878787878,0.9878787878787878,0.0
Bug,Reservation may not be allocated when the role has no quota.,"Reservations that belong to a role that has no quota may not be allocated even when the reserved resources are allocatable to the role._x000D_
_x000D_
This is because in the current implementation the reserved resources may be counted towards the headroom left for unallocated quota limit in the second stage allocation._x000D_
_x000D_
https://github.com/apache/mesos/blob/c844db9ac7c0cef59be87438c6781bfb71adcc42/src/master/allocator/mesos/hierarchical.cpp#L1764-L1767_x000D_
_x000D_
Roles with quota do not have this issue because currently their reservations are taken care of in the first stage.",3.0,0,0.0,0.0,0.6666666666666666,1.0,0.5714285714285714,0.0,0.0,0.5438596491228069,0.5357142857142857,0.3974358974358974,0.4,0.4,0.0
Documentation,Add documentation about fault domains,We need some user docs for fault domains.,3.0,0,0.5,0.0,0.3333333333333333,0.5,0.14285714285714285,0.0,0.2,0.5087719298245614,0.4357142857142857,0.8846153846153846,0.9878787878787878,0.9878787878787878,0.0
Bug,ReservationTest.MasterFailover is flaky when run with `RESOURCE_PROVIDER` capability.,"On a system under load, {{ResourceProviderCapability/ReservationTest.MasterFailover/1}} can fail. {{GLOG_v=2}} of the failure:_x000D_
{noformat}_x000D_
[ RUN      ] ResourceProviderCapability/ReservationTest.MasterFailover/1_x000D_
I1201 14:52:47.324741 122806272 process.cpp:2730] Dropping event for process hierarchical-allocator(34)@172.18.8.37:57116_x000D_
I1201 14:52:47.324816 122806272 process.cpp:2730] Dropping event for process slave(17)@172.18.8.37:57116_x000D_
I1201 14:52:47.324859 2720961344 clock.cpp:331] Clock paused at 2017-12-01 13:53:04.834857088+00:00_x000D_
I1201 14:52:47.326314 2720961344 clock.cpp:435] Clock of files@172.18.8.37:57116 updated to 2017-12-01 13:53:04.834857088+00:00_x000D_
I1201 14:52:47.326371 2720961344 clock.cpp:435] Clock of hierarchical-allocator(35)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.834857088+00:00_x000D_
I1201 14:52:47.326539 2720961344 cluster.cpp:170] Creating default 'local' authorizer_x000D_
I1201 14:52:47.326568 2720961344 clock.cpp:435] Clock of local-authorizer(52)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.834857088+00:00_x000D_
I1201 14:52:47.326671 2720961344 clock.cpp:435] Clock of standalone-master-detector(52)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.834857088+00:00_x000D_
I1201 14:52:47.326709 2720961344 clock.cpp:435] Clock of in-memory-storage(35)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.834857088+00:00_x000D_
I1201 14:52:47.326884 2720961344 clock.cpp:435] Clock of registrar(35)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.834857088+00:00_x000D_
I1201 14:52:47.327579 2720961344 clock.cpp:435] Clock of master@172.18.8.37:57116 updated to 2017-12-01 13:53:04.834857088+00:00_x000D_
I1201 14:52:47.330301 119050240 master.cpp:454] Master 209387ca-a9c3-4717-9769-a59d9fe927f1 (172.18.8.37) started on 172.18.8.37:57116_x000D_
I1201 14:52:47.330329 119050240 master.cpp:456] Flags at startup: --acls="""" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""5ms"" --allocator=""HierarchicalDRF"" --authenticate_agents=""true"" --authenticate_frameworks=""true"" --authenticate_http_frameworks=""true"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/private/var/folders/0b/srgwj7vd2037pygpz1fpyqgm0000gn/T/z44iHn/credentials"" --filter_gpu_resources=""true"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_unreachable_tasks_per_framework=""1000"" --port=""5050"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""in_memory"" --registry_fetch_timeout=""1mins"" --registry_gc_interval=""15mins"" --registry_max_agent_age=""2weeks"" --registry_max_agent_count=""102400"" --registry_store_timeout=""100secs"" --registry_strict=""false"" --roles=""role"" --root_submissions=""true"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/private/var/folders/0b/srgwj7vd2037pygpz1fpyqgm0000gn/T/z44iHn/master"" --zk_session_timeout=""10secs""_x000D_
I1201 14:52:47.330628 119050240 master.cpp:505] Master only allowing authenticated frameworks to register_x000D_
I1201 14:52:47.330638 119050240 master.cpp:511] Master only allowing authenticated agents to register_x000D_
I1201 14:52:47.330644 119050240 master.cpp:517] Master only allowing authenticated HTTP frameworks to register_x000D_
I1201 14:52:47.330652 119050240 credentials.hpp:37] Loading credentials for authentication from '/private/var/folders/0b/srgwj7vd2037pygpz1fpyqgm0000gn/T/z44iHn/credentials'_x000D_
I1201 14:52:47.330873 119050240 master.cpp:561] Using default 'crammd5' authenticator_x000D_
I1201 14:52:47.330927 119050240 clock.cpp:435] Clock of crammd5-authenticator(35)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.834857088+00:00_x000D_
I1201 14:52:47.330963 119050240 http.cpp:1045] Creating default 'basic' HTTP authenticator for realm 'mesos-master-readonly'_x000D_
I1201 14:52:47.330993 119050240 clock.cpp:435] Clock of __basic_authenticator__(137)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.834857088+00:00_x000D_
I1201 14:52:47.331056 119050240 http.cpp:1045] Creating default 'basic' HTTP authenticator for realm 'mesos-master-readwrite'_x000D_
I1201 14:52:47.331082 119050240 clock.cpp:435] Clock of __basic_authenticator__(138)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.834857088+00:00_x000D_
I1201 14:52:47.331151 119050240 http.cpp:1045] Creating default 'basic' HTTP authenticator for realm 'mesos-master-scheduler'_x000D_
I1201 14:52:47.331176 119050240 clock.cpp:435] Clock of __basic_authenticator__(139)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.834857088+00:00_x000D_
I1201 14:52:47.331228 119050240 master.cpp:640] Authorization enabled_x000D_
W1201 14:52:47.331238 119050240 master.cpp:703] The '--roles' flag is deprecated. This flag will be removed in the future. See the Mesos 0.27 upgrade notes for more information_x000D_
I1201 14:52:47.331326 119050240 clock.cpp:435] Clock of whitelist(35)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.834857088+00:00_x000D_
I1201 14:52:47.331521 118513664 hierarchical.cpp:173] Initialized hierarchical allocator process_x000D_
I1201 14:52:47.331560 122269696 whitelist_watcher.cpp:77] No whitelist given_x000D_
I1201 14:52:47.333322 119050240 master.cpp:2221] Elected as the leading master!_x000D_
I1201 14:52:47.333339 119050240 master.cpp:1701] Recovering from registrar_x000D_
I1201 14:52:47.333454 120659968 registrar.cpp:347] Recovering registrar_x000D_
I1201 14:52:47.333526 120659968 clock.cpp:435] Clock of __latch__(154)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.834857088+00:00_x000D_
I1201 14:52:47.333732 120659968 registrar.cpp:391] Successfully fetched the registry (0B) in 0ns_x000D_
I1201 14:52:47.333809 120659968 registrar.cpp:495] Applied 1 operations in 29366ns; attempting to update the registry_x000D_
I1201 14:52:47.333927 120659968 clock.cpp:435] Clock of __latch__(155)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.834857088+00:00_x000D_
I1201 14:52:47.334156 118513664 registrar.cpp:552] Successfully updated the registry in 0ns_x000D_
I1201 14:52:47.334218 118513664 registrar.cpp:424] Successfully recovered registrar_x000D_
I1201 14:52:47.334426 119050240 master.cpp:1814] Recovered 0 agents from the registry (131B); allowing 10mins for agents to re-register_x000D_
I1201 14:52:47.334503 119586816 hierarchical.cpp:211] Skipping recovery of hierarchical allocator: nothing to recover_x000D_
I1201 14:52:47.337247 2720961344 clock.cpp:435] Clock of standalone-master-detector(53)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.834857088+00:00_x000D_
I1201 14:52:47.338493 2720961344 clock.cpp:435] Clock of files@172.18.8.37:57116 updated to 2017-12-01 13:53:04.834857088+00:00_x000D_
W1201 14:52:47.338521 2720961344 process.cpp:2756] Attempted to spawn already running process files@172.18.8.37:57116_x000D_
I1201 14:52:47.338845 2720961344 clock.cpp:435] Clock of fetcher(18)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.834857088+00:00_x000D_
I1201 14:52:47.339238 2720961344 containerizer.cpp:304] Using isolation { environment_secret, filesystem/posix, posix/mem, posix/cpu }_x000D_
I1201 14:52:47.339455 2720961344 clock.cpp:435] Clock of copy-provisioner-backend(18)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.834857088+00:00_x000D_
I1201 14:52:47.339504 2720961344 provisioner.cpp:297] Using default backend 'copy'_x000D_
I1201 14:52:47.339524 2720961344 clock.cpp:435] Clock of mesos-provisioner(18)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.834857088+00:00_x000D_
I1201 14:52:47.339607 2720961344 clock.cpp:435] Clock of environment-secret-isolator(18)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.834857088+00:00_x000D_
I1201 14:52:47.339789 2720961344 clock.cpp:435] Clock of posix-filesystem-isolator(18)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.834857088+00:00_x000D_
I1201 14:52:47.339973 2720961344 clock.cpp:435] Clock of posix-mem-isolator(18)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.834857088+00:00_x000D_
I1201 14:52:47.340020 2720961344 clock.cpp:435] Clock of posix-cpu-isolator(18)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.834857088+00:00_x000D_
I1201 14:52:47.340062 2720961344 clock.cpp:435] Clock of sandbox-logger(18)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.834857088+00:00_x000D_
I1201 14:52:47.340103 2720961344 clock.cpp:435] Clock of (18)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.834857088+00:00_x000D_
I1201 14:52:47.340281 2720961344 clock.cpp:435] Clock of mesos-containerizer(18)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.834857088+00:00_x000D_
I1201 14:52:47.340481 2720961344 cluster.cpp:458] Creating default 'local' authorizer_x000D_
I1201 14:52:47.340513 2720961344 clock.cpp:435] Clock of local-authorizer(53)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.834857088+00:00_x000D_
I1201 14:52:47.340608 2720961344 clock.cpp:435] Clock of agent-garbage-collector(18)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.834857088+00:00_x000D_
I1201 14:52:47.340742 2720961344 clock.cpp:435] Clock of __executor__(18)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.834857088+00:00_x000D_
I1201 14:52:47.340839 2720961344 clock.cpp:435] Clock of task-status-update-manager(18)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.834857088+00:00_x000D_
I1201 14:52:47.341027 2720961344 clock.cpp:435] Clock of slave(18)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.834857088+00:00_x000D_
I1201 14:52:47.341190 2720961344 clock.cpp:435] Clock of __limiter__(18)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.834857088+00:00_x000D_
I1201 14:52:47.341687 2720961344 clock.cpp:435] Clock of resource-provider-manager(18)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.834857088+00:00_x000D_
I1201 14:52:47.341840 121196544 slave.cpp:253] Mesos agent started on (18)@172.18.8.37:57116_x000D_
I1201 14:52:47.341935 2720961344 clock.cpp:381] Clock advanced (10ms) to 2017-12-01 13:53:04.844857088+00:00_x000D_
I1201 14:52:47.342027 122806272 clock.cpp:435] Clock of hierarchical-allocator(35)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.839857088+00:00_x000D_
I1201 14:52:47.342051 122806272 clock.cpp:435] Clock of __reaper__(1)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.842657088+00:00_x000D_
I1201 14:52:47.341878 121196544 slave.cpp:254] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/var/folders/0b/srgwj7vd2037pygpz1fpyqgm0000gn/T/ResourceProviderCapability_ReservationTest_MasterFailover_1_DhNV1G/store/appc"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""true"" --authenticatee=""crammd5"" --authentication_backoff_factor=""1secs"" --authorizer=""local"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/var/folders/0b/srgwj7vd2037pygpz1fpyqgm0000gn/T/ResourceProviderCapability_ReservationTest_MasterFailover_1_DhNV1G/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/var/folders/0b/srgwj7vd2037pygpz1fpyqgm0000gn/T/ResourceProviderCapability_ReservationTest_MasterFailover_1_DhNV1G/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_reregistration_timeout=""2secs"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/var/folders/0b/srgwj7vd2037pygpz1fpyqgm0000gn/T/ResourceProviderCapability_ReservationTest_MasterFailover_1_DhNV1G/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_command_executor=""false"" --http_credentials=""/var/folders/0b/srgwj7vd2037pygpz1fpyqgm0000gn/T/ResourceProviderCapability_ReservationTest_MasterFailover_1_DhNV1G/http_credentials"" --http_heartbeat_interval=""30secs"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher=""posix"" --launcher_dir=""/Users/jan/Documents/mesos/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_executors_per_framework=""150"" --oversubscribed_resources_interval=""15secs"" --port=""5051"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:8;mem:2048"" --runtime_dir=""/var/folders/0b/srgwj7vd2037pygpz1fpyqgm0000gn/T/ResourceProviderCapability_ReservationTest_MasterFailover_1_DhNV1G"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --version=""false"" --work_dir=""/var/folders/0b/srgwj7vd2037pygpz1fpyqgm0000gn/T/ResourceProviderCapability_ReservationTest_MasterFailover_1_wjKAB3"" --zk_session_timeout=""10secs""_x000D_
I1201 14:52:47.342082 122806272 clock.cpp:435] Clock of hierarchical-allocator(35)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.844857088+00:00_x000D_
I1201 14:52:47.342172 121196544 credentials.hpp:86] Loading credential for authentication from '/var/folders/0b/srgwj7vd2037pygpz1fpyqgm0000gn/T/ResourceProviderCapability_ReservationTest_MasterFailover_1_DhNV1G/credential'_x000D_
I1201 14:52:47.342216 122806272 clock.cpp:435] Clock of __reaper__(1)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.844857088+00:00_x000D_
I1201 14:52:47.342301 121196544 slave.cpp:286] Agent using credential for: test-principal_x000D_
I1201 14:52:47.342316 121196544 credentials.hpp:37] Loading credentials for authentication from '/var/folders/0b/srgwj7vd2037pygpz1fpyqgm0000gn/T/ResourceProviderCapability_ReservationTest_MasterFailover_1_DhNV1G/http_credentials'_x000D_
I1201 14:52:47.342391 120659968 hierarchical.cpp:1890] No allocations performed_x000D_
I1201 14:52:47.342411 120659968 hierarchical.cpp:1431] Performed allocation for 0 agents in 45088ns_x000D_
I1201 14:52:47.342497 121196544 http.cpp:1045] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-readonly'_x000D_
I1201 14:52:47.342541 121196544 clock.cpp:435] Clock of __basic_authenticator__(140)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.834857088+00:00_x000D_
I1201 14:52:47.342600 121196544 http.cpp:1045] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-readwrite'_x000D_
I1201 14:52:47.342628 121196544 clock.cpp:435] Clock of __basic_authenticator__(141)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.834857088+00:00_x000D_
I1201 14:52:47.342682 121196544 clock.cpp:435] Clock of noop-resource-estimator(18)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.834857088+00:00_x000D_
I1201 14:52:47.342725 121196544 clock.cpp:435] Clock of qos-noop-controller(18)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.834857088+00:00_x000D_
I1201 14:52:47.342804 121196544 clock.cpp:435] Clock of local-resource-provider-daemon(18)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.834857088+00:00_x000D_
I1201 14:52:47.343531 121196544 slave.cpp:585] Agent resources: [{""name"":""cpus"",""scalar"":{""value"":8.0},""type"":""SCALAR""},{""name"":""mem"",""scalar"":{""value"":2048.0},""type"":""SCALAR""},{""name"":""disk"",""scalar"":{""value"":470537.0},""type"":""SCALAR""},{""name"":""ports"",""ranges"":{""range"":[{""begin"":31000,""end"":32000}]},""type"":""RANGES""}]_x000D_
I1201 14:52:47.343757 121196544 slave.cpp:593] Agent attributes: [  ]_x000D_
I1201 14:52:47.343766 121196544 slave.cpp:602] Agent hostname: 172.18.8.37_x000D_
I1201 14:52:47.343842 119050240 task_status_update_manager.cpp:181] Pausing sending task status updates_x000D_
I1201 14:52:47.344367 121196544 clock.cpp:435] Clock of __async_executor__(18)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.834857088+00:00_x000D_
I1201 14:52:47.344763 122269696 state.cpp:64] Recovering state from '/var/folders/0b/srgwj7vd2037pygpz1fpyqgm0000gn/T/ResourceProviderCapability_ReservationTest_MasterFailover_1_wjKAB3/meta'_x000D_
I1201 14:52:47.344969 120123392 task_status_update_manager.cpp:207] Recovering task status update manager_x000D_
I1201 14:52:47.345077 119050240 containerizer.cpp:672] Recovering containerizer_x000D_
I1201 14:52:47.345255 119050240 clock.cpp:435] Clock of __collect__(35)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.834857088+00:00_x000D_
I1201 14:52:47.345836 119050240 clock.cpp:435] Clock of __collect__(36)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.834857088+00:00_x000D_
I1201 14:52:47.345984 119586816 provisioner.cpp:493] Provisioner recovery complete_x000D_
I1201 14:52:47.346258 121196544 slave.cpp:6513] Finished recovery_x000D_
I1201 14:52:47.353896 121196544 slave.cpp:6701] Querying resource estimator for oversubscribable resources_x000D_
I1201 14:52:47.354270 120123392 slave.cpp:999] New master detected at master@172.18.8.37:57116_x000D_
I1201 14:52:47.354349 120123392 slave.cpp:1034] Detecting new master_x000D_
I1201 14:52:47.354396 120123392 slave.cpp:6715] Received oversubscribable resources {} from the resource estimator_x000D_
I1201 14:52:47.354427 119586816 task_status_update_manager.cpp:181] Pausing sending task status updates_x000D_
I1201 14:52:47.354418 122806272 clock.cpp:435] Clock of slave(18)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.844058370+00:00_x000D_
I1201 14:52:47.354485 122806272 clock.cpp:435] Clock of slave(18)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.844857088+00:00_x000D_
I1201 14:52:47.354533 122269696 slave.cpp:1061] Authenticating with master master@172.18.8.37:57116_x000D_
I1201 14:52:47.354569 122269696 slave.cpp:1070] Using default CRAM-MD5 authenticatee_x000D_
I1201 14:52:47.354616 122269696 clock.cpp:435] Clock of crammd5-authenticatee(69)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.844857088+00:00_x000D_
I1201 14:52:47.354712 121733120 authenticatee.cpp:121] Creating new client SASL connection_x000D_
I1201 14:52:47.354758 121733120 clock.cpp:435] Clock of master@172.18.8.37:57116 updated to 2017-12-01 13:53:04.844857088+00:00_x000D_
I1201 14:52:47.354904 121196544 master.cpp:8589] Authenticating slave(18)@172.18.8.37:57116_x000D_
I1201 14:52:47.354948 121196544 clock.cpp:435] Clock of crammd5-authenticator(35)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.844857088+00:00_x000D_
I1201 14:52:47.355032 120659968 authenticator.cpp:414] Starting authentication session for crammd5-authenticatee(69)@172.18.8.37:57116_x000D_
I1201 14:52:47.355062 120659968 clock.cpp:435] Clock of crammd5-authenticator-session(69)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.844857088+00:00_x000D_
I1201 14:52:47.355201 118513664 authenticator.cpp:98] Creating new server SASL connection_x000D_
I1201 14:52:47.355330 119586816 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5_x000D_
I1201 14:52:47.355350 119586816 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'_x000D_
I1201 14:52:47.355409 119050240 authenticator.cpp:204] Received SASL authentication start_x000D_
I1201 14:52:47.355455 119050240 authenticator.cpp:326] Authentication requires more steps_x000D_
I1201 14:52:47.355516 120123392 authenticatee.cpp:259] Received SASL authentication step_x000D_
I1201 14:52:47.355592 122269696 authenticator.cpp:232] Received SASL authentication step_x000D_
I1201 14:52:47.355625 122269696 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'nfnt.local' server FQDN: 'nfnt.local' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false _x000D_
I1201 14:52:47.355651 122269696 auxprop.cpp:181] Looking up auxiliary property '*userPassword'_x000D_
I1201 14:52:47.355687 122269696 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'_x000D_
I1201 14:52:47.355713 122269696 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'nfnt.local' server FQDN: 'nfnt.local' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true _x000D_
I1201 14:52:47.355736 122269696 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true_x000D_
I1201 14:52:47.355758 122269696 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true_x000D_
I1201 14:52:47.355783 122269696 authenticator.cpp:318] Authentication success_x000D_
I1201 14:52:47.355907 121733120 master.cpp:8619] Successfully authenticated principal 'test-principal' at slave(18)@172.18.8.37:57116_x000D_
I1201 14:52:47.355965 121196544 authenticator.cpp:432] Authentication session cleanup for crammd5-authenticatee(69)@172.18.8.37:57116_x000D_
I1201 14:52:47.355998 120659968 authenticatee.cpp:299] Authentication success_x000D_
I1201 14:52:47.356014 121196544 clock.cpp:435] Clock of help@172.18.8.37:57116 updated to 2017-12-01 13:53:04.844857088+00:00_x000D_
I1201 14:52:47.356144 119050240 slave.cpp:1153] Successfully authenticated with master master@172.18.8.37:57116_x000D_
I1201 14:52:47.356294 119050240 slave.cpp:1696] Will retry registration in 2.9532ms if necessary_x000D_
I1201 14:52:47.356449 121733120 master.cpp:6042] Received register agent message from slave(18)@172.18.8.37:57116 (172.18.8.37)_x000D_
I1201 14:52:47.356472 121733120 master.cpp:3878] Authorizing agent with principal 'test-principal'_x000D_
I1201 14:52:47.356495 121733120 clock.cpp:435] Clock of local-authorizer(52)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.844857088+00:00_x000D_
I1201 14:52:47.356734 120659968 master.cpp:6104] Authorized registration of agent at slave(18)@172.18.8.37:57116 (172.18.8.37)_x000D_
I1201 14:52:47.356797 120659968 master.cpp:6197] Registering agent at slave(18)@172.18.8.37:57116 (172.18.8.37) with id 209387ca-a9c3-4717-9769-a59d9fe927f1-S0_x000D_
I1201 14:52:47.356842 120659968 clock.cpp:435] Clock of registrar(35)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.844857088+00:00_x000D_
I1201 14:52:47.357019 121196544 registrar.cpp:495] Applied 1 operations in 51305ns; attempting to update the registry_x000D_
I1201 14:52:47.357128 121196544 clock.cpp:435] Clock of in-memory-storage(35)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.844857088+00:00_x000D_
I1201 14:52:47.357218 121196544 clock.cpp:435] Clock of __latch__(156)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.844857088+00:00_x000D_
I1201 14:52:47.357378 121196544 registrar.cpp:552] Successfully updated the registry in 0ns_x000D_
I1201 14:52:47.357507 119050240 master.cpp:6246] Admitted agent 209387ca-a9c3-4717-9769-a59d9fe927f1-S0 at slave(18)@172.18.8.37:57116 (172.18.8.37)_x000D_
I1201 14:52:47.357703 119050240 clock.cpp:435] Clock of slave-observer(35)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.844857088+00:00_x000D_
I1201 14:52:47.357873 120659968 slave.cpp:5160] Received ping from slave-observer(35)@172.18.8.37:57116_x000D_
I1201 14:52:47.357844 119050240 master.cpp:6282] Registered agent 209387ca-a9c3-4717-9769-a59d9fe927f1-S0 at slave(18)@172.18.8.37:57116 (172.18.8.37) with cpus:8; mem:2048; disk:470537; ports:[31000-32000]_x000D_
I1201 14:52:47.357964 120659968 slave.cpp:1199] Registered with master master@172.18.8.37:57116; given agent ID 209387ca-a9c3-4717-9769-a59d9fe927f1-S0_x000D_
I1201 14:52:47.357992 120659968 clock.cpp:435] Clock of task-status-update-manager(18)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.844857088+00:00_x000D_
I1201 14:52:47.358054 122269696 task_status_update_manager.cpp:188] Resuming sending task status updates_x000D_
I1201 14:52:47.358053 118513664 hierarchical.cpp:553] Added agent 209387ca-a9c3-4717-9769-a59d9fe927f1-S0 (172.18.8.37) with cpus:8; mem:2048; disk:470537; ports:[31000-32000] (allocated: {})_x000D_
I1201 14:52:47.358266 118513664 hierarchical.cpp:1890] No allocations performed_x000D_
I1201 14:52:47.358304 118513664 hierarchical.cpp:1431] Performed allocation for 1 agents in 118185ns_x000D_
I1201 14:52:47.358395 120659968 slave.cpp:1219] Checkpointing SlaveInfo to '/var/folders/0b/srgwj7vd2037pygpz1fpyqgm0000gn/T/ResourceProviderCapability_ReservationTest_MasterFailover_1_wjKAB3/meta/slaves/209387ca-a9c3-4717-9769-a59d9fe927f1-S0/slave.info'_x000D_
I1201 14:52:47.358955 120659968 clock.cpp:435] Clock of local-resource-provider-daemon(18)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.844857088+00:00_x000D_
I1201 14:52:47.359042 120659968 slave.cpp:1281] Forwarding total resources cpus:8; mem:2048; disk:470537; ports:[31000-32000]_x000D_
I1201 14:52:47.359097 120659968 slave.cpp:1298] Forwarding total oversubscribed resources {}_x000D_
I1201 14:52:47.359454 121196544 master.cpp:7036] Received update of agent 209387ca-a9c3-4717-9769-a59d9fe927f1-S0 at slave(18)@172.18.8.37:57116 (172.18.8.37) with total resources cpus:8; mem:2048; disk:470537; ports:[31000-32000]_x000D_
I1201 14:52:47.359508 121196544 master.cpp:7049] Received update of agent 209387ca-a9c3-4717-9769-a59d9fe927f1-S0 at slave(18)@172.18.8.37:57116 (172.18.8.37) with total oversubscribed resources {}_x000D_
I1201 14:52:47.359946 121733120 hierarchical.cpp:620] Agent 209387ca-a9c3-4717-9769-a59d9fe927f1-S0 (172.18.8.37) updated with total resources cpus:8; mem:2048; disk:470537; ports:[31000-32000]_x000D_
I1201 14:52:47.361738 2720961344 clock.cpp:435] Clock of version@172.18.8.37:57116 updated to 2017-12-01 13:53:04.844857088+00:00_x000D_
W1201 14:52:47.361764 2720961344 process.cpp:2756] Attempted to spawn already running process version@172.18.8.37:57116_x000D_
I1201 14:52:47.361790 2720961344 clock.cpp:435] Clock of __latch__(157)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.844857088+00:00_x000D_
I1201 14:52:47.366130 2720961344 clock.cpp:435] Clock of scheduler-430784ca-9418-4424-b431-5ce1a96a6fee@172.18.8.37:57116 updated to 2017-12-01 13:53:04.844857088+00:00_x000D_
I1201 14:52:47.366179 2720961344 clock.cpp:435] Clock of metrics@172.18.8.37:57116 updated to 2017-12-01 13:53:04.844857088+00:00_x000D_
I1201 14:52:47.366233 2720961344 sched.cpp:232] Version: 1.5.0_x000D_
I1201 14:52:47.366262 2720961344 clock.cpp:381] Clock advanced (5ms) to 2017-12-01 13:53:04.849857088+00:00_x000D_
I1201 14:52:47.366351 122806272 clock.cpp:435] Clock of hierarchical-allocator(35)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.849857088+00:00_x000D_
I1201 14:52:47.366603 118513664 clock.cpp:435] Clock of standalone-master-detector(53)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.844857088+00:00_x000D_
I1201 14:52:47.366729 122269696 hierarchical.cpp:1890] No allocations performed_x000D_
I1201 14:52:47.366744 120123392 sched.cpp:336] New master detected at master@172.18.8.37:57116_x000D_
I1201 14:52:47.366755 122269696 hierarchical.cpp:1431] Performed allocation for 1 agents in 84509ns_x000D_
I1201 14:52:47.366794 120123392 sched.cpp:396] Authenticating with master master@172.18.8.37:57116_x000D_
I1201 14:52:47.366806 120123392 sched.cpp:403] Using default CRAM-MD5 authenticatee_x000D_
I1201 14:52:47.366844 120123392 clock.cpp:435] Clock of crammd5-authenticatee(70)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.844857088+00:00_x000D_
I1201 14:52:47.366993 121196544 authenticatee.cpp:121] Creating new client SASL connection_x000D_
I1201 14:52:47.367127 119050240 master.cpp:8589] Authenticating scheduler-430784ca-9418-4424-b431-5ce1a96a6fee@172.18.8.37:57116_x000D_
I1201 14:52:47.367199 119586816 authenticator.cpp:414] Starting authentication session for crammd5-authenticatee(70)@172.18.8.37:57116_x000D_
I1201 14:52:47.367228 119586816 clock.cpp:435] Clock of crammd5-authenticator-session(70)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.844857088+00:00_x000D_
I1201 14:52:47.367327 118513664 authenticator.cpp:98] Creating new server SASL connection_x000D_
I1201 14:52:47.367401 120659968 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5_x000D_
I1201 14:52:47.367424 120659968 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'_x000D_
I1201 14:52:47.367471 122269696 authenticator.cpp:204] Received SASL authentication start_x000D_
I1201 14:52:47.367512 122269696 authenticator.cpp:326] Authentication requires more steps_x000D_
I1201 14:52:47.367568 120123392 authenticatee.cpp:259] Received SASL authentication step_x000D_
I1201 14:52:47.367624 121733120 authenticator.cpp:232] Received SASL authentication step_x000D_
I1201 14:52:47.367642 121733120 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'nfnt.local' server FQDN: 'nfnt.local' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false _x000D_
I1201 14:52:47.367652 121733120 auxprop.cpp:181] Looking up auxiliary property '*userPassword'_x000D_
I1201 14:52:47.367668 121733120 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'_x000D_
I1201 14:52:47.367678 121733120 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'nfnt.local' server FQDN: 'nfnt.local' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true _x000D_
I1201 14:52:47.367688 121733120 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true_x000D_
I1201 14:52:47.367694 121733120 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true_x000D_
I1201 14:52:47.367704 121733120 authenticator.cpp:318] Authentication success_x000D_
I1201 14:52:47.367748 121196544 authenticatee.cpp:299] Authentication success_x000D_
I1201 14:52:47.367775 119586816 master.cpp:8619] Successfully authenticated principal 'test-principal' at scheduler-430784ca-9418-4424-b431-5ce1a96a6fee@172.18.8.37:57116_x000D_
I1201 14:52:47.367806 118513664 authenticator.cpp:432] Authentication session cleanup for crammd5-authenticatee(70)@172.18.8.37:57116_x000D_
I1201 14:52:47.367884 119050240 sched.cpp:502] Successfully authenticated with master master@172.18.8.37:57116_x000D_
I1201 14:52:47.367899 119050240 sched.cpp:824] Sending SUBSCRIBE call to master@172.18.8.37:57116_x000D_
I1201 14:52:47.367959 119050240 sched.cpp:857] Will retry registration in 1.762124339secs if necessary_x000D_
I1201 14:52:47.368059 121733120 master.cpp:2969] Received SUBSCRIBE call for framework 'default' at scheduler-430784ca-9418-4424-b431-5ce1a96a6fee@172.18.8.37:57116_x000D_
I1201 14:52:47.368075 121733120 master.cpp:2286] Authorizing framework principal 'test-principal' to receive offers for roles '{ role }'_x000D_
I1201 14:52:47.368321 119586816 master.cpp:3049] Subscribing framework default with checkpointing disabled and capabilities [ RESERVATION_REFINEMENT ]_x000D_
I1201 14:52:47.368604 120659968 clock.cpp:435] Clock of metrics@172.18.8.37:57116 updated to 2017-12-01 13:53:04.849857088+00:00_x000D_
I1201 14:52:47.368692 120659968 hierarchical.cpp:293] Added framework 209387ca-a9c3-4717-9769-a59d9fe927f1-0000_x000D_
I1201 14:52:47.368717 120123392 sched.cpp:751] Framework registered with 209387ca-a9c3-4717-9769-a59d9fe927f1-0000_x000D_
I1201 14:52:47.368772 120123392 sched.cpp:765] Scheduler::registered took 45597ns_x000D_
I1201 14:52:47.368937 120659968 hierarchical.cpp:1860] Allocating cpus:8; mem:2048; disk:470537; ports:[31000-32000] on agent 209387ca-a9c3-4717-9769-a59d9fe927f1-S0 to role role of framework 209387ca-a9c3-4717-9769-a59d9fe927f1-0000_x000D_
I1201 14:52:47.369276 120659968 clock.cpp:435] Clock of master@172.18.8.37:57116 updated to 2017-12-01 13:53:04.849857088+00:00_x000D_
I1201 14:52:47.369351 120659968 hierarchical.cpp:1990] No inverse offers to send out!_x000D_
I1201 14:52:47.369380 120659968 hierarchical.cpp:1431] Performed allocation for 1 agents in 643932ns_x000D_
I1201 14:52:47.369691 119050240 master.cpp:8419] Sending 1 offers to framework 209387ca-a9c3-4717-9769-a59d9fe927f1-0000 (default) at scheduler-430784ca-9418-4424-b431-5ce1a96a6fee@172.18.8.37:57116_x000D_
I1201 14:52:47.369753 119050240 clock.cpp:435] Clock of scheduler-430784ca-9418-4424-b431-5ce1a96a6fee@172.18.8.37:57116 updated to 2017-12-01 13:53:04.849857088+00:00_x000D_
I1201 14:52:47.369946 121733120 sched.cpp:897] Received 1 offers_x000D_
I1201 14:52:47.370076 121733120 sched.cpp:921] Scheduler::resourceOffers took 82264ns_x000D_
I1201 14:52:47.372522 119586816 master.cpp:10331] Removing offer 209387ca-a9c3-4717-9769-a59d9fe927f1-O0_x000D_
I1201 14:52:47.372608 119586816 master.cpp:4236] Processing ACCEPT call for offers: [ 209387ca-a9c3-4717-9769-a59d9fe927f1-O0 ] on agent 209387ca-a9c3-4717-9769-a59d9fe927f1-S0 at slave(18)@172.18.8.37:57116 (172.18.8.37) for framework 209387ca-a9c3-4717-9769-a59d9fe927f1-0000 (default) at scheduler-430784ca-9418-4424-b431-5ce1a96a6fee@172.18.8.37:57116_x000D_
I1201 14:52:47.372666 119586816 clock.cpp:435] Clock of local-authorizer(52)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.849857088+00:00_x000D_
I1201 14:52:47.372692 119586816 master.cpp:3663] Authorizing principal 'test-principal' to reserve resources '[{""allocation_info"":{""role"":""role""},""name"":""cpus"",""reservations"":[{""principal"":""test-principal"",""role"":""role"",""type"":""DYNAMIC""}],""scalar"":{""value"":8.0},""type"":""SCALAR""},{""allocation_info"":{""role"":""role""},""name"":""mem"",""reservations"":[{""principal"":""test-principal"",""role"":""role"",""type"":""DYNAMIC""}],""scalar"":{""value"":2048.0},""type"":""SCALAR""}]'_x000D_
I1201 14:52:47.373001 119586816 clock.cpp:435] Clock of __await__(52)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.849857088+00:00_x000D_
I1201 14:52:47.373103 119586816 clock.cpp:435] Clock of __await__(53)@172.18.8.37:57116 updated to 2017-12-01 13:53:04.849857088+00:00_x000D_
I1201 14:52:47.373265 118513664 clock.cpp:435] Clock of help@172.18.8.37:57116 updated to 2017-12-01 13:53:04.849857088+00:00_x000D_
I1201 14:52:47.373839 119050240 master.cpp:4569] Applying RESERVE operation for resources [{""allocation_info"":{""role"":""r",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.2631578947368421,0.36428571428571427,0.28205128205128205,0.296969696969697,0.296969696969697,0.0
Bug,SlaveTest.IgnoreV0ExecutorIfItReregistersWithoutReconnect is flaky.,"{noformat}_x000D_
../../src/tests/slave_tests.cpp:7888_x000D_
Actual function call count doesn't match EXPECT_CALL(exec, shutdown(_))..._x000D_
         Expected: to be called once_x000D_
           Actual: never called - unsatisfied and active_x000D_
{noformat}_x000D_
Full log attached.",3.0,0,0.5,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.717948717948718,0.8242424242424242,0.7212121212121212,0.0
Bug,Mesos Containerizer GC should set 'layers' after checkpointing layer ids in provisioner.,"{noformat}_x000D_
11111_x000D_
222222_x000D_
333333_x000D_
444444_x000D_
11111_x000D_
222222_x000D_
333333_x000D_
444444_x000D_
I1129 23:24:45.469543  6592 registry_puller.cpp:395] Extracting layer tar ball '/tmp/mesos/store/docker/staging/MVgVC7/sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 to rootfs '/tmp/mesos/store/docker/staging/MVgVC7/38135e3743e6dcb66bd1394b633053714333c00007b7cf930bfeebfda660c06e/rootfs.overlay'_x000D_
I1129 23:24:45.473287  6592 registry_puller.cpp:395] Extracting layer tar ball '/tmp/mesos/store/docker/staging/MVgVC7/sha256:b56ae66c29370df48e7377c8f9baa744a3958058a766793f821dadcb144a4647 to rootfs '/tmp/mesos/store/docker/staging/MVgVC7/b5815a31a59b66c909dbf6c670de78690d4b52649b8e283fc2bfd2594f61cca3/rootfs.overlay'_x000D_
I1129 23:24:45.582002  6594 registry_puller.cpp:395] Extracting layer tar ball '/tmp/mesos/store/docker/staging/6Zbc17/sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 to rootfs '/tmp/mesos/store/docker/staging/6Zbc17/e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6/rootfs.overlay'_x000D_
I1129 23:24:45.589404  6595 metadata_manager.cpp:167] Successfully cached image 'alpine'_x000D_
I1129 23:24:45.590204  6594 registry_puller.cpp:395] Extracting layer tar ball '/tmp/mesos/store/docker/staging/6Zbc17/sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 to rootfs '/tmp/mesos/store/docker/staging/6Zbc17/be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e/rootfs.overlay'_x000D_
I1129 23:24:45.595190  6594 registry_puller.cpp:395] Extracting layer tar ball '/tmp/mesos/store/docker/staging/6Zbc17/sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 to rootfs '/tmp/mesos/store/docker/staging/6Zbc17/53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f/rootfs.overlay'_x000D_
I1129 23:24:45.599500  6594 registry_puller.cpp:395] Extracting layer tar ball '/tmp/mesos/store/docker/staging/6Zbc17/sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 to rootfs '/tmp/mesos/store/docker/staging/6Zbc17/a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721/rootfs.overlay'_x000D_
I1129 23:24:45.602047  6597 provisioner.cpp:506] Provisioning image rootfs '/tmp/provisioner/containers/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/01de09c5-d8e9-412e-8825-a592d2c875e5/backends/overlay/rootfses/b5d48445-848d-4274-a4f8-e909351ebc35' for container 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 using overlay backend_x000D_
I1129 23:24:45.602751  6594 registry_puller.cpp:395] Extracting layer tar ball '/tmp/mesos/store/docker/staging/6Zbc17/sha256:1db09adb5ddd7f1a07b6d585a7db747a51c7bd17418d47e91f901bdf420abd66 to rootfs '/tmp/mesos/store/docker/staging/6Zbc17/120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16/rootfs.overlay'_x000D_
I1129 23:24:45.603054  6596 overlay.cpp:168] Created symlink '/tmp/provisioner/containers/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/01de09c5-d8e9-412e-8825-a592d2c875e5/backends/overlay/scratch/b5d48445-848d-4274-a4f8-e909351ebc35/links' -> '/tmp/xAWQ8y'_x000D_
I1129 23:24:45.604398  6596 overlay.cpp:196] Provisioning image rootfs with overlayfs: 'lowerdir=/tmp/xAWQ8y/1:/tmp/xAWQ8y/0,upperdir=/tmp/provisioner/containers/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/01de09c5-d8e9-412e-8825-a592d2c875e5/backends/overlay/scratch/b5d48445-848d-4274-a4f8-e909351ebc35/upperdir,workdir=/tmp/provisioner/containers/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/01de09c5-d8e9-412e-8825-a592d2c875e5/backends/overlay/scratch/b5d48445-848d-4274-a4f8-e909351ebc35/workdir'_x000D_
I1129 23:24:45.607802  6594 registry_puller.cpp:395] Extracting layer tar ball '/tmp/mesos/store/docker/staging/6Zbc17/sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 to rootfs '/tmp/mesos/store/docker/staging/6Zbc17/42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229/rootfs.overlay'_x000D_
I1129 23:24:45.612139  6594 registry_puller.cpp:395] Extracting layer tar ball '/tmp/mesos/store/docker/staging/6Zbc17/sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4 to rootfs '/tmp/mesos/store/docker/staging/6Zbc17/511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158/rootfs.overlay'_x000D_
I1129 23:24:45.612253  6593 containerizer.cpp:1369] Checkpointed ContainerConfig at '/var/run/mesos/containers/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/01de09c5-d8e9-412e-8825-a592d2c875e5/config'_x000D_
I1129 23:24:45.612298  6593 containerizer.cpp:2926] Transitioning the state of container 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 from PROVISIONING to PREPARING_x000D_
I1129 23:24:45.625658  6596 containerizer.cpp:1838] Launching 'mesos-containerizer' with flags '--help=""false"" --launch_info=""{""clone_namespaces"":[131072],""command"":{""shell"":true,""value"":""sleep 1""},""environment"":{""variables"":[{""name"":""MESOS_SANDBOX"",""type"":""VALUE"",""value"":""\/mnt\/mesos\/sandbox""},{""name"":""PATH"",""type"":""VALUE"",""value"":""\/usr\/local\/sbin:\/usr\/local\/bin:\/usr\/sbin:\/usr\/bin:\/sbin:\/bin""},{""name"":""MESOS_CONTAINER_IP"",""type"":""VALUE"",""value"":""10.0.2.15""}]},""pre_exec_commands"":[{""arguments"":[""mesos-containerizer"",""mount"",""--help=false"",""--operation=make-rslave"",""--path=\/""],""shell"":false,""value"":""\/vagrant\/mesos\/build\/src\/mesos-containerizer""},{""arguments"":[""mount"",""-n"",""--rbind"",""\/tmp\/slaves\/65fa58c5-48c6-4998-b336-ceb9bcd2ec43-S0\/frameworks\/55b825d4-c922-46bb-ab8e-4e01abf1a756-0000\/executors\/default-executor\/runs\/3bbc3fd1-0138-43a9-94ba-d017d813daac\/containers\/01de09c5-d8e9-412e-8825-a592d2c875e5"",""\/tmp\/provisioner\/containers\/3bbc3fd1-0138-43a9-94ba-d017d813daac\/containers\/01de09c5-d8e9-412e-8825-a592d2c875e5\/backends\/overlay\/rootfses\/b5d48445-848d-4274-a4f8-e909351ebc35\/mnt\/mesos\/sandbox""],""shell"":false,""value"":""mount""}],""rootfs"":""\/tmp\/provisioner\/containers\/3bbc3fd1-0138-43a9-94ba-d017d813daac\/containers\/01de09c5-d8e9-412e-8825-a592d2c875e5\/backends\/overlay\/rootfses\/b5d48445-848d-4274-a4f8-e909351ebc35"",""task_environment"":{},""user"":""root"",""working_directory"":""\/mnt\/mesos\/sandbox""}"" --pipe_read=""12"" --pipe_write=""15"" --runtime_directory=""/var/run/mesos/containers/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/01de09c5-d8e9-412e-8825-a592d2c875e5"" --unshare_namespace_mnt=""false""'_x000D_
I1129 23:24:45.626317  6598 linux_launcher.cpp:438] Launching nested container 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 and cloning with namespaces CLONE_NEWNS_x000D_
I1129 23:24:45.633211  6598 systemd.cpp:96] Assigned child process '6745' to 'mesos_executors.slice'_x000D_
I1129 23:24:45.636270  6596 containerizer.cpp:2926] Transitioning the state of container 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 from PREPARING to ISOLATING_x000D_
I1129 23:24:45.691830  6597 metadata_manager.cpp:167] Successfully cached image 'mesosphere/inky'_x000D_
I1129 23:24:45.694399  6594 provisioner.cpp:506] Provisioning image rootfs '/tmp/provisioner/containers/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/7be37efd-7ec4-459a-a51a-97f6be99c3c3/backends/overlay/rootfses/1187cc83-a23a-4390-9c28-092a7b7690b5' for container 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 using overlay backend_x000D_
I1129 23:24:45.694919  6596 overlay.cpp:168] Created symlink '/tmp/provisioner/containers/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/7be37efd-7ec4-459a-a51a-97f6be99c3c3/backends/overlay/scratch/1187cc83-a23a-4390-9c28-092a7b7690b5/links' -> '/tmp/GXhXiT'_x000D_
I1129 23:24:45.695103  6596 overlay.cpp:196] Provisioning image rootfs with overlayfs: 'lowerdir=/tmp/GXhXiT/6:/tmp/GXhXiT/5:/tmp/GXhXiT/4:/tmp/GXhXiT/3:/tmp/GXhXiT/2:/tmp/GXhXiT/1:/tmp/GXhXiT/0,upperdir=/tmp/provisioner/containers/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/7be37efd-7ec4-459a-a51a-97f6be99c3c3/backends/overlay/scratch/1187cc83-a23a-4390-9c28-092a7b7690b5/upperdir,workdir=/tmp/provisioner/containers/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/7be37efd-7ec4-459a-a51a-97f6be99c3c3/backends/overlay/scratch/1187cc83-a23a-4390-9c28-092a7b7690b5/workdir'_x000D_
I1129 23:24:45.696255  6596 provisioner.cpp:714] Container 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 has no checkpointed layers_x000D_
I1129 23:24:45.696349  6596 provisioner.cpp:714] Container 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 has no checkpointed layers_x000D_
I1129 23:24:45.696449  6593 containerizer.cpp:1369] Checkpointed ContainerConfig at '/var/run/mesos/containers/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/7be37efd-7ec4-459a-a51a-97f6be99c3c3/config'_x000D_
I1129 23:24:45.696506  6593 containerizer.cpp:2926] Transitioning the state of container 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 from PROVISIONING to PREPARING_x000D_
I1129 23:24:45.697865  6595 store.cpp:531] Layer 'be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e' is retained by image store cache_x000D_
I1129 23:24:45.697918  6595 store.cpp:531] Layer '511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158' is retained by image store cache_x000D_
I1129 23:24:45.697968  6595 store.cpp:531] Layer '53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f' is retained by image store cache_x000D_
I1129 23:24:45.697999  6595 store.cpp:531] Layer 'e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6' is retained by image store cache_x000D_
I1129 23:24:45.698025  6595 store.cpp:531] Layer '120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16' is retained by image store cache_x000D_
I1129 23:24:45.698050  6595 store.cpp:531] Layer '38135e3743e6dcb66bd1394b633053714333c00007b7cf930bfeebfda660c06e' is retained by image store cache_x000D_
I1129 23:24:45.698076  6595 store.cpp:531] Layer 'a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721' is retained by image store cache_x000D_
I1129 23:24:45.698104  6595 store.cpp:531] Layer '42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229' is retained by image store cache_x000D_
I1129 23:24:45.698129  6595 store.cpp:531] Layer 'b5815a31a59b66c909dbf6c670de78690d4b52649b8e283fc2bfd2594f61cca3' is retained by image store cache_x000D_
I1129 23:24:45.698894  6596 provisioner.cpp:714] Container 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 has no checkpointed layers_x000D_
I1129 23:24:45.698966  6596 provisioner.cpp:714] Container 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 has no checkpointed layers_x000D_
I1129 23:24:45.700333  6596 store.cpp:531] Layer 'be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e' is retained by image store cache_x000D_
I1129 23:24:45.700394  6596 store.cpp:531] Layer '511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158' is retained by image store cache_x000D_
I1129 23:24:45.700412  6596 store.cpp:531] Layer '53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f' is retained by image store cache_x000D_
I1129 23:24:45.700428  6596 store.cpp:531] Layer 'e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6' is retained by image store cache_x000D_
I1129 23:24:45.700441  6596 store.cpp:531] Layer '120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16' is retained by image store cache_x000D_
I1129 23:24:45.700454  6596 store.cpp:531] Layer '38135e3743e6dcb66bd1394b633053714333c00007b7cf930bfeebfda660c06e' is retained by image store cache_x000D_
I1129 23:24:45.700467  6596 store.cpp:531] Layer 'a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721' is retained by image store cache_x000D_
I1129 23:24:45.700480  6596 store.cpp:531] Layer '42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229' is retained by image store cache_x000D_
I1129 23:24:45.700495  6596 store.cpp:531] Layer 'b5815a31a59b66c909dbf6c670de78690d4b52649b8e283fc2bfd2594f61cca3' is retained by image store cache_x000D_
I1129 23:24:45.702491  6594 provisioner.cpp:714] Container 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 has no checkpointed layers_x000D_
I1129 23:24:45.702554  6594 provisioner.cpp:714] Container 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 has no checkpointed layers_x000D_
I1129 23:24:45.703707  6592 store.cpp:531] Layer 'be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e' is retained by image store cache_x000D_
I1129 23:24:45.703783  6592 store.cpp:531] Layer '511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158' is retained by image store cache_x000D_
I1129 23:24:45.703812  6592 store.cpp:531] Layer '53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f' is retained by image store cache_x000D_
I1129 23:24:45.703816  6592 store.cpp:531] Layer 'e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6' is retained by image store cache_x000D_
I1129 23:24:45.703816  6592 store.cpp:531] Layer '120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16' is retained by image store cache_x000D_
I1129 23:24:45.704164  6592 store.cpp:531] Layer '38135e3743e6dcb66bd1394b633053714333c00007b7cf930bfeebfda660c06e' is retained by image store cache_x000D_
I1129 23:24:45.704208  6592 store.cpp:531] Layer 'a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721' is retained by image store cache_x000D_
I1129 23:24:45.704255  6592 store.cpp:531] Layer '42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229' is retained by image store cache_x000D_
I1129 23:24:45.704285  6592 store.cpp:531] Layer 'b5815a31a59b66c909dbf6c670de78690d4b52649b8e283fc2bfd2594f61cca3' is retained by image store cache_x000D_
I1129 23:24:45.704814  6595 provisioner.cpp:714] Container 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 has no checkpointed layers_x000D_
I1129 23:24:45.704861  6595 provisioner.cpp:714] Container 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 has no checkpointed layers_x000D_
I1129 23:24:45.708112  6592 store.cpp:531] Layer 'be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e' is retained by image store cache_x000D_
I1129 23:24:45.708204  6592 store.cpp:531] Layer '511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158' is retained by image store cache_x000D_
I1129 23:24:45.708238  6592 store.cpp:531] Layer '53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f' is retained by image store cache_x000D_
I1129 23:24:45.708264  6592 store.cpp:531] Layer 'e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6' is retained by image store cache_x000D_
I1129 23:24:45.708375  6592 store.cpp:531] Layer '120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16' is retained by image store cache_x000D_
I1129 23:24:45.708407  6592 store.cpp:531] Layer '38135e3743e6dcb66bd1394b633053714333c00007b7cf930bfeebfda660c06e' is retained by image store cache_x000D_
I1129 23:24:45.708472  6592 store.cpp:531] Layer 'a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721' is retained by image store cache_x000D_
I1129 23:24:45.708514  6592 store.cpp:531] Layer '42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229' is retained by image store cache_x000D_
I1129 23:24:45.708545  6592 store.cpp:531] Layer 'b5815a31a59b66c909dbf6c670de78690d4b52649b8e283fc2bfd2594f61cca3' is retained by image store cache_x000D_
I1129 23:24:45.709048  6596 provisioner.cpp:714] Container 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 has no checkpointed layers_x000D_
I1129 23:24:45.709161  6596 provisioner.cpp:714] Container 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 has no checkpointed layers_x000D_
I1129 23:24:45.710321  6594 store.cpp:531] Layer 'be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e' is retained by image store cache_x000D_
I1129 23:24:45.710382  6594 store.cpp:531] Layer '511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158' is retained by image store cache_x000D_
I1129 23:24:45.710412  6594 store.cpp:531] Layer '53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f' is retained by image store cache_x000D_
I1129 23:24:45.710436  6594 store.cpp:531] Layer 'e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6' is retained by image store cache_x000D_
I1129 23:24:45.710458  6594 store.cpp:531] Layer '120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16' is retained by image store cache_x000D_
I1129 23:24:45.710480  6594 store.cpp:531] Layer '38135e3743e6dcb66bd1394b633053714333c00007b7cf930bfeebfda660c06e' is retained by image store cache_x000D_
I1129 23:24:45.710522  6594 store.cpp:531] Layer 'a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721' is retained by image store cache_x000D_
I1129 23:24:45.710551  6594 store.cpp:531] Layer '42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229' is retained by image store cache_x000D_
I1129 23:24:45.710590  6594 store.cpp:531] Layer 'b5815a31a59b66c909dbf6c670de78690d4b52649b8e283fc2bfd2594f61cca3' is retained by image store cache_x000D_
I1129 23:24:45.713176  6594 containerizer.cpp:1838] Launching 'mesos-containerizer' with flags '--help=""false"" --launch_info=""{""clone_namespaces"":[131072],""command"":{""shell"":true,""value"":""sleep 100000""},""environment"":{""variables"":[{""name"":""MESOS_SANDBOX"",""type"":""VALUE"",""value"":""\/mnt\/mesos\/sandbox""},{""name"":""HOME"",""type"":""VALUE"",""value"":""\/""},{""name"":""PATH"",""type"":""VALUE"",""value"":""\/usr\/local\/sbin:\/usr\/local\/bin:\/usr\/sbin:\/usr\/bin:\/sbin:\/bin""},{""name"":""MESOS_CONTAINER_IP"",""type"":""VALUE"",""value"":""10.0.2.15""}]},""pre_exec_commands"":[{""arguments"":[""mesos-containerizer"",""mount"",""--help=false"",""--operation=make-rslave"",""--path=\/""],""shell"":false,""value"":""\/vagrant\/mesos\/build\/src\/mesos-containerizer""},{""arguments"":[""mount"",""-n"",""--rbind"",""\/tmp\/slaves\/65fa58c5-48c6-4998-b336-ceb9bcd2ec43-S0\/frameworks\/55b825d4-c922-46bb-ab8e-4e01abf1a756-0000\/executors\/default-executor\/runs\/3bbc3fd1-0138-43a9-94ba-d017d813daac\/containers\/7be37efd-7ec4-459a-a51a-97f6be99c3c3"",""\/tmp\/provisioner\/containers\/3bbc3fd1-0138-43a9-94ba-d017d813daac\/containers\/7be37efd-7ec4-459a-a51a-97f6be99c3c3\/backends\/overlay\/rootfses\/1187cc83-a23a-4390-9c28-092a7b7690b5\/mnt\/mesos\/sandbox""],""shell"":false,""value"":""mount""}],""rootfs"":""\/tmp\/provisioner\/containers\/3bbc3fd1-0138-43a9-94ba-d017d813daac\/containers\/7be37efd-7ec4-459a-a51a-97f6be99c3c3\/backends\/overlay\/rootfses\/1187cc83-a23a-4390-9c28-092a7b7690b5"",""task_environment"":{},""user"":""root"",""working_directory"":""\/mnt\/mesos\/sandbox""}"" --pipe_read=""13"" --pipe_write=""14"" --runtime_directory=""/var/run/mesos/containers/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/7be37efd-7ec4-459a-a51a-97f6be99c3c3"" --unshare_namespace_mnt=""false""'_x000D_
I1129 23:24:45.713954  6597 linux_launcher.cpp:438] Launching nested container 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 and cloning with namespaces CLONE_NEWNS_x000D_
I1129 23:24:45.721781  6597 systemd.cpp:96] Assigned child process '6775' to 'mesos_executors.slice'_x000D_
I1129 23:24:45.725494  6594 containerizer.cpp:2926] Transitioning the state of container 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 from PREPARING to ISOLATING_x000D_
I1129 23:24:45.791635  6595 containerizer.cpp:2926] Transitioning the state of container 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 from ISOLATING to FETCHING_x000D_
I1129 23:24:45.791880  6591 fetcher.cpp:379] Starting to fetch URIs for container: 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5, directory: /tmp/slaves/65fa58c5-48c6-4998-b336-ceb9bcd2ec43-S0/frameworks/55b825d4-c922-46bb-ab8e-4e01abf1a756-0000/executors/default-executor/runs/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/01de09c5-d8e9-412e-8825-a592d2c875e5_x000D_
I1129 23:24:45.792626  6591 containerizer.cpp:2926] Transitioning the state of container 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 from FETCHING to RUNNING_x000D_
11111_x000D_
222222_x000D_
333333_x000D_
444444_x000D_
I1129 23:24:45.807262  6592 provisioner.cpp:714] Container 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 has no checkpointed layers_x000D_
I1129 23:24:45.807375  6592 provisioner.cpp:714] Container 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 has no checkpointed layers_x000D_
I1129 23:24:45.808658  6591 store.cpp:531] Layer 'be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e' is retained by image store cache_x000D_
I1129 23:24:45.808843  6591 store.cpp:531] Layer '511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158' is retained by image store cache_x000D_
I1129 23:24:45.808869  6591 store.cpp:531] Layer '53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f' is retained by image store cache_x000D_
I1129 23:24:45.808897  6591 store.cpp:531] Layer 'e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6' is retained by image store cache_x000D_
I1129 23:24:45.808962  6591 store.cpp:531] Layer '120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16' is retained by image store cache_x000D_
I1129 23:24:45.808990  6591 store.cpp:531] Layer '38135e3743e6dcb66bd1394b633053714333c00007b7cf930bfeebfda660c06e' is retained by image store cache_x000D_
I1129 23:24:45.809012  6591 store.cpp:531] Layer 'a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721' is retained by image store cache_x000D_
I1129 23:24:45.809036  6591 store.cpp:531] Layer '42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229' is retained by image store cache_x000D_
I1129 23:24:45.809057  6591 store.cpp:531] Layer 'b5815a31a59b66c909dbf6c670de78690d4b52649b8e283fc2bfd2594f61cca3' is retained by image store cache_x000D_
I1129 23:24:45.893280  6596 containerizer.cpp:2926] Transitioning the state of container 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 from ISOLATING to FETCHING_x000D_
I1129 23:24:45.893523  6596 fetcher.cpp:379] Starting to fetch URIs for container: 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3, directory: /tmp/slaves/65fa58c5-48c6-4998-b336-ceb9bcd2ec43-S0/frameworks/55b825d4-c922-46bb-ab8e-4e01abf1a756-0000/executors/default-executor/runs/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/7be37efd-7ec4-459a-a51a-97f6be99c3c3_x000D_
I1129 23:24:45.894335  6594 containerizer.cpp:2926] Transitioning the state of container 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 from FETCHING to RUNNING_x000D_
I1129 23:24:45.902606  6598 process.cpp:3503] Handling HTTP event for process 'slave(1)' with path: '/slave(1)/api/v1/executor'_x000D_
I1129 23:24:45.903908  6597 process.cpp:3503] Handling HTTP event for process 'slave(1)' with path: '/slave(1)/api/v1'_x000D_
I1129 23:24:45.904618  6597 process.cpp:3503] Handling HTTP event for process 'slave(1)' with path: '/slave(1)/api/v1'_x000D_
I1129 23:24:45.908681  6597 http.cpp:1185] HTTP POST for /slave(1)/api/v1 from 10.0.2.15:57620_x000D_
I1129 23:24:45.909113  6597 http.cpp:1185] HTTP POST for /slave(1)/api/v1 from 10.0.2.15:57622_x000D_
I1129 23:24:45.909708  6597 http.cpp:2589] Processing WAIT_NESTED_CONTAINER call for container '3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5'_x000D_
I1129 23:24:45.910148  6597 http.cpp:2589] Processing WAIT_NESTED_CONTAINER call for container '3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3'_x000D_
I1129 23:24:45.938350  6596 process.cpp:3503] Handling HTTP event for process 'slave(1)' with path: '/slave(1)/api/v1/executor'_x000D_
I1129 23:24:45.938781  6596 http.cpp:1185] HTTP POST for /slave(1)/api/v1/executor from 10.0.2.15:57564_x000D_
I1129 23:24:45.939141  6596 slave.cpp:4584] Handling status update TASK_RUNNING (UUID: 6ba2641b-ff6a-4ac2-9123-23579534147d) for task 1 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000_x000D_
I1129 23:24:45.940809  6591 http.cpp:1185] HTTP POST for /slave(1)/api/v1/executor from 10.0.2.15:57564_x000D_
I1129 23:24:45.941130  6591 slave.cpp:4584] Handling status update TASK_RUNNING (UUID: f22fe2c5-2a2e-43c9-9f22-a7da351bab08) for task 2 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000_x000D_
I1129 23:24:45.942906  6591 task_status_update_manager.cpp:328] Received task status update TASK_RUNNING (UUID: 6ba2641b-ff6a-4ac2-9123-23579534147d) for task 1 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000_x000D_
I1129 23:24:45.943076  6591 task_status_update_manager.cpp:383] Forwarding task status update TASK_RUNNING (UUID: 6ba2641b-ff6a-4ac2-9123-23579534147d) for task 1 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000 to the agent_x000D_
I1129 23:24:45.943322  6595 slave.cpp:5067] Forwarding the update TASK_RUNNING (UUID: 6ba2641b-ff6a-4ac2-9123-23579534147d) for task 1 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000 to master@127.0.0.1:5050_x000D_
I1129 23:24:45.943332  6591 task_status_update_manager.cpp:328] Received task status update TASK_RUNNING (UUID: f22fe2c5-2a2e-43c9-9f22-a7da351bab08) for task 2 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000_x000D_
I1129 23:24:45.943506  6591 task_status_update_manager.cpp:383] Forwarding task status update TASK_RUNNING (UUID: f22fe2c5-2a2e-43c9-9f22-a7da351bab08) for task 2 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000 to the agent_x000D_
I1129 23:24:45.943717  6595 slave.cpp:4960] Task status update manager successfully handled status update TASK_RUNNING (UUID: 6ba2641b-ff6a-4ac2-9123-23579534147d) for task 1 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000_x000D_
I1129 23:24:45.943905  6595 slave.cpp:5067] Forwarding the update TASK_RUNNING (UUID: f22fe2c5-2a2e-43c9-9f22-a7da351bab08) for task 2 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000 to master@127.0.0.1:5050_x000D_
I1129 23:24:45.943905  6595 slave.cpp:4960] Task status update manager successfully handled status update TASK_RUNNING (UUID: f22fe2c5-2a2e-43c9-9f22-a7da351bab08) for task 2 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000_x000D_
I1129 23:24:45.992866  6591 task_status_update_manager.cpp:401] Received task status update acknowledgement (UUID: 6ba2641b-ff6a-4ac2-9123-23579534147d) for task 1 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000_x000D_
I1129 23:24:45.993261  6595 slave.cpp:3868] Task status update manager successfully handled status update acknowledgement (UUID: 6ba2641b-ff6a-4ac2-9123-23579534147d) for task 1 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000_x000D_
I1129 23:24:45.993968  6593 task_status_update_manager.cpp:401] Received task status update acknowledgement (UUID: f22fe2c5-2a2e-43c9-9f22-a7da351bab08) for task 2 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000_x000D_
I1129 23:24:45.994295  6598 slave.cpp:3868] Task status update manager successfully handled status update acknowledgement (UUID: f22fe2c5-2a2e-43c9-9f22-a7da351bab08) for task 2 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000_x000D_
11111_x000D_
222222_x000D_
333333_x000D_
444444_x000D_
I1129 23:24:46.808684  6592 provisioner.cpp:714] Container 3bbc3fd1-0138-43a9-94ba-d017d813daac.7be37efd-7ec4-459a-a51a-97f6be99c3c3 has no checkpointed layers_x000D_
I1129 23:24:46.808876  6592 provisioner.cpp:714] Container 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 has no checkpointed layers_x000D_
I1129 23:24:46.810683  6593 store.cpp:531] Layer 'be4ce2753831b8952a5b797cf45b2230e1befead6f5db0630bcb24a5f554255e' is retained by image store cache_x000D_
I1129 23:24:46.810751  6593 store.cpp:531] Layer '511136ea3c5a64f264b78b5433614aec563103b4d4702f3ba7d4d2698e22c158' is retained by image store cache_x000D_
I1129 23:24:46.810781  6593 store.cpp:531] Layer '53b5066c5a7dff5d6f6ef0c1945572d6578c083d550d2a3d575b4cdf7460306f' is retained by image store cache_x000D_
I1129 23:24:46.810808  6593 store.cpp:531] Layer 'e28617c6dd2169bfe2b10017dfaa04bd7183ff840c4f78ebe73fca2a89effeb6' is retained by image store cache_x000D_
I1129 23:24:46.810834  6593 store.cpp:531] Layer '120e218dd395ec314e7b6249f39d2853911b3d6def6ea164ae05722649f34b16' is retained by image store cache_x000D_
I1129 23:24:46.810860  6593 store.cpp:531] Layer '38135e3743e6dcb66bd1394b633053714333c00007b7cf930bfeebfda660c06e' is retained by image store cache_x000D_
I1129 23:24:46.810885  6593 store.cpp:531] Layer 'a9eb172552348a9a49180694790b33a1097f546456d041b6e82e4d7716ddb721' is retained by image store cache_x000D_
I1129 23:24:46.810911  6593 store.cpp:531] Layer '42eed7f1bf2ac3f1610c5e616d2ab1ee9c7290234240388d6297bc0f32c34229' is retained by image store cache_x000D_
I1129 23:24:46.810937  6593 store.cpp:531] Layer 'b5815a31a59b66c909dbf6c670de78690d4b52649b8e283fc2bfd2594f61cca3' is retained by image store cache_x000D_
I1129 23:24:47.057590  6596 containerizer.cpp:2775] Container 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 has exited_x000D_
I1129 23:24:47.057667  6596 containerizer.cpp:2324] Destroying container 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 in RUNNING state_x000D_
I1129 23:24:47.057695  6596 containerizer.cpp:2926] Transitioning the state of container 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5 from RUNNING to DESTROYING_x000D_
I1129 23:24:47.058082  6596 linux_launcher.cpp:514] Asked to destroy container 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5_x000D_
I1129 23:24:47.059027  6596 linux_launcher.cpp:560] Using freezer to destroy cgroup mesos/3bbc3fd1-0138-43a9-94ba-d017d813daac/mesos/01de09c5-d8e9-412e-8825-a592d2c875e5_x000D_
I1129 23:24:47.060667  6596 cgroups.cpp:3058] Freezing cgroup /sys/fs/cgroup/freezer/mesos/3bbc3fd1-0138-43a9-94ba-d017d813daac/mesos/01de09c5-d8e9-412e-8825-a592d2c875e5_x000D_
I1129 23:24:47.062700  6597 cgroups.cpp:1413] Successfully froze cgroup /sys/fs/cgroup/freezer/mesos/3bbc3fd1-0138-43a9-94ba-d017d813daac/mesos/01de09c5-d8e9-412e-8825-a592d2c875e5 after 1.838336ms_x000D_
I1129 23:24:47.064627  6592 cgroups.cpp:3076] Thawing cgroup /sys/fs/cgroup/freezer/mesos/3bbc3fd1-0138-43a9-94ba-d017d813daac/mesos/01de09c5-d8e9-412e-8825-a592d2c875e5_x000D_
I1129 23:24:47.066498  6598 cgroups.cpp:1442] Successfully thawed cgroup /sys/fs/cgroup/freezer/mesos/3bbc3fd1-0138-43a9-94ba-d017d813daac/mesos/01de09c5-d8e9-412e-8825-a592d2c875e5 after 1.642752ms_x000D_
I1129 23:24:47.071521  6592 provisioner.cpp:648] Destroying container rootfs at '/tmp/provisioner/containers/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/01de09c5-d8e9-412e-8825-a592d2c875e5/backends/overlay/rootfses/b5d48445-848d-4274-a4f8-e909351ebc35' for container 3bbc3fd1-0138-43a9-94ba-d017d813daac.01de09c5-d8e9-412e-8825-a592d2c875e5_x000D_
I1129 23:24:47.098203  6596 overlay.cpp:296] Removed temporary directory '/tmp/xAWQ8y' pointed by '/tmp/provisioner/containers/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/01de09c5-d8e9-412e-8825-a592d2c875e5/backends/overlay/scratch/b5d48445-848d-4274-a4f8-e909351ebc35/links'_x000D_
I1129 23:24:47.100265  6591 containerizer.cpp:2613] Checkpointing termination state to nested container's runtime directory '/var/run/mesos/containers/3bbc3fd1-0138-43a9-94ba-d017d813daac/containers/01de09c5-d8e9-412e-8825-a592d2c875e5/termination'_x000D_
I1129 23:24:47.107206  6594 process.cpp:3503] Handling HTTP event for process 'slave(1)' with path: '/slave(1)/api/v1/executor'_x000D_
I1129 23:24:47.151911  6594 http.cpp:1185] HTTP POST for /slave(1)/api/v1/executor from 10.0.2.15:57564_x000D_
I1129 23:24:47.152243  6594 slave.cpp:4584] Handling status update TASK_FINISHED (UUID: 288dbdf5-3dde-48fa-b2d4-bfd6c098ea44) for task 1 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000_x000D_
I1129 23:24:47.154391  6594 task_status_update_manager.cpp:328] Received task status update TASK_FINISHED (UUID: 288dbdf5-3dde-48fa-b2d4-bfd6c098ea44) for task 1 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000_x000D_
I1129 23:24:47.154578  6594 task_status_update_manager.cpp:383] Forwarding task status update TASK_FINISHED (UUID: 288dbdf5-3dde-48fa-b2d4-bfd6c098ea44) for task 1 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000 to the agent_x000D_
I1129 23:24:47.154810  6593 slave.cpp:5067] Forwarding the update TASK_FINISHED (UUID: 288dbdf5-3dde-48fa-b2d4-bfd6c098ea44) for task 1 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000 to master@127.0.0.1:5050_x000D_
I1129 23:24:47.157249  6593 slave.cpp:4960] Task status update manager successfully handled status update TASK_FINISHED (UUID: 288dbdf5-3dde-48fa-b2d4-bfd6c098ea44) for task 1 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000_x000D_
I1129 23:24:47.385977  6592 task_status_update_manager.cpp:401] Received task status update acknowledgement (UUID: 288dbdf5-3dde-48fa-b2d4-bfd6c098ea44) for task 1 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000_x000D_
I1129 23:24:47.386334  6592 task_status_update_manager.cpp:538] Cleaning up status update stream for task 1 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000_x000D_
I1129 23:24:47.387459  6592 slave.cpp:3868] Task status update manager successfully handled status update acknowledgement (UUID: 288dbdf5-3dde-48fa-b2d4-bfd6c098ea44) for task 1 of framework 55b825d4-c922-46bb-ab8e-4e01abf1a756-0000_x000D_
I1129 23:24:47.387568  6592 slave.cpp:8457] Completing task",3.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.4,0.07017543859649122,0.12142857142857143,0.6794871794871795,0.5939393939393939,0.5939393939393939,0.0
Task,Add an agent endpoint to list all active resource providers,Operators/Frameworks might need information about all resource providers currently running on an agent. An API endpoint should provide that information and include resource provider name and type.,3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.2631578947368421,0.36428571428571427,0.28205128205128205,0.296969696969697,0.296969696969697,0.0
Task,Support resource provider re-subscription in the resource provider manager,"Resource providers may re-subscribe by sending a {{SUBSCRIBE}} call that includes a resource provider ID. Support for this has to be added to the resource provider manager. E.g., the manager should check if a resource provider with the ID exists and use the updated HTTP connection.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.2631578947368421,0.36428571428571427,0.28205128205128205,0.296969696969697,0.296969696969697,0.0
Bug,ResourceProviderManagerHttpApiTest.ConvertResources is flaky,"From a ASF CI run:_x000D_
_x000D_
{noformat}_x000D_
3: [       OK ] ContentType/ResourceProviderManagerHttpApiTest.ConvertResources/0 (1048 ms)_x000D_
3: [ RUN      ] ContentType/ResourceProviderManagerHttpApiTest.ConvertResources/1_x000D_
3: I1123 08:06:04.233137 20036 cluster.cpp:162] Creating default 'local' authorizer_x000D_
3: I1123 08:06:04.237293 20060 master.cpp:448] Master 7c9d8e8c-3fb3-44c5-8505-488ada3e848e (dce3e4c418cb) started on 172.17.0.2:35090_x000D_
3: I1123 08:06:04.237325 20060 master.cpp:450] Flags at startup: --acls="""" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate_agents=""true"" --authenticate_frameworks=""true"" --authenticate_http_frameworks=""true"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/EpiTO7/credentials"" --filter_gpu_resources=""true"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_unreachable_tasks_per_framework=""1000"" --port=""5050"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""in_memory"" --registry_fetch_timeout=""1mins"" --registry_gc_interval=""15mins"" --registry_max_agent_age=""2weeks"" --registry_max_agent_count=""102400"" --registry_store_timeout=""100secs"" --registry_strict=""false"" --root_submissions=""true"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/EpiTO7/master"" --zk_session_timeout=""10secs""_x000D_
3: I1123 08:06:04.237727 20060 master.cpp:499] Master only allowing authenticated frameworks to register_x000D_
3: I1123 08:06:04.237743 20060 master.cpp:505] Master only allowing authenticated agents to register_x000D_
3: I1123 08:06:04.237753 20060 master.cpp:511] Master only allowing authenticated HTTP frameworks to register_x000D_
3: I1123 08:06:04.237764 20060 credentials.hpp:37] Loading credentials for authentication from '/tmp/EpiTO7/credentials'_x000D_
3: I1123 08:06:04.238149 20060 master.cpp:555] Using default 'crammd5' authenticator_x000D_
3: I1123 08:06:04.238358 20060 http.cpp:1045] Creating default 'basic' HTTP authenticator for realm 'mesos-master-readonly'_x000D_
3: I1123 08:06:04.238575 20060 http.cpp:1045] Creating default 'basic' HTTP authenticator for realm 'mesos-master-readwrite'_x000D_
3: I1123 08:06:04.238764 20060 http.cpp:1045] Creating default 'basic' HTTP authenticator for realm 'mesos-master-scheduler'_x000D_
3: I1123 08:06:04.238939 20060 master.cpp:634] Authorization enabled_x000D_
3: I1123 08:06:04.239159 20043 whitelist_watcher.cpp:77] No whitelist given_x000D_
3: I1123 08:06:04.239187 20045 hierarchical.cpp:173] Initialized hierarchical allocator process_x000D_
3: I1123 08:06:04.242822 20041 master.cpp:2215] Elected as the leading master!_x000D_
3: I1123 08:06:04.242857 20041 master.cpp:1695] Recovering from registrar_x000D_
3: I1123 08:06:04.243067 20052 registrar.cpp:347] Recovering registrar_x000D_
3: I1123 08:06:04.243808 20052 registrar.cpp:391] Successfully fetched the registry (0B) in 690944ns_x000D_
3: I1123 08:06:04.243953 20052 registrar.cpp:495] Applied 1 operations in 37370ns; attempting to update the registry_x000D_
3: I1123 08:06:04.244638 20052 registrar.cpp:552] Successfully updated the registry in 620032ns_x000D_
3: I1123 08:06:04.244798 20052 registrar.cpp:424] Successfully recovered registrar_x000D_
3: I1123 08:06:04.245352 20058 hierarchical.cpp:211] Skipping recovery of hierarchical allocator: nothing to recover_x000D_
3: I1123 08:06:04.245358 20057 master.cpp:1808] Recovered 0 agents from the registry (129B); allowing 10mins for agents to re-register_x000D_
3: W1123 08:06:04.251852 20036 process.cpp:2756] Attempted to spawn already running process files@172.17.0.2:35090_x000D_
3: I1123 08:06:04.253250 20036 containerizer.cpp:301] Using isolation { environment_secret, posix/cpu, posix/mem, filesystem/posix, network/cni }_x000D_
3: W1123 08:06:04.253965 20036 backend.cpp:76] Failed to create 'aufs' backend: AufsBackend requires root privileges_x000D_
3: W1123 08:06:04.254109 20036 backend.cpp:76] Failed to create 'bind' backend: BindBackend requires root privileges_x000D_
3: I1123 08:06:04.254148 20036 provisioner.cpp:259] Using default backend 'copy'_x000D_
3: I1123 08:06:04.256542 20036 cluster.cpp:448] Creating default 'local' authorizer_x000D_
3: I1123 08:06:04.260066 20057 slave.cpp:262] Mesos agent started on (784)@172.17.0.2:35090_x000D_
3: I1123 08:06:04.260093 20057 slave.cpp:263] Flags at startup: --acls="""" --agent_features=""capabilities {_x000D_
3:   type: MULTI_ROLE_x000D_
3: }_x000D_
3: capabilities {_x000D_
3:   type: HIERARCHICAL_ROLE_x000D_
3: }_x000D_
3: capabilities {_x000D_
3:   type: RESERVATION_REFINEMENT_x000D_
3: }_x000D_
3: capabilities {_x000D_
3:   type: RESOURCE_PROVIDER_x000D_
3: }_x000D_
3: "" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/ContentType_ResourceProviderManagerHttpApiTest_ConvertResources_1_Vr92Vg/store/appc"" --authenticate_http_executors=""true"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""false"" --authenticatee=""crammd5"" --authentication_backoff_factor=""1secs"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/ContentType_ResourceProviderManagerHttpApiTest_ConvertResources_1_Vr92Vg/credential"" --default_role=""*"" --disallow_sharing_agent_pid_namespace=""false"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/ContentType_ResourceProviderManagerHttpApiTest_ConvertResources_1_Vr92Vg/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_reregistration_timeout=""2secs"" --executor_secret_key=""/tmp/ContentType_ResourceProviderManagerHttpApiTest_ConvertResources_1_Vr92Vg/executor_secret_key"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/ContentType_ResourceProviderManagerHttpApiTest_ConvertResources_1_Vr92Vg/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_command_executor=""false"" --http_credentials=""/tmp/ContentType_ResourceProviderManagerHttpApiTest_ConvertResources_1_Vr92Vg/http_credentials"" --http_heartbeat_interval=""30secs"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher=""posix"" --launcher_dir=""/mesos/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_executors_per_framework=""150"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --port=""5051"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --runtime_dir=""/tmp/ContentType_ResourceProviderManagerHttpApiTest_ConvertResources_1_Vr92Vg"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/ContentType_ResourceProviderManagerHttpApiTest_ConvertResources_1_ycwsnc"" --zk_session_timeout=""10secs""_x000D_
3: I1123 08:06:04.260721 20057 credentials.hpp:86] Loading credential for authentication from '/tmp/ContentType_ResourceProviderManagerHttpApiTest_ConvertResources_1_Vr92Vg/credential'_x000D_
3: I1123 08:06:04.260936 20057 slave.cpp:295] Agent using credential for: test-principal_x000D_
3: I1123 08:06:04.260975 20057 credentials.hpp:37] Loading credentials for authentication from '/tmp/ContentType_ResourceProviderManagerHttpApiTest_ConvertResources_1_Vr92Vg/http_credentials'_x000D_
3: I1123 08:06:04.261390 20057 http.cpp:1045] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-executor'_x000D_
3: I1123 08:06:04.261538 20057 http.cpp:1066] Creating default 'jwt' HTTP authenticator for realm 'mesos-agent-executor'_x000D_
3: I1123 08:06:04.261780 20057 http.cpp:1045] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-readonly'_x000D_
3: I1123 08:06:04.261898 20057 http.cpp:1066] Creating default 'jwt' HTTP authenticator for realm 'mesos-agent-readonly'_x000D_
3: I1123 08:06:04.263828 20057 slave.cpp:593] Agent resources: [{""name"":""cpus"",""scalar"":{""value"":2.0},""type"":""SCALAR""},{""name"":""mem"",""scalar"":{""value"":1024.0},""type"":""SCALAR""},{""name"":""disk"",""scalar"":{""value"":1024.0},""type"":""SCALAR""},{""name"":""ports"",""ranges"":{""range"":[{""begin"":31000,""end"":32000}]},""type"":""RANGES""}]_x000D_
3: I1123 08:06:04.264111 20057 slave.cpp:601] Agent attributes: [  ]_x000D_
3: I1123 08:06:04.264124 20057 slave.cpp:610] Agent hostname: dce3e4c418cb_x000D_
3: I1123 08:06:04.264286 20054 task_status_update_manager.cpp:181] Pausing sending task status updates_x000D_
3: I1123 08:06:04.266237 20052 state.cpp:64] Recovering state from '/tmp/ContentType_ResourceProviderManagerHttpApiTest_ConvertResources_1_ycwsnc/meta'_x000D_
3: I1123 08:06:04.266593 20052 task_status_update_manager.cpp:207] Recovering task status update manager_x000D_
3: I1123 08:06:04.266840 20045 containerizer.cpp:668] Recovering containerizer_x000D_
3: I1123 08:06:04.268791 20048 provisioner.cpp:455] Provisioner recovery complete_x000D_
3: I1123 08:06:04.269258 20054 slave.cpp:6493] Finished recovery_x000D_
3: I1123 08:06:04.270414 20057 task_status_update_manager.cpp:181] Pausing sending task status updates_x000D_
3: I1123 08:06:04.270411 20055 slave.cpp:1007] New master detected at master@172.17.0.2:35090_x000D_
3: I1123 08:06:04.270539 20055 slave.cpp:1042] Detecting new master_x000D_
3: I1123 08:06:04.283004 20040 slave.cpp:1069] Authenticating with master master@172.17.0.2:35090_x000D_
3: I1123 08:06:04.283128 20040 slave.cpp:1078] Using default CRAM-MD5 authenticatee_x000D_
3: I1123 08:06:04.283475 20042 authenticatee.cpp:121] Creating new client SASL connection_x000D_
3: I1123 08:06:04.283859 20052 master.cpp:8312] Authenticating slave(784)@172.17.0.2:35090_x000D_
3: I1123 08:06:04.284049 20038 authenticator.cpp:414] Starting authentication session for crammd5-authenticatee(1421)@172.17.0.2:35090_x000D_
3: I1123 08:06:04.284358 20043 authenticator.cpp:98] Creating new server SASL connection_x000D_
3: I1123 08:06:04.284647 20049 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5_x000D_
3: I1123 08:06:04.284683 20049 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'_x000D_
3: I1123 08:06:04.284826 20060 authenticator.cpp:204] Received SASL authentication start_x000D_
3: I1123 08:06:04.284900 20060 authenticator.cpp:326] Authentication requires more steps_x000D_
3: I1123 08:06:04.285058 20044 authenticatee.cpp:259] Received SASL authentication step_x000D_
3: I1123 08:06:04.285233 20044 authenticator.cpp:232] Received SASL authentication step_x000D_
3: I1123 08:06:04.285275 20044 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'dce3e4c418cb' server FQDN: 'dce3e4c418cb' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false _x000D_
3: I1123 08:06:04.285287 20044 auxprop.cpp:181] Looking up auxiliary property '*userPassword'_x000D_
3: I1123 08:06:04.285326 20044 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'_x000D_
3: I1123 08:06:04.285348 20044 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'dce3e4c418cb' server FQDN: 'dce3e4c418cb' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true _x000D_
3: I1123 08:06:04.285357 20044 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true_x000D_
3: I1123 08:06:04.285363 20044 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true_x000D_
3: I1123 08:06:04.285380 20044 authenticator.cpp:318] Authentication success_x000D_
3: I1123 08:06:04.285506 20059 authenticatee.cpp:299] Authentication success_x000D_
3: I1123 08:06:04.285557 20045 master.cpp:8342] Successfully authenticated principal 'test-principal' at slave(784)@172.17.0.2:35090_x000D_
3: I1123 08:06:04.285658 20047 authenticator.cpp:432] Authentication session cleanup for crammd5-authenticatee(1421)@172.17.0.2:35090_x000D_
3: I1123 08:06:04.285847 20044 slave.cpp:1161] Successfully authenticated with master master@172.17.0.2:35090_x000D_
3: I1123 08:06:04.286111 20044 slave.cpp:1685] Will retry registration in 2.233007ms if necessary_x000D_
3: I1123 08:06:04.286402 20046 master.cpp:6036] Received register agent message from slave(784)@172.17.0.2:35090 (dce3e4c418cb)_x000D_
3: I1123 08:06:04.286550 20046 master.cpp:3872] Authorizing agent with principal 'test-principal'_x000D_
3: I1123 08:06:04.287089 20056 master.cpp:6098] Authorized registration of agent at slave(784)@172.17.0.2:35090 (dce3e4c418cb)_x000D_
3: I1123 08:06:04.287220 20056 master.cpp:6191] Registering agent at slave(784)@172.17.0.2:35090 (dce3e4c418cb) with id 7c9d8e8c-3fb3-44c5-8505-488ada3e848e-S0_x000D_
3: I1123 08:06:04.287748 20053 registrar.cpp:495] Applied 1 operations in 63340ns; attempting to update the registry_x000D_
3: I1123 08:06:04.288362 20053 registrar.cpp:552] Successfully updated the registry in 548864ns_x000D_
3: I1123 08:06:04.288583 20042 master.cpp:6240] Admitted agent 7c9d8e8c-3fb3-44c5-8505-488ada3e848e-S0 at slave(784)@172.17.0.2:35090 (dce3e4c418cb)_x000D_
3: I1123 08:06:04.289449 20042 master.cpp:6276] Registered agent 7c9d8e8c-3fb3-44c5-8505-488ada3e848e-S0 at slave(784)@172.17.0.2:35090 (dce3e4c418cb) with cpus:2; mem:1024; disk:1024; ports:[31000-32000]_x000D_
3: I1123 08:06:04.289716 20038 slave.cpp:1685] Will retry registration in 35.387012ms if necessary_x000D_
3: I1123 08:06:04.289844 20043 hierarchical.cpp:600] Added agent 7c9d8e8c-3fb3-44c5-8505-488ada3e848e-S0 (dce3e4c418cb) with cpus:2; mem:1024; disk:1024; ports:[31000-32000] (allocated: {})_x000D_
3: I1123 08:06:04.289922 20038 slave.cpp:1207] Registered with master master@172.17.0.2:35090; given agent ID 7c9d8e8c-3fb3-44c5-8505-488ada3e848e-S0_x000D_
3: I1123 08:06:04.289942 20060 master.cpp:6036] Received register agent message from slave(784)@172.17.0.2:35090 (dce3e4c418cb)_x000D_
3: I1123 08:06:04.290046 20060 master.cpp:3872] Authorizing agent with principal 'test-principal'_x000D_
3: I1123 08:06:04.290077 20050 task_status_update_manager.cpp:188] Resuming sending task status updates_x000D_
3: I1123 08:06:04.290155 20043 hierarchical.cpp:1457] Performed allocation for 1 agents in 152178ns_x000D_
3: I1123 08:06:04.290329 20038 slave.cpp:1227] Checkpointing SlaveInfo to '/tmp/ContentType_ResourceProviderManagerHttpApiTest_ConvertResources_1_ycwsnc/meta/slaves/7c9d8e8c-3fb3-44c5-8505-488ada3e848e-S0/slave.info'_x000D_
3: I1123 08:06:04.290479 20059 master.cpp:6098] Authorized registration of agent at slave(784)@172.17.0.2:35090 (dce3e4c418cb)_x000D_
3: I1123 08:06:04.290560 20059 master.cpp:6169] Agent 7c9d8e8c-3fb3-44c5-8505-488ada3e848e-S0 at slave(784)@172.17.0.2:35090 (dce3e4c418cb) already registered, resending acknowledgement_x000D_
3: I1123 08:06:04.290829 20038 slave.cpp:1288] Forwarding total resources cpus:2; mem:1024; disk:1024; ports:[31000-32000]_x000D_
3: I1123 08:06:04.290917 20038 slave.cpp:1298] Forwarding total oversubscribed resources {}_x000D_
3: W1123 08:06:04.291487 20038 slave.cpp:1265] Already registered with master master@172.17.0.2:35090_x000D_
3: I1123 08:06:04.291539 20038 slave.cpp:1288] Forwarding total resources cpus:2; mem:1024; disk:1024; ports:[31000-32000]_x000D_
3: I1123 08:06:04.291553 20037 master.cpp:7078] Received update of agent 7c9d8e8c-3fb3-44c5-8505-488ada3e848e-S0 at slave(784)@172.17.0.2:35090 (dce3e4c418cb) with total resources cpus:2; mem:1024; disk:1024; ports:[31000-32000]_x000D_
3: I1123 08:06:04.291610 20037 master.cpp:7091] Received update of agent 7c9d8e8c-3fb3-44c5-8505-488ada3e848e-S0 at slave(784)@172.17.0.2:35090 (dce3e4c418cb) with total oversubscribed resources {}_x000D_
3: I1123 08:06:04.291649 20038 slave.cpp:1298] Forwarding total oversubscribed resources {}_x000D_
3: I1123 08:06:04.291822 20037 master.cpp:7109] Ignoring update on agent 7c9d8e8c-3fb3-44c5-8505-488ada3e848e-S0 at slave(784)@172.17.0.2:35090 (dce3e4c418cb) as it reports no changes_x000D_
3: I1123 08:06:04.292245 20037 master.cpp:7078] Received update of agent 7c9d8e8c-3fb3-44c5-8505-488ada3e848e-S0 at slave(784)@172.17.0.2:35090 (dce3e4c418cb) with total resources cpus:2; mem:1024; disk:1024; ports:[31000-32000]_x000D_
3: I1123 08:06:04.292304 20037 master.cpp:7091] Received update of agent 7c9d8e8c-3fb3-44c5-8505-488ada3e848e-S0 at slave(784)@172.17.0.2:35090 (dce3e4c418cb) with total oversubscribed resources {}_x000D_
3: I1123 08:06:04.292381 20044 http_connection.hpp:221] New endpoint detected at http://172.17.0.2:35090/slave(784)/api/v1/resource_provider_x000D_
3: I1123 08:06:04.292474 20037 master.cpp:7109] Ignoring update on agent 7c9d8e8c-3fb3-44c5-8505-488ada3e848e-S0 at slave(784)@172.17.0.2:35090 (dce3e4c418cb) as it reports no changes_x000D_
3: I1123 08:06:04.292973 20036 scheduler.cpp:188] Version: 1.5.0_x000D_
3: I1123 08:06:04.293253 20055 scheduler.cpp:311] Using default 'basic' HTTP authenticatee_x000D_
3: I1123 08:06:04.293665 20049 scheduler.cpp:494] New master detected at master@172.17.0.2:35090_x000D_
3: I1123 08:06:04.293694 20049 scheduler.cpp:503] Waiting for 0ns before initiating a re-(connection) attempt with the master_x000D_
3: I1123 08:06:04.294288 20045 http_connection.hpp:277] Connected with the remote endpoint at http://172.17.0.2:35090/slave(784)/api/v1/resource_provider_x000D_
3: I1123 08:06:04.294934 20046 http_connection.hpp:129] Sending 1 call to http://172.17.0.2:35090/slave(784)/api/v1/resource_provider_x000D_
3: I1123 08:06:04.295969 20055 scheduler.cpp:385] Connected with the master at http://172.17.0.2:35090/master/api/v1/scheduler_x000D_
3: I1123 08:06:04.296830 20059 process.cpp:3503] Handling HTTP event for process 'slave(784)' with path: '/slave(784)/api/v1/resource_provider'_x000D_
3: I1123 08:06:04.297196 20054 scheduler.cpp:247] Sending SUBSCRIBE call to http://172.17.0.2:35090/master/api/v1/scheduler_x000D_
3: I1123 08:06:04.298148 20052 http.cpp:1185] HTTP POST for /slave(784)/api/v1/resource_provider from 172.17.0.2:38204_x000D_
3: I1123 08:06:04.298343 20052 process.cpp:3503] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler'_x000D_
3: I1123 08:06:04.298552 20056 manager.cpp:386] Subscribing resource provider {""name"":""test"",""type"":""org.apache.mesos.rp.test""}_x000D_
3: I1123 08:06:04.299580 20057 http.cpp:1185] HTTP POST for /master/api/v1/scheduler from 172.17.0.2:38208_x000D_
3: I1123 08:06:04.299873 20057 master.cpp:2615] Received subscription request for HTTP framework 'default'_x000D_
3: I1123 08:06:04.300007 20057 master.cpp:2280] Authorizing framework principal 'test-principal' to receive offers for roles '{ role }'_x000D_
3: I1123 08:06:04.300477 20050 master.cpp:2750] Subscribing framework 'default' with checkpointing disabled and capabilities [ RESERVATION_REFINEMENT ]_x000D_
3: I1123 08:06:04.300882 20056 http_connection.hpp:129] Sending 3 call to http://172.17.0.2:35090/slave(784)/api/v1/resource_provider_x000D_
3: I1123 08:06:04.301465 20047 hierarchical.cpp:306] Added framework 7c9d8e8c-3fb3-44c5-8505-488ada3e848e-0000_x000D_
3: I1123 08:06:04.301954 20057 scheduler.cpp:739] Enqueuing event SUBSCRIBED received from http://172.17.0.2:35090/master/api/v1/scheduler_x000D_
3: I1123 08:06:04.302021 20052 process.cpp:3503] Handling HTTP event for process 'slave(784)' with path: '/slave(784)/api/v1/resource_provider'_x000D_
3: I1123 08:06:04.302435 20057 scheduler.cpp:739] Enqueuing event HEARTBEAT received from http://172.17.0.2:35090/master/api/v1/scheduler_x000D_
3: I1123 08:06:04.303020 20055 http.cpp:1185] HTTP POST for /slave(784)/api/v1/resource_provider from 172.17.0.2:38202_x000D_
3: I1123 08:06:04.303211 20047 hierarchical.cpp:1457] Performed allocation for 1 agents in 1.549319ms_x000D_
3: I1123 08:06:04.303767 20060 master.cpp:8142] Sending 1 offers to framework 7c9d8e8c-3fb3-44c5-8505-488ada3e848e-0000 (default)_x000D_
3: I1123 08:06:04.303828 20050 slave.cpp:6780] Handling resource provider message 'UPDATE_TOTAL_RESOURCES: 582a6138-5ac1-4c38-b407-576be39d0a82 disk[RAW]:200'_x000D_
3: I1123 08:06:04.303941 20050 slave.cpp:6825] Forwarding new total resources cpus:2; mem:1024; disk:1024; ports:[31000-32000]; disk[RAW]:200_x000D_
3: I1123 08:06:04.304744 20060 master.cpp:7078] Received update of agent 7c9d8e8c-3fb3-44c5-8505-488ada3e848e-S0 at slave(784)@172.17.0.2:35090 (dce3e4c418cb) with total resources cpus:2; mem:1024; disk:1024; ports:[31000-32000]; disk[RAW]:200_x000D_
3: I1123 08:06:04.305366 20051 scheduler.cpp:739] Enqueuing event OFFERS received from http://172.17.0.2:35090/master/api/v1/scheduler_x000D_
3: I1123 08:06:04.305364 20060 master.cpp:7141] Removing offer 7c9d8e8c-3fb3-44c5-8505-488ada3e848e-O0 with resources cpus(allocated: role):2; mem(allocated: role):1024; disk(allocated: role):1024; ports(allocated: role):[31000-32000] on agent 7c9d8e8c-3fb3-44c5-8505-488ada3e848e-S0 at slave(784)@172.17.0.2:35090 (dce3e4c418cb)_x000D_
3: I1123 08:06:04.306197 20060 master.cpp:10063] Removing offer 7c9d8e8c-3fb3-44c5-8505-488ada3e848e-O0_x000D_
3: I1123 08:06:04.306350 20058 hierarchical.cpp:667] Agent 7c9d8e8c-3fb3-44c5-8505-488ada3e848e-S0 (dce3e4c418cb) updated with total resources cpus:2; mem:1024; disk:1024; ports:[31000-32000]; disk[RAW]:200_x000D_
3: /mesos/src/tests/resource_provider_manager_tests.cpp:713: Failure_x000D_
3: Value of: resources.empty()_x000D_
3:   Actual: true_x000D_
3: Expected: false_x000D_
3: I1123 08:06:04.307636 20058 hierarchical.cpp:1132] Recovered cpus(allocated: role):2; mem(allocated: role):1024; disk(allocated: role):1024; ports(allocated: role):[31000-32000] (total: cpus:2; mem:1024; disk:1024; ports:[31000-32000]; disk[RAW]:200, allocated: {}) on agent 7c9d8e8c-3fb3-44c5-8505-488ada3e848e-S0 from framework 7c9d8e8c-3fb3-44c5-8505-488ada3e848e-0000_x000D_
3: I1123 08:06:04.308148 20051 master.cpp:1425] Framework 7c9d8e8c-3fb3-44c5-8505-488ada3e848e-0000 (default) disconnected_x000D_
3: I1123 08:06:04.308176 20051 master.cpp:3333] Deactivating framework 7c9d8e8c-3fb3-44c5-8505-488ada3e848e-0000 (default)_x000D_
3: I1123 08:06:04.308253 20051 master.cpp:3310] Disconnecting framework 7c9d8e8c-3fb3-44c5-8505-488ada3e848e-0000 (default)_x000D_
3: I1123 08:06:04.308284 20051 master.cpp:1440] Giving framework 7c9d8e8c-3fb3-44c5-8505-488ada3e848e-0000 (default) 0ns to failover_x000D_
3: I1123 08:06:04.308549 20055 master.cpp:7974] Framework failover timeout, removing framework 7c9d8e8c-3fb3-44c5-8505-488ada3e848e-0000 (default)_x000D_
3: I1123 08:06:04.308575 20055 master.cpp:8831] Removing framework 7c9d8e8c-3fb3-44c5-8505-488ada3e848e-0000 (default)_x000D_
3: I1123 08:06:04.308782 20038 slave.cpp:3270] Asked to shut down framework 7c9d8e8c-3fb3-44c5-8505-488ada3e848e-0000 by master@172.17.0.2:35090_x000D_
3: I1123 08:06:04.308811 20038 slave.cpp:3285] Cannot shut down unknown framework 7c9d8e8c-3fb3-44c5-8505-488ada3e848e-0000_x000D_
3: W1123 08:06:04.309376 20037 master.cpp:7988] Master returning resources offered to framework 7c9d8e8c-3fb3-44c5-8505-488ada3e848e-0000 because the framework has terminated or is inactive_x000D_
3: I1123 08:06:04.309382 20058 hierarchical.cpp:1457] Performed allocation for 1 agents in 1.597792ms_x000D_
3: I1123 08:06:04.309464 20058 hierarchical.cpp:419] Deactivated framework 7c9d8e8c-3fb3-44c5-8505-488ada3e848e-0000_x000D_
3: I1123 08:06:04.310418 20058 hierarchical.cpp:358] Removed framework 7c9d8e8c-3fb3-44c5-8505-488ada3e848e-0000_x000D_
3: I1123 08:06:04.310714 20058 hierarchical.cpp:1132] Recovered cpus(allocated: role):2; mem(allocated: role):1024; disk(allocated: role):1024; ports(allocated: role):[31000-32000]; disk(allocated: role)[RAW]:200 (total: cpus:2; mem:1024; disk:1024; ports:[31000-32000]; disk[RAW]:200, allocated: {}) on agent 7c9d8e8c-3fb3-44c5-8505-488ada3e848e-S0 from framework 7c9d8e8c-3fb3-44c5-8505-488ada3e848e-0000_x000D_
3: I1123 08:06:04.311563 20054 slave.cpp:883] Agent terminating_x000D_
3: I1123 08:06:04.311776 20038 master.cpp:1311] Agent 7c9d8e8c-3fb3-44c5-8505-488ada3e848e-S0 at slave(784)@172.17.0.2:35090 (dce3e4c418cb) disconnected_x000D_
3: I1123 08:06:04.311800 20038 master.cpp:3370] Disconnecting agent 7c9d8e8c-3fb3-44c5-8505-488ada3e848e-S0 at slave(784)@172.17.0.2:35090 (dce3e4c418cb)_x000D_
3: I1123 08:06:04.311849 20038 master.cpp:3389] Deactivating agent 7c9d8e8c-3fb3-44c5-8505-488ada3e848e-S0 at slave(784)@172.17.0.2:35090 (dce3e4c418cb)_x000D_
3: I1123 08:06:04.311949 20058 hierarchical.cpp:697] Agent 7c9d8e8c-3fb3-44c5-8505-488ada3e848e-S0 deactivated_x000D_
3: I1123 08:06:04.318100 20036 master.cpp:1153] Master terminating_x000D_
3: I1123 08:06:04.318877 20047 hierarchical.cpp:633] Removed agent 7c9d8e8c-3fb3-44c5-8505-488ada3e848e-S0_x000D_
3: [  FAILED  ] ContentType/ResourceProviderManagerHttpApiTest.ConvertResources/1, where_x000D_
{noformat}_x000D_
",2.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.2631578947368421,0.36428571428571427,0.28205128205128205,0.296969696969697,0.296969696969697,0.0
Task,"Introduce a way to resolve the ""profile"" for disk resources","This builds off MESOS-8060 which introduced the {{profile}} field to disk resources._x000D_
_x000D_
A volume profile will consist of a {{string}} which maps to a list of {{VolumeCapability}} and some free-form parameters (string key-value pairs).  See the {{message CreateVolumeResponse}} under https://github.com/container-storage-interface/spec/blob/master/csi.proto for more information about the fields._x000D_
_x000D_
We will introduce a module which will allow the operator to specify this mapping.  Unfortunately, due to the nature of volume profiles, we cannot provide a reasonable ""default"" mapping of profiles (because we don't have any idea what volumes are available).  Instead, there will be two implementations of the module:_x000D_
1) The default will essentially turn volume profiles off.  Any attempt to use volume profiles will simply fail._x000D_
2) An optional module will allow the operator to specify a URI (file or link) that contains a JSON representation of the mapping.  The module will have some knobs to tune the frequency at which this URI is fetched from.",3.0,0,0.0,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.0
Bug,SlaveRecoveryTest/0.ReconnectExecutor is flaky.,Observed it today in our CI. Logs attached.,3.0,0,0.5,0.0,0.3333333333333333,0.5,0.14285714285714285,0.0,0.2,0.5087719298245614,0.4357142857142857,0.717948717948718,0.8242424242424242,0.7212121212121212,0.0
Task,mesos.interface 1.4.0 cannot be installed with pip,"This breaks some framework development tooling._x000D_
_x000D_
WIth latest pip:_x000D_
{noformat}_x000D_
$ python -m pip -V_x000D_
pip 9.0.1 from /Users/wfarner/code/aurora/build-support/python/pycharm.venv/lib/python2.7/site-packages (python 2.7)_x000D_
{noformat}_x000D_
_x000D_
This works fine for previous releases:_x000D_
{noformat}_x000D_
$ python -m pip install mesos.interface==1.3.0_x000D_
Collecting mesos.interface==1.3.0_x000D_
..._x000D_
Installing collected packages: mesos.interface_x000D_
Successfully installed mesos.interface-1.3.0_x000D_
{noformat}_x000D_
_x000D_
But it does not for 1.4.0:_x000D_
{noformat}_x000D_
$ python -m pip install mesos.interface==1.4.0_x000D_
Collecting mesos.interface==1.4.0_x000D_
  Could not find a version that satisfies the requirement mesos.interface==1.4.0 (from versions: 0.21.2.linux-x86_64, 0.22.1.2.linux-x86_64, 0.22.2.linux-x86_64, 0.23.1.linux-x86_64, 0.24.1.linux-x86_64, 0.24.2.linux-x86_64, 0.25.0.linux-x86_64, 0.25.1.linux-x86_64, 0.26.1.linux-x86_64, 0.27.0.linux-x86_64, 0.27.1.linux-x86_64, 0.27.2.linux-x86_64, 0.28.0.linux-x86_64, 0.28.1.linux-x86_64, 0.28.2.linux-x86_64, 1.0.0.linux-x86_64, 1.0.1.linux-x86_64, 1.1.0.linux-x86_64, 1.2.0.linux-x86_64, 1.3.0.linux-x86_64, 0.20.0, 0.20.1, 0.21.0, 0.21.1, 0.21.2, 0.22.0, 0.22.1.2, 0.22.2, 0.23.0, 0.23.1, 0.24.0, 0.24.1, 0.24.2, 0.25.0, 0.25.1, 0.26.0, 0.26.1, 0.27.0, 0.27.1, 0.27.2, 0.28.0, 0.28.1, 0.28.2, 1.0.0, 1.0.1, 1.1.0, 1.2.0, 1.3.0)_x000D_
No matching distribution found for mesos.interface==1.4.0_x000D_
{noformat}_x000D_
_x000D_
Verbose output shows that pip skips the 1.4.0 distribution:_x000D_
{noformat}_x000D_
$ python -m pip install -v mesos.interface==1.4.0 | grep 1.4.0_x000D_
Collecting mesos.interface==1.4.0_x000D_
    Skipping link https://pypi.python.org/packages/ef/1b/d5b0c1456f755ad42477eaa9667e22d1f5fd8e2fce0f9b26937f93743f6c/mesos.interface-1.4.0-py2.7.egg#md5=32113860961d49c31f69f7b13a9bc063 (from https://pypi.python.org/simple/mesos-interface/); unsupported archive format: .egg_x000D_
{noformat}",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01282051282051282,0.024242424242424242,0.024242424242424242,0.0
Task,Validate that any offer operation is only applied on resources from a single provider,Offer operations can only be applied to resources from one single resource provider. A number of places in the implementation assume that the provider ID obtained from any {Resource} in an offer operation is equivalent to the one from any other resource. We should update the master to validate that invariant and reject malformed operations.,1.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.2,0.2631578947368421,0.36428571428571427,1.0,0.8121212121212121,0.8121212121212121,0.0
Improvement,Add the pip-requirements from other modules to the pylint virtual environment,"We have a virtual environment in {{/support}} that is used by Pylint and ESlint. Pylint will then lint modules that need a different set of dependencies to work. If we do not pull the pip-requirements from other modules to the linter virtual environment, we will see {{import-error}}s when linting Python files._x000D_
_x000D_
One solution would be to use the {{--init-hook}} flag of Pylint in {{PyLinter}}. When using {{run_lint}}, we would group the modified files per module and then run pylint for all the modified files of each module.",3.0,0,0.0,0.25037707390648567,0.3333333333333333,0.5,0.14285714285714285,0.5,0.0,0.2631578947368421,0.29285714285714287,0.10256410256410256,0.1515151515151515,0.1515151515151515,0.0
Task,Update the ReconcileOfferOperations protos,"Some protos have been committed, but they follow an event-based API._x000D_
_x000D_
We decided to follow the request/response model for this API, so we need to update the protos.",1.0,0,0.5,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.28205128205128205,0.2606060606060606,0.2606060606060606,0.0
Task,"Implement explicit offer operation reconciliation between the master, agent and RPs.","Upon receiving an {{UpdateSlave}} message the master should compare its list of pending operations for the agent/LRPs to the list of pending operations contained in the message. It should then build a {{ReconcileOfferOperations}} message with all the operations missing in the {{UpdateSlave}} message and send it to the agent._x000D_
_x000D_
The agent will receive these messages and should handle them by itself if the operations affect the default resources, or forward them to the RP manager otherwise._x000D_
_x000D_
The agent/RP handler should check if the operations are pending. If an operation is not pending, then an {{ApplyOfferOperation}} message got dropped, and the agent/LRP should send an {{OFFER_OPERATION_DROPPED}} status update to the master.",3.0,0,0.5,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.28205128205128205,0.2606060606060606,0.2606060606060606,0.0
Task,Agents should handle acks for operations affecting default resources.,The agent's acknowledgement handler should be updated to be able to handle acks for updates related to operations affecting agent default resources.,3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.28205128205128205,0.2606060606060606,0.2606060606060606,0.0
Task,Update the master to accept OfferOperationIDs from frameworks.,Master’s {{ACCEPT}} handler should send failed operation updates when a framework sets the {{OfferOperationID}} on an operation destined for an agent without the {{RESOURCE_PROVIDER}} capability.,3.0,0,0.5,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.28205128205128205,0.2606060606060606,0.2606060606060606,0.0
Task,Implement the agent's AcknowledgeOfferOperationMessage handler.,"The handler should forward acks to the RP manager for resource provider operations._x000D_
Handling of operations on agent default resources will be taken care of as part of MESOS-8194",3.0,0,0.5,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.28205128205128205,0.2606060606060606,0.2606060606060606,0.0
Bug,Scheduler library has incorrect assumptions about connections.,"Scheduler library assumes that a connection cannot be interrupted between continuations, for example {{send()}} and {{_send()}}: [https://github.com/apache/mesos/blob/509a1ab3226bbec7c369f431656f4ec692da00ba/src/scheduler/scheduler.cpp#L553]. This is not true, {{detected()}} can fire in-between, leading to disconnection:_x000D_
{noformat}_x000D_
I1107 18:50:57.154796 2138112 scheduler.cpp:496] New master detected at master@192.168.9.40:59063_x000D_
..._x000D_
I1107 18:50:57.160935 2138112 scheduler.cpp:505] Waiting for 0ns before initiating a re-(connection) attempt with the master_x000D_
I1107 18:50:57.161245 1064960 clock.cpp:435] Clock of __collect__(7)@192.168.9.40:59063 updated to 2017-11-07 17:50:57.159954176+00:00_x000D_
I1107 18:50:57.161285 1898086400 clock.cpp:361] Clock resumed at 2017-11-07 17:50:57.159954176+00:00_x000D_
I1107 18:50:57.161602 1064960 scheduler.cpp:387] Connected with the master at http://192.168.9.40:59063/master/api/v1/scheduler_x000D_
I1107 18:50:57.161779 2138112 scheduler.cpp:249] Sending SUBSCRIBE call to http://192.168.9.40:59063/master/api/v1/scheduler_x000D_
I1107 18:50:57.162037 2138112 scheduler.cpp:496] New master detected at master@192.168.9.40:59063_x000D_
I1107 18:50:57.162055 2138112 scheduler.cpp:505] Waiting for 0ns before initiating a re-(connection) attempt with the master_x000D_
I1107 18:50:57.162164 4820992 process.cpp:3167] Dropping event for process __http_connection__(14)@192.168.9.40:59063_x000D_
F1107 18:50:57.162214 2138112 scheduler.cpp:553] CHECK_SOME(connections): is NONE _x000D_
*** Check failure stack trace: ***_x000D_
E1107 18:50:57.162240 4820992 process.cpp:2576] Failed to shutdown socket with fd 9, address 192.168.9.40:59063: Socket is not connected_x000D_
    @        0x10ed262b4  google::LogMessage::Flush()_x000D_
    @        0x10ed2a21f  google::LogMessageFatal::~LogMessageFatal()_x000D_
    @        0x10ed26ef9  google::LogMessageFatal::~LogMessageFatal()_x000D_
E1107 18:50:57.162304 4820992 process.cpp:2576] Failed to shutdown socket with fd 10, address 192.168.9.40:59063: Socket is not connected_x000D_
    @        0x1078efaea  _CheckFatal::~_CheckFatal()_x000D_
    @        0x1078ea675  _CheckFatal::~_CheckFatal()_x000D_
    @        0x109dfcabf  mesos::v1::scheduler::MesosProcess::_send()_x000D_
    @        0x109e07438  _ZZN7process8dispatchIN5mesos2v19scheduler12MesosProcessERKNS3_4CallERKNS_6FutureINS_4http7RequestEEES7_SD_EEvRKNS_3PIDIT_EEMSF_FvT0_T1_EOT2_OT3_ENKUlRS5_RSB_PNS_11ProcessBaseEE_clESR_SS_SU__x000D_
    @        0x109e072b7  _ZNSt3__128__invoke_void_return_wrapperIvE6__callIJRNS_6__bindIZN7process8dispatchIN5mesos2v19scheduler12MesosProcessERKNS8_4CallERKNS4_6FutureINS4_4http7RequestEEESC_SI_EEvRKNS4_3PIDIT_EEMSK_FvT0_T1_EOT2_OT3_EUlRSA_RSG_PNS4_11ProcessBaseEE_JSC_SI_RNS_12placeholders4__phILi1EEEEEESZ_EEEvDpOT__x000D_
    @        0x109e06ba9  _ZNSt3__110__function6__funcINS_6__bindIZN7process8dispatchIN5mesos2v19scheduler12MesosProcessERKNS7_4CallERKNS3_6FutureINS3_4http7RequestEEESB_SH_EEvRKNS3_3PIDIT_EEMSJ_FvT0_T1_EOT2_OT3_EUlRS9_RSF_PNS3_11ProcessBaseEE_JSB_SH_RNS_12placeholders4__phILi1EEEEEENS_9allocatorIS14_EEFvSY_EEclEOSY__x000D_
    @        0x10de77d3a  std::__1::function<>::operator()()_x000D_
    @        0x10e307abc  process::ProcessBase::visit()_x000D_
    @        0x10e3b804e  process::DispatchEvent::visit()_x000D_
    @        0x107a4b991  process::ProcessBase::serve()_x000D_
    @        0x10e300191  process::ProcessManager::resume()_x000D_
    @        0x10e42d27d  process::ProcessManager::init_threads()::$_2::operator()()_x000D_
    @        0x10e42ce12  _ZNSt3__114__thread_proxyINS_5tupleIJZN7process14ProcessManager12init_threadsEvE3$_2EEEEEPvS6__x000D_
    @     0x7fff8591499d  _pthread_body_x000D_
    @     0x7fff8591491a  _pthread_start_x000D_
    @     0x7fff85912351  thread_start_x000D_
zsh: abort      GLOG_v=2 GTEST_FILTER=""*SchedulerTest.MasterFailover*"" ./bin/mesos-tests.sh  _x000D_
{noformat}_x000D_
_x000D_
The bug has been introduced in https://reviews.apache.org/r/62594",1.0,1.4.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.717948717948718,0.8242424242424242,0.7212121212121212,0.9541284403669724
Bug,Using a failoverTimeout of 0 with Mesos native scheduler client can result in infinite subscribe loop,"Over the past year, the Marathon team has been plagued with an issue that hits our CI builds periodically in which the scheduler driver enters a tight loop, sending 10,000s of SUBSCRIBE calls to the master per second. I turned on debug logging for the client and the server, and it pointed to an issue with the {{doReliableRegistration}} method in sched.cpp. Here's the logs:_x000D_
_x000D_
{code}_x000D_
WARN [05:39:39 EventsIntegrationTest-LocalMarathon-32858] I1104 05:39:39.099815 13397 process.cpp:1383] libprocess is initialized on 127.0.1.1:60957 with 8 worker threads_x000D_
WARN [05:39:39 EventsIntegrationTest-LocalMarathon-32858] I1104 05:39:39.118237 13397 logging.cpp:199] Logging to STDERR_x000D_
WARN [05:39:39 EventsIntegrationTest-LocalMarathon-32858] I1104 05:39:39.128921 13416 sched.cpp:232] Version: 1.4.0_x000D_
WARN [05:39:39 EventsIntegrationTest-LocalMarathon-32858] I1104 05:39:39.151785 13791 group.cpp:341] Group process (zookeeper-group(1)@127.0.1.1:60957) connected to ZooKeeper_x000D_
WARN [05:39:39 EventsIntegrationTest-LocalMarathon-32858] I1104 05:39:39.151823 13791 group.cpp:831] Syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0)_x000D_
WARN [05:39:39 EventsIntegrationTest-LocalMarathon-32858] I1104 05:39:39.151837 13791 group.cpp:419] Trying to create path '/mesos' in ZooKeeper_x000D_
WARN [05:39:39 EventsIntegrationTest-LocalMarathon-32858] I1104 05:39:39.152586 13791 group.cpp:758] Found non-sequence node 'log_replicas' at '/mesos' in ZooKeeper_x000D_
WARN [05:39:39 EventsIntegrationTest-LocalMarathon-32858] I1104 05:39:39.152662 13791 detector.cpp:152] Detected a new leader: (id='0')_x000D_
WARN [05:39:39 EventsIntegrationTest-LocalMarathon-32858] I1104 05:39:39.152762 13791 group.cpp:700] Trying to get '/mesos/json.info_0000000000' in ZooKeeper_x000D_
WARN [05:39:39 EventsIntegrationTest-LocalMarathon-32858] I1104 05:39:39.157148 13791 zookeeper.cpp:262] A new leading master (UPID=master@172.16.10.95:32856) is detected_x000D_
WARN [05:39:39 EventsIntegrationTest-LocalMarathon-32858] I1104 05:39:39.157347 13787 sched.cpp:336] New master detected at master@172.16.10.95:32856_x000D_
WARN [05:39:39 EventsIntegrationTest-LocalMarathon-32858] I1104 05:39:39.157557 13787 sched.cpp:352] No credentials provided. Attempting to register without authentication_x000D_
WARN [05:39:39 EventsIntegrationTest-LocalMarathon-32858] I1104 05:39:39.157565 13787 sched.cpp:836] Sending SUBSCRIBE call to master@172.16.10.95:32856_x000D_
WARN [05:39:39 EventsIntegrationTest-LocalMarathon-32858] I1104 05:39:39.157635 13787 sched.cpp:869] Will retry registration in 0ns if necessary_x000D_
WARN [05:39:39 EventsIntegrationTest-LocalMarathon-32858] I1104 05:39:39.158979 13785 sched.cpp:836] Sending SUBSCRIBE call to master@172.16.10.95:32856_x000D_
WARN [05:39:39 EventsIntegrationTest-LocalMarathon-32858] I1104 05:39:39.159029 13785 sched.cpp:869] Will retry registration in 0ns if necessary_x000D_
WARN [05:39:39 EventsIntegrationTest-LocalMarathon-32858] I1104 05:39:39.159265 13790 sched.cpp:836] Sending SUBSCRIBE call to master@172.16.10.95:32856_x000D_
WARN [05:39:39 EventsIntegrationTest-LocalMarathon-32858] I1104 05:39:39.159303 13790 sched.cpp:869] Will retry registration in 0ns if necessary_x000D_
WARN [05:39:39 EventsIntegrationTest-LocalMarathon-32858] I1104 05:39:39.159479 13786 sched.cpp:836] Sending SUBSCRIBE call to master@172.16.10.95:32856_x000D_
WARN [05:39:39 EventsIntegrationTest-LocalMarathon-32858] I1104 05:39:39.159521 13786 sched.cpp:869] Will retry registration in 0ns if necessary_x000D_
WARN [05:39:39 EventsIntegrationTest-LocalMarathon-32858] I1104 05:39:39.159622 13788 sched.cpp:836] Sending SUBSCRIBE call to master@172.16.10.95:32856_x000D_
WARN [05:39:39 EventsIntegrationTest-LocalMarathon-32858] I1104 05:39:39.159658 13788 sched.cpp:869] Will retry registration in 0ns if necessary_x000D_
WARN [05:39:39 EventsIntegrationTest-LocalMarathon-32858] I1104 05:39:39.159749 13789 sched.cpp:836] Sending SUBSCRIBE call to master@172.16.10.95:32856_x000D_
WARN [05:39:39 EventsIntegrationTest-LocalMarathon-32858] I1104 05:39:39.159785 13789 sched.cpp:869] Will retry registration in 0ns if necessary_x000D_
WARN [05:39:39 EventsIntegrationTest-LocalMarathon-32858] I1104 05:39:39.159878 13792 sched.cpp:836] Sending SUBSCRIBE call to master@172.16.10.95:32856_x000D_
WARN [05:39:39 EventsIntegrationTest-LocalMarathon-32858] I1104 05:39:39.159916 13792 sched.cpp:869] Will retry registration in 0ns if necessary_x000D_
{code}_x000D_
_x000D_
In Marathon, when we are running our tests, we set the failoverTimeout to 0 in order to cause the Mesos master to immediately forget about a framework when it disconnects._x000D_
_x000D_
On line 860 of sched.cpp, the retry-delay is set to 1/10th the failoverTimeout, which provides the best explanation for why the value is 0:_x000D_
_x000D_
{code}_x000D_
./mesos/src/sched/sched.cpp_x000D_
_x000D_
 818 |   void doReliableRegistration(Duration maxBackoff)_x000D_
 819 |   {_x000D_
..._x000D_
 851 |     // Bound the maximum backoff by 'REGISTRATION_RETRY_INTERVAL_MAX'._x000D_
 852 |     maxBackoff =_x000D_
 853 |       std::min(maxBackoff, scheduler::REGISTRATION_RETRY_INTERVAL_MAX);_x000D_
 854 | _x000D_
 855 |     // If failover timeout is present, bound the maximum backoff_x000D_
 856 |     // by 1/10th of the failover timeout._x000D_
 857 |     if (framework.has_failover_timeout()) {_x000D_
 858 |       Try<Duration> duration = Duration::create(framework.failover_timeout());_x000D_
 859 |       if (duration.isSome()) {_x000D_
 860 |         maxBackoff = std::min(maxBackoff, duration.get() / 10);_x000D_
 861 |       }_x000D_
 862 |     }_x000D_
 863 | _x000D_
 864 |     // Determine the delay for next attempt by picking a random_x000D_
 865 |     // duration between 0 and 'maxBackoff'._x000D_
 866 |     // TODO(vinod): Use random numbers from <random> header._x000D_
 867 |     Duration delay = maxBackoff * ((double) os::random() / RAND_MAX);_x000D_
 868 | _x000D_
 869 |     VLOG(1) << ""Will retry registration in "" << delay << "" if necessary"";_x000D_
 870 | _x000D_
 871 |     // Backoff._x000D_
 872 |     frameworkRegistrationTimer = process::delay(_x000D_
 873 |         delay, self(), &Self::doReliableRegistration, maxBackoff * 2);_x000D_
 874 |   }_x000D_
 875 | _x000D_
{code}_x000D_
_x000D_
Reading through the code, it seems that once this value is 0, it will always be zero, since backoff is multiplicative (0 * 2 == 0), and the failover_timeout / 10 limit is applied each time._x000D_
_x000D_
To make matters worse, failoverTimeout of {{0}} is the default:_x000D_
_x000D_
{code}_x000D_
./mesos/include/mesos/mesos.proto_x000D_
_x000D_
 238 |   // The amount of time (in seconds) that the master will wait for the_x000D_
 239 |   // scheduler to failover before it tears down the framework by_x000D_
 240 |   // killing all its tasks/executors. This should be non-zero if a_x000D_
 241 |   // framework expects to reconnect after a failure and not lose its_x000D_
 242 |   // tasks/executors._x000D_
 243 |   //_x000D_
 244 |   // NOTE: To avoid accidental destruction of tasks, production_x000D_
 245 |   // frameworks typically set this to a large value (e.g., 1 week)._x000D_
 246 |   optional double failover_timeout = 4 [default = 0.0];_x000D_
 247 | _x000D_
{code}_x000D_
_x000D_
I've confirmed that when using 1.4.0 of the Mesos client java jar, this default is used if failoverTimeout is not set:_x000D_
_x000D_
{code}_x000D_
@ import $ivy.`org.apache.mesos:mesos:1.4.0`_x000D_
import $ivy.$_x000D_
_x000D_
@ import org.apache.mesos.Protos_x000D_
import org.apache.mesos.Protos_x000D_
_x000D_
@ Protos.FrameworkInfo.newBuilder.setName(""test"").setUser(""user"").build.getFailoverTimeout_x000D_
res3: Double = 0.0_x000D_
{code}",2.0,"1.1.3,1.2.2,1.3.1,1.4.0",0.5,0.0,0.6666666666666666,1.0,0.5714285714285714,0.0,0.0,0.5438596491228069,0.5357142857142857,0.02564102564102564,0.01212121212121212,0.01212121212121212,0.9405045871559633
Task,Design a library to send offer operation status updates,"As detailed in the [offer operation feedback design doc|https://docs.google.com/document/d/1GGh14SbPTItjiweSZfann4GZ6PCteNrn-1y4pxOjgcI/edit#], we need to add a library to do the following:_x000D_
* Send offer operation status updates_x000D_
* Checkpoint pending/unacknowledged operations_x000D_
* Retry operation status updates until an acknowledgement is received_x000D_
_x000D_
This should be a common library which can be used by the agent (for its default resources) and by local resource providers. In the future, it can also be used by external resource providers._x000D_
_x000D_
We should write a short design doc to explore precisely how this will be implemented. It can probably be modeled after the task status update manager in the agent.",3.0,0,0.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Task,Add new protobuf messages for offer operation feedback,We should add the necessary protobuf messages for offer operation feedback as detailed in the [offer operation feedback design doc|https://docs.google.com/document/d/1GGh14SbPTItjiweSZfann4GZ6PCteNrn-1y4pxOjgcI/edit#].,2.0,0,0.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Task,Add placeholder handlers for offer operation feedback,"In order to sketch out the flow of messages necessary to facilitate offer operation feedback, we should add some empty placeholder handlers to the master and agent as detailed in the [offer operation feedback design doc|https://docs.google.com/document/d/1GGh14SbPTItjiweSZfann4GZ6PCteNrn-1y4pxOjgcI/edit#].",2.0,0,0.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Bug,Make os::pipe file descriptors O_CLOEXEC.,File descriptors from {{os::pipe}} will be inherited across exec. On Linux we can use [pipe2|http://man7.org/linux/man-pages/man2/pipe.2.html] to atomically make the pipe {{O_CLOEXEC}}.,3.0,0,0.0,0.5444947209653093,0.0,0.0,0.0,0.5,1.0,0.05263157894736842,0.03571428571428571,0.11538461538461538,0.05454545454545455,0.05454545454545455,0.0
Bug,GPU tests are failing due to TASK_STARTING.,"For instance: NvidiaGpuTest.ROOT_CGROUPS_NVIDIA_GPU_VerifyDeviceAccess_x000D_
_x000D_
{noformat}_x000D_
I1020 22:18:46.180371  1480 exec.cpp:237] Executor registered on agent ca0e7b44-c621-4442-a62e-15f7bf02064b-S0_x000D_
I1020 22:18:46.185027  1486 executor.cpp:171] Received SUBSCRIBED event_x000D_
I1020 22:18:46.186005  1486 executor.cpp:175] Subscribed executor on core-dev_x000D_
I1020 22:18:46.186189  1486 executor.cpp:171] Received LAUNCH event_x000D_
I1020 22:18:46.188908  1486 executor.cpp:637] Starting task 3c08cf78-575d-4813-82b6-3ace272db35e_x000D_
I1020 22:18:46.192939  1316 slave.cpp:4407] Handling status update TASK_STARTING (UUID: 87cee290-b2fe-4459-9b75-b9f03aab6492) for task 3c08cf78-575d-4813-82b6-3ace272db35e of fra_x000D_
mework ca0e7b44-c621-4442-a62e-15f7bf02064b-0000 from executor(1)@10.0.49.2:42711_x000D_
I1020 22:18:46.196228  1330 status_update_manager.cpp:323] Received status update TASK_STARTING (UUID: 87cee290-b2fe-4459-9b75-b9f03aab6492) for task 3c08cf78-575d-4813-82b6-3ace_x000D_
272db35e of framework ca0e7b44-c621-4442-a62e-15f7bf02064b-0000_x000D_
I1020 22:18:46.197510  1329 slave.cpp:4888] Forwarding the update TASK_STARTING (UUID: 87cee290-b2fe-4459-9b75-b9f03aab6492) for task 3c08cf78-575d-4813-82b6-3ace272db35e of fram_x000D_
ework ca0e7b44-c621-4442-a62e-15f7bf02064b-0000 to master@10.0.49.2:34819_x000D_
I1020 22:18:46.197927  1329 slave.cpp:4798] Sending acknowledgement for status update TASK_STARTING (UUID: 87cee290-b2fe-4459-9b75-b9f03aab6492) for task 3c08cf78-575d-4813-82b6-_x000D_
3ace272db35e of framework ca0e7b44-c621-4442-a62e-15f7bf02064b-0000 to executor(1)@10.0.49.2:42711_x000D_
I1020 22:18:46.198098  1332 master.cpp:6998] Status update TASK_STARTING (UUID: 87cee290-b2fe-4459-9b75-b9f03aab6492) for task 3c08cf78-575d-4813-82b6-3ace272db35e of framework c_x000D_
a0e7b44-c621-4442-a62e-15f7bf02064b-0000 from agent ca0e7b44-c621-4442-a62e-15f7bf02064b-S0 at slave(1)@10.0.49.2:34819 (core-dev)_x000D_
I1020 22:18:46.198187  1332 master.cpp:7060] Forwarding status update TASK_STARTING (UUID: 87cee290-b2fe-4459-9b75-b9f03aab6492) for task 3c08cf78-575d-4813-82b6-3ace272db35e of _x000D_
framework ca0e7b44-c621-4442-a62e-15f7bf02064b-0000_x000D_
I1020 22:18:46.198463  1332 master.cpp:9162] Updating the state of task 3c08cf78-575d-4813-82b6-3ace272db35e of framework ca0e7b44-c621-4442-a62e-15f7bf02064b-0000 (latest state:_x000D_
 TASK_STARTING, status update state: TASK_STARTING)_x000D_
I1020 22:18:46.199198  1331 master.cpp:5566] Processing ACKNOWLEDGE call 87cee290-b2fe-4459-9b75-b9f03aab6492 for task 3c08cf78-575d-4813-82b6-3ace272db35e of framework ca0e7b44-_x000D_
c621-4442-a62e-15f7bf02064b-0000 (default) at scheduler-f2b66689-382a-4b8c-bdc9-978cff922409@10.0.49.2:34819 on agent ca0e7b44-c621-4442-a62e-15f7bf02064b-S0_x000D_
/home/jie/workspace/mesos/src/tests/containerizer/nvidia_gpu_isolator_tests.cpp:142: Failure_x000D_
      Expected: TASK_RUNNING_x000D_
To be equal to: statusRunning1->state()_x000D_
      Which is: TASK_STARTING_x000D_
{noformat}",1.0,0,0.5,0.0,0.3333333333333333,0.5,0.14285714285714285,0.0,0.2,0.5087719298245614,0.4357142857142857,0.7948717948717948,1.0,1.0,0.0
Bug,Unified Containerizer Auto backend should check xfs ftype for overlayfs backend.,"when using xfs as the backing filesystem in unified containerizer, the `ftype` has to be equal to 1 if we are using the overlay fs backend. we should add the detection in auto backend logic because some OS (like centos 7.2) has xfs ftype=0 by default._x000D_
_x000D_
https://docs.docker.com/engine/userguide/storagedriver/overlayfs-driver/",3.0,0,0.5,0.0,0.6666666666666666,1.0,0.5714285714285714,0.0,0.0,0.5438596491228069,0.5357142857142857,0.6794871794871795,0.5939393939393939,0.5939393939393939,0.0
Bug,ROOT_DOCKER_DockerHealthyTask segfaults in debian 8.,"This test consistently cannot recover the agent on two debian 8 builds: with SSL and CMake based. The error is always the same (full logs attached):_x000D_
{noformat}_x000D_
19:40:59 E1019 19:40:58.581372 16873 slave.cpp:6301] EXIT with status 1: Failed to perform recovery: Failed to run 'docker -H unix:///var/run/docker.sock ps -a': exited with status 1; stderr='error during connect: Get http://%2Fvar%2Frun%2Fdocker.sock/v1.31/containers/json?all=1: read unix @->/var/run/docker.sock: read: connection reset by peer_x000D_
{noformat}",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.717948717948718,0.8242424242424242,0.7212121212121212,0.0
Improvement,Add a master flag to disallow agents that are not configured with fault domain,"Once mesos masters and agents in a cluster are *all* upgraded to a version where the fault domains feature is available, it is beneficial to enforce that agents without a fault domain configured are not allowed to join the cluster. _x000D_
_x000D_
This is a safety net for operators who could forget to configure the fault domain of a remote agent and let it join the cluster. If this happens, an agent in a remote region will be considered a local agent by the master and frameworks (because agent's fault domain is not configured) causing tasks to potentially land in a remote agent which is undesirable._x000D_
_x000D_
Note that this has to be a configurable flag and not enforced by default because otherwise upgrades from a fault domain non-configured cluster to a configured cluster will not be possible.",3.0,0,0.5,0.0,0.3333333333333333,0.5,0.14285714285714285,0.0,0.2,0.5087719298245614,0.4357142857142857,0.8846153846153846,0.9878787878787878,0.9878787878787878,0.0
Improvement,Docker fetcher plugin unsupported scheme failure message is not accurate.,"https://github.com/apache/mesos/blob/1.4.0/src/uri/fetchers/docker.cpp#L843_x000D_
_x000D_
This failure message is not accurate. For such a case, if the user/operator give a wrong credential to communicate to a BASIC auth based docker private registry. The authentication failed but the log is still saying: ""Unsupported auth-scheme: BASIC""_x000D_
_x000D_
",3.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.007142857142857143,0.6794871794871795,0.5939393939393939,0.5939393939393939,0.0
Bug,Some tests miss subscribed event because expectation is set after event fires.,"Tests_x000D_
{noformat}_x000D_
CgroupsIsolatorTest.ROOT_CGROUPS_LimitSwap_x000D_
DefaultExecutorCniTest.ROOT_VerifyContainerIP_x000D_
DockerRuntimeIsolatorTest.ROOT_INTERNET_CURL_NestedSimpleCommand_x000D_
DockerRuntimeIsolatorTest.ROOT_NestedDockerDefaultCmdLocalPuller_x000D_
DockerRuntimeIsolatorTest.ROOT_NestedDockerDefaultEntryptLocalPuller_x000D_
{noformat}_x000D_
all have the same problem. They initiate a scheduler subscribe call in reaction to {{connected}} event. However, an expectation for {{subscribed}} event is created _afterwards_, which might lead to an uninteresting mock function call for {{subscribed}} followed by a failure to wait for {{subscribed}}, see attached log excerpt for more details. Problematic code is here: https://github.com/apache/mesos/blob/1c51c98638bb9ea0e8ec6a3f284b33d6c1a4e8ef/src/tests/containerizer/runtime_isolator_tests.cpp#L593-L615_x000D_
_x000D_
A possible solution is to await for {{subscribed}} only, without {{connected}}, setting the expectation before a connection is attempted, see https://github.com/apache/mesos/blob/1c51c98638bb9ea0e8ec6a3f284b33d6c1a4e8ef/src/tests/default_executor_tests.cpp#L139-L159.",2.0,0,0.5,0.25037707390648567,0.3333333333333333,0.5,0.14285714285714285,0.5,0.0,0.2631578947368421,0.29285714285714287,0.717948717948718,0.8242424242424242,0.7212121212121212,0.0
Bug,Double free corruption in tests due to parallel manipulation of signal and control handlers.,"As mentioned in https://reviews.apache.org/r/51122, {{installCtrlHandler}} and {{configureSignal}} might be called from parallel threads, triggering double free corruption. This makes our example framework tests flaky.",3.0,0,0.5,0.05429864253393665,0.0,0.0,0.0,0.0,0.0,0.017543859649122806,0.014285714285714285,0.717948717948718,0.8242424242424242,0.7212121212121212,0.0
Improvement,Some fields went missing with no replacement in api/v1.,"Hi friends, _x000D_
_x000D_
These fields are available via the state.json but went missing in the v1 of the API:_x000D_
-leader_info- -> available via GET_MASTER which should always return leading master info_x000D_
start_time_x000D_
elected_time_x000D_
_x000D_
As we're showing them on the Overview page of the DC/OS UI, yet would like not be using state.json, it would be great to have them somewhere in V1.",2.0,0,0.5,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.0,0.0,0.0,0.0
Bug,PersistentVolumeTest.SharedPersistentVolumeRescindOnDestroy is flaky.,"I'm observing {{ROOT_MountDiskResource/PersistentVolumeTest.SharedPersistentVolumeRescindOnDestroy/0}} being flaky on our internal CI. From what I see in the logs, when {{framework1}} accepts an offer, creates volumes, launches a task, and kills it right after, the executor might manage to register in-between and hence an unexpected {{TASK_RUNNING}} status update is sent. To fix this, one approach is to explicitly wait for {{TASK_RUNNING}} before attempting to kill the task.",2.0,1.5.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.15789473684210525,0.15,0.717948717948718,0.8242424242424242,0.7212121212121212,0.963302752293578
Improvement,Change Libprocess actor state transitions verbose logs to use VLOG(3) instead of 2,"Without claiming a general change or a holistic approach, the amount of logs concerning states being resumed when running a Mesos cluster with {{GLOG_v=2}} is quite noisy. We should thus use {{VLOG(3)}} for such messages.",1.0,0,0.0,0.25037707390648567,0.3333333333333333,0.5,0.14285714285714285,0.5,0.0,0.2631578947368421,0.29285714285714287,0.10256410256410256,0.1515151515151515,0.1515151515151515,0.0
Improvement,Change Mesos common events verbose logs to use VLOG(2) instead of 1,"The original commit https://github.com/apache/mesos/commit/fa6ffdfcd22136c171b43aed2e7949a07fd263d7 that started using VLOG(1) for the allocator does not state why this level was chosen and the periodic messages such as ""No allocations performed"" should be displayed at a higher level to simplify debugging.",2.0,0,0.0,0.25037707390648567,0.3333333333333333,0.5,0.14285714285714285,0.5,0.0,0.2631578947368421,0.29285714285714287,0.10256410256410256,0.1515151515151515,0.1515151515151515,0.0
Bug,Pylint report errors in apply-reviews.py on Ubuntu 14.04,"{code}_x000D_
Building remotely on ubuntu-4 (ubuntu trusty) in workspace <https://builds.apache.org/job/Mesos-Buildbot/BUILDTOOL=cmake,COMPILER=gcc,CONFIGURATION=--verbose%20--disable-libtool-wrappers%20--enable-libevent%20--enable-ssl,ENVIRONMENT=GLOG_v=1%20MESOS_VERBOSE=1,OS=centos%3A7,label_exp=(ubuntu)&&(!ubuntu-us1)&&(!ubuntu-eu2)&&(!qnode3)&&(!H23)/ws/>_x000D_
Wiping out workspace first._x000D_
Cloning the remote Git repository_x000D_
Cloning repository https://git-wip-us.apache.org/repos/asf/mesos.git_x000D_
 > git init <https://builds.apache.org/job/Mesos-Buildbot/BUILDTOOL=cmake,COMPILER=gcc,CONFIGURATION=--verbose%20--disable-libtool-wrappers%20--enable-libevent%20--enable-ssl,ENVIRONMENT=GLOG_v=1%20MESOS_VERBOSE=1,OS=centos%3A7,label_exp=(ubuntu)&&(!ubuntu-us1)&&(!ubuntu-eu2)&&(!qnode3)&&(!H23)/ws/> # timeout=10_x000D_
Fetching upstream changes from https://git-wip-us.apache.org/repos/asf/mesos.git_x000D_
 > git --version # timeout=10_x000D_
 > git fetch --tags --progress https://git-wip-us.apache.org/repos/asf/mesos.git +refs/heads/*:refs/remotes/origin/*_x000D_
 > git config remote.origin.url https://git-wip-us.apache.org/repos/asf/mesos.git # timeout=10_x000D_
 > git config --add remote.origin.fetch +refs/heads/*:refs/remotes/origin/* # timeout=10_x000D_
 > git config remote.origin.url https://git-wip-us.apache.org/repos/asf/mesos.git # timeout=10_x000D_
Fetching upstream changes from https://git-wip-us.apache.org/repos/asf/mesos.git_x000D_
 > git fetch --tags --progress https://git-wip-us.apache.org/repos/asf/mesos.git +refs/heads/*:refs/remotes/origin/*_x000D_
Checking out Revision 4fd018be674ad6badd23f6a1f0baea2d63fd7974 (origin/master)_x000D_
Commit message: ""Formatted 'synchronized' like loop instead of function.""_x000D_
 > git config core.sparsecheckout # timeout=10_x000D_
 > git checkout -f 4fd018be674ad6badd23f6a1f0baea2d63fd7974_x000D_
 > git rev-list 5e5ed4db273179101eee90d752c1622315987949 # timeout=10_x000D_
[76454b83] $ /bin/bash -xe /tmp/jenkins5012206270681846605.sh_x000D_
+ '[' origin/master = origin/1.0.x ']'_x000D_
+ ./support/jenkins/buildbot.sh_x000D_
Requirement already satisfied (use --upgrade to upgrade): virtualenv in /usr/lib/python2.7/dist-packages_x000D_
Cleaning up..._x000D_
Total errors found: 0_x000D_
<https://builds.apache.org/job/Mesos-Buildbot/BUILDTOOL=cmake,COMPILER=gcc,CONFIGURATION=--verbose%20--disable-libtool-wrappers%20--enable-libevent%20--enable-ssl,ENVIRONMENT=GLOG_v=1%20MESOS_VERBOSE=1,OS=centos%3A7,label_exp=(ubuntu)&&(!ubuntu-us1)&&(!ubuntu-eu2)&&(!qnode3)&&(!H23)/ws/src/python/cli_new/.virtualenv/local/lib/python2.7/site-packages/pip/_vendor/requests/packages/urllib3/util/ssl_.py>:318: SNIMissingWarning: An HTTPS request has been made, but the SNI (Subject Name Indication) extension to TLS is not available on this platform. This may cause the server to present an incorrect TLS certificate, which can cause validation failures. You can upgrade to a newer version of Python to solve this. For more information, see https://urllib3.readthedocs.io/en/latest/security.html#snimissingwarning._x000D_
  SNIMissingWarning_x000D_
<https://builds.apache.org/job/Mesos-Buildbot/BUILDTOOL=cmake,COMPILER=gcc,CONFIGURATION=--verbose%20--disable-libtool-wrappers%20--enable-libevent%20--enable-ssl,ENVIRONMENT=GLOG_v=1%20MESOS_VERBOSE=1,OS=centos%3A7,label_exp=(ubuntu)&&(!ubuntu-us1)&&(!ubuntu-eu2)&&(!qnode3)&&(!H23)/ws/src/python/cli_new/.virtualenv/local/lib/python2.7/site-packages/pip/_vendor/requests/packages/urllib3/util/ssl_.py>:122: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. You can upgrade to a newer version of Python to solve this. For more information, see https://urllib3.readthedocs.io/en/latest/security.html#insecureplatformwarning._x000D_
  InsecurePlatformWarning_x000D_
<https://builds.apache.org/job/Mesos-Buildbot/BUILDTOOL=cmake,COMPILER=gcc,CONFIGURATION=--verbose%20--disable-libtool-wrappers%20--enable-libevent%20--enable-ssl,ENVIRONMENT=GLOG_v=1%20MESOS_VERBOSE=1,OS=centos%3A7,label_exp=(ubuntu)&&(!ubuntu-us1)&&(!ubuntu-eu2)&&(!qnode3)&&(!H23)/ws/src/python/cli_new/.virtualenv/local/lib/python2.7/site-packages/pip/_vendor/requests/packages/urllib3/util/ssl_.py>:122: InsecurePlatformWarning: A true SSLContext object is not available. This prevents urllib3 from configuring SSL appropriately and may cause certain SSL connections to fail. You can upgrade to a newer version of Python to solve this. For more information, see https://urllib3.readthedocs.io/en/latest/security.html#insecureplatformwarning._x000D_
  InsecurePlatformWarning_x000D_
************* Module apply-reviews_x000D_
E:169,14: Module 'ssl' has no 'SSLContext' member (no-member)_x000D_
E:189,19: Unexpected keyword argument 'context' in function call (unexpected-keyword-arg)_x000D_
Total errors found: 2_x000D_
{code}_x000D_
_x000D_
Error 1, ""Module 'ssl' has no 'SSLContext' member (no-member)"":_x000D_
The line caught is part of a function that is only used on Windows (https://github.com/apache/mesos/blob/4fd018be674ad6badd23f6a1f0baea2d63fd7974/support/apply-reviews.py#L188). Having this code requires Python 2.7.9: https://docs.python.org/2/library/ssl.html#_x000D_
_x000D_
Error 2, ""Unexpected keyword argument 'context' in function call (unexpected-keyword-arg)"":_x000D_
This keyword argument is indeed added in Python 2.7.9._x000D_
",1.0,0,0.0,0.25037707390648567,0.3333333333333333,0.5,0.14285714285714285,0.5,0.0,0.2631578947368421,0.29285714285714287,0.10256410256410256,0.1515151515151515,0.1515151515151515,0.0
Bug,ReservationEndpointsTest.GoodReserveAndUnreserveACL is flaky.,"As just observed on our internal CI;_x000D_
_x000D_
Error Message_x000D_
{noformat}_x000D_
../../src/tests/reservation_endpoints_tests.cpp:1026_x000D_
Value of: (response).get().status_x000D_
  Actual: ""409 Conflict""_x000D_
Expected: Accepted().status_x000D_
Which is: ""202 Accepted""_x000D_
{noformat}_x000D_
_x000D_
Log:_x000D_
{noformat}_x000D_
00:42:35  [ RUN      ] ReservationEndpointsTest.GoodReserveAndUnreserveACL_x000D_
00:42:35  I0930 00:42:35.517658  7413 cluster.cpp:162] Creating default 'local' authorizer_x000D_
00:42:35  I0930 00:42:35.518507  7433 master.cpp:445] Master 938119f3-8007-4d6f-a45b-d49bf76a0590 (ip-172-16-10-96.ec2.internal) started on 172.16.10.96:46227_x000D_
00:42:35  I0930 00:42:35.518523  7433 master.cpp:447] Flags at startup: --acls=""reserve_resources {_x000D_
00:42:35    principals {_x000D_
00:42:35      values: ""test-principal""_x000D_
00:42:35    }_x000D_
00:42:35    roles {_x000D_
00:42:35      type: ANY_x000D_
00:42:35    }_x000D_
00:42:35  }_x000D_
00:42:35  unreserve_resources {_x000D_
00:42:35    principals {_x000D_
00:42:35      values: ""test-principal""_x000D_
00:42:35    }_x000D_
00:42:35    reserver_principals {_x000D_
00:42:35      values: ""test-principal""_x000D_
00:42:35    }_x000D_
00:42:35  }_x000D_
00:42:35  "" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""50ms"" --allocator=""HierarchicalDRF"" --authenticate_agents=""true"" --authenticate_frameworks=""true"" --authenticate_http_frameworks=""true"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/zFIYus/credentials"" --filter_gpu_resources=""true"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_unreachable_tasks_per_framework=""1000"" --port=""5050"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""in_memory"" --registry_fetch_timeout=""1mins"" --registry_gc_interval=""15mins"" --registry_max_agent_age=""2weeks"" --registry_max_agent_count=""102400"" --registry_store_timeout=""100secs"" --registry_strict=""false"" --roles=""role"" --root_submissions=""true"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/zFIYus/master"" --zk_session_timeout=""10secs""_x000D_
00:42:35  I0930 00:42:35.518672  7433 master.cpp:497] Master only allowing authenticated frameworks to register_x000D_
00:42:35  I0930 00:42:35.518681  7433 master.cpp:511] Master only allowing authenticated agents to register_x000D_
00:42:35  I0930 00:42:35.518685  7433 master.cpp:524] Master only allowing authenticated HTTP frameworks to register_x000D_
00:42:35  I0930 00:42:35.518689  7433 credentials.hpp:37] Loading credentials for authentication from '/tmp/zFIYus/credentials'_x000D_
00:42:35  I0930 00:42:35.518784  7433 master.cpp:569] Using default 'crammd5' authenticator_x000D_
00:42:35  I0930 00:42:35.518823  7433 http.cpp:1045] Creating default 'basic' HTTP authenticator for realm 'mesos-master-readonly'_x000D_
00:42:35  I0930 00:42:35.518853  7433 http.cpp:1045] Creating default 'basic' HTTP authenticator for realm 'mesos-master-readwrite'_x000D_
00:42:35  I0930 00:42:35.518877  7433 http.cpp:1045] Creating default 'basic' HTTP authenticator for realm 'mesos-master-scheduler'_x000D_
00:42:35  I0930 00:42:35.518898  7433 master.cpp:649] Authorization enabled_x000D_
00:42:35  W0930 00:42:35.518905  7433 master.cpp:712] The '--roles' flag is deprecated. This flag will be removed in the future. See the Mesos 0.27 upgrade notes for more information_x000D_
00:42:35  I0930 00:42:35.519016  7438 whitelist_watcher.cpp:77] No whitelist given_x000D_
00:42:35  I0930 00:42:35.519018  7439 hierarchical.cpp:171] Initialized hierarchical allocator process_x000D_
00:42:35  I0930 00:42:35.519625  7433 master.cpp:2216] Elected as the leading master!_x000D_
00:42:35  I0930 00:42:35.519640  7433 master.cpp:1705] Recovering from registrar_x000D_
00:42:35  I0930 00:42:35.519677  7433 registrar.cpp:347] Recovering registrar_x000D_
00:42:35  I0930 00:42:35.519762  7438 registrar.cpp:391] Successfully fetched the registry (0B) in 70144ns_x000D_
00:42:35  I0930 00:42:35.519783  7438 registrar.cpp:495] Applied 1 operations in 3246ns; attempting to update the registry_x000D_
00:42:35  I0930 00:42:35.519870  7439 registrar.cpp:552] Successfully updated the registry in 78080ns_x000D_
00:42:35  I0930 00:42:35.519899  7439 registrar.cpp:424] Successfully recovered registrar_x000D_
00:42:35  I0930 00:42:35.519975  7439 master.cpp:1809] Recovered 0 agents from the registry (168B); allowing 10mins for agents to re-register_x000D_
00:42:35  I0930 00:42:35.520007  7435 hierarchical.cpp:209] Skipping recovery of hierarchical allocator: nothing to recover_x000D_
00:42:35  W0930 00:42:35.521775  7413 process.cpp:3194] Attempted to spawn already running process files@172.16.10.96:46227_x000D_
00:42:35  I0930 00:42:35.522099  7413 containerizer.cpp:292] Using isolation { environment_secret, posix/cpu, posix/mem, filesystem/posix, network/cni }_x000D_
00:42:35  I0930 00:42:35.527375  7413 linux_launcher.cpp:146] Using /sys/fs/cgroup/freezer as the freezer hierarchy for the Linux launcher_x000D_
00:42:35  I0930 00:42:35.527729  7413 provisioner.cpp:255] Using default backend 'overlay'_x000D_
00:42:35  I0930 00:42:35.528136  7413 cluster.cpp:448] Creating default 'local' authorizer_x000D_
00:42:35  I0930 00:42:35.528524  7439 slave.cpp:254] Mesos agent started on (409)@172.16.10.96:46227_x000D_
00:42:35  I0930 00:42:35.528540  7439 slave.cpp:255] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/ReservationEndpointsTest_GoodReserveAndUnreserveACL_o9Veil/store/appc"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""true"" --authenticatee=""crammd5"" --authentication_backoff_factor=""1secs"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/ReservationEndpointsTest_GoodReserveAndUnreserveACL_o9Veil/credential"" --default_role=""*"" --disallow_sharing_agent_pid_namespace=""false"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/ReservationEndpointsTest_GoodReserveAndUnreserveACL_o9Veil/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_reregistration_timeout=""2secs"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/ReservationEndpointsTest_GoodReserveAndUnreserveACL_o9Veil/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_command_executor=""false"" --http_credentials=""/tmp/ReservationEndpointsTest_GoodReserveAndUnreserveACL_o9Veil/http_credentials"" --http_heartbeat_interval=""30secs"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher=""linux"" --launcher_dir=""/home/centos/workspace/mesos/Mesos_CI-build/FLAG/Plain/label/mesos-ec2-centos-7/mesos/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_executors_per_framework=""150"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --port=""5051"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:1;mem:512"" --revocable_cpu_low_priority=""true"" --runtime_dir=""/tmp/ReservationEndpointsTest_GoodReserveAndUnreserveACL_o9Veil"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/ReservationEndpointsTest_GoodReserveAndUnreserveACL_wtxBTT"" --zk_session_timeout=""10secs""_x000D_
00:42:35  I0930 00:42:35.528676  7439 credentials.hpp:86] Loading credential for authentication from '/tmp/ReservationEndpointsTest_GoodReserveAndUnreserveACL_o9Veil/credential'_x000D_
00:42:35  I0930 00:42:35.528724  7439 slave.cpp:287] Agent using credential for: test-principal_x000D_
00:42:35  I0930 00:42:35.528730  7439 credentials.hpp:37] Loading credentials for authentication from '/tmp/ReservationEndpointsTest_GoodReserveAndUnreserveACL_o9Veil/http_credentials'_x000D_
00:42:35  I0930 00:42:35.528782  7439 http.cpp:1045] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-readonly'_x000D_
00:42:35  I0930 00:42:35.528818  7439 http.cpp:1045] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-readwrite'_x000D_
00:42:35  I0930 00:42:35.529382  7439 slave.cpp:585] Agent resources: [{""name"":""cpus"",""scalar"":{""value"":1.0},""type"":""SCALAR""},{""name"":""mem"",""scalar"":{""value"":512.0},""type"":""SCALAR""},{""name"":""disk"",""scalar"":{""value"":35823.0},""type"":""SCALAR""},{""name"":""ports"",""ranges"":{""range"":[{""begin"":31000,""end"":32000}]},""type"":""RANGES""}]_x000D_
00:42:35  I0930 00:42:35.529448  7439 slave.cpp:593] Agent attributes: [  ]_x000D_
00:42:35  I0930 00:42:35.529454  7439 slave.cpp:602] Agent hostname: ip-172-16-10-96.ec2.internal_x000D_
00:42:35  I0930 00:42:35.529676  7435 status_update_manager.cpp:177] Pausing sending status updates_x000D_
00:42:35  I0930 00:42:35.529700  7437 state.cpp:64] Recovering state from '/tmp/ReservationEndpointsTest_GoodReserveAndUnreserveACL_wtxBTT/meta'_x000D_
00:42:35  I0930 00:42:35.529767  7437 status_update_manager.cpp:203] Recovering status update manager_x000D_
00:42:35  I0930 00:42:35.529822  7437 containerizer.cpp:648] Recovering containerizer_x000D_
00:42:35  I0930 00:42:35.530825  7437 provisioner.cpp:416] Provisioner recovery complete_x000D_
00:42:35  I0930 00:42:35.530910  7437 slave.cpp:6313] Finished recovery_x000D_
00:42:35  I0930 00:42:35.531136  7437 slave.cpp:6495] Querying resource estimator for oversubscribable resources_x000D_
00:42:35  I0930 00:42:35.531191  7439 status_update_manager.cpp:177] Pausing sending status updates_x000D_
00:42:35  I0930 00:42:35.531201  7436 slave.cpp:993] New master detected at master@172.16.10.96:46227_x000D_
00:42:35  I0930 00:42:35.531231  7436 slave.cpp:1028] Detecting new master_x000D_
00:42:35  I0930 00:42:35.531263  7436 slave.cpp:6509] Received oversubscribable resources {} from the resource estimator_x000D_
00:42:35  I0930 00:42:35.539541  7435 slave.cpp:1055] Authenticating with master master@172.16.10.96:46227_x000D_
00:42:35  I0930 00:42:35.539571  7435 slave.cpp:1066] Using default CRAM-MD5 authenticatee_x000D_
00:42:35  I0930 00:42:35.539618  7435 authenticatee.cpp:121] Creating new client SASL connection_x000D_
00:42:35  I0930 00:42:35.540117  7435 master.cpp:7915] Authenticating slave(409)@172.16.10.96:46227_x000D_
00:42:35  I0930 00:42:35.540163  7435 authenticator.cpp:414] Starting authentication session for crammd5-authenticatee(861)@172.16.10.96:46227_x000D_
00:42:35  I0930 00:42:35.540216  7435 authenticator.cpp:98] Creating new server SASL connection_x000D_
00:42:35  I0930 00:42:35.540619  7435 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5_x000D_
00:42:35  I0930 00:42:35.540637  7435 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'_x000D_
00:42:35  I0930 00:42:35.540665  7435 authenticator.cpp:204] Received SASL authentication start_x000D_
00:42:35  I0930 00:42:35.540694  7435 authenticator.cpp:326] Authentication requires more steps_x000D_
00:42:35  I0930 00:42:35.540724  7435 authenticatee.cpp:259] Received SASL authentication step_x000D_
00:42:35  I0930 00:42:35.540766  7435 authenticator.cpp:232] Received SASL authentication step_x000D_
00:42:35  I0930 00:42:35.540782  7435 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'ip-172-16-10-96.ec2.internal' server FQDN: 'ip-172-16-10-96.ec2.internal' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false _x000D_
00:42:35  I0930 00:42:35.540791  7435 auxprop.cpp:181] Looking up auxiliary property '*userPassword'_x000D_
00:42:35  I0930 00:42:35.540804  7435 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'_x000D_
00:42:35  I0930 00:42:35.540813  7435 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'ip-172-16-10-96.ec2.internal' server FQDN: 'ip-172-16-10-96.ec2.internal' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true _x000D_
00:42:35  I0930 00:42:35.540819  7435 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true_x000D_
00:42:35  I0930 00:42:35.540824  7435 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true_x000D_
00:42:35  I0930 00:42:35.540835  7435 authenticator.cpp:318] Authentication success_x000D_
00:42:35  I0930 00:42:35.540881  7436 authenticatee.cpp:299] Authentication success_x000D_
00:42:35  I0930 00:42:35.540894  7435 master.cpp:7945] Successfully authenticated principal 'test-principal' at slave(409)@172.16.10.96:46227_x000D_
00:42:35  I0930 00:42:35.540925  7432 authenticator.cpp:432] Authentication session cleanup for crammd5-authenticatee(861)@172.16.10.96:46227_x000D_
00:42:35  I0930 00:42:35.540977  7436 slave.cpp:1150] Successfully authenticated with master master@172.16.10.96:46227_x000D_
00:42:35  I0930 00:42:35.541028  7436 slave.cpp:1629] Will retry registration in 11.469653ms if necessary_x000D_
00:42:35  I0930 00:42:35.541095  7438 master.cpp:5819] Received register agent message from slave(409)@172.16.10.96:46227 (ip-172-16-10-96.ec2.internal)_x000D_
00:42:35  I0930 00:42:35.541121  7438 master.cpp:3856] Authorizing agent with principal 'test-principal'_x000D_
00:42:35  I0930 00:42:35.541203  7432 master.cpp:5879] Authorized registration of agent at slave(409)@172.16.10.96:46227 (ip-172-16-10-96.ec2.internal)_x000D_
00:42:35  I0930 00:42:35.541246  7432 master.cpp:5972] Registering agent at slave(409)@172.16.10.96:46227 (ip-172-16-10-96.ec2.internal) with id 938119f3-8007-4d6f-a45b-d49bf76a0590-S0_x000D_
00:42:35  I0930 00:42:35.541350  7432 registrar.cpp:495] Applied 1 operations in 10060ns; attempting to update the registry_x000D_
00:42:35  I0930 00:42:35.541492  7432 registrar.cpp:552] Successfully updated the registry in 120064ns_x000D_
00:42:35  I0930 00:42:35.541545  7437 master.cpp:6019] Admitted agent 938119f3-8007-4d6f-a45b-d49bf76a0590-S0 at slave(409)@172.16.10.96:46227 (ip-172-16-10-96.ec2.internal)_x000D_
00:42:35  I0930 00:42:35.541666  7437 master.cpp:6050] Registered agent 938119f3-8007-4d6f-a45b-d49bf76a0590-S0 at slave(409)@172.16.10.96:46227 (ip-172-16-10-96.ec2.internal) with [{""name"":""cpus"",""scalar"":{""value"":1.0},""type"":""SCALAR""},{""name"":""mem"",""scalar"":{""value"":512.0},""type"":""SCALAR""},{""name"":""disk"",""scalar"":{""value"":35823.0},""type"":""SCALAR""},{""name"":""ports"",""ranges"":{""range"":[{""begin"":31000,""end"":32000}]},""type"":""RANGES""}]_x000D_
00:42:35  I0930 00:42:35.541733  7439 hierarchical.cpp:593] Added agent 938119f3-8007-4d6f-a45b-d49bf76a0590-S0 (ip-172-16-10-96.ec2.internal) with cpus:1; mem:512; disk:35823; ports:[31000-32000] (allocated: {})_x000D_
00:42:35  I0930 00:42:35.541798  7437 slave.cpp:1196] Registered with master master@172.16.10.96:46227; given agent ID 938119f3-8007-4d6f-a45b-d49bf76a0590-S0_x000D_
00:42:35  I0930 00:42:35.541828  7439 hierarchical.cpp:1943] No allocations performed_x000D_
00:42:35  I0930 00:42:35.541842  7439 hierarchical.cpp:1486] Performed allocation for 1 agents in 32679ns_x000D_
00:42:35  I0930 00:42:35.541879  7439 status_update_manager.cpp:184] Resuming sending status updates_x000D_
00:42:35  I0930 00:42:35.542570  7433 process.cpp:3929] Handling HTTP event for process 'master' with path: '/master/reserve'_x000D_
00:42:35  I0930 00:42:35.542874  7438 http.cpp:1185] HTTP POST for /master/reserve from 172.16.10.96:54256_x000D_
00:42:35  I0930 00:42:35.543103  7437 slave.cpp:1216] Checkpointing SlaveInfo to '/tmp/ReservationEndpointsTest_GoodReserveAndUnreserveACL_wtxBTT/meta/slaves/938119f3-8007-4d6f-a45b-d49bf76a0590-S0/slave.info'_x000D_
00:42:35  I0930 00:42:35.543090  7438 master.cpp:3641] Authorizing principal 'test-principal' to reserve resources '[{""name"":""cpus"",""reservations"":[{""principal"":""test-principal"",""role"":""role"",""type"":""DYNAMIC""}],""scalar"":{""value"":1.0},""type"":""SCALAR""},{""name"":""mem"",""reservations"":[{""principal"":""test-principal"",""role"":""role"",""type"":""DYNAMIC""}],""scalar"":{""value"":512.0},""type"":""SCALAR""}]'_x000D_
00:42:35  I0930 00:42:35.543285  7437 slave.cpp:1265] Forwarding total oversubscribed resources {}_x000D_
00:42:35  I0930 00:42:35.543319  7437 slave.cpp:4969] Received ping from slave-observer(413)@172.16.10.96:46227_x000D_
00:42:35  I0930 00:42:35.543632  7438 master.cpp:6814] Received update of agent 938119f3-8007-4d6f-a45b-d49bf76a0590-S0 at slave(409)@172.16.10.96:46227 (ip-172-16-10-96.ec2.internal) with total oversubscribed resources {}_x000D_
00:42:35  I0930 00:42:35.543754  7438 master.cpp:9314] Sending updated checkpointed resources cpus(reservations: [(DYNAMIC,role,test-principal)]):1; mem(reservations: [(DYNAMIC,role,test-principal)]):512 to agent 938119f3-8007-4d6f-a45b-d49bf76a0590-S0 at slave(409)@172.16.10.96:46227 (ip-172-16-10-96.ec2.internal)_x000D_
00:42:35  I0930 00:42:35.543889  7437 hierarchical.cpp:660] Agent 938119f3-8007-4d6f-a45b-d49bf76a0590-S0 (ip-172-16-10-96.ec2.internal) updated with total resources cpus:1; mem:512; disk:35823; ports:[31000-32000]_x000D_
00:42:35  I0930 00:42:35.543952  7437 hierarchical.cpp:1943] No allocations performed_x000D_
00:42:35  I0930 00:42:35.543967  7437 hierarchical.cpp:1486] Performed allocation for 1 agents in 29057ns_x000D_
00:42:35  I0930 00:42:35.544109  7438 slave.cpp:3522] Updated checkpointed resources from {} to cpus(reservations: [(DYNAMIC,role,test-principal)]):1; mem(reservations: [(DYNAMIC,role,test-principal)]):512_x000D_
00:42:35  I0930 00:42:35.544886  7439 process.cpp:3929] Handling HTTP event for process 'master' with path: '/master/unreserve'_x000D_
00:42:35  I0930 00:42:35.545197  7437 http.cpp:1185] HTTP POST for /master/unreserve from 172.16.10.96:54258_x000D_
00:42:35  I0930 00:42:35.545383  7437 master.cpp:3709] Authorizing principal 'test-principal' to unreserve resources '[{""name"":""cpus"",""reservations"":[{""principal"":""test-principal"",""role"":""role"",""type"":""DYNAMIC""}],""scalar"":{""value"":1.0},""type"":""SCALAR""},{""name"":""mem"",""reservations"":[{""principal"":""test-principal"",""role"":""role"",""type"":""DYNAMIC""}],""scalar"":{""value"":512.0},""type"":""SCALAR""}]'_x000D_
00:42:35  ../../src/tests/reservation_endpoints_tests.cpp:1026: Failure_x000D_
00:42:35  Value of: (response).get().status_x000D_
00:42:35    Actual: ""409 Conflict""_x000D_
00:42:35  Expected: Accepted().status_x000D_
00:42:35  Which is: ""202 Accepted""_x000D_
00:42:35  I0930 00:42:35.546277  7413 slave.cpp:869] Agent terminating_x000D_
00:42:35  I0930 00:42:35.546371  7439 master.cpp:1321] Agent 938119f3-8007-4d6f-a45b-d49bf76a0590-S0 at slave(409)@172.16.10.96:46227 (ip-172-16-10-96.ec2.internal) disconnected_x000D_
00:42:35  I0930 00:42:35.546391  7439 master.cpp:3354] Disconnecting agent 938119f3-8007-4d6f-a45b-d49bf76a0590-S0 at slave(409)@172.16.10.96:46227 (ip-172-16-10-96.ec2.internal)_x000D_
00:42:35  I0930 00:42:35.546404  7439 master.cpp:3373] Deactivating agent 938119f3-8007-4d6f-a45b-d49bf76a0590-S0 at slave(409)@172.16.10.96:46227 (ip-172-16-10-96.ec2.internal)_x000D_
00:42:35  I0930 00:42:35.546520  7438 hierarchical.cpp:690] Agent 938119f3-8007-4d6f-a45b-d49bf76a0590-S0 deactivated_x000D_
00:42:35  I0930 00:42:35.547936  7413 master.cpp:1163] Master terminating_x000D_
00:42:35  I0930 00:42:35.548065  7439 hierarchical.cpp:626] Removed agent 938119f3-8007-4d6f-a45b-d49bf76a0590-S0_x000D_
00:42:35  [  FAILED  ] ReservationEndpointsTest.GoodReserveAndUnreserveACL (33 ms)_x000D_
{noformat}",2.0,1.5.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.3717948717948718,0.24848484848484845,0.24848484848484845,0.963302752293578
Bug,MasterTestPrePostReservationRefinement.ReserveAndUnreserveResourcesV1 is flaky.,"As seen on our internal CI._x000D_
_x000D_
Error Message_x000D_
{noformat}_x000D_
../../src/tests/master_tests.cpp:8682_x000D_
Value of: (v1UnreserveResourcesResponse).get().status_x000D_
  Actual: ""409 Conflict""_x000D_
Expected: Accepted().status_x000D_
Which is: ""202 Accepted""_x000D_
{noformat}_x000D_
_x000D_
_x000D_
Log:_x000D_
{noformat}_x000D_
00:33:08  [ RUN      ] bool/MasterTestPrePostReservationRefinement.ReserveAndUnreserveResourcesV1/0_x000D_
00:33:08  I0929 17:33:08.670744 2067726336 cluster.cpp:162] Creating default 'local' authorizer_x000D_
00:33:08  I0929 17:33:08.672592 3211264 master.cpp:445] Master 71fce4a3-01f6-43a7-b512-28980b04e51f (10.0.49.4) started on 10.0.49.4:54887_x000D_
00:33:08  I0929 17:33:08.672621 3211264 master.cpp:447] Flags at startup: --acls="""" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate_agents=""true"" --authenticate_frameworks=""true"" --authenticate_http_frameworks=""true"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/private/var/folders/6w/rw03zh013y38ys6cyn8qppf80000gn/T/YdqFmR/credentials"" --filter_gpu_resources=""true"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_unreachable_tasks_per_framework=""1000"" --port=""5050"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""in_memory"" --registry_fetch_timeout=""1mins"" --registry_gc_interval=""15mins"" --registry_max_agent_age=""2weeks"" --registry_max_agent_count=""102400"" --registry_store_timeout=""100secs"" --registry_strict=""false"" --root_submissions=""true"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/private/var/folders/6w/rw03zh013y38ys6cyn8qppf80000gn/T/YdqFmR/master"" --zk_session_timeout=""10secs""_x000D_
00:33:08  I0929 17:33:08.672792 3211264 master.cpp:497] Master only allowing authenticated frameworks to register_x000D_
00:33:08  I0929 17:33:08.672804 3211264 master.cpp:511] Master only allowing authenticated agents to register_x000D_
00:33:08  I0929 17:33:08.672821 3211264 master.cpp:524] Master only allowing authenticated HTTP frameworks to register_x000D_
00:33:08  I0929 17:33:08.672829 3211264 credentials.hpp:37] Loading credentials for authentication from '/private/var/folders/6w/rw03zh013y38ys6cyn8qppf80000gn/T/YdqFmR/credentials'_x000D_
00:33:08  I0929 17:33:08.672997 3211264 master.cpp:569] Using default 'crammd5' authenticator_x000D_
00:33:08  I0929 17:33:08.673053 3211264 http.cpp:1045] Creating default 'basic' HTTP authenticator for realm 'mesos-master-readonly'_x000D_
00:33:08  I0929 17:33:08.673136 3211264 http.cpp:1045] Creating default 'basic' HTTP authenticator for realm 'mesos-master-readwrite'_x000D_
00:33:08  I0929 17:33:08.673174 3211264 http.cpp:1045] Creating default 'basic' HTTP authenticator for realm 'mesos-master-scheduler'_x000D_
00:33:08  I0929 17:33:08.673226 3211264 master.cpp:649] Authorization enabled_x000D_
00:33:08  I0929 17:33:08.673306 2674688 hierarchical.cpp:171] Initialized hierarchical allocator process_x000D_
00:33:08  I0929 17:33:08.673326 1601536 whitelist_watcher.cpp:77] No whitelist given_x000D_
00:33:08  I0929 17:33:08.674684 1601536 master.cpp:2216] Elected as the leading master!_x000D_
00:33:08  I0929 17:33:08.674708 1601536 master.cpp:1705] Recovering from registrar_x000D_
00:33:08  I0929 17:33:08.674787 2674688 registrar.cpp:347] Recovering registrar_x000D_
00:33:08  I0929 17:33:08.674944 2674688 registrar.cpp:391] Successfully fetched the registry (0B) in 134912ns_x000D_
00:33:08  I0929 17:33:08.675014 2674688 registrar.cpp:495] Applied 1 operations in 17us; attempting to update the registry_x000D_
00:33:08  I0929 17:33:08.675209 2674688 registrar.cpp:552] Successfully updated the registry in 157184ns_x000D_
00:33:08  I0929 17:33:08.675252 2674688 registrar.cpp:424] Successfully recovered registrar_x000D_
00:33:08  I0929 17:33:08.675377 2138112 master.cpp:1809] Recovered 0 agents from the registry (121B); allowing 10mins for agents to re-register_x000D_
00:33:08  I0929 17:33:08.675418 528384 hierarchical.cpp:209] Skipping recovery of hierarchical allocator: nothing to recover_x000D_
00:33:08  W0929 17:33:08.678066 2067726336 process.cpp:3194] Attempted to spawn already running process files@10.0.49.4:54887_x000D_
00:33:08  I0929 17:33:08.678484 2067726336 containerizer.cpp:292] Using isolation { environment_secret, filesystem/posix, posix/cpu, posix/mem }_x000D_
00:33:08  I0929 17:33:08.678678 2067726336 provisioner.cpp:255] Using default backend 'copy'_x000D_
00:33:08  I0929 17:33:08.679306 2067726336 cluster.cpp:448] Creating default 'local' authorizer_x000D_
00:33:08  I0929 17:33:08.680037 3747840 slave.cpp:254] Mesos agent started on (751)@10.0.49.4:54887_x000D_
00:33:08  I0929 17:33:08.680061 3747840 slave.cpp:255] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/var/folders/6w/rw03zh013y38ys6cyn8qppf80000gn/T/bool_MasterTestPrePostReservationRefinement_ReserveAndUnreserveResourcesV1_0_b1Sm7Z/store/appc"" --authenticate_http_executors=""true"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""true"" --authenticatee=""crammd5"" --authentication_backoff_factor=""1secs"" --authorizer=""local"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/var/folders/6w/rw03zh013y38ys6cyn8qppf80000gn/T/bool_MasterTestPrePostReservationRefinement_ReserveAndUnreserveResourcesV1_0_b1Sm7Z/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/var/folders/6w/rw03zh013y38ys6cyn8qppf80000gn/T/bool_MasterTestPrePostReservationRefinement_ReserveAndUnreserveResourcesV1_0_b1Sm7Z/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_reregistration_timeout=""2secs"" --executor_secret_key=""/var/folders/6w/rw03zh013y38ys6cyn8qppf80000gn/T/bool_MasterTestPrePostReservationRefinement_ReserveAndUnreserveResourcesV1_0_b1Sm7Z/executor_secret_key"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/var/folders/6w/rw03zh013y38ys6cyn8qppf80000gn/T/bool_MasterTestPrePostReservationRefinement_ReserveAndUnreserveResourcesV1_0_b1Sm7Z/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_command_executor=""false"" --http_credentials=""/var/folders/6w/rw03zh013y38ys6cyn8qppf80000gn/T/bool_MasterTestPrePostReservationRefinement_ReserveAndUnreserveResourcesV1_0_b1Sm7Z/http_credentials"" --http_heartbeat_interval=""30secs"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher=""posix"" --launcher_dir=""/Users/jenkins/workspace/workspace/mesos/Mesos_CI-build/FLAG/SSL/label/mac/mesos/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_executors_per_framework=""150"" --oversubscribed_resources_interval=""15secs"" --port=""5051"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]"" --runtime_dir=""/var/folders/6w/rw03zh013y38ys6cyn8qppf80000gn/T/bool_MasterTestPrePostReservationRefinement_ReserveAndUnreserveResourcesV1_0_b1Sm7Z"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --version=""false"" --work_dir=""/var/folders/6w/rw03zh013y38ys6cyn8qppf80000gn/T/bool_MasterTestPrePostReservationRefinement_ReserveAndUnreserveResourcesV1_0_rx4DfE"" --zk_session_timeout=""10secs""_x000D_
00:33:08  I0929 17:33:08.680240 3747840 credentials.hpp:86] Loading credential for authentication from '/var/folders/6w/rw03zh013y38ys6cyn8qppf80000gn/T/bool_MasterTestPrePostReservationRefinement_ReserveAndUnreserveResourcesV1_0_b1Sm7Z/credential'_x000D_
00:33:08  I0929 17:33:08.680353 3747840 slave.cpp:287] Agent using credential for: test-principal_x000D_
00:33:08  I0929 17:33:08.680364 3747840 credentials.hpp:37] Loading credentials for authentication from '/var/folders/6w/rw03zh013y38ys6cyn8qppf80000gn/T/bool_MasterTestPrePostReservationRefinement_ReserveAndUnreserveResourcesV1_0_b1Sm7Z/http_credentials'_x000D_
00:33:08  I0929 17:33:08.680552 3747840 http.cpp:1045] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-executor'_x000D_
00:33:08  I0929 17:33:08.680621 3747840 http.cpp:1066] Creating default 'jwt' HTTP authenticator for realm 'mesos-agent-executor'_x000D_
00:33:08  I0929 17:33:08.680690 3747840 http.cpp:1045] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-readonly'_x000D_
00:33:08  I0929 17:33:08.680723 3747840 http.cpp:1066] Creating default 'jwt' HTTP authenticator for realm 'mesos-agent-readonly'_x000D_
00:33:08  I0929 17:33:08.680783 3747840 http.cpp:1045] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-readwrite'_x000D_
00:33:08  I0929 17:33:08.680831 3747840 http.cpp:1066] Creating default 'jwt' HTTP authenticator for realm 'mesos-agent-readwrite'_x000D_
00:33:08  I0929 17:33:08.681946 3747840 slave.cpp:585] Agent resources: [{""name"":""cpus"",""scalar"":{""value"":2.0},""type"":""SCALAR""},{""name"":""mem"",""scalar"":{""value"":1024.0},""type"":""SCALAR""},{""name"":""disk"",""scalar"":{""value"":1024.0},""type"":""SCALAR""},{""name"":""ports"",""ranges"":{""range"":[{""begin"":31000,""end"":32000}]},""type"":""RANGES""}]_x000D_
00:33:08  I0929 17:33:08.682111 3747840 slave.cpp:593] Agent attributes: [  ]_x000D_
00:33:08  I0929 17:33:08.682119 3747840 slave.cpp:602] Agent hostname: 10.0.49.4_x000D_
00:33:08  I0929 17:33:08.682204 3211264 status_update_manager.cpp:177] Pausing sending status updates_x000D_
00:33:08  I0929 17:33:08.682720 2138112 state.cpp:64] Recovering state from '/var/folders/6w/rw03zh013y38ys6cyn8qppf80000gn/T/bool_MasterTestPrePostReservationRefinement_ReserveAndUnreserveResourcesV1_0_rx4DfE/meta'_x000D_
00:33:08  I0929 17:33:08.682868 3211264 status_update_manager.cpp:203] Recovering status update manager_x000D_
00:33:08  I0929 17:33:08.682950 4284416 containerizer.cpp:648] Recovering containerizer_x000D_
00:33:08  I0929 17:33:08.683574 528384 provisioner.cpp:416] Provisioner recovery complete_x000D_
00:33:08  I0929 17:33:08.683806 1064960 slave.cpp:6313] Finished recovery_x000D_
00:33:08  I0929 17:33:08.684301 1064960 slave.cpp:6495] Querying resource estimator for oversubscribable resources_x000D_
00:33:08  I0929 17:33:08.684377 2674688 slave.cpp:6509] Received oversubscribable resources {} from the resource estimator_x000D_
00:33:08  I0929 17:33:08.684469 3747840 status_update_manager.cpp:177] Pausing sending status updates_x000D_
00:33:08  I0929 17:33:08.684484 2674688 slave.cpp:993] New master detected at master@10.0.49.4:54887_x000D_
00:33:08  I0929 17:33:08.684527 2674688 slave.cpp:1028] Detecting new master_x000D_
00:33:08  I0929 17:33:08.688560 528384 slave.cpp:1055] Authenticating with master master@10.0.49.4:54887_x000D_
00:33:08  I0929 17:33:08.688627 528384 slave.cpp:1066] Using default CRAM-MD5 authenticatee_x000D_
00:33:08  I0929 17:33:08.688720 4284416 authenticatee.cpp:121] Creating new client SASL connection_x000D_
00:33:08  I0929 17:33:08.688834 1064960 master.cpp:7915] Authenticating slave(751)@10.0.49.4:54887_x000D_
00:33:08  I0929 17:33:08.688892 3211264 authenticator.cpp:414] Starting authentication session for crammd5-authenticatee(1388)@10.0.49.4:54887_x000D_
00:33:08  I0929 17:33:08.688968 1601536 authenticator.cpp:98] Creating new server SASL connection_x000D_
00:33:08  I0929 17:33:08.689050 3747840 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5_x000D_
00:33:08  I0929 17:33:08.689090 3747840 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'_x000D_
00:33:08  I0929 17:33:08.689149 2674688 authenticator.cpp:204] Received SASL authentication start_x000D_
00:33:08  I0929 17:33:08.689199 2674688 authenticator.cpp:326] Authentication requires more steps_x000D_
00:33:08  I0929 17:33:08.689280 2138112 authenticatee.cpp:259] Received SASL authentication step_x000D_
00:33:08  I0929 17:33:08.689344 528384 authenticator.cpp:232] Received SASL authentication step_x000D_
00:33:08  I0929 17:33:08.689388 528384 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'Jenkinss-Mac-mini.local' server FQDN: 'Jenkinss-Mac-mini.local' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false _x000D_
00:33:08  I0929 17:33:08.689400 528384 auxprop.cpp:181] Looking up auxiliary property '*userPassword'_x000D_
00:33:08  I0929 17:33:08.689424 528384 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'_x000D_
00:33:08  I0929 17:33:08.689436 528384 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'Jenkinss-Mac-mini.local' server FQDN: 'Jenkinss-Mac-mini.local' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true _x000D_
00:33:08  I0929 17:33:08.689445 528384 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true_x000D_
00:33:08  I0929 17:33:08.689450 528384 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true_x000D_
00:33:08  I0929 17:33:08.689458 528384 authenticator.cpp:318] Authentication success_x000D_
00:33:08  I0929 17:33:08.689518 4284416 authenticatee.cpp:299] Authentication success_x000D_
00:33:08  I0929 17:33:08.689532 1064960 master.cpp:7945] Successfully authenticated principal 'test-principal' at slave(751)@10.0.49.4:54887_x000D_
00:33:08  I0929 17:33:08.689558 3211264 authenticator.cpp:432] Authentication session cleanup for crammd5-authenticatee(1388)@10.0.49.4:54887_x000D_
00:33:08  I0929 17:33:08.689643 1601536 slave.cpp:1150] Successfully authenticated with master master@10.0.49.4:54887_x000D_
00:33:08  I0929 17:33:08.689721 1601536 slave.cpp:1629] Will retry registration in 8.587419ms if necessary_x000D_
00:33:08  I0929 17:33:08.689782 528384 master.cpp:5819] Received register agent message from slave(751)@10.0.49.4:54887 (10.0.49.4)_x000D_
00:33:08  I0929 17:33:08.689800 528384 master.cpp:3856] Authorizing agent with principal 'test-principal'_x000D_
00:33:08  I0929 17:33:08.689968 4284416 master.cpp:5879] Authorized registration of agent at slave(751)@10.0.49.4:54887 (10.0.49.4)_x000D_
00:33:08  I0929 17:33:08.690037 4284416 master.cpp:5972] Registering agent at slave(751)@10.0.49.4:54887 (10.0.49.4) with id 71fce4a3-01f6-43a7-b512-28980b04e51f-S0_x000D_
00:33:08  I0929 17:33:08.690204 3747840 registrar.cpp:495] Applied 1 operations in 43us; attempting to update the registry_x000D_
00:33:08  I0929 17:33:08.690408 528384 registrar.cpp:552] Successfully updated the registry in 168960ns_x000D_
00:33:08  I0929 17:33:08.690484 1064960 master.cpp:6019] Admitted agent 71fce4a3-01f6-43a7-b512-28980b04e51f-S0 at slave(751)@10.0.49.4:54887 (10.0.49.4)_x000D_
00:33:08  I0929 17:33:08.690712 2674688 slave.cpp:4969] Received ping from slave-observer(678)@10.0.49.4:54887_x000D_
00:33:08  I0929 17:33:08.690850 2674688 slave.cpp:1196] Registered with master master@10.0.49.4:54887; given agent ID 71fce4a3-01f6-43a7-b512-28980b04e51f-S0_x000D_
00:33:08  I0929 17:33:08.690891 3747840 hierarchical.cpp:593] Added agent 71fce4a3-01f6-43a7-b512-28980b04e51f-S0 (10.0.49.4) with cpus:2; mem:1024; disk:1024; ports:[31000-32000] (allocated: {})_x000D_
00:33:08  I0929 17:33:08.690696 1064960 master.cpp:6050] Registered agent 71fce4a3-01f6-43a7-b512-28980b04e51f-S0 at slave(751)@10.0.49.4:54887 (10.0.49.4) with [{""name"":""cpus"",""scalar"":{""value"":2.0},""type"":""SCALAR""},{""name"":""mem"",""scalar"":{""value"":1024.0},""type"":""SCALAR""},{""name"":""disk"",""scalar"":{""value"":1024.0},""type"":""SCALAR""},{""name"":""ports"",""ranges"":{""range"":[{""begin"":31000,""end"":32000}]},""type"":""RANGES""}]_x000D_
00:33:08  I0929 17:33:08.690943 528384 status_update_manager.cpp:184] Resuming sending status updates_x000D_
00:33:08  I0929 17:33:08.691010 3747840 hierarchical.cpp:1943] No allocations performed_x000D_
00:33:08  I0929 17:33:08.691033 3747840 hierarchical.cpp:1486] Performed allocation for 1 agents in 74us_x000D_
00:33:08  I0929 17:33:08.692587 4284416 process.cpp:3929] Handling HTTP event for process 'master' with path: '/master/api/v1'_x000D_
00:33:08  I0929 17:33:08.693039 3211264 http.cpp:1185] HTTP POST for /master/api/v1 from 10.0.49.4:57137_x000D_
00:33:08  I0929 17:33:08.693147 2674688 slave.cpp:1216] Checkpointing SlaveInfo to '/var/folders/6w/rw03zh013y38ys6cyn8qppf80000gn/T/bool_MasterTestPrePostReservationRefinement_ReserveAndUnreserveResourcesV1_0_rx4DfE/meta/slaves/71fce4a3-01f6-43a7-b512-28980b04e51f-S0/slave.info'_x000D_
00:33:08  I0929 17:33:08.693150 3211264 http.cpp:673] Processing call RESERVE_RESOURCES_x000D_
00:33:08  I0929 17:33:08.693291 3211264 master.cpp:3641] Authorizing principal 'test-principal' to reserve resources '[{""name"":""cpus"",""reservations"":[{""principal"":""test-principal"",""role"":""default-role"",""type"":""DYNAMIC""}],""scalar"":{""value"":1.0},""type"":""SCALAR""},{""name"":""mem"",""reservations"":[{""principal"":""test-principal"",""role"":""default-role"",""type"":""DYNAMIC""}],""scalar"":{""value"":512.0},""type"":""SCALAR""}]'_x000D_
00:33:08  I0929 17:33:08.693671 2674688 slave.cpp:1265] Forwarding total oversubscribed resources {}_x000D_
00:33:08  I0929 17:33:08.693742 1064960 master.cpp:6814] Received update of agent 71fce4a3-01f6-43a7-b512-28980b04e51f-S0 at slave(751)@10.0.49.4:54887 (10.0.49.4) with total oversubscribed resources {}_x000D_
00:33:08  I0929 17:33:08.694505 528384 master.cpp:9314] Sending updated checkpointed resources cpus(reservations: [(DYNAMIC,default-role,test-principal)]):1; mem(reservations: [(DYNAMIC,default-role,test-principal)]):512 to agent 71fce4a3-01f6-43a7-b512-28980b04e51f-S0 at slave(751)@10.0.49.4:54887 (10.0.49.4)_x000D_
00:33:08  I0929 17:33:08.694834 2138112 hierarchical.cpp:660] Agent 71fce4a3-01f6-43a7-b512-28980b04e51f-S0 (10.0.49.4) updated with total resources cpus:2; mem:1024; disk:1024; ports:[31000-32000]_x000D_
00:33:08  I0929 17:33:08.694988 2138112 hierarchical.cpp:1943] No allocations performed_x000D_
00:33:08  I0929 17:33:08.695013 2138112 hierarchical.cpp:1486] Performed allocation for 1 agents in 80us_x000D_
00:33:08  I0929 17:33:08.695297 3211264 slave.cpp:3522] Updated checkpointed resources from {} to cpus(reservations: [(DYNAMIC,default-role,test-principal)]):1; mem(reservations: [(DYNAMIC,default-role,test-principal)]):512_x000D_
00:33:08  I0929 17:33:08.696081 1601536 process.cpp:3929] Handling HTTP event for process 'master' with path: '/master/api/v1'_x000D_
00:33:08  I0929 17:33:08.696606 528384 http.cpp:1185] HTTP POST for /master/api/v1 from 10.0.49.4:57138_x000D_
00:33:08  I0929 17:33:08.696708 528384 http.cpp:673] Processing call UNRESERVE_RESOURCES_x000D_
00:33:08  I0929 17:33:08.696887 528384 master.cpp:3709] Authorizing principal 'test-principal' to unreserve resources '[{""name"":""cpus"",""reservations"":[{""principal"":""test-principal"",""role"":""default-role"",""type"":""DYNAMIC""}],""scalar"":{""value"":1.0},""type"":""SCALAR""},{""name"":""mem"",""reservations"":[{""principal"":""test-principal"",""role"":""default-role"",""type"":""DYNAMIC""}],""scalar"":{""value"":512.0},""type"":""SCALAR""}]'_x000D_
00:33:08  ../../src/tests/master_tests.cpp:8682: Failure_x000D_
00:33:08  Value of: (v1UnreserveResourcesResponse).get().status_x000D_
00:33:08    Actual: ""409 Conflict""_x000D_
00:33:08  Expected: Accepted().status_x000D_
00:33:08  Which is: ""202 Accepted""_x000D_
00:33:08  I0929 17:33:08.698737 2067726336 slave.cpp:869] Agent terminating_x000D_
00:33:08  I0929 17:33:08.698833 1601536 master.cpp:1321] Agent 71fce4a3-01f6-43a7-b512-28980b04e51f-S0 at slave(751)@10.0.49.4:54887 (10.0.49.4) disconnected_x000D_
00:33:08  I0929 17:33:08.698854 1601536 master.cpp:3354] Disconnecting agent 71fce4a3-01f6-43a7-b512-28980b04e51f-S0 at slave(751)@10.0.49.4:54887 (10.0.49.4)_x000D_
00:33:08  I0929 17:33:08.698876 1601536 master.cpp:3373] Deactivating agent 71fce4a3-01f6-43a7-b512-28980b04e51f-S0 at slave(751)@10.0.49.4:54887 (10.0.49.4)_x000D_
00:33:08  I0929 17:33:08.698952 2138112 hierarchical.cpp:690] Agent 71fce4a3-01f6-43a7-b512-28980b04e51f-S0 deactivated_x000D_
00:33:08  I0929 17:33:08.701202 2067726336 master.cpp:1163] Master terminating_x000D_
00:33:08  I0929 17:33:08.701479 1601536 hierarchical.cpp:626] Removed agent 71fce4a3-01f6-43a7-b512-28980b04e51f-S0_x000D_
00:33:08  [  FAILED  ] bool/MasterTestPrePostReservationRefinement.ReserveAndUnreserveResourcesV1/0, where GetParam() = true (39 ms)_x000D_
{noformat}",2.0,1.5.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.3717948717948718,0.24848484848484845,0.24848484848484845,0.963302752293578
Task,Add Mesos CLI command to list agents,We should have a command listing the agents in a Mesos cluster.,2.0,0,0.0,0.25037707390648567,0.3333333333333333,0.5,0.14285714285714285,0.5,0.0,0.2631578947368421,0.29285714285714287,0.10256410256410256,0.1515151515151515,0.1515151515151515,0.0
Improvement,Update HTTP scheduler library to allow for modularized authenticatee.,Allow the scheduler library to load an HTTP authenticatee module providing custom mechanisms for authentication.,3.0,0,0.0,0.3740573152337858,0.0,0.0,0.0,0.2,0.4,0.43859649122807015,0.2714285714285714,0.3717948717948718,0.24848484848484845,0.24848484848484845,0.0
Improvement,Introduce a basic HTTP authenticatee.,Refactor the hardcoded basic HTTP authentication code from within the scheduler library into the (modularized) interface provided by MESOS-8016,2.0,0,0.0,0.3740573152337858,0.0,0.0,0.0,0.2,0.4,0.43859649122807015,0.2714285714285714,0.3717948717948718,0.24848484848484845,0.24848484848484845,0.0
Improvement,Introduce modularized HTTP authenticatee.,"Define the implementation of a modularized interface for the scheduler library authentication, providing the means of an authenticatee. This interface will allow consumers of HTTP APIs to use replaceable authentication mechanisms via a defined interface.",2.0,0,0.0,0.3740573152337858,0.0,0.0,0.0,0.2,0.4,0.43859649122807015,0.2714285714285714,0.3717948717948718,0.24848484848484845,0.24848484848484845,0.0
Improvement,Design a scheduler (V1) HTTP API authenticatee mechanism.,Provide a design proposal for a scheduler HTTP API authenticatee module.,2.0,0,0.0,0.3740573152337858,0.0,0.0,0.0,0.2,0.4,0.43859649122807015,0.2714285714285714,0.3717948717948718,0.24848484848484845,0.24848484848484845,0.0
Improvement,Support Znode paths for masters in the new CLI,Right now the new Mesos CLI only works in single master mode with a single master IP and port. We should add support for finding the mesos leader in HA mode by hitting a set of zk instances similar to how {{mesos-resolve}} works.,3.0,0,0.5,0.25037707390648567,0.3333333333333333,0.5,0.14285714285714285,0.5,0.0,0.2631578947368421,0.29285714285714287,0.3205128205128205,0.6,0.6,0.0
Bug,PersistentVolumeEndpointsTest.SlavesEndpointFullResources is flaky.,"Observed on internal CI:_x000D_
{noformat}_x000D_
../../src/tests/persistent_volume_endpoints_tests.cpp:1952_x000D_
Value of: (response).get().status_x000D_
  Actual: ""409 Conflict""_x000D_
Expected: Accepted().status_x000D_
Which is: ""202 Accepted""_x000D_
{noformat}_x000D_
Full log attached.",2.0,1.5.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.717948717948718,0.8242424242424242,0.7212121212121212,0.963302752293578
Bug,PersistentVolumeEndpointsTest.NoAuthentication is flaky.,"Observed a failure on internal CI:_x000D_
{noformat}_x000D_
../../src/tests/persistent_volume_endpoints_tests.cpp:1385_x000D_
Value of: (response).get().status_x000D_
  Actual: ""409 Conflict""_x000D_
Expected: Accepted().status_x000D_
Which is: ""202 Accepted""_x000D_
{noformat}_x000D_
Full log attached.",2.0,1.5.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.717948717948718,0.8242424242424242,0.7212121212121212,0.963302752293578
Bug,PersistentVolumeEndpointsTest.UnreserveVolumeResources is flaky.,"Observed a failure on the internal CI:_x000D_
{noformat}_x000D_
../../src/tests/persistent_volume_endpoints_tests.cpp:450_x000D_
Value of: (response).get().status_x000D_
  Actual: ""409 Conflict""_x000D_
Expected: Accepted().status_x000D_
Which is: ""202 Accepted""_x000D_
{noformat}_x000D_
Full log attached.",2.0,1.5.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.717948717948718,0.8242424242424242,0.7212121212121212,0.963302752293578
Bug,"fatal, check failed !framework->recovered()","mesos master crashed on what appears to be framework recovery_x000D_
_x000D_
mesos master version: 1.3.1_x000D_
mesos agent version: 1.3.1_x000D_
_x000D_
{code}_x000D_
W0920 14:58:54.756364 25452 master.cpp:7568] Task 862181ec-dffb-4c03-8807-5fb4c4e9a907 of framework 889aae9d-1aab-4268-ba42-9d5c2461d871 unknown to the agent a498d458-bbca-426e-b076-b328f5b035da-S5225 at slave(1)_x000D_
@10.0.239.217:5051 (ip-10-0-239-217) during re-registration: reconciling with the agent_x000D_
W0920 14:58:54.756369 25452 master.cpp:7568] Task 9c21c48a-63ad-4d58-9e22-f720af19a644 of framework 889aae9d-1aab-4268-ba42-9d5c2461d871 unknown to the agent a498d458-bbca-426e-b076-b328f5b035da-S5225 at slave(1)_x000D_
@10.0.239.217:5051 (ip-10-0-239-217) during re-registration: reconciling with the agent_x000D_
W0920 14:58:54.756376 25452 master.cpp:7568] Task 05c451f8-c48a-47bd-a235-0ceb9b3f8d0c of framework 889aae9d-1aab-4268-ba42-9d5c2461d871 unknown to the agent a498d458-bbca-426e-b076-b328f5b035da-S5225 at slave(1)_x000D_
@10.0.239.217:5051 (ip-10-0-239-217) during re-registration: reconciling with the agent_x000D_
W0920 14:58:54.756381 25452 master.cpp:7568] Task e8641b1f-f67f-42fe-821c-09e5a290fc60 of framework 889aae9d-1aab-4268-ba42-9d5c2461d871 unknown to the agent a498d458-bbca-426e-b076-b328f5b035da-S5225 at slave(1)_x000D_
@10.0.239.217:5051 (ip-10-0-239-217) during re-registration: reconciling with the agent_x000D_
W0920 14:58:54.756386 25452 master.cpp:7568] Task f838a03c-5cd4-47eb-8606-69b004d89808 of framework 889aae9d-1aab-4268-ba42-9d5c2461d871 unknown to the agent a498d458-bbca-426e-b076-b328f5b035da-S5225 at slave(1)_x000D_
@10.0.239.217:5051 (ip-10-0-239-217) during re-registration: reconciling with the agent_x000D_
W0920 14:58:54.756392 25452 master.cpp:7568] Task 685ca5da-fa24-494d-a806-06e03bbf00bd of framework 889aae9d-1aab-4268-ba42-9d5c2461d871 unknown to the agent a498d458-bbca-426e-b076-b328f5b035da-S5225 at slave(1)_x000D_
@10.0.239.217:5051 (ip-10-0-239-217) during re-registration: reconciling with the agent_x000D_
W0920 14:58:54.756397 25452 master.cpp:7568] Task 65ccf39b-5c46-4121-9fdd-21570e8068e6 of framework 889aae9d-1aab-4268-ba42-9d5c2461d871 unknown to the agent a498d458-bbca-426e-b076-b328f5b035da-S5225 at slave(1)_x000D_
@10.0.239.217:5051 (ip-10-0-239-217) during re-registration: reconciling with the agent_x000D_
F0920 14:58:54.756404 25452 master.cpp:7601] Check failed: !framework->recovered()_x000D_
*** Check failure stack trace: ***_x000D_
    @     0x7f7bf80087ed  google::LogMessage::Fail()_x000D_
    @     0x7f7bf800a5a0  google::LogMessage::SendToLog()_x000D_
    @     0x7f7bf80083d3  google::LogMessage::Flush()_x000D_
    @     0x7f7bf800afc9  google::LogMessageFatal::~LogMessageFatal()_x000D_
    @     0x7f7bf736fe7e  mesos::internal::master::Master::reconcileKnownSlave()_x000D_
    @     0x7f7bf739e612  mesos::internal::master::Master::_reregisterSlave()_x000D_
    @     0x7f7bf73a580e  _ZNSt17_Function_handlerIFvPN7process11ProcessBaseEEZNS0_8dispatchIN5mesos8internal6master6MasterERKNS5_9SlaveInfoERKNS0_4UPIDERK6OptionINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIc_x000D_
EEEERKSt6vectorINS5_8ResourceESaISQ_EERKSP_INS5_12ExecutorInfoESaISV_EERKSP_INS5_4TaskESaIS10_EERKSP_INS5_13FrameworkInfoESaIS15_EERKSP_INS6_17Archive_FrameworkESaIS1A_EERKSL_RKSP_INS5_20SlaveInfo_CapabilityESaIS_x000D_
1H_EERKNS0_6FutureIbEES9_SC_SM_SS_SX_S12_S17_S1C_SL_S1J_S1N_EEvRKNS0_3PIDIT_EEMS1R_FvT0_T1_T2_T3_T4_T5_T6_T7_T8_T9_T10_ET11_T12_T13_T14_T15_T16_T17_T18_T19_T20_T21_EUlS2_E_E9_M_invokeERKSt9_Any_dataOS2__x000D_
    @     0x7f7bf7f5e69c  process::ProcessBase::visit()_x000D_
    @     0x7f7bf7f71403  process::ProcessManager::resume()_x000D_
    @     0x7f7bf7f7c127  _ZNSt6thread5_ImplISt12_Bind_simpleIFZN7process14ProcessManager12init_threadsEvEUt_vEEE6_M_runEv_x000D_
    @     0x7f7bf60b5c80  (unknown)_x000D_
    @     0x7f7bf58c86ba  start_thread_x000D_
    @     0x7f7bf55fe3dd  (unknown)_x000D_
mesos-master.service: Main process exited, code=killed, status=6/ABRT_x000D_
mesos-master.service: Unit entered failed state._x000D_
mesos-master.service: Failed with result 'signal'._x000D_
{code}_x000D_
_x000D_
The issue happened again on Mesos 1.5 (docker mesos master from the mesosphere docker repo):_x000D_
{code}_x000D_
Mar 11 10:04:33 research docker[4503]: I0311 10:04:33.815433    13 http.cpp:1185] HTTP POST for /master/api/v1/scheduler from 10.142.0.5:55133_x000D_
Mar 11 10:04:33 research docker[4503]: I0311 10:04:33.815588    13 master.cpp:5467] Processing DECLINE call for offers: [ 5e57f633-a69c-4009-b773-990b4b8984ad-O58323 ] for framework 5e57f633-a69c-4009-b7_x000D_
Mar 11 10:04:33 research docker[4503]: I0311 10:04:33.815693    13 master.cpp:10703] Removing offer 5e57f633-a69c-4009-b773-990b4b8984ad-O58323_x000D_
Mar 11 10:04:35 research docker[4503]: I0311 10:04:35.820142    10 master.cpp:8227] Marking agent 5e57f633-a69c-4009-b773-990b4b8984ad-S49 at slave(1)@10.142.0.10:5051 (tf-mesos-agent-t7c8.c.bitcoin-engi_x000D_
Mar 11 10:04:35 research docker[4503]: I0311 10:04:35.820367    10 registrar.cpp:495] Applied 1 operations in 86528ns; attempting to update the registry_x000D_
Mar 11 10:04:35 research docker[4503]: I0311 10:04:35.820572    10 registrar.cpp:552] Successfully updated the registry in 175872ns_x000D_
Mar 11 10:04:35 research docker[4503]: I0311 10:04:35.820642    11 master.cpp:8275] Marked agent 5e57f633-a69c-4009-b773-990b4b8984ad-S49 at slave(1)@10.142.0.10:5051 (tf-mesos-agent-t7c8.c.bitcoin-engin_x000D_
Mar 11 10:04:35 research docker[4503]: I0311 10:04:35.820957     9 hierarchical.cpp:609] Removed agent 5e57f633-a69c-4009-b773-990b4b8984ad-S49_x000D_
Mar 11 10:04:35 research docker[4503]: F0311 10:04:35.851961    11 master.cpp:10018] Check failed: 'framework' Must be non NULL_x000D_
Mar 11 10:04:35 research docker[4503]: *** Check failure stack trace: ***_x000D_
Mar 11 10:04:36 research docker[4503]: @     0x7f96c6044a7d  google::LogMessage::Fail()_x000D_
Mar 11 10:04:36 research docker[4503]: @     0x7f96c6046830  google::LogMessage::SendToLog()_x000D_
Mar 11 10:04:36 research docker[4503]: @     0x7f96c6044663  google::LogMessage::Flush()_x000D_
Mar 11 10:04:36 research docker[4503]: @     0x7f96c6047259  google::LogMessageFatal::~LogMessageFatal()_x000D_
Mar 11 10:04:36 research docker[4503]: @     0x7f96c5258e14  google::CheckNotNull<>()_x000D_
Mar 11 10:04:36 research docker[4503]: @     0x7f96c521dfc8  mesos::internal::master::Master::__removeSlave()_x000D_
Mar 11 10:04:36 research docker[4503]: @     0x7f96c521f1a2  mesos::internal::master::Master::_markUnreachable()_x000D_
Mar 11 10:04:36 research docker[4503]: @     0x7f96c5f98f11  process::ProcessBase::consume()_x000D_
Mar 11 10:04:36 research docker[4503]: @     0x7f96c5fb2a4a  process::ProcessManager::resume()_x000D_
Mar 11 10:04:36 research docker[4503]: @     0x7f96c5fb65d6  _ZNSt6thread5_ImplISt12_Bind_simpleIFZN7process14ProcessManager12init_threadsEvEUlvE_vEEE6_M_runEv_x000D_
Mar 11 10:04:36 research docker[4503]: @     0x7f96c35d4c80  (unknown)_x000D_
Mar 11 10:04:36 research docker[4503]: @     0x7f96c2de76ba  start_thread_x000D_
Mar 11 10:04:36 research docker[4503]: @     0x7f96c2b1d41d  (unknown)_x000D_
Mar 11 10:04:36 research docker[4503]: *** Aborted at 1520762676 (unix time) try ""date -d @1520762676"" if you are using GNU date ***_x000D_
Mar 11 10:04:36 research docker[4503]: PC: @     0x7f96c2a4d196 (unknown)_x000D_
Mar 11 10:04:36 research docker[4503]: *** SIGSEGV (@0x0) received by PID 1 (TID 0x7f96b986d700) from PID 0; stack trace: ***_x000D_
Mar 11 10:04:36 research docker[4503]: @     0x7f96c2df1390 (unknown)_x000D_
Mar 11 10:04:36 research docker[4503]: @     0x7f96c2a4d196 (unknown)_x000D_
Mar 11 10:04:36 research docker[4503]: @     0x7f96c604ce2c google::DumpStackTraceAndExit()_x000D_
Mar 11 10:04:36 research docker[4503]: @     0x7f96c6044a7d google::LogMessage::Fail()_x000D_
Mar 11 10:04:36 research docker[4503]: @     0x7f96c6046830 google::LogMessage::SendToLog()_x000D_
Mar 11 10:04:36 research docker[4503]: @     0x7f96c6044663 google::LogMessage::Flush()_x000D_
Mar 11 10:04:36 research docker[4503]: @     0x7f96c6047259 google::LogMessageFatal::~LogMessageFatal()_x000D_
Mar 11 10:04:36 research docker[4503]: @     0x7f96c5258e14 google::CheckNotNull<>()_x000D_
Mar 11 10:04:36 research docker[4503]: @     0x7f96c521dfc8 mesos::internal::master::Master::__removeSlave()_x000D_
Mar 11 10:04:36 research docker[4503]: @     0x7f96c521f1a2 mesos::internal::master::Master::_markUnreachable()_x000D_
Mar 11 10:04:36 research docker[4503]: @     0x7f96c5f98f11 process::ProcessBase::consume()_x000D_
Mar 11 10:04:36 research docker[4503]: @     0x7f96c5fb2a4a process::ProcessManager::resume()_x000D_
Mar 11 10:04:36 research docker[4503]: @     0x7f96c5fb65d6 _ZNSt6thread5_ImplISt12_Bind_simpleIFZN7process14ProcessManager12init_threadsEvEUlvE_vEEE6_M_runEv_x000D_
Mar 11 10:04:36 research docker[4503]: @     0x7f96c35d4c80 (unknown)_x000D_
Mar 11 10:04:36 research docker[4503]: @     0x7f96c2de76ba start_thread_x000D_
Mar 11 10:04:36 research docker[4503]: @     0x7f96c2b1d41d (unknown)_x000D_
Mar 11 10:04:38 research systemd[1]: mesos-master2.service: main process exited, code=exited, status=139/n/a_x000D_
Mar 11 10:04:38 research docker[18886]: mesos-master_x000D_
Mar 11 10:04:38 research systemd[1]: Unit mesos-master2.service entered failed state._x000D_
{code}_x000D_
The failure in this case seems to happen right after an agent drops out of the cluster - which is a similar failure condition to the first time.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01282051282051282,0.0,0.0,0.0
Task,Activate apache/mesos org on bintray,This would allow us to host community-managed rpm/deb packages.,3.0,0,0.0,0.3453996983408748,0.0,0.0,0.0,0.0,0.2,0.38596491228070173,0.35714285714285715,0.28205128205128205,0.29090909090909095,0.28484848484848485,0.0
Bug,Stout fails to compile with libc >= 2.26.,"Glibc 2.26 removes ""xlocale.h"" [1] which makes stout fail to compile. Stout should be using 'locale.h' instead.


[1]: https://sourceware.org/glibc/wiki/Release/2.26#Removal_of_.27xlocale.h.27",2.0,0,0.0,0.3453996983408748,0.0,0.0,0.0,0.0,0.2,0.38596491228070173,0.35714285714285715,0.28205128205128205,0.29090909090909095,0.28484848484848485,0.0
Bug,Lint javascript files to enable linting.,"To enable the linting of our javascript codebase, the javascript files should first be linted so that new commits will not have to include fixes for current issues.",1.0,0,0.0,0.25037707390648567,0.3333333333333333,0.5,0.14285714285714285,0.5,0.0,0.2631578947368421,0.29285714285714287,0.10256410256410256,0.1515151515151515,0.1515151515151515,0.0
Bug,SlaveTest.HTTPSchedulerSlaveRestart test is flaky.,"Saw this on ASF CI when testing 1.4.0-rc5_x000D_
_x000D_
{code}_x000D_
[ RUN      ] SlaveTest.HTTPSchedulerSlaveRestart_x000D_
I0912 05:40:15.280185 32547 cluster.cpp:162] Creating default 'local' authorizer_x000D_
I0912 05:40:15.282783 32554 master.cpp:442] Master c23ff8cf-cb2f-40d0-8f18-871a41f128cf (b909d5e22907) started on 172.17.0.2:58922_x000D_
I0912 05:40:15.282804 32554 master.cpp:444] Flags at startup: --acls="""" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate_agents=""true"" --authenticate_frameworks=""true"" --authenticate_http_frameworks=""true"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/he1E9j/credentials"" --filter_gpu_resources=""true"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_unreachable_tasks_per_framework=""1000"" --port=""5050"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""in_memory"" --registry_fetch_timeout=""1mins"" --registry_gc_interval=""15mins"" --registry_max_agent_age=""2weeks"" --registry_max_agent_count=""102400"" --registry_store_timeout=""100secs"" --registry_strict=""false"" --root_submissions=""true"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/he1E9j/master"" --zk_session_timeout=""10secs""_x000D_
I0912 05:40:15.283092 32554 master.cpp:494] Master only allowing authenticated frameworks to register_x000D_
I0912 05:40:15.283110 32554 master.cpp:508] Master only allowing authenticated agents to register_x000D_
I0912 05:40:15.283118 32554 master.cpp:521] Master only allowing authenticated HTTP frameworks to register_x000D_
I0912 05:40:15.283123 32554 credentials.hpp:37] Loading credentials for authentication from '/tmp/he1E9j/credentials'_x000D_
I0912 05:40:15.283394 32554 master.cpp:566] Using default 'crammd5' authenticator_x000D_
I0912 05:40:15.283543 32554 http.cpp:1026] Creating default 'basic' HTTP authenticator for realm 'mesos-master-readonly'_x000D_
I0912 05:40:15.283731 32554 http.cpp:1026] Creating default 'basic' HTTP authenticator for realm 'mesos-master-readwrite'_x000D_
I0912 05:40:15.283887 32554 http.cpp:1026] Creating default 'basic' HTTP authenticator for realm 'mesos-master-scheduler'_x000D_
I0912 05:40:15.284021 32554 master.cpp:646] Authorization enabled_x000D_
I0912 05:40:15.284293 32552 whitelist_watcher.cpp:77] No whitelist given_x000D_
I0912 05:40:15.284335 32550 hierarchical.cpp:171] Initialized hierarchical allocator process_x000D_
I0912 05:40:15.287078 32561 master.cpp:2163] Elected as the leading master!_x000D_
I0912 05:40:15.287103 32561 master.cpp:1702] Recovering from registrar_x000D_
I0912 05:40:15.287214 32557 registrar.cpp:347] Recovering registrar_x000D_
I0912 05:40:15.287703 32557 registrar.cpp:391] Successfully fetched the registry (0B) in 455936ns_x000D_
I0912 05:40:15.287791 32557 registrar.cpp:495] Applied 1 operations in 24179ns; attempting to update the registry_x000D_
I0912 05:40:15.288317 32557 registrar.cpp:552] Successfully updated the registry in 473088ns_x000D_
I0912 05:40:15.288435 32557 registrar.cpp:424] Successfully recovered registrar_x000D_
I0912 05:40:15.288789 32548 master.cpp:1801] Recovered 0 agents from the registry (129B); allowing 10mins for agents to re-register_x000D_
I0912 05:40:15.288822 32559 hierarchical.cpp:209] Skipping recovery of hierarchical allocator: nothing to recover_x000D_
I0912 05:40:15.292457 32547 containerizer.cpp:246] Using isolation: posix/cpu,posix/mem,filesystem/posix,network/cni,environment_secret_x000D_
W0912 05:40:15.293053 32547 backend.cpp:76] Failed to create 'aufs' backend: AufsBackend requires root privileges_x000D_
W0912 05:40:15.293184 32547 backend.cpp:76] Failed to create 'bind' backend: BindBackend requires root privileges_x000D_
I0912 05:40:15.293220 32547 provisioner.cpp:255] Using default backend 'copy'_x000D_
W0912 05:40:15.297993 32547 process.cpp:3196] Attempted to spawn already running process files@172.17.0.2:58922_x000D_
I0912 05:40:15.298338 32547 cluster.cpp:448] Creating default 'local' authorizer_x000D_
I0912 05:40:15.300554 32551 slave.cpp:250] Mesos agent started on (198)@172.17.0.2:58922_x000D_
I0912 05:40:15.300576 32551 slave.cpp:251] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/SlaveTest_HTTPSchedulerSlaveRestart_n3xE7x/store/appc"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""true"" --authenticatee=""crammd5"" --authentication_backoff_factor=""1secs"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/SlaveTest_HTTPSchedulerSlaveRestart_n3xE7x/credential"" --default_role=""*"" --disallow_sharing_agent_pid_namespace=""false"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/SlaveTest_HTTPSchedulerSlaveRestart_n3xE7x/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_reregistration_timeout=""2:_x000D_
secs"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/SlaveTest_HTTPSchedulerSlaveRestart_n3xE7x/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_command_executor=""false"" --http_credentials=""/tmp/SlaveTest_HTTPSchedulerSlaveRestart_n3xE7x/http_credentials"" --http_heartbeat_interval=""30secs"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher=""posix"" --launcher_dir=""/mesos/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_executors_per_framework=""150"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --port=""5051"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --runtime_dir=""/tmp/SlaveTest_HTTPSchedulerSlaveRestart_n3xE7x"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/SlaveTest_HTTPSchedulerSlaveRestart_68fE8V""_x000D_
I0912 05:40:15.301059 32551 credentials.hpp:86] Loading credential for authentication from '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_n3xE7x/credential'_x000D_
W0912 05:40:15.301174 32547 process.cpp:3196] Attempted to spawn already running process version@172.17.0.2:58922_x000D_
I0912 05:40:15.301239 32551 slave.cpp:283] Agent using credential for: test-principal_x000D_
I0912 05:40:15.301256 32551 credentials.hpp:37] Loading credentials for authentication from '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_n3xE7x/http_credentials'_x000D_
I0912 05:40:15.301512 32551 http.cpp:1026] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-readonly'_x000D_
I0912 05:40:15.301681 32551 http.cpp:1026] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-readwrite'_x000D_
I0912 05:40:15.301935 32547 sched.cpp:232] Version: 1.4.0_x000D_
I0912 05:40:15.302479 32557 sched.cpp:336] New master detected at master@172.17.0.2:58922_x000D_
I0912 05:40:15.302592 32557 sched.cpp:407] Authenticating with master master@172.17.0.2:58922_x000D_
I0912 05:40:15.302614 32557 sched.cpp:414] Using default CRAM-MD5 authenticatee_x000D_
I0912 05:40:15.302922 32553 authenticatee.cpp:121] Creating new client SASL connection_x000D_
I0912 05:40:15.303220 32562 master.cpp:7832] Authenticating scheduler-228abf3d-36ea-4900-94a1-8b18d253716c@172.17.0.2:58922_x000D_
I0912 05:40:15.303400 32556 authenticator.cpp:414] Starting authentication session for crammd5-authenticatee(406)@172.17.0.2:58922_x000D_
I0912 05:40:15.303673 32554 authenticator.cpp:98] Creating new server SASL connection_x000D_
I0912 05:40:15.303473 32551 slave.cpp:565] Agent resources: [{""name"":""cpus"",""scalar"":{""value"":2.0},""type"":""SCALAR""},{""name"":""mem"",""scalar"":{""value"":1024.0},""type"":""SCALAR""},{""name"":""disk"",""scalar"":{""value"":1024.0},""type"":""SCALAR""},{""name"":""ports"",""ranges"":{""range"":[{""begin"":31000,""end"":32000}]},""type"":""RANGES""}]_x000D_
I0912 05:40:15.303707 32551 slave.cpp:573] Agent attributes: [  ]_x000D_
I0912 05:40:15.303717 32551 slave.cpp:582] Agent hostname: b909d5e22907_x000D_
I0912 05:40:15.303900 32559 status_update_manager.cpp:177] Pausing sending status updates_x000D_
I0912 05:40:15.304033 32548 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5_x000D_
I0912 05:40:15.304070 32548 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'_x000D_
I0912 05:40:15.304189 32548 authenticator.cpp:204] Received SASL authentication start_x000D_
I0912 05:40:15.304265 32548 authenticator.cpp:326] Authentication requires more steps_x000D_
I0912 05:40:15.304404 32561 authenticatee.cpp:259] Received SASL authentication step_x000D_
I0912 05:40:15.304566 32549 authenticator.cpp:232] Received SASL authentication step_x000D_
I0912 05:40:15.304603 32549 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'b909d5e22907' server FQDN: 'b909d5e22907' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false _x000D_
I0912 05:40:15.304615 32549 auxprop.cpp:181] Looking up auxiliary property '*userPassword'_x000D_
I0912 05:40:15.304647 32549 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'_x000D_
I0912 05:40:15.304671 32549 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'b909d5e22907' server FQDN: 'b909d5e22907' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true _x000D_
I0912 05:40:15.304682 32549 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true_x000D_
I0912 05:40:15.304697 32549 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true_x000D_
I0912 05:40:15.304715 32549 authenticator.cpp:318] Authentication success_x000D_
I0912 05:40:15.304852 32563 authenticatee.cpp:299] Authentication success_x000D_
I0912 05:40:15.304916 32552 master.cpp:7862] Successfully authenticated principal 'test-principal' at scheduler-228abf3d-36ea-4900-94a1-8b18d253716c@172.17.0.2:58922_x000D_
I0912 05:40:15.305004 32557 authenticator.cpp:432] Authentication session cleanup for crammd5-authenticatee(406)@172.17.0.2:58922_x000D_
I0912 05:40:15.305253 32549 sched.cpp:513] Successfully authenticated with master master@172.17.0.2:58922_x000D_
I0912 05:40:15.305269 32549 sched.cpp:836] Sending SUBSCRIBE call to master@172.17.0.2:58922_x000D_
I0912 05:40:15.305433 32549 sched.cpp:869] Will retry registration in 237.896638ms if necessary_x000D_
I0912 05:40:15.305629 32555 state.cpp:64] Recovering state from '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_68fE8V/meta'_x000D_
I0912 05:40:15.305652 32559 master.cpp:2894] Received SUBSCRIBE call for framework 'default' at scheduler-228abf3d-36ea-4900-94a1-8b18d253716c@172.17.0.2:58922_x000D_
I0912 05:40:15.305742 32559 master.cpp:2228] Authorizing framework principal 'test-principal' to receive offers for roles '{ * }'_x000D_
I0912 05:40:15.305963 32560 status_update_manager.cpp:203] Recovering status update manager_x000D_
I0912 05:40:15.306152 32550 containerizer.cpp:609] Recovering containerizer_x000D_
I0912 05:40:15.306252 32553 master.cpp:2974] Subscribing framework default with checkpointing enabled and capabilities [ RESERVATION_REFINEMENT ]_x000D_
I0912 05:40:15.306928 32559 sched.cpp:759] Framework registered with c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000_x000D_
I0912 05:40:15.307013 32559 sched.cpp:773] Scheduler::registered took 58136ns_x000D_
I0912 05:40:15.307162 32552 hierarchical.cpp:303] Added framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000_x000D_
I0912 05:40:15.307384 32552 hierarchical.cpp:1925] No allocations performed_x000D_
I0912 05:40:15.307423 32552 hierarchical.cpp:2015] No inverse offers to send out!_x000D_
I0912 05:40:15.307464 32552 hierarchical.cpp:1468] Performed allocation for 0 agents in 124365ns_x000D_
I0912 05:40:15.308010 32557 provisioner.cpp:416] Provisioner recovery complete_x000D_
I0912 05:40:15.308349 32556 slave.cpp:6295] Finished recovery_x000D_
I0912 05:40:15.308863 32556 slave.cpp:6477] Querying resource estimator for oversubscribable resources_x000D_
I0912 05:40:15.309139 32562 slave.cpp:6491] Received oversubscribable resources {} from the resource estimator_x000D_
I0912 05:40:15.309347 32562 slave.cpp:971] New master detected at master@172.17.0.2:58922_x000D_
I0912 05:40:15.309409 32550 status_update_manager.cpp:177] Pausing sending status updates_x000D_
I0912 05:40:15.309500 32562 slave.cpp:1006] Detecting new master_x000D_
I0912 05:40:15.311897 32559 slave.cpp:1033] Authenticating with master master@172.17.0.2:58922_x000D_
I0912 05:40:15.311975 32559 slave.cpp:1044] Using default CRAM-MD5 authenticatee_x000D_
I0912 05:40:15.312253 32560 authenticatee.cpp:121] Creating new client SASL connection_x000D_
I0912 05:40:15.312513 32560 master.cpp:7832] Authenticating slave(198)@172.17.0.2:58922_x000D_
I0912 05:40:15.312654 32548 authenticator.cpp:414] Starting authentication session for crammd5-authenticatee(407)@172.17.0.2:58922_x000D_
I0912 05:40:15.312940 32558 authenticator.cpp:98] Creating new server SASL connection_x000D_
I0912 05:40:15.313187 32552 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5_x000D_
I0912 05:40:15.313213 32552 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'_x000D_
I0912 05:40:15.313313 32552 authenticator.cpp:204] Received SASL authentication start_x000D_
I0912 05:40:15.313364 32552 authenticator.cpp:326] Authentication requires more steps_x000D_
I0912 05:40:15.313478 32551 authenticatee.cpp:259] Received SASL authentication step_x000D_
I0912 05:40:15.313613 32553 authenticator.cpp:232] Received SASL authentication step_x000D_
I0912 05:40:15.313649 32553 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'b909d5e22907' server FQDN: 'b909d5e22907' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false _x000D_
I0912 05:40:15.313673 32553 auxprop.cpp:181] Looking up auxiliary property '*userPassword'_x000D_
I0912 05:40:15.313743 32553 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'_x000D_
I0912 05:40:15.313788 32553 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'b909d5e22907' server FQDN: 'b909d5e22907' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true _x000D_
I0912 05:40:15.313808 32553 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true_x000D_
I0912 05:40:15.313817 32553 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true_x000D_
I0912 05:40:15.313833 32553 authenticator.cpp:318] Authentication success_x000D_
I0912 05:40:15.313931 32557 authenticatee.cpp:299] Authentication success_x000D_
I0912 05:40:15.314019 32554 master.cpp:7862] Successfully authenticated principal 'test-principal' at slave(198)@172.17.0.2:58922_x000D_
I0912 05:40:15.314079 32553 authenticator.cpp:432] Authentication session cleanup for crammd5-authenticatee(407)@172.17.0.2:58922_x000D_
I0912 05:40:15.314239 32555 slave.cpp:1128] Successfully authenticated with master master@172.17.0.2:58922_x000D_
I0912 05:40:15.314457 32555 slave.cpp:1607] Will retry registration in 9.221574ms if necessary_x000D_
I0912 05:40:15.314672 32561 master.cpp:5714] Received register agent message from slave(198)@172.17.0.2:58922 (b909d5e22907)_x000D_
I0912 05:40:15.314810 32561 master.cpp:3803] Authorizing agent with principal 'test-principal'_x000D_
I0912 05:40:15.315261 32548 master.cpp:5774] Authorized registration of agent at slave(198)@172.17.0.2:58922 (b909d5e22907)_x000D_
I0912 05:40:15.315383 32548 master.cpp:5867] Registering agent at slave(198)@172.17.0.2:58922 (b909d5e22907) with id c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0_x000D_
I0912 05:40:15.315827 32558 registrar.cpp:495] Applied 1 operations in 55999ns; attempting to update the registry_x000D_
I0912 05:40:15.316412 32558 registrar.cpp:552] Successfully updated the registry in 528896ns_x000D_
I0912 05:40:15.316654 32557 master.cpp:5914] Admitted agent c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0 at slave(198)@172.17.0.2:58922 (b909d5e22907)_x000D_
I0912 05:40:15.317286 32554 slave.cpp:4970] Received ping from slave-observer(191)@172.17.0.2:58922_x000D_
I0912 05:40:15.317461 32554 slave.cpp:1174] Registered with master master@172.17.0.2:58922; given agent ID c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0_x000D_
I0912 05:40:15.317587 32553 status_update_manager.cpp:184] Resuming sending status updates_x000D_
I0912 05:40:15.317279 32557 master.cpp:5945] Registered agent c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0 at slave(198)@172.17.0.2:58922 (b909d5e22907) with [{""name"":""cpus"",""scalar"":{""value"":2.0},""type"":""SCALAR""},{""name"":""mem"",""scalar"":{""value"":1024.0},""type"":""SCALAR""},{""name"":""disk"",""scalar"":{""value"":1024.0},""type"":""SCALAR""},{""name"":""ports"",""ranges"":{""range"":[{""begin"":31000,""end"":32000}]},""type"":""RANGES""}]_x000D_
I0912 05:40:15.317819 32562 hierarchical.cpp:593] Added agent c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0 (b909d5e22907) with cpus:2; mem:1024; disk:1024; ports:[31000-32000] (allocated: {})_x000D_
I0912 05:40:15.317857 32554 slave.cpp:1194] Checkpointing SlaveInfo to '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_68fE8V/meta/slaves/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0/slave.info'_x000D_
I0912 05:40:15.318280 32554 slave.cpp:1243] Forwarding total oversubscribed resources {}_x000D_
I0912 05:40:15.318450 32554 master.cpp:6683] Received update of agent c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0 at slave(198)@172.17.0.2:58922 (b909d5e22907) with total oversubscribed resources {}_x000D_
I0912 05:40:15.319030 32562 hierarchical.cpp:2015] No inverse offers to send out!_x000D_
I0912 05:40:15.319090 32562 hierarchical.cpp:1468] Performed allocation for 1 agents in 1.101144ms_x000D_
I0912 05:40:15.319267 32562 hierarchical.cpp:660] Agent c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0 (b909d5e22907) updated with total resources cpus:2; mem:1024; disk:1024; ports:[31000-32000]_x000D_
I0912 05:40:15.319643 32555 master.cpp:7662] Sending 1 offers to framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 (default) at scheduler-228abf3d-36ea-4900-94a1-8b18d253716c@172.17.0.2:58922_x000D_
I0912 05:40:15.320127 32561 sched.cpp:933] Scheduler::resourceOffers took 109341ns_x000D_
I0912 05:40:15.322115 32550 master.cpp:9159] Removing offer c23ff8cf-cb2f-40d0-8f18-871a41f128cf-O0_x000D_
I0912 05:40:15.322265 32550 master.cpp:4153] Processing ACCEPT call for offers: [ c23ff8cf-cb2f-40d0-8f18-871a41f128cf-O0 ] on agent c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0 at slave(198)@172.17.0.2:58922 (b909d5e22907) for framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 (default) at scheduler-228abf3d-36ea-4900-94a1-8b18d253716c@172.17.0.2:58922_x000D_
I0912 05:40:15.322368 32550 master.cpp:3530] Authorizing framework principal 'test-principal' to launch task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec_x000D_
I0912 05:40:15.324560 32550 master.cpp:9719] Adding task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec with resources [{""allocation_info"":{""role"":""*""},""name"":""cpus"",""scalar"":{""value"":2.0},""type"":""SCALAR""},{""allocation_info"":{""role"":""*""},""name"":""mem"",""scalar"":{""value"":1024.0},""type"":""SCALAR""},{""allocation_info"":{""role"":""*""},""name"":""disk"",""scalar"":{""value"":1024.0},""type"":""SCALAR""},{""allocation_info"":{""role"":""*""},""name"":""ports"",""ranges"":{""range"":[{""begin"":31000,""end"":32000}]},""type"":""RANGES""}] on agent c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0 at slave(198)@172.17.0.2:58922 (b909d5e22907)_x000D_
I0912 05:40:15.325297 32550 master.cpp:4816] Launching task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 (default) at scheduler-228abf3d-36ea-4900-94a1-8b18d253716c@172.17.0.2:58922 with resources [{""allocation_info"":{""role"":""*""},""name"":""cpus"",""scalar"":{""value"":2.0},""type"":""SCALAR""},{""allocation_info"":{""role"":""*""},""name"":""mem"",""scalar"":{""value"":1024.0},""type"":""SCALAR""},{""allocation_info"":{""role"":""*""},""name"":""disk"",""scalar"":{""value"":1024.0},""type"":""SCALAR""},{""allocation_info"":{""role"":""*""},""name"":""ports"",""ranges"":{""range"":[{""begin"":31000,""end"":32000}]},""type"":""RANGES""}] on agent c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0 at slave(198)@172.17.0.2:58922 (b909d5e22907)_x000D_
I0912 05:40:15.327203 32560 slave.cpp:1736] Got assigned task '8fc99bc8-a2b6-498b-8bb2-af5d92e78cec' for framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000_x000D_
I0912 05:40:15.327380 32560 slave.cpp:7175] Checkpointing FrameworkInfo to '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_68fE8V/meta/slaves/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0/frameworks/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000/framework.info'_x000D_
I0912 05:40:15.327888 32560 slave.cpp:7186] Checkpointing framework pid '@0.0.0.0:0' to '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_68fE8V/meta/slaves/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0/frameworks/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000/framework.pid'_x000D_
I0912 05:40:15.327944 32550 hierarchical.cpp:887] Updated allocation of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 on agent c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0 from cpus(allocated: *):2; mem(allocated: *):1024; disk(allocated: *):1024; ports(allocated: *):[31000-32000] to cpus(allocated: *):2; mem(allocated: *):1024; disk(allocated: *):1024; ports(allocated: *):[31000-32000]_x000D_
I0912 05:40:15.328968 32560 slave.cpp:2003] Authorizing task '8fc99bc8-a2b6-498b-8bb2-af5d92e78cec' for framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000_x000D_
I0912 05:40:15.329071 32560 slave.cpp:6794] Authorizing framework principal 'test-principal' to launch task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec_x000D_
I0912 05:40:15.330121 32553 slave.cpp:2171] Launching task '8fc99bc8-a2b6-498b-8bb2-af5d92e78cec' for framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000_x000D_
I0912 05:40:15.330823 32553 paths.cpp:578] Trying to chown '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_68fE8V/slaves/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0/frameworks/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000/executors/8fc99bc8-a2b6-498b-8bb2-af5d92e78cec/runs/69e9c3b3-65c9-4c04-b38d-ef2266c2cdf5' to user 'mesos'_x000D_
I0912 05:40:15.331084 32553 slave.cpp:7757] Checkpointing ExecutorInfo to '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_68fE8V/meta/slaves/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0/frameworks/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000/executors/8fc99bc8-a2b6-498b-8bb2-af5d92e78cec/executor.info'_x000D_
I0912 05:40:15.331904 32553 slave.cpp:7256] Launching executor '8fc99bc8-a2b6-498b-8bb2-af5d92e78cec' of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 with resources [{""allocation_info"":{""role"":""*""},""name"":""cpus"",""scalar"":{""value"":0.1},""type"":""SCALAR""},{""allocation_info"":{""role"":""*""},""name"":""mem"",""scalar"":{""value"":32.0},""type"":""SCALAR""}] in work directory '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_68fE8V/slaves/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0/frameworks/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000/executors/8fc99bc8-a2b6-498b-8bb2-af5d92e78cec/runs/69e9c3b3-65c9-4c04-b38d-ef2266c2cdf5'_x000D_
I0912 05:40:15.332718 32553 slave.cpp:2858] Launching container 69e9c3b3-65c9-4c04-b38d-ef2266c2cdf5 for executor '8fc99bc8-a2b6-498b-8bb2-af5d92e78cec' of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000_x000D_
I0912 05:40:15.333190 32554 containerizer.cpp:1083] Starting container 69e9c3b3-65c9-4c04-b38d-ef2266c2cdf5_x000D_
I0912 05:40:15.333230 32553 slave.cpp:7800] Checkpointing TaskInfo to '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_68fE8V/meta/slaves/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0/frameworks/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000/executors/8fc99bc8-a2b6-498b-8bb2-af5d92e78cec/runs/69e9c3b3-65c9-4c04-b38d-ef2266c2cdf5/tasks/8fc99bc8-a2b6-498b-8bb2-af5d92e78cec/task.info'_x000D_
I0912 05:40:15.333696 32554 containerizer.cpp:2712] Transitioning the state of container 69e9c3b3-65c9-4c04-b38d-ef2266c2cdf5 from PROVISIONING to PREPARING_x000D_
I0912 05:40:15.333937 32553 slave.cpp:2400] Queued task '8fc99bc8-a2b6-498b-8bb2-af5d92e78cec' for executor '8fc99bc8-a2b6-498b-8bb2-af5d92e78cec' of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000_x000D_
I0912 05:40:15.334064 32553 slave.cpp:924] Successfully attached file '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_68fE8V/slaves/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0/frameworks/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000/executors/8fc99bc8-a2b6-498b-8bb2-af5d92e78cec/runs/69e9c3b3-65c9-4c04-b38d-ef2266c2cdf5'_x000D_
I0912 05:40:15.334168 32553 slave.cpp:924] Successfully attached file '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_68fE8V/slaves/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0/frameworks/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000/executors/8fc99bc8-a2b6-498b-8bb2-af5d92e78cec/runs/69e9c3b3-65c9-4c04-b38d-ef2266c2cdf5'_x000D_
I0912 05:40:15.338408 32556 containerizer.cpp:1681] Launching 'mesos-containerizer' with flags '--help=""false"" --launch_info=""{""command"":{""arguments"":[""mesos-executor"",""--launcher_dir=\/mesos\/build\/src""],""shell"":false,""value"":""\/mesos\/build\/src\/mesos-executor""},""environment"":{""variables"":[{""name"":""LIBPROCESS_PORT"",""type"":""VALUE"",""value"":""0""},{""name"":""MESOS_AGENT_ENDPOINT"",""type"":""VALUE"",""value"":""172.17.0.2:58922""},{""name"":""MESOS_CHECKPOINT"",""type"":""VALUE"",""value"":""1""},{""name"":""MESOS_DIRECTORY"",""type"":""VALUE"",""value"":""\/tmp\/SlaveTest_HTTPSchedulerSlaveRestart_68fE8V\/slaves\/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0\/frameworks\/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000\/executors\/8fc99bc8-a2b6-498b-8bb2-af5d92e78cec\/runs\/69e9c3b3-65c9-4c04-b38d-ef2266c2cdf5""},{""name"":""MESOS_EXECUTOR_ID"",""type"":""VALUE"",""value"":""8fc99bc8-a2b6-498b-8bb2-af5d92e78cec""},{""name"":""MESOS_EXECUTOR_SHUTDOWN_GRACE_PERIOD"",""type"":""VALUE"",""value"":""5secs""},{""name"":""MESOS_FRAMEWORK_ID"",""type"":""VALUE"",""value"":""c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000""},{""name"":""MESOS_HTTP_COMMAND_EXECUTOR"",""type"":""VALUE"",""value"":""0""},{""name"":""MESOS_RECOVERY_TIMEOUT"",""type"":""VALUE"",""value"":""15mins""},{""name"":""MESOS_SLAVE_ID"",""type"":""VALUE"",""value"":""c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0""},{""name"":""MESOS_SLAVE_PID"",""type"":""VALUE"",""value"":""slave(198)@172.17.0.2:58922""},{""name"":""MESOS_SUBSCRIPTION_BACKOFF_MAX"",""type"":""VALUE"",""value"":""2secs""},{""name"":""MESOS_SANDBOX"",""type"":""VALUE"",""value"":""\/tmp\/SlaveTest_HTTPSchedulerSlaveRestart_68fE8V\/slaves\/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0\/frameworks\/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000\/executors\/8fc99bc8-a2b6-498b-8bb2-af5d92e78cec\/runs\/69e9c3b3-65c9-4c04-b38d-ef2266c2cdf5""}]},""task_environment"":{},""user"":""mesos"",""working_directory"":""\/tmp\/SlaveTest_HTTPSchedulerSlaveRestart_68fE8V\/slaves\/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0\/frameworks\/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000\/executors\/8fc99bc8-a2b6-498b-8bb2-af5d92e78cec\/runs\/69e9c3b3-65c9-4c04-b38d-ef2266c2cdf5""}"" --pipe_read=""6"" --pipe_write=""7"" --runtime_directory=""/tmp/SlaveTest_HTTPSchedulerSlaveRestart_n3xE7x/containers/69e9c3b3-65c9-4c04-b38d-ef2266c2cdf5"" --unshare_namespace_mnt=""false""'_x000D_
I0912 05:40:15.340767 32556 launcher.cpp:140] Forked child with pid '1772' for container '69e9c3b3-65c9-4c04-b38d-ef2266c2cdf5'_x000D_
I0912 05:40:15.340893 32556 containerizer.cpp:1773] Checkpointing container's forked pid 1772 to '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_68fE8V/meta/slaves/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0/frameworks/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000/executors/8fc99bc8-a2b6-498b-8bb2-af5d92e78cec/runs/69e9c3b3-65c9-4c04-b38d-ef2266c2cdf5/pids/forked.pid'_x000D_
I0912 05:40:15.341821 32556 containerizer.cpp:2712] Transitioning the state of container 69e9c3b3-65c9-4c04-b38d-ef2266c2cdf5 from PREPARING to ISOLATING_x000D_
I0912 05:40:15.343189 32558 containerizer.cpp:2712] Transitioning the state of container 69e9c3b3-65c9-4c04-b38d-ef2266c2cdf5 from ISOLATING to FETCHING_x000D_
I0912 05:40:15.343369 32560 fetcher.cpp:377] Starting to fetch URIs for container: 69e9c3b3-65c9-4c04-b38d-ef2266c2cdf5, directory: /tmp/SlaveTest_HTTPSchedulerSlaveRestart_68fE8V/slaves/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0/frameworks/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000/executors/8fc99bc8-a2b6-498b-8bb2-af5d92e78cec/runs/69e9c3b3-65c9-4c04-b38d-ef2266c2cdf5_x000D_
I0912 05:40:15.344462 32549 containerizer.cpp:2712] Transitioning the state of container 69e9c3b3-65c9-4c04-b38d-ef2266c2cdf5 from FETCHING to RUNNING_x000D_
I0912 05:40:15.504098  1787 exec.cpp:162] Version: 1.4.0_x000D_
I0912 05:40:15.510535 32550 slave.cpp:3935] Got registration for executor '8fc99bc8-a2b6-498b-8bb2-af5d92e78cec' of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 from executor(1)@172.17.0.2:33722_x000D_
I0912 05:40:15.511157 32550 slave.cpp:4021] Checkpointing executor pid 'executor(1)@172.17.0.2:33722' to '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_68fE8V/meta/slaves/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0/frameworks/c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000/executors/8fc99bc8-a2b6-498b-8bb2-af5d92e78cec/runs/69e9c3b3-65c9-4c04-b38d-ef2266c2cdf5/pids/libprocess.pid'_x000D_
I0912 05:40:15.513628 32552 slave.cpp:2605] Sending queued task '8fc99bc8-a2b6-498b-8bb2-af5d92e78cec' to executor '8fc99bc8-a2b6-498b-8bb2-af5d92e78cec' of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 at executor(1)@172.17.0.2:33722_x000D_
I0912 05:40:15.517511  1780 exec.cpp:237] Executor registered on agent c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0_x000D_
I0912 05:40:15.521653  1774 executor.cpp:171] Received SUBSCRIBED event_x000D_
I0912 05:40:15.522095  1774 executor.cpp:175] Subscribed executor on b909d5e22907_x000D_
I0912 05:40:15.522334  1774 executor.cpp:171] Received LAUNCH event_x000D_
I0912 05:40:15.522544  1774 executor.cpp:633] Starting task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec_x000D_
I0912 05:40:15.528475  1774 executor.cpp:477] Running '/mesos/build/src/mesos-containerizer launch <POSSIBLY-SENSITIVE-DATA>'_x000D_
I0912 05:40:15.531814  1774 executor.cpp:646] Forked command at 1791_x000D_
I0912 05:40:15.538535 32556 slave.cpp:4399] Handling status update TASK_RUNNING (UUID: b4d60b7e-a8d0-448e-aaf6-4f83dbcb642e) for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 from executor(1)@172.17.0.2:33722_x000D_
I0912 05:40:15.540377 32548 status_update_manager.cpp:323] Received status update TASK_RUNNING (UUID: b4d60b7e-a8d0-448e-aaf6-4f83dbcb642e) for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000_x000D_
I0912 05:40:15.540426 32548 status_update_manager.cpp:500] Creating StatusUpdate stream for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000_x000D_
I0912 05:40:15.541287 32548 status_update_manager.cpp:834] Checkpointing UPDATE for status update TASK_RUNNING (UUID: b4d60b7e-a8d0-448e-aaf6-4f83dbcb642e) for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000_x000D_
I0912 05:40:15.541561 32548 status_update_manager.cpp:377] Forwarding update TASK_RUNNING (UUID: b4d60b7e-a8d0-448e-aaf6-4f83dbcb642e) for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 to the agent_x000D_
I0912 05:40:15.541859 32559 slave.cpp:4880] Forwarding the update TASK_RUNNING (UUID: b4d60b7e-a8d0-448e-aaf6-4f83dbcb642e) for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 to master@172.17.0.2:58922_x000D_
I0912 05:40:15.542114 32559 slave.cpp:4774] Status update manager successfully handled status update TASK_RUNNING (UUID: b4d60b7e-a8d0-448e-aaf6-4f83dbcb642e) for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000_x000D_
I0912 05:40:15.542174 32559 slave.cpp:4790] Sending acknowledgement for status update TASK_RUNNING (UUID: b4d60b7e-a8d0-448e-aaf6-4f83dbcb642e) for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 to executor(1)@172.17.0.2:33722_x000D_
I0912 05:40:15.542295 32552 master.cpp:6841] Status update TASK_RUNNING (UUID: b4d60b7e-a8d0-448e-aaf6-4f83dbcb642e) for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 from agent c23ff8cf-cb2f-40d0-8f18-871a41f128cf-S0 at slave(198)@172.17.0.2:58922 (b909d5e22907)_x000D_
I0912 05:40:15.542371 32552 master.cpp:6903] Forwarding status update TASK_RUNNING (UUID: b4d60b7e-a8d0-448e-aaf6-4f83dbcb642e) for task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000_x000D_
I0912 05:40:15.542628 32552 master.cpp:8928] Updating the state of task 8fc99bc8-a2b6-498b-8bb2-af5d92e78cec of framework c23ff8cf-cb2f-40d0-8f18-871a41f128cf-0000 (latest st:_x000D_
ate: TASK_RUNNING, status update state: T",1.0,1.4.0,0.5,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.8846153846153846,0.9878787878787878,0.9878787878787878,0.9541284403669724
Bug,Handle cgroups v2 hierarchy when parsing /proc/self/cgroups.,"Cgroups v2 hierarchies don't list the ""controllers"" field (e.g., ""0::/user.slice/user-1000.slice/session-5.scope)  is empty [1] and hence the cgroup parser failes. We should simply skip over fields with empty controller field.

{code}
[ RUN      ] CgroupsAnyHierarchyTest.ROOT_CGROUPS_Read
../../src/tests/containerizer/cgroups_tests.cpp:464: Failure
cgroup: Unexpected format in /proc/20668/cgroup
[  FAILED  ] CgroupsAnyHierarchyTest.ROOT_CGROUPS_Read (8 ms)
{code}

[1] See 6-2 in https://www.kernel.org/doc/Documentation/cgroup-v2.txt",1.0,"1.2.3,1.3.2,1.4.1,1.5.0",0.0,0.3453996983408748,0.0,0.0,0.0,0.0,0.2,0.38596491228070173,0.35714285714285715,0.28205128205128205,0.29090909090909095,0.28484848484848485,0.9496788990825688
Bug,Handle `/proc/self/ns/pid_for_children` when parsing available namespace.,"Since Linux 4.12, /proc/self/ns/pid_for_children is a handle for the PID namespace of child processes created by this process. Since this is not a namespace type in its own, we should ignore this file when listing namespaces via `ls /proc/self/ns`.",1.0,"1.2.3,1.3.2,1.4.1,1.5.0",0.0,0.3453996983408748,0.0,0.0,0.0,0.0,0.2,0.38596491228070173,0.35714285714285715,0.28205128205128205,0.29090909090909095,0.28484848484848485,0.9496788990825688
Bug,MasterAPITest.EventAuthorizationFiltering is flaky.,"{noformat}_x000D_
commit e4d56bcb65f7bf9805eff18e6a9249eb7512f745_x000D_
Author: Quinn Leng quinn.leng.666@gmail.com_x000D_
Date:   Tue Aug 29 13:13:19 2017 -0700_x000D_
_x000D_
Added authorization for V1 events._x000D_
_x000D_
Added authorization filtering for the master V1 operator event_x000D_
stream. Subscribers will only receive events that their_x000D_
principal is authorized to see. The new test_x000D_
'MasterAPITest.EventAuthorizationFiltering' verifies this_x000D_
behavior._x000D_
_x000D_
Review: https://reviews.apache.org/r/61189/_x000D_
{noformat}_x000D_
_x000D_
The above commit introduced the test {{MasterAPITest.EventAuthorizationFiltering}} which is flaky.",2.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.11538461538461538,0.05454545454545455,0.03636363636363637,0.0
Bug,`Metrics()` hangs on second call on Windows,"An unfortunately difficult to debug problem has cropped up on Windows. While running the {{mesos-tests}} they will hang at:

{noformat}
[==========] Running 2 tests from 2 test cases.
[----------] Global test environment set-up.
[----------] 1 test from FetcherTest
[ RUN      ] FetcherTest.MalformedURI
[       OK ] FetcherTest.MalformedURI (48 ms)
[----------] 1 test from FetcherTest (63 ms total)

[----------] 1 test from GarbageCollectorTest
[ RUN      ] GarbageCollectorTest.Schedule
C:\Users\andschwa\src\mesos-master\src\tests\utils.cpp(64): error: Failed to wait 15secs for response
C:\Users\andschwa\src\mesos-master\src\tests\utils.cpp(65): error: Failed to wait 15secs for response
{noformat}

{{GarbageCollectorTest.Schedule}} is the first test that will hang in an unfiltered run of mesos-tests.

This can be minimally reproduced by running any two tests which call {{Metrics()}} from {{utils.cpp}}. The following have been confirmed:

{noformat}
--gtest_filter=""GarbageCollectorTest.Schedule:HierarchicalAllocatorTest.OfferFilter""
--gtest_filter=""GarbageCollectorTest.Schedule:FetcherTest.MalformedURI""
--gtest_filter=""HierarchicalAllocatorTest.OfferFilter:FetcherTest.MalformedURI""
{noformat}

The second test will hang (indicating a race condition), waiting for a {{GET}} to {{/metrics/snapshot}} that never returns.

There appears to be a timing problem to this bug as well. If your CPU is heavily utilized (say, by running another build in the background), the tests will pass. They will pass if you attach Application Verifier to {{mesos-tests.exe}}, which slows down execution enough. Very slow machines (such as those used for CI) will also not exhibit this hang. Increasing the log verbosity using {{GLOG_v=2}} or higher will make it disappear (although {{--verbose}} has no effect).

Oddly, the bug will reproduce under the Visual Studio debugger, but all it shows us is a pending future waiting for the metrics request to come back.

In {{metrics.cpp}} there is a note that the request might timeout, but we're unsure if this is the same problem, or a different problem manifesting in the same way:

{noformat}
  // TODO(neilc): This request might timeout if the current value of a
  // metric cannot be determined. In tests, a common cause for this is
  // MESOS-6231 when multiple scheduler drivers are in use.
{noformat}

A {{git bisect}} revealed that:

{noformat}
20c5311434e45a631ffc6036d327e00b2228ad26 is the first bad commit
commit 20c5311434e45a631ffc6036d327e00b2228ad26
Author: James Peach <jpeach@apache.org>
Date:   Tue Aug 22 16:19:47 2017 -0700

    Added agent garbage collection metrics.

    Added some basic sandbox garbage collection metrics to track the number
    of successful, failed and pending path removals.

    Review: https://reviews.apache.org/r/61260/
{noformat}

Caused this bug to appear (but does not necessarily mean it created the bug). Reverting this commit allows all the tests to pass, but we believe this just hides the bug.

This bug has reproduced on Windows machines with and without Docker (and Windows containers) installed. (I only mention this because it was a variable on my machine when the bug first appeared, but have since ruled it out as relevant.)

We do not think that it is specific to {{libevent}}, as the bug does not appear to reproduce on a Linux VM built with {{libevent}} instead of {{libev}}.",2.0,0,0.5,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.11538461538461538,0.05454545454545455,0.03636363636363637,0.0
Bug,Make args optional in mesos port mapper plugin,"Current implementation of the mesos-port-mapper plugin fails if the args field is absent in the cni config which makes it very specific to mesos. Instead, if args could be optional then this plugin could be used in a more generic environment. ",1.0,0,0.0,0.006033182503770739,0.3333333333333333,0.5,0.14285714285714285,0.0,0.0,0.017543859649122806,0.007142857142857143,0.01282051282051282,0.0,0.0,0.0
Bug,Fix communication between old masters and new agents.,"For re-registration, agents currently send the resources in tasks
and executors to the master in the ""post-reservation-refinement"" format,
which is incompatible for pre-1.4 masters. We should change the agent
such that it always downgrades the resources to
the ""pre-reservation-refinement"" format, and the master unconditionally
upgrade the resources to ""post-reservation-refinement"" format.",2.0,0,0.0,0.6726998491704375,0.0,0.0,0.0,0.1,0.6000000000000001,0.2807017543859649,0.29285714285714287,0.21794871794871795,0.26666666666666666,0.26666666666666666,0.0
Bug,Docker statistics not reported on Windows.,"On Windows, the JSON information provided by the agent at the /container API does not contain the expected {{statistics}} object for Docker containers on Windows. This breaks the dcos-metrics tool, required for DC/OS integration on Windows.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.8,0.19298245614035087,0.09999999999999999,0.11538461538461538,0.05454545454545455,0.03636363636363637,0.0
Bug,Quota heuristic check not accounting for mount volumes,"This may be expected but came as a surprise to us. We are unable to create a quota bigger than the root disk space on slaves._x000D_
_x000D_
Given two clusters with the same number of slaves and root disk size, but one that also has mount volumes, is what the disk resources look like:_x000D_
_x000D_
{noformat}_x000D_
[root@fin-fang-foom-master-1 ~]# curl -s master.mesos:5050/state | jq '.slaves[] .resources .disk'_x000D_
28698_x000D_
28699_x000D_
28698_x000D_
28698_x000D_
28697_x000D_
{noformat}_x000D_
_x000D_
{noformat}_x000D_
[root@hydra-master-1 ~]# curl -s master.mesos:5050/state | jq '.slaves[] .resources .disk'_x000D_
50817_x000D_
50817_x000D_
50814_x000D_
50819_x000D_
50817_x000D_
{noformat}_x000D_
_x000D_
In {{fin-fang-foom}}, I was able to create a quota for {{143490mb}} which is the total of available disk resources, root in this case, as reported by Mesos. For {{hydra}}, I am only able to create a quota for {{143489mb}}. This is equivalent to the total of root disks available in {{hydra}} rather than the total available disks reported by Mesos resources which is {{254084mb}}._x000D_
_x000D_
With a modified Mesos that adds logging to {{quota_handler}}, we can see that only the {{disk(*)}} number increases in {{nonStaticClusterResources}} after every iteration. The final iteration is {{disk(*):143489}} which is the maximum quota I was able to create on {{hydra}}. We expected that quota heuristic check would also include resources such as {{disk(*)[MOUNT:/dcos/volume2]:7373}}_x000D_
_x000D_
{noformat}_x000D_
Aug 11 12:54:18 hydra-master-1 mesos-master[24896]: I0811 12:54:18.763764 24902 quota_handler.cpp:71] Performing capacity heuristic check for a set quota request_x000D_
Aug 11 12:54:18 hydra-master-1 mesos-master[24896]: I0811 12:54:18.763783 24902 quota_handler.cpp:87] heuristic: total quota 'disk(*):143489'_x000D_
_x000D_
Aug 11 12:54:18 hydra-master-1 mesos-master[24896]: I0811 12:54:18.763870 24902 quota_handler.cpp:111] heuristic: nonStaticAgentResources = 'ports(*):[1025-2180, 2182-3887, 3889-5049, 5052-8079, 8082-8180, 8182-32000]; disk(*)[MOUNT:/dcos/volume0]:7373; disk(*)[MOUNT:/dcos/volume1]:7373; disk(*)[MOUNT:/dcos/volume2]:7373; disk(*):28698; cpus(*):4; mem(*):15023'_x000D_
Aug 11 12:54:18 hydra-master-1 mesos-master[24896]: I0811 12:54:18.763923 24902 quota_handler.cpp:113] heuristic: nonStaticClusterResources = 'ports(*):[1025-2180, 2182-3887, 3889-5049, 5052-8079, 8082-8180, 8182-32000]; disk(*)[MOUNT:/dcos/volume0]:7373; disk(*)[MOUNT:/dcos/volume1]:7373; disk(*)[MOUNT:/dcos/volume2]:7373; disk(*):28698; cpus(*):4; mem(*):15023'_x000D_
_x000D_
_x000D_
Aug 11 12:54:18 hydra-master-1 mesos-master[24896]: I0811 12:54:18.763989 24902 quota_handler.cpp:111] heuristic: nonStaticAgentResources = 'ports(*):[1025-2180, 2182-3887, 3889-5049, 5052-8079, 8082-8180, 8182-32000]; disk(*)[MOUNT:/dcos/volume0]:7373; disk(*)[MOUNT:/dcos/volume1]:7373; disk(*)[MOUNT:/dcos/volume2]:7373; disk(*):28698; cpus(*):4; mem(*):15023'_x000D_
Aug 11 12:54:18 hydra-master-1 mesos-master[24896]: I0811 12:54:18.764022 24902 quota_handler.cpp:113] heuristic: nonStaticClusterResources = 'ports(*):[1025-2180, 2182-3887, 3889-5049, 5052-8079, 8082-8180, 8182-32000]; disk(*)[MOUNT:/dcos/volume0]:7373; disk(*)[MOUNT:/dcos/volume1]:7373; disk(*)[MOUNT:/dcos/volume2]:7373; disk(*):57396; cpus(*):8; mem(*):30046; disk(*)[MOUNT:/dcos/volume0]:7373; disk(*)[MOUNT:/dcos/volume1]:7373; disk(*)[MOUNT:/dcos/volume2]:7373'_x000D_
_x000D_
Aug 11 12:54:18 hydra-master-1 mesos-master[24896]: I0811 12:54:18.764077 24902 quota_handler.cpp:111] heuristic: nonStaticAgentResources = 'ports(*):[1025-2180, 2182-3887, 3889-5049, 5052-8079, 8082-8180, 8182-32000]; disk(*)[MOUNT:/dcos/volume0]:7373; disk(*)[MOUNT:/dcos/volume1]:7373; disk(*)[MOUNT:/dcos/volume2]:7373; disk(*):28695; cpus(*):4; mem(*):15023'_x000D_
Aug 11 12:54:18 hydra-master-1 mesos-master[24896]: I0811 12:54:18.764119 24902 quota_handler.cpp:113] heuristic: nonStaticClusterResources = 'ports(*):[1025-2180, 2182-3887, 3889-5049, 5052-8079, 8082-8180, 8182-32000]; disk(*)[MOUNT:/dcos/volume0]:7373; disk(*)[MOUNT:/dcos/volume1]:7373; disk(*)[MOUNT:/dcos/volume2]:7373; disk(*):86091; cpus(*):12; mem(*):45069; disk(*)[MOUNT:/dcos/volume0]:7373; disk(*)[MOUNT:/dcos/volume1]:7373; disk(*)[MOUNT:/dcos/volume2]:7373; disk(*)[MOUNT:/dcos/volume0]:7373; disk(*)[MOUNT:/dcos/volume1]:7373; disk(*)[MOUNT:/dcos/volume2]:7373'_x000D_
_x000D_
Aug 11 12:54:18 hydra-master-1 mesos-master[24896]: I0811 12:54:18.764225 24902 quota_handler.cpp:111] heuristic: nonStaticAgentResources = 'ports(*):[1025-2180, 2182-3887, 3889-5049, 5052-8079, 8082-8180, 8182-32000]; disk(*)[MOUNT:/dcos/volume0]:7373; disk(*)[MOUNT:/dcos/volume1]:7373; disk(*)[MOUNT:/dcos/volume2]:7373; disk(*):28700; cpus(*):4; mem(*):15023'_x000D_
Aug 11 12:54:18 hydra-master-1 mesos-master[24896]: I0811 12:54:18.764307 24902 quota_handler.cpp:113] heuristic: nonStaticClusterResources = 'ports(*):[1025-2180, 2182-3887, 3889-5049, 5052-8079, 8082-8180, 8182-32000]; disk(*)[MOUNT:/dcos/volume0]:7373; disk(*)[MOUNT:/dcos/volume1]:7373; disk(*)[MOUNT:/dcos/volume2]:7373; disk(*):114791; cpus(*):16; mem(*):60092; disk(*)[MOUNT:/dcos/volume0]:7373; disk(*)[MOUNT:/dcos/volume1]:7373; disk(*)[MOUNT:/dcos/volume2]:7373; disk(*)[MOUNT:/dcos/volume0]:7373; disk(*)[MOUNT:/dcos/volume1]:7373; disk(*)[MOUNT:/dcos/volume2]:7373; disk(*)[MOUNT:/dcos/volume0]:7373; disk(*)[MOUNT:/dcos/volume1]:7373; disk(*)[MOUNT:/dcos/volume2]:7373'_x000D_
_x000D_
Aug 11 12:54:18 hydra-master-1 mesos-master[24896]: I0811 12:54:18.764434 24902 quota_handler.cpp:111] heuristic: nonStaticAgentResources = 'ports(*):[1025-2180, 2182-3887, 3889-5049, 5052-8079, 8082-8180, 8182-32000]; disk(*)[MOUNT:/dcos/volume0]:7373; disk(*)[MOUNT:/dcos/volume1]:7373; disk(*)[MOUNT:/dcos/volume2]:7373; disk(*):28698; cpus(*):4; mem(*):15023'_x000D_
Aug 11 12:54:18 hydra-master-1 mesos-master[24896]: I0811 12:54:18.764492 24902 quota_handler.cpp:113] heuristic: nonStaticClusterResources = 'ports(*):[1025-2180, 2182-3887, 3889-5049, 5052-8079, 8082-8180, 8182-32000]; disk(*)[MOUNT:/dcos/volume0]:7373; disk(*)[MOUNT:/dcos/volume1]:7373; disk(*)[MOUNT:/dcos/volume2]:7373; disk(*):143489; cpus(*):20; mem(*):75115; disk(*)[MOUNT:/dcos/volume0]:7373; disk(*)[MOUNT:/dcos/volume1]:7373; disk(*)[MOUNT:/dcos/volume2]:7373; disk(*)[MOUNT:/dcos/volume0]:7373; disk(*)[MOUNT:/dcos/volume1]:7373; disk(*)[MOUNT:/dcos/volume2]:7373; disk(*)[MOUNT:/dcos/volume0]:7373; disk(*)[MOUNT:/dcos/volume1]:7373; disk(*)[MOUNT:/dcos/volume2]:7373; disk(*)[MOUNT:/dcos/volume0]:7373; disk(*)[MOUNT:/dcos/volume1]:7373; disk(*)[MOUNT:/dcos/volume2]:7373'_x000D_
_x000D_
Aug 11 12:54:18 hydra-master-1 mesos-master[24896]: I0811 12:54:18.764562 24902 quota_handler.cpp:118] heuristic: nonStaticClusterResources.contains(totalQuota)_x000D_
{noformat}_x000D_
",1.0,0,0.5,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.01282051282051282,0.0,0.0,0.0
Bug,Agent may process a kill task and still launch the task.,"Based on the investigation of MESOS-7744, the agent has a race in which ""queued"" tasks can still be launched after the agent has processed a kill task for them. This race was introduced when {{Slave::statusUpdate}} was made asynchronous:

(1) {{Slave::__run}} completes, task is now within {{Executor::queuedTasks}}
(2) {{Slave::killTask}} locates the executor based on the task ID residing in queuedTasks, calls {{Slave::statusUpdate()}} with {{TASK_KILLED}}
(3) {{Slave::___run}} assumes that killed tasks have been removed from {{Executor::queuedTasks}}, but this now occurs asynchronously in {{Slave::_statusUpdate}}. So, the executor still sees the queued task and delivers it and adds the task to {{Executor::launchedTasks}}.
(3) {{Slave::_statusUpdate}} runs, removes the task from {{Executor::launchedTasks}} and adds it to {{Executor::terminatedTasks}}.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.0
Bug,Master stores old resource format in the registry,"We intend for the master to store all internal resource representations in the new, post-reservation-refinement format. However, [when persisting registered agents to the registrar|https://github.com/apache/mesos/blob/498a000ac1bb8f51dc871f22aea265424a407a17/src/master/master.cpp#L5861-L5876], the master does not convert the resources; agents provide resources in the pre-reservation-refinement format, and these resources are stored as-is. This means that after recovery, any agents in the master's {{slaves.recovered}} map will have {{SlaveInfo.resources}} in the pre-reservation-refinement format._x000D_
_x000D_
We should update the master to convert these resources before persisting them to the registry.",3.0,0,0.5,0.6726998491704375,0.0,0.0,0.0,0.1,0.6000000000000001,0.2807017543859649,0.29285714285714287,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Improvement,Add Mesos CLI command to list active tasks,We need to add a command to list all the tasks running in a Mesos cluster by checking the endpoint {{/tasks}} and reporting the results.,3.0,0,0.0,0.25037707390648567,0.3333333333333333,0.5,0.14285714285714285,0.5,0.0,0.2631578947368421,0.29285714285714287,0.10256410256410256,0.1515151515151515,0.1515151515151515,0.0
Task,Add HTTP connection handling to the resource provider driver,"The {{resource_provider::Driver}} is responsible for establishing a connection with an agent/master resource provider API and provide calls to the API, receive events from the API. This is done using HTTP and should be implemented similar to how it's done for schedulers and executors (see {{src/executor/executor.cpp, src/scheduler/scheduler.cpp}}).",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.2631578947368421,0.36428571428571427,0.28205128205128205,0.296969696969697,0.296969696969697,0.0
Improvement,Improve the test frameworks.,"These improvements include three main points:
* Adding a {{name}} flag to certain frameworks to distinguish between instances.
* Cleaning up the code style of the frameworks.
* For frameworks with custom executors, such as balloon framework, adding a {{executor_extra_uris}} flag containing URIs that will be passed to the {{command_info}} of the executor.",3.0,0,0.0,0.25037707390648567,0.3333333333333333,0.5,0.14285714285714285,0.5,0.0,0.2631578947368421,0.29285714285714287,0.10256410256410256,0.1515151515151515,0.1515151515151515,0.0
Task,Docker executor needs to return multiple IP addresses for the container,`Docker executor` currently returns only a single IP address for each docker container. In a world where container has a v4 and v6 address the executor needs to return all the addresses it sees for the container else we won't be able to support dual-stack containers.,1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2807017543859649,0.39285714285714285,0.1282051282051282,0.3090909090909091,0.3090909090909091,0.0
Task,Add copy assignment operator to `net::IP::Network`,"Currently, we can't extend the class `net::IP::Network` with out adding a copy assignment operator in the derived class, due to the use of `std::unique_ptr` in the base class. Hence, need to introduce a copy assignment operator into the base class.",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2807017543859649,0.39285714285714285,0.1282051282051282,0.3090909090909091,0.3090909090909091,0.0
Bug,fs::list drops path components on Windows,"fs::list(/foo/bar/*.txt) returns a.txt, b.txt, not /foo/bar/a.txt, /foo/bar/b.txt_x000D_
_x000D_
This breaks a ZooKeeper test on Windows.",2.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.8,0.19298245614035087,0.09999999999999999,0.11538461538461538,0.05454545454545455,0.03636363636363637,0.0
Bug,Copy-n-paste error in slave/main.cpp,"Coverity diagnosed a copy-n-paste error in {{slave/main.cpp}} (https://scan5.coverity.com/reports.htm#v10074/p10429/fileInstanceId=120155401&defectInstanceId=33592186&mergedDefectId=1414687+1+Comment),

{noformat}
323  } else if (flags.ip6.isSome()) {
CID 1414687 (#1 of 1): Copy-paste error (COPY_PASTE_ERROR)
copy_paste_error: ip in flags.ip looks like a copy-paste error.
   	Should it say ip6 instead?
324    os::setenv(""LIBPROCESS_IP6"", flags.ip.get());
325  }
{noformat}

We check the incorrect IP for some value here (check on {{ip6}}, but use of {{ip}}), and it seems extremely likely we intended to use {{flags.ip6}}.",1.0,1.4.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.2807017543859649,0.39285714285714285,1.0,0.8121212121212121,0.8121212121212121,0.9541284403669724
Bug,libprocess initializes to bind to random port if --ip is not specified,"When running current [HEAD|https://github.com/apache/mesos/commit/c90bea80486c089e933bef64aca341e4cfaaef25],

{noformat:title=without --ip}
./mesos-master.sh --work_dir=/tmp/mesos-test1
...
I0707 14:14:05.927870  5820 master.cpp:438] Master db2a2d26-a9a9-4e6f-9909-b9eca47a2862 (<host>) started on <addr>:36839
{noformat}

{noformat:title=with --ip}
./mesos-master.sh --ip=<addr> --work_dir=/tmp/mesos-test1
I0707 14:09:56.851483  5729 master.cpp:438] Master 963e0f42-9767-4629-8e3d-02c6ab6ad225 (<host>) started on <addr>:5050
{noformat}

It would be great this is caught by tests/CI.",1.0,1.4.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.2807017543859649,0.39285714285714285,0.20512820512820512,0.13939393939393938,0.13939393939393938,0.9541284403669724
Task,Make `net::IP` fields protected to allow for inheritance,"Correctly the properties of `net::IP` are `private` making it hard for classes to inherit `net::IP`. Although `net::IPv4` and `net::IPv6` already inherit `net::IP` they don't have access to the storage structures stored within `net::IP`. While this works for the current API this is not very extensible. 

Hence, we should make the properties of `net::IP` protected instead of private.",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2807017543859649,0.39285714285714285,0.1282051282051282,0.3090909090909091,0.3090909090909091,0.0
Bug,MasterTest.KillUnknownTask is failling due to a bug in `net::IPv4::ANY()`,"Seeing the following failure when running `MasterTest.KillUnknownTask`:
```
I0706 14:08:20.724071 25596 sched.cpp:1041] Scheduler::statusUpdate took 19411ns
[libprotobuf FATAL google/protobuf/message_lite.cc:294] CHECK failed: IsInitialized(): Can't serialize message of type ""mesos.scheduler.Call"" because it is missing required fields: acknowledge.slave_id.value
libprocess: scheduler-5cca230e-e4c9-466e-b2cd-bde7b7d7ed71@127.0.0.1:44650 terminating due to CHECK failed: IsInitialized(): Can't serialize message of type ""mesos.scheduler.Call"" because it is missing required fields: acknowledge.slave_id.valueI0706 14:08:20.724196 25570 sched.cpp:2021] Asked to stop the driver
```

Looks we introduced a bug when we create the `net::IPv4` class. The `ANY` method of this class returns `INADDR_LOOPBACK` instead of `INADDR_ANY`. This ends up causing weird issues in terms of connectivity. We need to fix `net::IPv4::ANY` to return `INADDR_ANY`.",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2807017543859649,0.39285714285714285,0.1282051282051282,0.3090909090909091,0.3090909090909091,0.0
Bug,net::IP::Network not building on Windows,"Building master (well, 2c1be9ced) is currently broken on Windows. Repro:

{noformat}
git checkout 2c1be9ced 
mkdir build
cd build 
cmake .. -DENABLE_LIBEVENT=1 -DHAS_AUTHENTICATION=0 -G ""Visual Studio 15 2017 Win64"" -T ""host=x64""
cmake --build . --target stout-tests
{noformat}

(Build instructions here: https://github.com/apache/mesos/blob/master/docs/windows.md)

Get a bunch of compilation errors:

{noformat}
""C:\Users\andschwa\src\mesos-copy2\build\3rdparty\stout\tests\stout-tests.vcxproj"" (default target) (1) ->
(ClCompile target) ->
  C:\Users\andschwa\src\mesos-copy2\3rdparty\stout\include\stout/windows/ip.hpp(31): error C2065: 'IPNetwork': undeclared identifier (compiling source file C:\Users\andschwa\src\mesos-copy2\3rdparty\stout\tests\ip_tests.cpp) [C:\Users\ands
chwa\src\mesos-copy2\build\3rdparty\stout\tests\stout-tests.vcxproj]
  C:\Users\andschwa\src\mesos-copy2\3rdparty\stout\include\stout/windows/ip.hpp(31): error C2923: 'Result': 'IPNetwork' is not a valid template type argument for parameter 'T' (compiling source file C:\Users\andschwa\src\mesos-copy2\3rdpar
ty\stout\tests\ip_tests.cpp) [C:\Users\andschwa\src\mesos-copy2\build\3rdparty\stout\tests\stout-tests.vcxproj]
  C:\Users\andschwa\src\mesos-copy2\3rdparty\stout\include\stout/windows/ip.hpp(31): error C2653: 'IPNetwork': is not a class or namespace name (compiling source file C:\Users\andschwa\src\mesos-copy2\3rdparty\stout\tests\ip_tests.cpp) [C:
\Users\andschwa\src\mesos-copy2\build\3rdparty\stout\tests\stout-tests.vcxproj]
  C:\Users\andschwa\src\mesos-copy2\3rdparty\stout\include\stout/windows/ip.hpp(34): error C2079: 'net::fromLinkDevice' uses undefined class 'Result' (compiling source file C:\Users\andschwa\src\mesos-copy2\3rdparty\stout\tests\ip_tests.cp
p) [C:\Users\andschwa\src\mesos-copy2\build\3rdparty\stout\tests\stout-tests.vcxproj]
  C:\Users\andschwa\src\mesos-copy2\3rdparty\stout\include\stout/windows/ip.hpp(41): error C2440: 'return': cannot convert from 'Error' to 'Result' (compiling source file C:\Users\andschwa\src\mesos-copy2\3rdparty\stout\tests\ip_tests.cpp)
 [C:\Users\andschwa\src\mesos-copy2\build\3rdparty\stout\tests\stout-tests.vcxproj]
  C:\Users\andschwa\src\mesos-copy2\3rdparty\stout\include\stout/windows/ip.hpp(49): error C2440: 'return': cannot convert from 'WindowsError' to 'Result' (compiling source file C:\Users\andschwa\src\mesos-copy2\3rdparty\stout\tests\ip_tes
ts.cpp) [C:\Users\andschwa\src\mesos-copy2\build\3rdparty\stout\tests\stout-tests.vcxproj]
  C:\Users\andschwa\src\mesos-copy2\3rdparty\stout\include\stout/windows/ip.hpp(58): error C2440: 'return': cannot convert from 'WindowsError' to 'Result' (compiling source file C:\Users\andschwa\src\mesos-copy2\3rdparty\stout\tests\ip_tes
ts.cpp) [C:\Users\andschwa\src\mesos-copy2\build\3rdparty\stout\tests\stout-tests.vcxproj]
  C:\Users\andschwa\src\mesos-copy2\3rdparty\stout\include\stout/windows/ip.hpp(70): error C2065: 'IPNetwork': undeclared identifier (compiling source file C:\Users\andschwa\src\mesos-copy2\3rdparty\stout\tests\ip_tests.cpp) [C:\Users\ands
chwa\src\mesos-copy2\build\3rdparty\stout\tests\stout-tests.vcxproj]
  C:\Users\andschwa\src\mesos-copy2\3rdparty\stout\include\stout/windows/ip.hpp(70): error C2923: 'Try': 'IPNetwork' is not a valid template type argument for parameter 'T' (compiling source file C:\Users\andschwa\src\mesos-copy2\3rdparty\
stout\tests\ip_tests.cpp) [C:\Users\andschwa\src\mesos-copy2\build\3rdparty\stout\tests\stout-tests.vcxproj]
  C:\Users\andschwa\src\mesos-copy2\3rdparty\stout\include\stout/windows/ip.hpp(70): error C2653: 'IPNetwork': is not a class or namespace name (compiling source file C:\Users\andschwa\src\mesos-copy2\3rdparty\stout\tests\ip_tests.cpp) [C:
\Users\andschwa\src\mesos-copy2\build\3rdparty\stout\tests\stout-tests.vcxproj]
  C:\Users\andschwa\src\mesos-copy2\3rdparty\stout\include\stout/windows/ip.hpp(70): error C3861: 'create': identifier not found (compiling source file C:\Users\andschwa\src\mesos-copy2\3rdparty\stout\tests\ip_tests.cpp) [C:\Users\andschwa
\src\mesos-copy2\build\3rdparty\stout\tests\stout-tests.vcxproj]
...
{noformat}",1.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.2807017543859649,0.39285714285714285,0.11538461538461538,0.05454545454545455,0.03636363636363637,0.0
Bug,RegisterSlaveValidationTest.DropInvalidReregistration is flaky.,"Observed this on ASF CI.

Seems a bit different from MESOS-7441.

{code}
[ RUN      ] RegisterSlaveValidationTest.DropInvalidReregistration
I0629 05:23:17.367363  2252 cluster.cpp:162] Creating default 'local' authorizer
I0629 05:23:17.370198  2276 master.cpp:436] Master 25091bef-3845-4bb6-ae23-e18ac0f4d174 (b3c104d65da7) started on 172.17.0.3:42034
I0629 05:23:17.370234  2276 master.cpp:438] Flags at startup: --acls="""" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""1secs"" -
-allocator=""HierarchicalDRF"" --authenticate_agents=""true"" --authenticate_frameworks=""true"" --authenticate_http_frameworks=""true"" --authenticate_http_readonly=""true"" --au
thenticate_http_readwrite=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/V0UvSM/credentials"" --framework_sorter=""drf"" --help=""false"" --hostn
ame_lookup=""true"" --http_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" 
--logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_unreachable_tasks_per_framework=""1000"" --port=""5050"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""in_memory"" --registry_fetch_timeout=""1mins"" --registry_gc_interval=""15mins"" --registry_max_agent_age=""2weeks"" --registry_max_agent_count=""102400"" --registry_store_timeout=""100secs"" --registry_strict=""false"" --root_submissions=""true"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-1.3.1/_inst/share/mesos/webui"" --work_dir=""/tmp/V0UvSM/master"" --zk_session_timeout=""10secs""
I0629 05:23:17.370513  2276 master.cpp:488] Master only allowing authenticated frameworks to register
I0629 05:23:17.370525  2276 master.cpp:502] Master only allowing authenticated agents to register
I0629 05:23:17.370534  2276 master.cpp:515] Master only allowing authenticated HTTP frameworks to register
I0629 05:23:17.370543  2276 credentials.hpp:37] Loading credentials for authentication from '/tmp/V0UvSM/credentials'
I0629 05:23:17.370806  2276 master.cpp:560] Using default 'crammd5' authenticator
I0629 05:23:17.370929  2276 http.cpp:975] Creating default 'basic' HTTP authenticator for realm 'mesos-master-readonly'
I0629 05:23:17.371073  2276 http.cpp:975] Creating default 'basic' HTTP authenticator for realm 'mesos-master-readwrite'
I0629 05:23:17.371193  2276 http.cpp:975] Creating default 'basic' HTTP authenticator for realm 'mesos-master-scheduler'
I0629 05:23:17.371318  2276 master.cpp:640] Authorization enabled
I0629 05:23:17.371455  2272 hierarchical.cpp:158] Initialized hierarchical allocator process
I0629 05:23:17.371477  2290 whitelist_watcher.cpp:77] No whitelist given
I0629 05:23:17.373731  2277 master.cpp:2161] Elected as the leading master!
I0629 05:23:17.373760  2277 master.cpp:1700] Recovering from registrar
I0629 05:23:17.373891  2280 registrar.cpp:345] Recovering registrar
I0629 05:23:17.374527  2280 registrar.cpp:389] Successfully fetched the registry (0B) in 593152ns
I0629 05:23:17.374625  2280 registrar.cpp:493] Applied 1 operations in 19216ns; attempting to update the registry
I0629 05:23:17.375228  2280 registrar.cpp:550] Successfully updated the registry in 555008ns
I0629 05:23:17.375336  2280 registrar.cpp:422] Successfully recovered registrar
I0629 05:23:17.375826  2282 hierarchical.cpp:185] Skipping recovery of hierarchical allocator: nothing to recover
I0629 05:23:17.375850  2290 master.cpp:1799] Recovered 0 agents from the registry (129B); allowing 10mins for agents to re-register
I0629 05:23:17.380674  2252 containerizer.cpp:221] Using isolation: posix/cpu,posix/mem,filesystem/posix,network/cni
W0629 05:23:17.381237  2252 backend.cpp:76] Failed to create 'aufs' backend: AufsBackend requires root privileges
W0629 05:23:17.381350  2252 backend.cpp:76] Failed to create 'bind' backend: BindBackend requires root privileges
I0629 05:23:17.381384  2252 provisioner.cpp:249] Using default backend 'copy'
I0629 05:23:17.383884  2252 cluster.cpp:448] Creating default 'local' authorizer
I0629 05:23:17.385763  2281 slave.cpp:231] Mesos agent started on (287)@172.17.0.3:42034
I0629 05:23:17.385787  2281 slave.cpp:232] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http_executors=""true"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""true"" --authenticatee=""crammd5"" --authentication_backoff_factor=""1secs"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/RegisterSlaveValidationTest_DropInvalidReregistration_f1Jc3i/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/is:
olators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_reregistration_timeout=""2secs"" --executor_secret_key=""/tmp/RegisterSlaveValidationTest_DropInvalidReregistration_f1Jc3i/executor_secret_key"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/RegisterSlaveValidationTest_DropInvalidReregistration_f1Jc3i/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_command_executor=""false"" --http_credentials=""/tmp/RegisterSlaveValidationTest_DropInvalidReregistration_f1Jc3i/http_credentials"" --http_heartbeat_interval=""30secs"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher=""posix"" --launcher_dir=""/mesos/mesos-1.3.1/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_executors_per_framework=""150"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --port=""5051"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --runtime_dir=""/tmp/RegisterSlaveValidationTest_DropInvalidReregistration_f1Jc3i"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/RegisterSlaveValidationTest_DropInvalidReregistration_RVRQXx""
I0629 05:23:17.386165  2281 credentials.hpp:86] Loading credential for authentication from '/tmp/RegisterSlaveValidationTest_DropInvalidReregistration_f1Jc3i/credential'
I0629 05:23:17.386319  2281 slave.cpp:264] Agent using credential for: test-principal
I0629 05:23:17.386339  2281 credentials.hpp:37] Loading credentials for authentication from '/tmp/RegisterSlaveValidationTest_DropInvalidReregistration_f1Jc3i/http_credentials'
I0629 05:23:17.386600  2281 http.cpp:975] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-executor'
I0629 05:23:17.386703  2281 http.cpp:996] Creating default 'jwt' HTTP authenticator for realm 'mesos-agent-executor'
I0629 05:23:17.386885  2281 http.cpp:975] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-readonly'
I0629 05:23:17.386973  2281 http.cpp:996] Creating default 'jwt' HTTP authenticator for realm 'mesos-agent-readonly'
I0629 05:23:17.387302  2281 http.cpp:975] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-readwrite'
I0629 05:23:17.387409  2281 http.cpp:996] Creating default 'jwt' HTTP authenticator for realm 'mesos-agent-readwrite'
I0629 05:23:17.388684  2281 slave.cpp:531] Agent resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0629 05:23:17.388783  2281 slave.cpp:539] Agent attributes: [  ]
I0629 05:23:17.388801  2281 slave.cpp:544] Agent hostname: b3c104d65da7
I0629 05:23:17.388916  2294 status_update_manager.cpp:177] Pausing sending status updates
I0629 05:23:17.390480  2288 state.cpp:62] Recovering state from '/tmp/RegisterSlaveValidationTest_DropInvalidReregistration_RVRQXx/meta'
I0629 05:23:17.390751  2275 status_update_manager.cpp:203] Recovering status update manager
I0629 05:23:17.391083  2286 containerizer.cpp:608] Recovering containerizer
I0629 05:23:17.392966  2285 provisioner.cpp:410] Provisioner recovery complete
I0629 05:23:17.393373  2272 slave.cpp:6075] Finished recovery
I0629 05:23:17.393777  2272 slave.cpp:6257] Querying resource estimator for oversubscribable resources
I0629 05:23:17.394049  2284 status_update_manager.cpp:177] Pausing sending status updates
I0629 05:23:17.394065  2272 slave.cpp:924] New master detected at master@172.17.0.3:42034
I0629 05:23:17.394129  2272 slave.cpp:959] Detecting new master
I0629 05:23:17.394268  2272 slave.cpp:6271] Received oversubscribable resources {} from the resource estimator
I0629 05:23:17.399830  2292 slave.cpp:986] Authenticating with master master@172.17.0.3:42034
I0629 05:23:17.399900  2292 slave.cpp:997] Using default CRAM-MD5 authenticatee
I0629 05:23:17.400095  2289 authenticatee.cpp:121] Creating new client SASL connection
I0629 05:23:17.400344  2275 master.cpp:7475] Authenticating slave(287)@172.17.0.3:42034
I0629 05:23:17.400452  2282 authenticator.cpp:414] Starting authentication session for crammd5-authenticatee(624)@172.17.0.3:42034
I0629 05:23:17.400650  2271 authenticator.cpp:98] Creating new server SASL connection
I0629 05:23:17.400858  2294 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
I0629 05:23:17.400883  2294 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
I0629 05:23:17.400981  2291 authenticator.cpp:204] Received SASL authentication start
I0629 05:23:17.401043  2291 authenticator.cpp:326] Authentication requires more steps
I0629 05:23:17.401151  2293 authenticatee.cpp:259] Received SASL authentication step
I0629 05:23:17.401382  2283 authenticator.cpp:232] Received SASL authentication step
I0629 05:23:17.401419  2283 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'b3c104d65da7' server FQDN: 'b3c104d65da7' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0629 05:23:17.401434  2283 auxprop.cpp:181] Looking up auxiliary property '*userPassword'
I0629 05:23:17.401470  2283 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0629 05:23:17.401492  2283 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'b3c104d65da7' server FQDN: 'b3c104d65da7' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0629 05:23:17.401506  2283 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0629 05:23:17.401515  2283 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0629 05:23:17.401532  2283 authenticator.cpp:318] Authentication success
I0629 05:23:17.401620  2289 authenticatee.cpp:299] Authentication success
I0629 05:23:17.401682  2281 master.cpp:7505] Successfully authenticated principal 'test-principal' at slave(287)@172.17.0.3:42034
I0629 05:23:17.401913  2273 authenticator.cpp:432] Authentication session cleanup for crammd5-authenticatee(624)@172.17.0.3:42034
I0629 05:23:17.401921  2284 slave.cpp:1081] Successfully authenticated with master master@172.17.0.3:42034
I0629 05:23:17.402212  2284 slave.cpp:1509] Will retry registration in 1.256299ms if necessary
I0629 05:23:17.402345  2294 master.cpp:5429] Received register agent message from slave(287)@172.17.0.3:42034 (b3c104d65da7)
I0629 05:23:17.402477  2294 master.cpp:3659] Authorizing agent with principal 'test-principal'
I0629 05:23:17.402930  2271 master.cpp:5564] Registering agent at slave(287)@172.17.0.3:42034 (b3c104d65da7) with id 25091bef-3845-4bb6-ae23-e18ac0f4d174-S0
I0629 05:23:17.403379  2281 registrar.cpp:493] Applied 1 operations in 62566ns; attempting to update the registry
I0629 05:23:17.404207  2281 registrar.cpp:550] Successfully updated the registry in 769024ns
I0629 05:23:17.404608  2286 slave.cpp:1509] Will retry registration in 25.850396ms if necessary
I0629 05:23:17.405179  2276 slave.cpp:4794] Received ping from slave-observer(289)@172.17.0.3:42034
I0629 05:23:17.405144  2294 master.cpp:5639] Registered agent 25091bef-3845-4bb6-ae23-e18ac0f4d174-S0 at slave(287)@172.17.0.3:42034 (b3c104d65da7) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0629 05:23:17.405441  2276 slave.cpp:1127] Registered with master master@172.17.0.3:42034; given agent ID 25091bef-3845-4bb6-ae23-e18ac0f4d174-S0
I0629 05:23:17.405465  2276 fetcher.cpp:94] Clearing fetcher cache
I0629 05:23:17.405508  2293 hierarchical.cpp:525] Added agent 25091bef-3845-4bb6-ae23-e18ac0f4d174-S0 (b3c104d65da7) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: {})
I0629 05:23:17.405594  2294 master.cpp:5429] Received register agent message from slave(287)@172.17.0.3:42034 (b3c104d65da7)
I0629 05:23:17.405743  2294 master.cpp:3659] Authorizing agent with principal 'test-principal'
I0629 05:23:17.405750  2288 status_update_manager.cpp:184] Resuming sending status updates
I0629 05:23:17.405933  2293 hierarchical.cpp:1850] No allocations performed
I0629 05:23:17.405971  2276 slave.cpp:1155] Checkpointing SlaveInfo to '/tmp/RegisterSlaveValidationTest_DropInvalidReregistration_RVRQXx/meta/slaves/25091bef-3845-4bb6-ae23-e18ac0f4d174-S0/slave.info'
I0629 05:23:17.405989  2293 hierarchical.cpp:1434] Performed allocation for 1 agents in 192285ns
I0629 05:23:17.406347  2276 slave.cpp:1193] Forwarding total oversubscribed resources {}
I0629 05:23:17.406415  2271 master.cpp:5542] Agent 25091bef-3845-4bb6-ae23-e18ac0f4d174-S0 at slave(287)@172.17.0.3:42034 (b3c104d65da7) already registered, resending acknowledgement
I0629 05:23:17.406491  2276 slave.cpp:924] New master detected at master@172.17.0.3:42034
I0629 05:23:17.406496  2280 status_update_manager.cpp:177] Pausing sending status updates
I0629 05:23:17.406548  2276 slave.cpp:959] Detecting new master
I0629 05:23:17.406570  2271 master.cpp:6324] Received update of agent 25091bef-3845-4bb6-ae23-e18ac0f4d174-S0 at slave(287)@172.17.0.3:42034 (b3c104d65da7) with total oversubscribed resources {}
I0629 05:23:17.406694  2276 slave.cpp:1127] Registered with master master@172.17.0.3:42034; given agent ID 25091bef-3845-4bb6-ae23-e18ac0f4d174-S0
I0629 05:23:17.406720  2276 fetcher.cpp:94] Clearing fetcher cache
I0629 05:23:17.406842  2275 status_update_manager.cpp:184] Resuming sending status updates
I0629 05:23:17.406961  2276 slave.cpp:1155] Checkpointing SlaveInfo to '/tmp/RegisterSlaveValidationTest_DropInvalidReregistration_RVRQXx/meta/slaves/25091bef-3845-4bb6-ae23-e18ac0f4d174-S0/slave.info'
I0629 05:23:17.407268  2276 slave.cpp:1193] Forwarding total oversubscribed resources {}
I0629 05:23:17.407431  2278 master.cpp:6324] Received update of agent 25091bef-3845-4bb6-ae23-e18ac0f4d174-S0 at slave(287)@172.17.0.3:42034 (b3c104d65da7) with total oversubscribed resources {}
I0629 05:23:17.413727  2276 slave.cpp:986] Authenticating with master master@172.17.0.3:42034
I0629 05:23:17.413785  2276 slave.cpp:997] Using default CRAM-MD5 authenticatee
I0629 05:23:17.413978  2278 authenticatee.cpp:121] Creating new client SASL connection
I0629 05:23:17.414268  2290 master.cpp:7475] Authenticating slave(287)@172.17.0.3:42034
I0629 05:23:17.414409  2273 authenticator.cpp:414] Starting authentication session for crammd5-authenticatee(625)@172.17.0.3:42034
I0629 05:23:17.414752  2292 authenticator.cpp:98] Creating new server SASL connection
I0629 05:23:17.414988  2272 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
I0629 05:23:17.415014  2272 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
I0629 05:23:17.415158  2278 authenticator.cpp:204] Received SASL authentication start
I0629 05:23:17.415225  2278 authenticator.cpp:326] Authentication requires more steps
I0629 05:23:17.415345  2285 authenticatee.cpp:259] Received SASL authentication step
I0629 05:23:17.415586  2271 authenticator.cpp:232] Received SASL authentication step
I0629 05:23:17.415616  2271 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'b3c104d65da7' server FQDN: 'b3c104d65da7' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0629 05:23:17.415629  2271 auxprop.cpp:181] Looking up auxiliary property '*userPassword'
I0629 05:23:17.415665  2271 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0629 05:23:17.415689  2271 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'b3c104d65da7' server FQDN: 'b3c104d65da7' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0629 05:23:17.415701  2271 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0629 05:23:17.415711  2271 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0629 05:23:17.415726  2271 authenticator.cpp:318] Authentication success
I0629 05:23:17.415807  2278 authenticatee.cpp:299] Authentication success
I0629 05:23:17.415865  2294 master.cpp:7505] Successfully authenticated principal 'test-principal' at slave(287)@172.17.0.3:42034
I0629 05:23:17.415910  2288 authenticator.cpp:432] Authentication session cleanup for crammd5-authenticatee(625)@172.17.0.3:42034
I0629 05:23:17.416126  2274 slave.cpp:1081] Successfully authenticated with master master@172.17.0.3:42034
I0629 05:23:18.372947  2275 hierarchical.cpp:1850] No allocations performed
I0629 05:23:18.373133  2275 hierarchical.cpp:1434] Performed allocation for 1 agents in 578948ns
I0629 05:23:19.374871  2289 hierarchical.cpp:1850] No allocations performed
I0629 05:23:19.375056  2289 hierarchical.cpp:1434] Performed allocation for 1 agents in 520444ns
I0629 05:23:20.375833  2290 hierarchical.cpp:1850] No allocations performed
I0629 05:23:20.375975  2290 hierarchical.cpp:1434] Performed allocation for 1 agents in 435113ns
I0629 05:23:21.377182  2279 hierarchical.cpp:1850] No allocations performed
I0629 05:23:21.377399  2279 hierarchical.cpp:1434] Performed allocation for 1 agents in 639655ns
I0629 05:23:22.378278  2277 hierarchical.cpp:1850] No allocations performed
I0629 05:23:22.378437  2277 hierarchical.cpp:1434] Performed allocation for 1 agents in 427151ns
I0629 05:23:23.380287  2292 hierarchical.cpp:1850] No allocations performed
I0629 05:23:23.380481  2292 hierarchical.cpp:1434] Performed allocation for 1 agents in 578747ns
I0629 05:23:24.381932  2273 hierarchical.cpp:1850] No allocations performed
I0629 05:23:24.382097  2273 hierarchical.cpp:1434] Performed allocation for 1 agents in 476981ns
I0629 05:23:25.383314  2275 hierarchical.cpp:1850] No allocations performed
I0629 05:23:25.383499  2275 hierarchical.cpp:1434] Performed allocation for 1 agents in 500084ns
I0629 05:23:26.384625  2281 hierarchical.cpp:1850] No allocations performed
I0629 05:23:26.384855  2281 hierarchical.cpp:1434] Performed allocation for 1 agents in 515354ns
I0629 05:23:27.385927  2291 hierarchical.cpp:1850] No allocations performed
I0629 05:23:27.386139  2291 hierarchical.cpp:1434] Performed allocation for 1 agents in 562796ns
I0629 05:23:28.387132  2285 hierarchical.cpp:1850] No allocations performed
I0629 05:23:28.387297  2285 hierarchical.cpp:1434] Performed allocation for 1 agents in 438370ns
I0629 05:23:29.388170  2286 hierarchical.cpp:1850] No allocations performed
I0629 05:23:29.388309  2286 hierarchical.cpp:1434] Performed allocation for 1 agents in 317943ns
I0629 05:23:30.389729  2278 hierarchical.cpp:1850] No allocations performed
I0629 05:23:30.389909  2278 hierarchical.cpp:1434] Performed allocation for 1 agents in 475524ns
I0629 05:23:31.390977  2280 hierarchical.cpp:1850] No allocations performed
I0629 05:23:31.391084  2280 hierarchical.cpp:1434] Performed allocation for 1 agents in 266216ns
I0629 05:23:32.391724  2287 hierarchical.cpp:1850] No allocations performed
I0629 05:23:32.391827  2287 hierarchical.cpp:1434] Performed allocation for 1 agents in 270448ns
I0629 05:23:32.394664  2276 slave.cpp:6257] Querying resource estimator for oversubscribable resources
I0629 05:23:32.395167  2293 slave.cpp:6271] Received oversubscribable resources {} from the resource estimator
I0629 05:23:32.406095  2288 slave.cpp:4794] Received ping from slave-observer(289)@172.17.0.3:42034
../../src/tests/master_validation_tests.cpp:3757: Failure
Failed to wait 15secs for reregisterSlaveMessage
I0629 05:23:32.409525  2291 slave.cpp:796] Agent terminating
I0629 05:23:32.410306  2292 master.cpp:1313] Agent 25091bef-3845-4bb6-ae23-e18ac0f4d174-S0 at slave(287)@172.17.0.3:42034 (b3c104d65da7) disconnected
I0629 05:23:32.410360  2292 master.cpp:3197] Disconnecting agent 25091bef-3845-4bb6-ae23-e18ac0f4d174-S0 at slave(287)@172.17.0.3:42034 (b3c104d65da7)
I0629 05:23:32.410907  2292 master.cpp:3216] Deactivating agent 25091bef-3845-4bb6-ae23-e18ac0f4d174-S0 at slave(287)@172.17.0.3:42034 (b3c104d65da7)
I0629 05:23:32.411128  2288 hierarchical.cpp:653] Agent 25091bef-3845-4bb6-ae23-e18ac0f4d174-S0 deactivated
I0629 05:23:32.418536  2278 master.cpp:1155] Master terminating
I0629 05:23:32.419867  2289 hierarchical.cpp:558] Removed agent 25091bef-3845-4bb6-ae23-e18ac0f4d174-S0
../../3rdparty/libprocess/include/process/gmock.hpp:446: Failure
Actual function call count doesn't match EXPECT_CALL(filter->mock, filter(testing::A<const MessageEvent&>()))...
    Expected args: message matcher (8-byte object <58-BF 04-20 F5-7F 00-00>, 1, 1-byte object <E8>)
         Expected: to be called once
           Actual: never called - unsatisfied and active
[  FAILED  ] RegisterSlaveValidationTest.DropInvalidReregistration (15061 ms)
{code}

Observed on macOS when testing 1.3.1:

{noformat}
[ RUN      ] RegisterSlaveValidationTest.DropInvalidReregistration
I0802 13:34:21.685421 2516382656 cluster.cpp:162] Creating default 'local' authorizer
I0802 13:34:21.686971 224620544 master.cpp:436] Master 21402191-4b94-4a3c-9c84-cf9942b0b81a (192.168.1.15) started on 192.168.1.15:53913
I0802 13:34:21.686991 224620544 master.cpp:438] Flags at startup: --acls="""" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate_agents=""true"" --authenticate_frameworks=""true"" --authenticate_http_frameworks=""true"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/private/var/folders/xb/nkj68dnn2wqd8mgfb_mvsds00000gn/T/7eFkbc/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_unreachable_tasks_per_framework=""1000"" --port=""5050"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""in_memory"" --registry_fetch_timeout=""1mins"" --registry_gc_interval=""15mins"" --registry_max_agent_age=""2weeks"" --registry_max_agent_count=""102400"" --registry_store_timeout=""100secs"" --registry_strict=""false"" --root_submissions=""true"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/private/var/folders/xb/nkj68dnn2wqd8mgfb_mvsds00000gn/T/7eFkbc/master"" --zk_session_timeout=""10secs""
I0802 13:34:21.687160 224620544 master.cpp:488] Master only allowing authenticated frameworks to register
I0802 13:34:21.687182 224620544 master.cpp:502] Master only allowing authenticated agents to register
I0802 13:34:21.687191 224620544 master.cpp:515] Master only allowing authenticated HTTP frameworks to register
I0802 13:34:21.687198 224620544 credentials.hpp:37] Loading credentials for authentication from '/private/var/folders/xb/nkj68dnn2wqd8mgfb_mvsds00000gn/T/7eFkbc/credentials'
I0802 13:34:21.687373 224620544 master.cpp:560] Using default 'crammd5' authenticator
I0802 13:34:21.687434 224620544 http.cpp:975] Creating default 'basic' HTTP authenticator for realm 'mesos-master-readonly'
I0802 13:34:21.687558 224620544 http.cpp:975] Creating default 'basic' HTTP authenticator for realm 'mesos-master-readwrite'
I0802 13:34:21.687672 224620544 http.cpp:975] Creating default 'basic' HTTP authenticator for realm 'mesos-master-scheduler'
I0802 13:34:21.687784 224620544 master.cpp:640] Authorization enabled
I0802 13:34:21.689018 227303424 master.cpp:2161] Elected as the leading master!
I0802 13:34:21.689031 227303424 master.cpp:1700] Recovering from registrar
I0802 13:34:21.689425 224083968 registrar.cpp:389] Successfully fetched the registry (0B) in 326912ns
I0802 13:34:21.689481 224083968 registrar.cpp:493] Applied 1 operations in 17us; attempting to update the registry
I0802 13:34:21.689767 224083968 registrar.cpp:550] Successfully updated the registry in 263168ns
I0802 13:34:21.689812 224083968 registrar.cpp:422] Successfully recovered registrar
I0802 13:34:21.690094 226766848 master.cpp:1799] Recovered 0 agents from the registry (135B); allowing 10mins for agents to re-register
I0802 13:34:21.692261 2516382656 containerizer.cpp:221] Using isolation: posix/cpu,posix/mem,filesystem/posix
I0802 13:34:21.692390 2516382656 provisioner.cpp:249] Using default backend 'copy'
I0802 13:34:21.693034 2516382656 cluster.cpp:448] Creating default 'local' authorizer
I0802 13:34:21.693660 226766848 slave.cpp:231] Mesos agent started on (5)@192.168.1.15:53913
I0802 13:34:21.693699 226766848 slave.cpp:232] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/var/folders/xb/nkj68dnn2wqd8mgfb_mvsds00000gn/T/mesos/store/appc"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""true"" --authenticatee=""crammd5"" --authentication_backoff_factor=""1secs"" --authorizer=""local"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/var/folders/xb/nkj68dnn2wqd8mgfb_mvsds00000gn/T/RegisterSlaveValidationTest_DropInvalidReregistration_FaKGv2/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/var/folders/xb/nkj68dnn2wqd8mgfb_mvsds00000gn/T/mesos/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_reregistration_timeout=""2secs"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/var/folders/xb/nkj68dnn2wqd8mgfb_mvsds00000gn/T/RegisterSlaveValidationTest_DropInvalidReregistration_FaKGv2/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_command_executor=""false"" --http_credentials=""/var/folders/xb/nkj68dnn2wqd8mgfb_mvsds00000gn/T/RegisterSlaveValidationTest_DropInvalidReregistration_FaKGv2/http_credentials"" --http_heartbeat_interval=""30secs"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher=""posix"" --launcher_dir=""/Users/bmahler/Downloads/mesos-1.3.1/src"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_executors_per_framework=""150"" --oversubscribed_resources_interval=""15secs"" --port=""5051"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]"" --runtime_dir=""/var/folders/xb/nkj68dnn2wqd8mgfb_mvsds00000gn/T/RegisterSlaveValidationTest_DropInvalidReregistration_FaKGv2"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --version=""false"" --work_dir=""/var/folders/xb/nkj68dnn2wqd8mgfb_mvsds00000gn/T/RegisterSlaveValidationTest_DropInvalidReregistration_JdMGIN""
I0802 13:34:21.693886 226766848 credentials.hpp:86] Loading credential for authentication from '/var/folders/xb/nkj68dnn2wqd8mgfb_mvsds00000gn/T/RegisterSlaveValidationTest_DropInvalidReregistration_FaKGv2/credential'
I0802 13:34:21.693975 226766848 slave.cpp:264] Agent using credential for: test-principal
I0802 13:34:21.693989 226766848 credentials.hpp:37] Loading credentials for authentication from '/var/folders/xb/nkj68dnn2wqd8mgfb_mvsds00000gn/T/RegisterSlaveValidationTest_DropInvalidReregistration_FaKGv2/http_credentials'
I0802 13:34:21.694102 226766848 http.cpp:975] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-readonly'
I0802 13:34:21.694186 226766848 http.cpp:975] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-readwrite'
I0802 13:34:21.694964 226766848 slave.cpp:531] Agent resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0802 13:34:21.695008 226766848 slave.cpp:539] Agent attributes: [  ]
I0802 13:34:21.695016 226766848 slave.cpp:544] Agent hostname: 192.168.1.15
I0802 13:34:21.695087 227840000 status_update_manager.cpp:177] Pausing sending status updates
I0802 13:34:21.695673 225693696 state.cpp:62] Recovering state from '/var/folders/xb/nkj68dnn2wqd8mgfb_mvsds00000gn/T/RegisterSlaveValidationTest_DropInvalidReregistration_JdMGIN/meta'
I0802 13:34:21.695814 226230272 status_update_manager.cpp:203] Recovering status update manager
I0802 13:34:21.695937 225693696 containerizer.cpp:608] Recovering containerizer
I0802 13:34:21.696552 224620544 provisioner.cpp:410] Provisioner recovery complete
I0802 13:34:21.696770 226766848 slave.cpp:6075] Finished recovery
I0802 13:34:21.697232 225157120 status_update_manager.cpp:177] Pausing sending status updates
I0802 13:34:21.697247 226766848 slave.cpp:924] New master detected at master@192.168.1.15:53913
I0802 13:34:21.697314 226766848 slave.cpp:959] Detecting new master
I0802 13:34:21.706558 227303424 slave.cpp:986] Authenticating with master master@192.168.1.15:53913
I0802 13:34:21.706626 227303424 slave.cpp:997] Using default CRAM-MD5 authenticatee
I0802 13:34:21.706876 227840000 authenticatee.cpp:121] Creating new client SASL connection
I0802 13:34:21.707221 227303424 master.cpp:7475] Authenticating slave(5)@192.168.1.15:53913
I0802 13:34:21.707667 224083968 authenticator.cpp:98] Creating new server SASL connection
I0802 13:34:21.707852 227840000 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
I0802 13:34:21.707921 227840000 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
I0802 13:34:21.708161 227840000 authenticator.cpp:204] Received SASL authentication start
I0802 13:34:21.708250 227840000 authenticator.cpp:326] Authentication requires more steps
I0802 13:34:21.708449 225693696 authenticatee.cpp:259] Received SASL authentication step
I0802 13:34:21.708734 227303424 authenticator.cpp:232] Received SASL authentication step
I0802 13:34:21.708827 227303424 authenticator.cpp:318] Authentication success
I0802 13:34:21.708992 225693696 authenticatee.cpp:299] Authentication success
I0802 13:34:21.709059 224083968 master.cpp:7505] Successfully authenticated principal 'test-principal' at slave(5)@192.168.1.15:53913
I0802 13:34:21.709574 226766848 slave.cpp:1081] Successfully authenticated with master master@192.168.1.15:53913
I0802 13:34:21.709900 224083968 master.cpp:5429] Received register agent message from slave(5)@192.168.1.15:53913 (192.168.1.15)
I0802 13:34:21.709947 224083968 master.cpp:3659] Authorizing agent with principal 'test-principal'
I0802 13:34:21.710321 225693696 master.cpp:5564] Registering agent at slave(5)@192.168.1.15:53913 (192.168.1.15) with id 21402191-4b94-4a3c-9c84-cf9942b0b81a-S0
I0802 13:34:21.710543 224620544 registrar.cpp:493] Applied 1 operations in 47us; attempting to update the registry
I0802 13:34:21.710937 224620544 registrar.cpp:550] Successfully updated the registry in 355840ns
I0802 13:34:21.711361 225157120 master.cpp:5639] Registered agent 21402191-4b94-4a3c-9c84-cf9942b0b81a-S0 at slave(5)@192.168.1.15:53913 (192.168.1.15) with cpus(*)",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.8846153846153846,0.9878787878787878,0.9878787878787878,0.0
Bug,Java HTTP adapter crashes JVM when leading master disconnects.,"When a Java scheduler using HTTP v0-v1 adapter loses the leading Mesos master, {{V0ToV1AdapterProcess::disconnected()}} is invoked, which in turn invokes Java scheduler [code via JNI|https://github.com/apache/mesos/blob/87c38b9e2bc5b1030a071ddf0aab69db70d64781/src/java/jni/org_apache_mesos_v1_scheduler_V0Mesos.cpp#L446]. This call uses the wrong object, {{jmesos}} instead of {{jscheduler}}, which crashes JVM:
{noformat}
#
# A fatal error has been detected by the Java Runtime Environment:
#
#  SIGSEGV (0xb) at pc=0x00007f4bca3849bf, pid=21, tid=0x00007f4b2ac45700
#
# JRE version: Java(TM) SE Runtime Environment (8.0_131-b11) (build 1.8.0_131-b11)
# Java VM: Java HotSpot(TM) 64-Bit Server VM (25.131-b11 mixed mode linux-amd64 compressed oops)
# Problematic frame:
# V  [libjvm.so+0x6d39bf]  jni_invoke_nonstatic(JNIEnv_*, JavaValue*, _jobject*, JNICallType, _jmethodID*, JNI_ArgumentPusher*, Thread*)+0x1af
{noformat}
{noformat}
Stack: [0x00007f4b2a445000,0x00007f4b2ac46000],  sp=0x00007f4b2ac44a80,  free space=8190k
Native frames: (J=compiled Java code, j=interpreted, Vv=VM code, C=native code)
V  [libjvm.so+0x6d39bf]  jni_invoke_nonstatic(JNIEnv_*, JavaValue*, _jobject*, JNICallType, _jmethodID*, JNI_ArgumentPusher*, Thread*)+0x1af
V  [libjvm.so+0x6d7fef]  jni_CallVoidMethodV+0x10f
C  [libmesos-1.2.0.so+0x1aa32d3]  JNIEnv_::CallVoidMethod(_jobject*, _jmethodID*, ...)+0x93
{noformat}",3.0,"1.1.2,1.2.1,1.3.0",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.717948717948718,0.8242424242424242,0.7212121212121212,0.9358715596330275
Bug,MasterTest.IgnoreOldAgentReregistration test is flaky,"Observed this on ASF CI._x000D_
_x000D_
{code}_x000D_
[ RUN      ] MasterTest.IgnoreOldAgentReregistration_x000D_
I0627 05:23:06.031154  4917 cluster.cpp:162] Creating default 'local' authorizer_x000D_
I0627 05:23:06.033433  4945 master.cpp:438] Master a8778782-0da1-49a5-9cb8-9f6d11701733 (c43debbe7e32) started on 172.17.0.4:41747_x000D_
I0627 05:23:06.033457  4945 master.cpp:440] Flags at startup: --acls="""" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate_agents=""true"" --authenticate_frameworks=""true"" --authenticate_http_frameworks=""true"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/2BARnF/credentials"" --filter_gpu_resources=""true"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_unreachable_tasks_per_framework=""1000"" --port=""5050"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""in_memory"" --registry_fetch_timeout=""1mins"" --registry_gc_interval=""15mins"" --registry_max_agent_age=""2weeks"" --registry_max_agent_count=""102400"" --registry_store_timeout=""100secs"" --registry_strict=""false"" --root_submissions=""true"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-1.4.0/_inst/share/mesos/webui"" --work_dir=""/tmp/2BARnF/master"" --zk_session_timeout=""10secs""_x000D_
I0627 05:23:06.033771  4945 master.cpp:490] Master only allowing authenticated frameworks to register_x000D_
I0627 05:23:06.033787  4945 master.cpp:504] Master only allowing authenticated agents to register_x000D_
I0627 05:23:06.033798  4945 master.cpp:517] Master only allowing authenticated HTTP frameworks to register_x000D_
I0627 05:23:06.033812  4945 credentials.hpp:37] Loading credentials for authentication from '/tmp/2BARnF/credentials'_x000D_
I0627 05:23:06.034080  4945 master.cpp:562] Using default 'crammd5' authenticator_x000D_
I0627 05:23:06.034221  4945 http.cpp:974] Creating default 'basic' HTTP authenticator for realm 'mesos-master-readonly'_x000D_
I0627 05:23:06.034409  4945 http.cpp:974] Creating default 'basic' HTTP authenticator for realm 'mesos-master-readwrite'_x000D_
I0627 05:23:06.034569  4945 http.cpp:974] Creating default 'basic' HTTP authenticator for realm 'mesos-master-scheduler'_x000D_
I0627 05:23:06.034688  4945 master.cpp:642] Authorization enabled_x000D_
I0627 05:23:06.034862  4938 whitelist_watcher.cpp:77] No whitelist given_x000D_
I0627 05:23:06.034868  4950 hierarchical.cpp:169] Initialized hierarchical allocator process_x000D_
I0627 05:23:06.037211  4957 master.cpp:2161] Elected as the leading master!_x000D_
I0627 05:23:06.037236  4957 master.cpp:1700] Recovering from registrar_x000D_
I0627 05:23:06.037333  4938 registrar.cpp:345] Recovering registrar_x000D_
I0627 05:23:06.038146  4938 registrar.cpp:389] Successfully fetched the registry (0B) in 768256ns_x000D_
I0627 05:23:06.038290  4938 registrar.cpp:493] Applied 1 operations in 30798ns; attempting to update the registry_x000D_
I0627 05:23:06.038861  4938 registrar.cpp:550] Successfully updated the registry in 510976ns_x000D_
I0627 05:23:06.038960  4938 registrar.cpp:422] Successfully recovered registrar_x000D_
I0627 05:23:06.039364  4941 hierarchical.cpp:207] Skipping recovery of hierarchical allocator: nothing to recover_x000D_
I0627 05:23:06.039594  4958 master.cpp:1799] Recovered 0 agents from the registry (129B); allowing 10mins for agents to re-register_x000D_
I0627 05:23:06.043999  4917 containerizer.cpp:230] Using isolation: posix/cpu,posix/mem,filesystem/posix,network/cni,environment_secret_x000D_
W0627 05:23:06.044456  4917 backend.cpp:76] Failed to create 'aufs' backend: AufsBackend requires root privileges_x000D_
W0627 05:23:06.044548  4917 backend.cpp:76] Failed to create 'bind' backend: BindBackend requires root privileges_x000D_
I0627 05:23:06.044580  4917 provisioner.cpp:255] Using default backend 'copy'_x000D_
I0627 05:23:06.046222  4917 cluster.cpp:448] Creating default 'local' authorizer_x000D_
I0627 05:23:06.047572  4950 slave.cpp:249] Mesos agent started on (269)@172.17.0.4:41747_x000D_
I0627 05:23:06.047591  4950 slave.cpp:250] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/MasterTest_IgnoreOldAgentReregistration_Bgz7OK/store/appc"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""true"" --authenticatee=""crammd5"" --authentication_backoff_factor=""1secs"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/MasterTest_IgnoreOldAgentReregistration_Bgz7OK/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/MasterTest_IgnoreOldAgentReregistration_Bgz7OK/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_reregistration_timeout=""2secs"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/MasterTest_IgnoreOldAgentReregistration_Bgz7OK/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_command_executor=""false"" --http_credentials=""/tmp/MasterTest_IgnoreOldAgentReregistration_Bgz7OK/http_credentials"" --http_heartbeat_interval=""30secs"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher=""posix"" --launcher_dir=""/mesos/mesos-1.4.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_executors_per_framework=""150"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --port=""5051"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --runtime_dir=""/tmp/MasterTest_IgnoreOldAgentReregistration_Bgz7OK"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/MasterTest_IgnoreOldAgentReregistration_US8t6c""_x000D_
I0627 05:23:06.047937  4950 credentials.hpp:86] Loading credential for authentication from '/tmp/MasterTest_IgnoreOldAgentReregistration_Bgz7OK/credential'_x000D_
I0627 05:23:06.048065  4950 slave.cpp:282] Agent using credential for: test-principal_x000D_
I0627 05:23:06.048085  4950 credentials.hpp:37] Loading credentials for authentication from '/tmp/MasterTest_IgnoreOldAgentReregistration_Bgz7OK/http_credentials'_x000D_
I0627 05:23:06.048274  4950 http.cpp:974] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-readonly'_x000D_
I0627 05:23:06.048396  4950 http.cpp:974] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-readwrite'_x000D_
I0627 05:23:06.049597  4950 slave.cpp:553] Agent resources: [{""name"":""cpus"",""scalar"":{""value"":2.0},""type"":""SCALAR""},{""name"":""mem"",""scalar"":{""value"":1024.0},""type"":""SCALAR""},{""name"":""disk"",""scalar"":{""value"":1024.0},""type"":""SCALAR""},{""name"":""ports"",""ranges"":{""range"":[{""begin"":31000,""end"":32000}]},""type"":""RANGES""}]_x000D_
I0627 05:23:06.049815  4950 slave.cpp:561] Agent attributes: [  ]_x000D_
I0627 05:23:06.049830  4950 slave.cpp:566] Agent hostname: c43debbe7e32_x000D_
I0627 05:23:06.049934  4941 status_update_manager.cpp:177] Pausing sending status updates_x000D_
I0627 05:23:06.051394  4957 state.cpp:64] Recovering state from '/tmp/MasterTest_IgnoreOldAgentReregistration_US8t6c/meta'_x000D_
I0627 05:23:06.051775  4954 status_update_manager.cpp:203] Recovering status update manager_x000D_
I0627 05:23:06.052050  4959 containerizer.cpp:582] Recovering containerizer_x000D_
I0627 05:23:06.053479  4946 provisioner.cpp:416] Provisioner recovery complete_x000D_
I0627 05:23:06.053802  4937 slave.cpp:6168] Finished recovery_x000D_
I0627 05:23:06.054203  4937 slave.cpp:6350] Querying resource estimator for oversubscribable resources_x000D_
I0627 05:23:06.054503  4954 slave.cpp:946] New master detected at master@172.17.0.4:41747_x000D_
I0627 05:23:06.054517  4950 status_update_manager.cpp:177] Pausing sending status updates_x000D_
I0627 05:23:06.054641  4954 slave.cpp:981] Detecting new master_x000D_
I0627 05:23:06.054774  4954 slave.cpp:6364] Received oversubscribable resources {} from the resource estimator_x000D_
I0627 05:23:06.065029  4943 slave.cpp:1008] Authenticating with master master@172.17.0.4:41747_x000D_
I0627 05:23:06.065158  4943 slave.cpp:1019] Using default CRAM-MD5 authenticatee_x000D_
I0627 05:23:06.065412  4945 authenticatee.cpp:121] Creating new client SASL connection_x000D_
I0627 05:23:06.065798  4938 master.cpp:7640] Authenticating slave(269)@172.17.0.4:41747_x000D_
I0627 05:23:06.065937  4954 authenticator.cpp:414] Starting authentication session for crammd5-authenticatee(577)@172.17.0.4:41747_x000D_
I0627 05:23:06.066174  4937 authenticator.cpp:98] Creating new server SASL connection_x000D_
I0627 05:23:06.066419  4958 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5_x000D_
I0627 05:23:06.066448  4958 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'_x000D_
I0627 05:23:06.066550  4958 authenticator.cpp:204] Received SASL authentication start_x000D_
I0627 05:23:06.066612  4958 authenticator.cpp:326] Authentication requires more steps_x000D_
I0627 05:23:06.066845  4952 authenticatee.cpp:259] Received SASL authentication step_x000D_
I0627 05:23:06.067041  4936 authenticator.cpp:232] Received SASL authentication step_x000D_
I0627 05:23:06.067081  4936 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'c43debbe7e32' server FQDN: 'c43debbe7e32' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false _x000D_
I0627 05:23:06.067096  4936 auxprop.cpp:181] Looking up auxiliary property '*userPassword'_x000D_
I0627 05:23:06.067129  4936 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'_x000D_
I0627 05:23:06.067149  4936 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'c43debbe7e32' server FQDN: 'c43debbe7e32' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true _x000D_
I0627 05:23:06.067159  4936 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true_x000D_
I0627 05:23:06.067167  4936 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true_x000D_
I0627 05:23:06.067184  4936 authenticator.cpp:318] Authentication success_x000D_
I0627 05:23:06.067267  4945 authenticatee.cpp:299] Authentication success_x000D_
I0627 05:23:06.067364  4954 master.cpp:7670] Successfully authenticated principal 'test-principal' at slave(269)@172.17.0.4:41747_x000D_
I0627 05:23:06.067433  4943 authenticator.cpp:432] Authentication session cleanup for crammd5-authenticatee(577)@172.17.0.4:41747_x000D_
I0627 05:23:06.067656  4939 slave.cpp:1103] Successfully authenticated with master master@172.17.0.4:41747_x000D_
I0627 05:23:06.067855  4939 slave.cpp:1545] Will retry registration in 1.835348ms if necessary_x000D_
I0627 05:23:06.068022  4949 master.cpp:5602] Received register agent message from slave(269)@172.17.0.4:41747 (c43debbe7e32)_x000D_
I0627 05:23:06.068155  4949 master.cpp:3742] Authorizing agent with principal 'test-principal'_x000D_
I0627 05:23:06.068568  4957 master.cpp:5662] Authorized registration of agent at slave(269)@172.17.0.4:41747 (c43debbe7e32)_x000D_
I0627 05:23:06.068668  4957 master.cpp:5741] Registering agent at slave(269)@172.17.0.4:41747 (c43debbe7e32) with id a8778782-0da1-49a5-9cb8-9f6d11701733-S0_x000D_
I0627 05:23:06.069067  4937 registrar.cpp:493] Applied 1 operations in 56568ns; attempting to update the registry_x000D_
I0627 05:23:06.069788  4937 registrar.cpp:550] Successfully updated the registry in 659968ns_x000D_
I0627 05:23:06.069957  4946 master.cpp:5788] Admitted agent a8778782-0da1-49a5-9cb8-9f6d11701733-S0 at slave(269)@172.17.0.4:41747 (c43debbe7e32)_x000D_
I0627 05:23:06.070394  4950 slave.cpp:1545] Will retry registration in 34.487053ms if necessary_x000D_
I0627 05:23:06.070508  4950 slave.cpp:4866] Received ping from slave-observer(272)@172.17.0.4:41747_x000D_
I0627 05:23:06.070776  4950 slave.cpp:1149] Registered with master master@172.17.0.4:41747; given agent ID a8778782-0da1-49a5-9cb8-9f6d11701733-S0_x000D_
I0627 05:23:06.070569  4946 master.cpp:5819] Registered agent a8778782-0da1-49a5-9cb8-9f6d11701733-S0 at slave(269)@172.17.0.4:41747 (c43debbe7e32) with [{""name"":""cpus"",""scalar"":{""value"":2.0},""type"":""SCALAR""},{""name"":""mem"",""scalar"":{""value"":1024.0},""type"":""SCALAR""},{""name"":""disk"",""scalar"":{""value"":1024.0},""type"":""SCALAR""},{""name"":""ports"",""ranges"":{""range"":[{""begin"":31000,""end"":32000}]},""type"":""RANGES""}]_x000D_
I0627 05:23:06.070894  4942 status_update_manager.cpp:184] Resuming sending status updates_x000D_
I0627 05:23:06.070890  4949 hierarchical.cpp:587] Added agent a8778782-0da1-49a5-9cb8-9f6d11701733-S0 (c43debbe7e32) with cpus:2; mem:1024; disk:1024; ports:[31000-32000] (allocated: {})_x000D_
I0627 05:23:06.071032  4946 master.cpp:5602] Received register agent message from slave(269)@172.17.0.4:41747 (c43debbe7e32)_x000D_
I0627 05:23:06.071089  4950 slave.cpp:1169] Checkpointing SlaveInfo to '/tmp/MasterTest_IgnoreOldAgentReregistration_US8t6c/meta/slaves/a8778782-0da1-49a5-9cb8-9f6d11701733-S0/slave.info'_x000D_
I0627 05:23:06.071142  4946 master.cpp:3742] Authorizing agent with principal 'test-principal'_x000D_
I0627 05:23:06.071179  4949 hierarchical.cpp:1938] No allocations performed_x000D_
I0627 05:23:06.071238  4949 hierarchical.cpp:1493] Performed allocation for 1 agents in 180904ns_x000D_
I0627 05:23:06.071403  4950 slave.cpp:1207] Forwarding total oversubscribed resources {}_x000D_
I0627 05:23:06.071549  4944 master.cpp:5662] Authorized registration of agent at slave(269)@172.17.0.4:41747 (c43debbe7e32)_x000D_
I0627 05:23:06.071624  4952 status_update_manager.cpp:177] Pausing sending status updates_x000D_
I0627 05:23:06.071624  4944 master.cpp:5719] Agent a8778782-0da1-49a5-9cb8-9f6d11701733-S0 at slave(269)@172.17.0.4:41747 (c43debbe7e32) already registered, resending acknowledgement_x000D_
I0627 05:23:06.071624  4950 slave.cpp:946] New master detected at master@172.17.0.4:41747_x000D_
I0627 05:23:06.071753  4950 slave.cpp:981] Detecting new master_x000D_
I0627 05:23:06.071769  4944 master.cpp:6508] Received update of agent a8778782-0da1-49a5-9cb8-9f6d11701733-S0 at slave(269)@172.17.0.4:41747 (c43debbe7e32) with total oversubscribed resources {}_x000D_
I0627 05:23:06.071905  4950 slave.cpp:1149] Registered with master master@172.17.0.4:41747; given agent ID a8778782-0da1-49a5-9cb8-9f6d11701733-S0_x000D_
I0627 05:23:06.072108  4943 status_update_manager.cpp:184] Resuming sending status updates_x000D_
I0627 05:23:06.072243  4950 slave.cpp:1169] Checkpointing SlaveInfo to '/tmp/MasterTest_IgnoreOldAgentReregistration_US8t6c/meta/slaves/a8778782-0da1-49a5-9cb8-9f6d11701733-S0/slave.info'_x000D_
I0627 05:23:06.072391  4948 hierarchical.cpp:1938] No allocations performed_x000D_
I0627 05:23:06.072443  4948 hierarchical.cpp:1493] Performed allocation for 1 agents in 158723ns_x000D_
I0627 05:23:06.072525  4950 slave.cpp:1207] Forwarding total oversubscribed resources {}_x000D_
I0627 05:23:06.072618  4950 slave.cpp:1008] Authenticating with master master@172.17.0.4:41747_x000D_
I0627 05:23:06.072661  4950 slave.cpp:1019] Using default CRAM-MD5 authenticatee_x000D_
I0627 05:23:06.072693  4959 master.cpp:6508] Received update of agent a8778782-0da1-49a5-9cb8-9f6d11701733-S0 at slave(269)@172.17.0.4:41747 (c43debbe7e32) with total oversubscribed resources {}_x000D_
I0627 05:23:06.072882  4946 authenticatee.cpp:121] Creating new client SASL connection_x000D_
I0627 05:23:06.073169  4944 master.cpp:7640] Authenticating slave(269)@172.17.0.4:41747_x000D_
I0627 05:23:06.073289  4950 authenticator.cpp:414] Starting authentication session for crammd5-authenticatee(578)@172.17.0.4:41747_x000D_
I0627 05:23:06.073531  4952 authenticator.cpp:98] Creating new server SASL connection_x000D_
I0627 05:23:06.073796  4949 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5_x000D_
I0627 05:23:06.073827  4949 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'_x000D_
I0627 05:23:06.073998  4936 authenticator.cpp:204] Received SASL authentication start_x000D_
I0627 05:23:06.074061  4936 authenticator.cpp:326] Authentication requires more steps_x000D_
I0627 05:23:06.074195  4957 authenticatee.cpp:259] Received SASL authentication step_x000D_
I0627 05:23:06.074335  4953 authenticator.cpp:232] Received SASL authentication step_x000D_
I0627 05:23:06.074370  4953 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'c43debbe7e32' server FQDN: 'c43debbe7e32' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false _x000D_
I0627 05:23:06.074388  4953 auxprop.cpp:181] Looking up auxiliary property '*userPassword'_x000D_
I0627 05:23:06.074416  4953 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'_x000D_
I0627 05:23:06.074436  4953 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'c43debbe7e32' server FQDN: 'c43debbe7e32' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true _x000D_
I0627 05:23:06.074446  4953 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true_x000D_
I0627 05:23:06.074455  4953 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true_x000D_
I0627 05:23:06.074468  4953 authenticator.cpp:318] Authentication success_x000D_
I0627 05:23:06.074553  4956 authenticatee.cpp:299] Authentication success_x000D_
I0627 05:23:06.074594  4945 master.cpp:7670] Successfully authenticated principal 'test-principal' at slave(269)@172.17.0.4:41747_x000D_
I0627 05:23:06.074642  4952 authenticator.cpp:432] Authentication session cleanup for crammd5-authenticatee(578)@172.17.0.4:41747_x000D_
I0627 05:23:06.074981  4951 slave.cpp:1103] Successfully authenticated with master master@172.17.0.4:41747_x000D_
../../src/tests/master_tests.cpp:7110: Failure_x000D_
Failed to wait 15secs for reregisterSlaveMessage_x000D_
I0627 05:23:21.082969  4953 slave.cpp:818] Agent terminating_x000D_
I0627 05:23:21.083305  4936 master.cpp:1316] Agent a8778782-0da1-49a5-9cb8-9f6d11701733-S0 at slave(269)@172.17.0.4:41747 (c43debbe7e32) disconnected_x000D_
I0627 05:23:21.083349  4936 master.cpp:3240] Disconnecting agent a8778782-0da1-49a5-9cb8-9f6d11701733-S0 at slave(269)@172.17.0.4:41747 (c43debbe7e32)_x000D_
I0627 05:23:21.083443  4936 master.cpp:3259] Deactivating agent a8778782-0da1-49a5-9cb8-9f6d11701733-S0 at slave(269)@172.17.0.4:41747 (c43debbe7e32)_x000D_
I0627 05:23:21.083665  4936 hierarchical.cpp:715] Agent a8778782-0da1-49a5-9cb8-9f6d11701733-S0 deactivated_x000D_
I0627 05:23:21.089186  4917 master.cpp:1158] Master terminating_x000D_
I0627 05:23:21.090092  4944 hierarchical.cpp:620] Removed agent a8778782-0da1-49a5-9cb8-9f6d11701733-S0_x000D_
../../3rdparty/libprocess/include/process/gmock.hpp:441: Failure_x000D_
Actual function call count doesn't match EXPECT_CALL(filter->mock, filter(testing::A<const MessageEvent&>()))..._x000D_
    Expected args: message matcher (8-byte object <E8-AB 06-80 6C-2B 00-00>, 1-byte object <AB>, 1-byte object <E8>)_x000D_
         Expected: to be called once_x000D_
           Actual: never called - unsatisfied and active_x000D_
[  FAILED  ] MasterTest.IgnoreOldAgentReregistration (15070 ms)_x000D_
_x000D_
{code}",3.0,1.4.3,0.5,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.8846153846153846,0.9878787878787878,0.9878787878787878,0.9544036697247705
Bug,PersistentVolumeEndpointsTest.ReserveAndSlaveRemoval test is flaky,"Observed this on ASF CI.

{code}
[ RUN      ] PersistentVolumeEndpointsTest.ReserveAndSlaveRemoval
I0627 15:20:33.687146 30773 cluster.cpp:162] Creating default 'local' authorizer
I0627 15:20:33.691745 30795 master.cpp:438] Master d8d232e5-1689-4780-b232-c91e5c3277b1 (0b1049f05548) started on 172.17.0.2:44357
I0627 15:20:33.691800 30795 master.cpp:440] Flags at startup: --acls="""" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""50ms"" --allocator=""HierarchicalDRF"" --authenticate_agents=""true"" --authenticate_frameworks=""true"" --authenticate_http_frameworks=""true"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/Wg4Ouh/credentials"" --filter_gpu_resources=""true"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_unreachable_tasks_per_framework=""1000"" --port=""5050"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""in_memory"" --registry_fetch_timeout=""1mins"" --registry_gc_interval=""15mins"" --registry_max_agent_age=""2weeks"" --registry_max_agent_count=""102400"" --registry_store_timeout=""100secs"" --registry_strict=""false"" --roles=""role1"" --root_submissions=""true"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/Wg4Ouh/master"" --zk_session_timeout=""10secs""
I0627 15:20:33.692142 30795 master.cpp:490] Master only allowing authenticated frameworks to register
I0627 15:20:33.692150 30795 master.cpp:504] Master only allowing authenticated agents to register
I0627 15:20:33.692154 30795 master.cpp:517] Master only allowing authenticated HTTP frameworks to register
I0627 15:20:33.692160 30795 credentials.hpp:37] Loading credentials for authentication from '/tmp/Wg4Ouh/credentials'
I0627 15:20:33.692463 30795 master.cpp:562] Using default 'crammd5' authenticator
I0627 15:20:33.692612 30795 http.cpp:974] Creating default 'basic' HTTP authenticator for realm 'mesos-master-readonly'
I0627 15:20:33.692831 30795 http.cpp:974] Creating default 'basic' HTTP authenticator for realm 'mesos-master-readwrite'
I0627 15:20:33.692942 30795 http.cpp:974] Creating default 'basic' HTTP authenticator for realm 'mesos-master-scheduler'
I0627 15:20:33.693061 30795 master.cpp:642] Authorization enabled
W0627 15:20:33.693076 30795 master.cpp:705] The '--roles' flag is deprecated. This flag will be removed in the future. See the Mesos 0.27 upgrade notes for more information
I0627 15:20:33.693354 30780 hierarchical.cpp:169] Initialized hierarchical allocator process
I0627 15:20:33.693359 30782 whitelist_watcher.cpp:77] No whitelist given
I0627 15:20:33.695943 30795 master.cpp:2161] Elected as the leading master!
I0627 15:20:33.695960 30795 master.cpp:1700] Recovering from registrar
I0627 15:20:33.696193 30795 registrar.cpp:345] Recovering registrar
I0627 15:20:33.697032 30795 registrar.cpp:389] Successfully fetched the registry (0B) in 811008ns
I0627 15:20:33.697147 30795 registrar.cpp:493] Applied 1 operations in 40183ns; attempting to update the registry
I0627 15:20:33.697922 30792 registrar.cpp:550] Successfully updated the registry in 709120ns
I0627 15:20:33.698020 30792 registrar.cpp:422] Successfully recovered registrar
I0627 15:20:33.698490 30789 master.cpp:1799] Recovered 0 agents from the registry (129B); allowing 10mins for agents to re-register
I0627 15:20:33.698511 30784 hierarchical.cpp:207] Skipping recovery of hierarchical allocator: nothing to recover
I0627 15:20:33.707849 30773 containerizer.cpp:230] Using isolation: posix/cpu,posix/mem,filesystem/posix,network/cni,environment_secret
W0627 15:20:33.708729 30773 backend.cpp:76] Failed to create 'aufs' backend: AufsBackend requires root privileges
W0627 15:20:33.708909 30773 backend.cpp:76] Failed to create 'bind' backend: BindBackend requires root privileges
I0627 15:20:33.708955 30773 provisioner.cpp:255] Using default backend 'copy'
I0627 15:20:33.711526 30773 cluster.cpp:448] Creating default 'local' authorizer
I0627 15:20:33.714450 30776 slave.cpp:249] Mesos agent started on (451)@172.17.0.2:44357
I0627 15:20:33.714649 30776 slave.cpp:250] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/PersistentVolumeEndpointsTest_ReserveAndSlaveRemoval_RnxQRd/store/appc"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""true"" --authenticatee=""crammd5"" --authentication_backoff_factor=""1secs"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/PersistentVolumeEndpointsTest_ReserveAndSlaveRemoval_RnxQRd/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/PersistentVolumeEndpointsTest_ReserveAndSlaveRemoval_RnxQRd/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_reregistration_timeout=""2secs"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/PersistentVolumeEndpointsTest_ReserveAndSlaveRemoval_RnxQRd/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_command_executor=""false"" --http_credentials=""/tmp/PersistentVolumeEndpointsTest_ReserveAndSlaveRemoval_RnxQRd/http_credentials"" --http_heartbeat_interval=""30secs"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher=""posix"" --launcher_dir=""/mesos/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_executors_per_framework=""150"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --port=""5051"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:4"" --revocable_cpu_low_priority=""true"" --runtime_dir=""/tmp/PersistentVolumeEndpointsTest_ReserveAndSlaveRemoval_RnxQRd"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/PersistentVolumeEndpointsTest_ReserveAndSlaveRemoval_buPjGf""
I0627 15:20:33.715536 30776 credentials.hpp:86] Loading credential for authentication from '/tmp/PersistentVolumeEndpointsTest_ReserveAndSlaveRemoval_RnxQRd/credential'
I0627 15:20:33.715909 30776 slave.cpp:282] Agent using credential for: test-principal
I0627 15:20:33.716008 30776 credentials.hpp:37] Loading credentials for authentication from '/tmp/PersistentVolumeEndpointsTest_ReserveAndSlaveRemoval_RnxQRd/http_credentials'
I0627 15:20:33.716431 30776 http.cpp:974] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-readonly'
I0627 15:20:33.716806 30776 http.cpp:974] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-readwrite'
I0627 15:20:33.717967 30776 slave.cpp:553] Agent resources: [{""name"":""cpus"",""scalar"":{""value"":4.0},""type"":""SCALAR""},{""name"":""mem"",""scalar"":{""value"":95614.0},""type"":""SCALAR""},{""name"":""disk"",""scalar"":{""value"":367489.0},""type"":""SCALAR""},{""name"":""ports"",""ranges"":{""range"":[{""begin"":31000,""end"":32000}]},""type"":""RANGES""}]
I0627 15:20:33.718209 30776 slave.cpp:561] Agent attributes: [  ]
I0627 15:20:33.718225 30776 slave.cpp:566] Agent hostname: 0b1049f05548
I0627 15:20:33.718375 30783 status_update_manager.cpp:177] Pausing sending status updates
I0627 15:20:33.720067 30795 state.cpp:64] Recovering state from '/tmp/PersistentVolumeEndpointsTest_ReserveAndSlaveRemoval_buPjGf/meta'
I0627 15:20:33.720432 30778 status_update_manager.cpp:203] Recovering status update manager
I0627 15:20:33.720608 30795 containerizer.cpp:582] Recovering containerizer
I0627 15:20:33.722419 30790 provisioner.cpp:416] Provisioner recovery complete
I0627 15:20:33.722766 30786 slave.cpp:6168] Finished recovery
I0627 15:20:33.723409 30786 slave.cpp:6350] Querying resource estimator for oversubscribable resources
I0627 15:20:33.724009 30784 slave.cpp:946] New master detected at master@172.17.0.2:44357
I0627 15:20:33.724221 30784 slave.cpp:981] Detecting new master
I0627 15:20:33.724370 30784 slave.cpp:6364] Received oversubscribable resources {} from the resource estimator
I0627 15:20:33.725513 30784 slave.cpp:1008] Authenticating with master master@172.17.0.2:44357
I0627 15:20:33.725639 30784 slave.cpp:1019] Using default CRAM-MD5 authenticatee
I0627 15:20:33.725975 30784 authenticatee.cpp:121] Creating new client SASL connection
I0627 15:20:33.726358 30784 master.cpp:7640] Authenticating slave(451)@172.17.0.2:44357
I0627 15:20:33.726567 30784 authenticator.cpp:414] Starting authentication session for crammd5-authenticatee(938)@172.17.0.2:44357
I0627 15:20:33.726917 30784 authenticator.cpp:98] Creating new server SASL connection
I0627 15:20:33.727162 30784 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
I0627 15:20:33.727192 30784 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
I0627 15:20:33.727285 30784 authenticator.cpp:204] Received SASL authentication start
I0627 15:20:33.727344 30784 authenticator.cpp:326] Authentication requires more steps
I0627 15:20:33.727429 30784 authenticatee.cpp:259] Received SASL authentication step
I0627 15:20:33.727546 30781 status_update_manager.cpp:177] Pausing sending status updates
I0627 15:20:33.727591 30784 authenticator.cpp:232] Received SASL authentication step
I0627 15:20:33.727624 30784 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: '0b1049f05548' server FQDN: '0b1049f05548' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0627 15:20:33.727640 30784 auxprop.cpp:181] Looking up auxiliary property '*userPassword'
I0627 15:20:33.727690 30784 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0627 15:20:33.727715 30784 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: '0b1049f05548' server FQDN: '0b1049f05548' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0627 15:20:33.727726 30784 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0627 15:20:33.727737 30784 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0627 15:20:33.727757 30784 authenticator.cpp:318] Authentication success
I0627 15:20:33.727963 30784 authenticatee.cpp:299] Authentication success
I0627 15:20:33.728052 30784 master.cpp:7670] Successfully authenticated principal 'test-principal' at slave(451)@172.17.0.2:44357
I0627 15:20:33.728145 30784 authenticator.cpp:432] Authentication session cleanup for crammd5-authenticatee(938)@172.17.0.2:44357
I0627 15:20:33.728500 30784 slave.cpp:1103] Successfully authenticated with master master@172.17.0.2:44357
I0627 15:20:33.728786 30784 slave.cpp:1545] Will retry registration in 1.216522ms if necessary
I0627 15:20:33.729225 30784 master.cpp:5602] Received register agent message from slave(451)@172.17.0.2:44357 (0b1049f05548)
I0627 15:20:33.729265 30784 master.cpp:3742] Authorizing agent with principal 'test-principal'
I0627 15:20:33.729861 30784 master.cpp:5662] Authorized registration of agent at slave(451)@172.17.0.2:44357 (0b1049f05548)
I0627 15:20:33.729984 30784 master.cpp:5741] Registering agent at slave(451)@172.17.0.2:44357 (0b1049f05548) with id d8d232e5-1689-4780-b232-c91e5c3277b1-S0
I0627 15:20:33.730517 30784 registrar.cpp:493] Applied 1 operations in 89878ns; attempting to update the registry
I0627 15:20:33.731552 30784 registrar.cpp:550] Successfully updated the registry in 956928ns
I0627 15:20:33.731950 30784 slave.cpp:1545] Will retry registration in 4.853741ms if necessary
I0627 15:20:33.732209 30784 master.cpp:5788] Admitted agent d8d232e5-1689-4780-b232-c91e5c3277b1-S0 at slave(451)@172.17.0.2:44357 (0b1049f05548)
I0627 15:20:33.732918 30784 master.cpp:5819] Registered agent d8d232e5-1689-4780-b232-c91e5c3277b1-S0 at slave(451)@172.17.0.2:44357 (0b1049f05548) with [{""name"":""cpus"",""scalar"":{""value"":4.0},""type"":""SCALAR""},{""name"":""mem"",""scalar"":{""value"":95614.0},""type"":""SCALAR""},{""name"":""disk"",""scalar"":{""value"":367489.0},""type"":""SCALAR""},{""name"":""ports"",""ranges"":{""range"":[{""begin"":31000,""end"":32000}]},""type"":""RANGES""}]
I0627 15:20:33.733402 30784 master.cpp:5602] Received register agent message from slave(451)@172.17.0.2:44357 (0b1049f05548)
I0627 15:20:33.733438 30784 master.cpp:3742] Authorizing agent with principal 'test-principal'
I0627 15:20:33.734184 30784 hierarchical.cpp:587] Added agent d8d232e5-1689-4780-b232-c91e5c3277b1-S0 (0b1049f05548) with cpus:4; mem:95614; disk:367489; ports:[31000-32000] (allocated: {})
I0627 15:20:33.734509 30784 hierarchical.cpp:1938] No allocations performed
I0627 15:20:33.734563 30784 hierarchical.cpp:1493] Performed allocation for 1 agents in 203705ns
I0627 15:20:33.734794 30784 slave.cpp:1149] Registered with master master@172.17.0.2:44357; given agent ID d8d232e5-1689-4780-b232-c91e5c3277b1-S0
I0627 15:20:33.735222 30784 slave.cpp:1169] Checkpointing SlaveInfo to '/tmp/PersistentVolumeEndpointsTest_ReserveAndSlaveRemoval_buPjGf/meta/slaves/d8d232e5-1689-4780-b232-c91e5c3277b1-S0/slave.info'
I0627 15:20:33.735703 30784 slave.cpp:1207] Forwarding total oversubscribed resources {}
I0627 15:20:33.735824 30784 slave.cpp:4866] Received ping from slave-observer(449)@172.17.0.2:44357
I0627 15:20:33.736479 30784 status_update_manager.cpp:184] Resuming sending status updates
I0627 15:20:33.736613 30784 master.cpp:6508] Received update of agent d8d232e5-1689-4780-b232-c91e5c3277b1-S0 at slave(451)@172.17.0.2:44357 (0b1049f05548) with total oversubscribed resources {}
I0627 15:20:33.736809 30784 master.cpp:5662] Authorized registration of agent at slave(451)@172.17.0.2:44357 (0b1049f05548)
I0627 15:20:33.736912 30784 master.cpp:5719] Agent d8d232e5-1689-4780-b232-c91e5c3277b1-S0 at slave(451)@172.17.0.2:44357 (0b1049f05548) already registered, resending acknowledgement
W0627 15:20:33.737525 30777 slave.cpp:1193] Already registered with master master@172.17.0.2:44357
I0627 15:20:33.737548 30777 slave.cpp:1207] Forwarding total oversubscribed resources {}
I0627 15:20:33.737752 30775 master.cpp:6508] Received update of agent d8d232e5-1689-4780-b232-c91e5c3277b1-S0 at slave(451)@172.17.0.2:44357 (0b1049f05548) with total oversubscribed resources {}
I0627 15:20:33.744215 30775 hierarchical.cpp:1938] No allocations performed
I0627 15:20:33.744280 30775 hierarchical.cpp:1493] Performed allocation for 1 agents in 249949ns
I0627 15:20:33.745524 30773 containerizer.cpp:230] Using isolation: posix/cpu,posix/mem,filesystem/posix,network/cni,environment_secret
W0627 15:20:33.746350 30773 backend.cpp:76] Failed to create 'aufs' backend: AufsBackend requires root privileges
W0627 15:20:33.746516 30773 backend.cpp:76] Failed to create 'bind' backend: BindBackend requires root privileges
I0627 15:20:33.746562 30773 provisioner.cpp:255] Using default backend 'copy'
I0627 15:20:33.749130 30773 cluster.cpp:448] Creating default 'local' authorizer
I0627 15:20:33.751713 30774 slave.cpp:249] Mesos agent started on (452)@172.17.0.2:44357
I0627 15:20:33.751760 30774 slave.cpp:250] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/PersistentVolumeEndpointsTest_ReserveAndSlaveRemoval_g5ycV6/store/appc"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""true"" --authenticatee=""crammd5"" --authentication_backoff_factor=""1secs"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/PersistentVolumeEndpointsTest_ReserveAndSlaveRemoval_g5ycV6/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/PersistentVolumeEndpointsTest_ReserveAndSlaveRemoval_g5ycV6/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_reregistration_timeout=""2secs"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/PersistentVolumeEndpointsTest_ReserveAndSlaveRemoval_g5ycV6/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_command_executor=""false"" --http_credentials=""/tmp/PersistentVolumeEndpointsTest_ReserveAndSlaveRemoval_g5ycV6/http_credentials"" --http_heartbeat_interval=""30secs"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher=""posix"" --launcher_dir=""/mesos/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_executors_per_framework=""150"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --port=""5051"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:3"" --revocable_cpu_low_priority=""true"" --runtime_dir=""/tmp/PersistentVolumeEndpointsTest_ReserveAndSlaveRemoval_g5ycV6"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/PersistentVolumeEndpointsTest_ReserveAndSlaveRemoval_QWO1D8""
I0627 15:20:33.752180 30774 credentials.hpp:86] Loading credential for authentication from '/tmp/PersistentVolumeEndpointsTest_ReserveAndSlaveRemoval_g5ycV6/credential'
I0627 15:20:33.752401 30774 slave.cpp:282] Agent using credential for: test-principal
I0627 15:20:33.752419 30774 credentials.hpp:37] Loading credentials for authentication from '/tmp/PersistentVolumeEndpointsTest_ReserveAndSlaveRemoval_g5ycV6/http_credentials'
I0627 15:20:33.752648 30774 http.cpp:974] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-readonly'
I0627 15:20:33.752821 30774 http.cpp:974] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-readwrite'
I0627 15:20:33.753873 30774 slave.cpp:553] Agent resources: [{""name"":""cpus"",""scalar"":{""value"":3.0},""type"":""SCALAR""},{""name"":""mem"",""scalar"":{""value"":95614.0},""type"":""SCALAR""},{""name"":""disk"",""scalar"":{""value"":367489.0},""type"":""SCALAR""},{""name"":""ports"",""ranges"":{""range"":[{""begin"":31000,""end"":32000}]},""type"":""RANGES""}]
I0627 15:20:33.754041 30774 slave.cpp:561] Agent attributes: [  ]
I0627 15:20:33.754051 30774 slave.cpp:566] Agent hostname: 0b1049f05548
I0627 15:20:33.754200 30788 status_update_manager.cpp:177] Pausing sending status updates
I0627 15:20:33.755672 30792 state.cpp:64] Recovering state from '/tmp/PersistentVolumeEndpointsTest_ReserveAndSlaveRemoval_QWO1D8/meta'
I0627 15:20:33.755934 30790 status_update_manager.cpp:203] Recovering status update manager
I0627 15:20:33.756114 30789 containerizer.cpp:582] Recovering containerizer
I0627 15:20:33.756492 30778 process.cpp:3779] Handling HTTP event for process 'master' with path: '/master/reserve'
I0627 15:20:33.757879 30783 provisioner.cpp:416] Provisioner recovery complete
I0627 15:20:33.758180 30784 slave.cpp:6168] Finished recovery
I0627 15:20:33.758538 30780 http.cpp:1114] HTTP POST for /master/reserve from 172.17.0.2:44414
I0627 15:20:33.758766 30784 slave.cpp:6350] Querying resource estimator for oversubscribable resources
I0627 15:20:33.758973 30795 status_update_manager.cpp:177] Pausing sending status updates
I0627 15:20:33.758972 30784 slave.cpp:946] New master detected at master@172.17.0.2:44357
I0627 15:20:33.759085 30784 slave.cpp:981] Detecting new master
I0627 15:20:33.759176 30784 slave.cpp:6364] Received oversubscribable resources {} from the resource estimator
I0627 15:20:33.759441 30780 master.cpp:3527] Authorizing principal 'test-principal' to reserve resources '[{""name"":""cpus"",""reservations"":[{""principal"":""test-principal"",""role"":""role1"",""type"":""DYNAMIC""}],""scalar"":{""value"":4.0},""type"":""SCALAR""}]'
I0627 15:20:33.761719 30779 master.cpp:8908] Sending updated checkpointed resources cpus(reservations: [(DYNAMIC,role1,test-principal)]):4 to agent d8d232e5-1689-4780-b232-c91e5c3277b1-S0 at slave(451)@172.17.0.2:44357 (0b1049f05548)
I0627 15:20:33.762620 30794 slave.cpp:3426] Updated checkpointed resources from {} to cpus(reservations: [(DYNAMIC,role1,test-principal)]):4
I0627 15:20:33.763795 30775 slave.cpp:1008] Authenticating with master master@172.17.0.2:44357
I0627 15:20:33.763928 30775 slave.cpp:1019] Using default CRAM-MD5 authenticatee
I0627 15:20:33.764186 30795 authenticatee.cpp:121] Creating new client SASL connection
I0627 15:20:33.764468 30796 master.cpp:7640] Authenticating slave(452)@172.17.0.2:44357
I0627 15:20:33.764649 30785 authenticator.cpp:414] Starting authentication session for crammd5-authenticatee(939)@172.17.0.2:44357
I0627 15:20:33.764925 30795 authenticator.cpp:98] Creating new server SASL connection
I0627 15:20:33.765175 30777 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
I0627 15:20:33.765203 30777 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
I0627 15:20:33.765386 30785 authenticator.cpp:204] Received SASL authentication start
I0627 15:20:33.765439 30785 authenticator.cpp:326] Authentication requires more steps
I0627 15:20:33.765538 30784 authenticatee.cpp:259] Received SASL authentication step
I0627 15:20:33.765709 30789 authenticator.cpp:232] Received SASL authentication step
I0627 15:20:33.765758 30789 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: '0b1049f05548' server FQDN: '0b1049f05548' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0627 15:20:33.765774 30789 auxprop.cpp:181] Looking up auxiliary property '*userPassword'
I0627 15:20:33.765825 30789 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0627 15:20:33.765854 30789 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: '0b1049f05548' server FQDN: '0b1049f05548' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0627 15:20:33.765867 30789 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0627 15:20:33.765877 30789 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0627 15:20:33.765902 30789 authenticator.cpp:318] Authentication success
I0627 15:20:33.766007 30773 sched.cpp:232] Version: 1.4.0
I0627 15:20:33.766126 30790 authenticatee.cpp:299] Authentication success
I0627 15:20:33.766155 30788 master.cpp:7670] Successfully authenticated principal 'test-principal' at slave(452)@172.17.0.2:44357
I0627 15:20:33.766187 30789 authenticator.cpp:432] Authentication session cleanup for crammd5-authenticatee(939)@172.17.0.2:44357
I0627 15:20:33.766543 30776 slave.cpp:1103] Successfully authenticated with master master@172.17.0.2:44357
I0627 15:20:33.766687 30793 sched.cpp:336] New master detected at master@172.17.0.2:44357
I0627 15:20:33.766753 30793 sched.cpp:407] Authenticating with master master@172.17.0.2:44357
I0627 15:20:33.766767 30793 sched.cpp:414] Using default CRAM-MD5 authenticatee
I0627 15:20:33.766770 30776 slave.cpp:1545] Will retry registration in 2.625286ms if necessary
I0627 15:20:33.766901 30789 master.cpp:5602] Received register agent message from slave(452)@172.17.0.2:44357 (0b1049f05548)
I0627 15:20:33.766932 30789 master.cpp:3742] Authorizing agent with principal 'test-principal'
I0627 15:20:33.766964 30797 authenticatee.cpp:121] Creating new client SASL connection
I0627 15:20:33.767232 30789 master.cpp:7640] Authenticating scheduler-f690f5d5-752a-499b-aca5-a83801ba224a@172.17.0.2:44357
I0627 15:20:33.767307 30776 authenticator.cpp:414] Starting authentication session for crammd5-authenticatee(940)@172.17.0.2:44357
I0627 15:20:33.767468 30774 master.cpp:5662] Authorized registration of agent at slave(452)@172.17.0.2:44357 (0b1049f05548)
I0627 15:20:33.767489 30797 authenticator.cpp:98] Creating new server SASL connection
I0627 15:20:33.767608 30774 master.cpp:5741] Registering agent at slave(452)@172.17.0.2:44357 (0b1049f05548) with id d8d232e5-1689-4780-b232-c91e5c3277b1-S1
I0627 15:20:33.767666 30778 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
I0627 15:20:33.767691 30778 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
I0627 15:20:33.767819 30797 authenticator.cpp:204] Received SASL authentication start
I0627 15:20:33.767864 30797 authenticator.cpp:326] Authentication requires more steps
I0627 15:20:33.767958 30786 authenticatee.cpp:259] Received SASL authentication step
I0627 15:20:33.767989 30781 registrar.cpp:493] Applied 1 operations in 89291ns; attempting to update the registry
I0627 15:20:33.768164 30788 authenticator.cpp:232] Received SASL authentication step
I0627 15:20:33.768187 30788 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: '0b1049f05548' server FQDN: '0b1049f05548' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0627 15:20:33.768198 30788 auxprop.cpp:181] Looking up auxiliary property '*userPassword'
I0627 15:20:33.768239 30788 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0627 15:20:33.768260 30788 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: '0b1049f05548' server FQDN: '0b1049f05548' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0627 15:20:33.768267 30788 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0627 15:20:33.768272 30788 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0627 15:20:33.768285 30788 authenticator.cpp:318] Authentication success
I0627 15:20:33.768394 30792 authenticatee.cpp:299] Authentication success
I0627 15:20:33.768615 30777 master.cpp:7670] Successfully authenticated principal 'test-principal' at scheduler-f690f5d5-752a-499b-aca5-a83801ba224a@172.17.0.2:44357
I0627 15:20:33.768641 30790 sched.cpp:513] Successfully authenticated with master master@172.17.0.2:44357
I0627 15:20:33.768661 30790 sched.cpp:836] Sending SUBSCRIBE call to master@172.17.0.2:44357
I0627 15:20:33.768661 30776 authenticator.cpp:432] Authentication session cleanup for crammd5-authenticatee(940)@172.17.0.2:44357
I0627 15:20:33.768782 30781 registrar.cpp:550] Successfully updated the registry in 722176ns
I0627 15:20:33.768887 30790 sched.cpp:869] Will retry registration in 424.478509ms if necessary
I0627 15:20:33.769176 30785 master.cpp:2853] Received SUBSCRIBE call for framework 'default' at scheduler-f690f5d5-752a-499b-aca5-a83801ba224a@172.17.0.2:44357
I0627 15:20:33.769222 30785 master.cpp:2197] Authorizing framework principal 'test-principal' to receive offers for roles '{ role1 }'
I0627 15:20:33.770375 30796 slave.cpp:1545] Will retry registration in 37.054423ms if necessary
I0627 15:20:33.770567 30785 master.cpp:5788] Admitted agent d8d232e5-1689-4780-b232-c91e5c3277b1-S1 at slave(452)@172.17.0.2:44357 (0b1049f05548)
I0627 15:20:33.771241 30784 slave.cpp:4866] Received ping from slave-observer(450)@172.17.0.2:44357
I0627 15:20:33.771467 30787 slave.cpp:1149] Registered with master master@172.17.0.2:44357; given agent ID d8d232e5-1689-4780-b232-c91e5c3277b1-S1
I0627 15:20:33.771585 30789 status_update_manager.cpp:184] Resuming sending status updates
I0627 15:20:33.771392 30785 master.cpp:5819] Registered agent d8d232e5-1689-4780-b232-c91e5c3277b1-S1 at slave(452)@172.17.0.2:44357 (0b1049f05548) with [{""name"":""cpus"",""scalar"":{""value"":3.0},""type"":""SCALAR""},{""name"":""mem"",""scalar"":{""value"":95614.0},""type"":""SCALAR""},{""name"":""disk"",""scalar"":{""value"":367489.0},""type"":""SCALAR""},{""name"":""ports"",""ranges"":{""range"":[{""begin"":31000,""end"":32000}]},""type"":""RANGES""}]
I0627 15:20:33.771682 30777 hierarchical.cpp:587] Added agent d8d232e5-1689-4780-b232-c91e5c3277b1-S1 (0b1049f05548) with cpus:3; mem:95614; disk:367489; ports:[31000-32000] (allocated: {})
I0627 15:20:33.771832 30787 slave.cpp:1169] Checkpointing SlaveInfo to '/tmp/PersistentVolumeEndpointsTest_ReserveAndSlaveRemoval_QWO1D8/meta/slaves/d8d232e5-1689-4780-b232-c91e5c3277b1-S1/slave.info'
I0627 15:20:33.771932 30777 hierarchical.cpp:1938] No allocations performed
I0627 15:20:33.771953 30785 master.cpp:5602] Received register agent message from slave(452)@172.17.0.2:44357 (0b1049f05548)
I0627 15:20:33.771975 30777 hierarchical.cpp:1493] Performed allocation for 1 agents in 164163ns
I0627 15:20:33.771996 30785 master.cpp:3742] Authorizing agent with principal 'test-principal'
I0627 15:20:33.772150 30787 slave.cpp:1207] Forwarding total oversubscribed resources {}
I0627 15:20:33.772339 30785 master.cpp:2933] Subscribing framework default with checkpointing disabled and capabilities [ RESERVATION_REFINEMENT ]
I0627 15:20:33.773007 30785 master.cpp:6508] Received update of agent d8d232e5-1689-4780-b232-c91e5c3277b1-S1 at slave(452)@172.17.0.2:44357 (0b1049f05548) with total oversubscribed resources {}
I0627 15:20:33.773047 30794 sched.cpp:759] Framework registered with d8d232e5-1689-4780-b232-c91e5c3277b1-0000
I0627 15:20:33.773095 30794 sched.cpp:773] Scheduler::registered took 31019ns
I0627 15:20:33.773164 30785 master.cpp:5662] Authorized registration of agent at slave(452)@172.17.0.2:44357 (0b1049f05548)
I0627 15:20:33.773227 30776 hierarchical.cpp:301] Added framework d8d232e5-1689-4780-b232-c91e5c3277b1-0000
I0627 15:20:33.773247 30785 master.cpp:5719] Agent d8d232e5-1689-4780-b232-c91e5c3277b1-S1 at slave(452)@172.17.0.2:44357 (0b1049f05548) already registered, resending acknowledgement
W0627 15:20:33.773438 30783 slave.cpp:1193] Already registered with master master@172.17.0.2:44357
I0627 15:20:33.773470 30783 slave.cpp:1207] Forwarding total oversubscribed resources {}
I0627 15:20:33.773676 30781 master.cpp:6508] Received update of agent d8d232e5-1689-4780-b232-c91e5c3277b1-S1 at slave(452)@172.17.0.2:44357 (0b1049f05548) with total oversubscribed resources {}
I0627 15:20:33.774871 30776 hierarchical.cpp:2028] No inverse offers to send out!
I0627 15:20:33.774912 30776 hierarchical.cpp:1493] Performed allocation for 2 agents in 1.518176ms
I0627 15:20:33.776294 30783 master.cpp:7470] Sending 2 offers to framework d8d232e5-1689-4780-b232-c91e5c3277b1-0000 (default) at scheduler-f690f5d5-752a-499b-aca5-a83801ba224a@172.17.0.2:44357
I0627 15:20:33.776959 30776 sched.cpp:933] Scheduler::resourceOffers took 133421ns
/mesos/src/tests/persistent_volume_endpoints_tests.cpp:1859: Failure
Value of: offeredSlaveId == slaveId1 || offeredSlaveId == slaveId2
  Actual: false
Expected: true
I0627 15:20:33.778265 30791 master.cpp:1430] Framework d8d232e5-1689-4780-b232-c91e5c3277b1-0000 (default) at scheduler-f690f5d5-752a-499b-aca5-a83801ba224a@172.17.0.2:44357 disconnected
I0627 15:20:33.778303 30791 master.cpp:3203] Deactivating framework d8d232e5-1689-4780-b232-c91e5c3277b1-0000 (default) at scheduler-f690f5d5-752a-499b-aca5-a83801ba224a@172.17.0.2:44357
I0627 15:20:33.779554 30787 hierarchical.cpp:410] Deactivated framework d8d232e5-1689-4780-b232-c91e5c3277b1-0000
I0627 15:20:33.779958 30791 master.cpp:3180] Disconnecting framework d8d232e5-1689-4780-b232-c91e5c3277b1-0000 (default) at scheduler-f690f5d5-752a-499b-aca5-a83801ba224a@172.17.0.2:44357
I0627 15:20:33.780009 30791 master.cpp:1445] Giving framework d8d232e5-1689-4780-b232-c91e5c3277b1-0000 (default) at scheduler-f690f5d5-752a-499b-aca5-a83801ba224a@172.17.0.2:44357 0ns to fa",3.0,0,0.5,1.0,0.0,0.0,0.0,0.0,0.8,0.5438596491228069,0.39999999999999997,0.8846153846153846,0.9878787878787878,0.9878787878787878,0.0
Bug,distclean error due to core dump file,"Observed this failure a bunch in ASF CI (Buildbot and Reviewbot).

{code}
make[2]: Leaving directory `/mesos/mesos-1.4.0/_build/src'
rm -f config.status config.cache config.log configure.lineno config.status.lineno
rm -f Makefile
ERROR: files left in build directory after distclean:
./src/core
make[1]: *** [distcleancheck] Error 1
make[1]: Leaving directory `/mesos/mesos-1.4.0/_build'
make: *** [distcheck] Error 1
{code}

Likely due to perf core dump failure during configure phase
{code}
E0626 19:44:39.541741  4914 perf.cpp:245] Failed to get perf version: Failed to execute perf: terminated with signal Aborted (core dumped)
-------------------------------------------------------------
Could not find the 'perf' command or its version lower that 2.6.39 so tests using it to sample the 'cpu-cycles' hardware event will not be run.
-------------------------------------------------------------
E0626 19:44:40.147486  4914 perf.cpp:245] Failed to get perf version: Failed to execute perf: terminated with signal Aborted (core dumped)
-------------------------------------------------------------
require 'perf' version >= 2.6.39 so no 'perf' tests will be run
{code}",3.0,0,0.5,0.3453996983408748,0.0,0.0,0.0,0.0,0.2,0.38596491228070173,0.35714285714285715,0.8846153846153846,0.9878787878787878,0.9878787878787878,0.0
Task,Optimize number of copies made in dispatch/defer mechanism,"Profiling agents reregistration for a large cluster shows, that many CPU cycles are spent on copying protobuf objects. This is partially due to copies made by a code like this:
{code}
future.then(defer(self(), &Process::method, param);
{code}
{{param}} could be copied 8-10 times before it reaches {{method}}. Specifically, {{reregisterSlave}} accepts vectors of rather complex objects, which are passed to {{defer}}.
Currently there are some places in {{defer}}, {{dispatch}} and {{Future}} code, which could use {{std::move}} and {{std::forward}} to evade some of the copies.",3.0,"1.2.0,1.2.1,1.3.0",0.0,0.05429864253393665,0.0,0.0,0.0,0.0,0.0,0.017543859649122806,0.014285714285714285,0.0,0.0,0.0,0.938868501529052
Improvement,Add heartbeats to master stream API,"Just like master uses heartbeats for scheduler API to keep the connection alive, it should do the same for the streaming API.",3.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02857142857142857,0.8846153846153846,0.9878787878787878,0.9878787878787878,0.0
Documentation,Update the documentation to reflect the addition of reservation refinement.,"There are a few things we need to be sure to document:_x000D_
_x000D_
* What reservation refinement is._x000D_
* The new ""format"" for Resource, when using the RESERVATION_REFINEMENT capability._x000D_
* The filtering of resources if a framework is not RESERVATION_REFINEMENT capable._x000D_
* The current limitations that only a single reservation can be pushed / popped within a single RESERVE / UNRESERVE operation.",2.0,0,0.5,0.6726998491704375,0.0,0.0,0.0,0.1,0.6000000000000001,0.2807017543859649,0.29285714285714287,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.0
Improvement,The order of isolators provided in '--isolation' flag is not preserved and instead sorted alphabetically,"According to documentation and comments in code the order of the entries in the --isolation flag should specify the ordering of the isolators. Specifically, the `create` and `prepare` calls for each isolator should run serially in the order in which they appear in the --isolation flag, while the `cleanup` call should be serialized in reverse order (with exception of filesystem isolator which is always first)._x000D_
But in fact, the isolators provided in '--isolation' flag are sorted alphabetically._x000D_
That happens in [this line of code|https://github.com/apache/mesos/blob/master/src/slave/containerizer/mesos/containerizer.cpp#L377]. In this line use of 'set<string>' is done (apparently instead of list or vector) and set is a sorted container.",2.0,"1.1.2,1.2.0,1.3.0",0.5,0.5444947209653093,0.0,0.0,0.0,0.5,1.0,0.05263157894736842,0.03571428571428571,0.0,0.0,0.0,0.9358409785932721
Bug,Docker containerizer fails to set sandbox logs ownership correctly.,"When using the docker containerizer in connection with a task that has no command user set but a framework that has ""nobody"" set as its default user, the resulting sandbox logs will be owned by the agent owning user (e.g. root) but not the framework owner (e.g. nobody).",2.0,1.4.0,0.0,0.3740573152337858,0.0,0.0,0.0,0.2,0.4,0.43859649122807015,0.2714285714285714,0.3717948717948718,0.24848484848484845,0.24848484848484845,0.9541284403669724
Bug,SlaveTest.ExecutorReregistrationTimeoutFlag aborts on Windows,"{code}_x000D_
[ RUN      ] SlaveTest.ExecutorReregistrationTimeoutFlag_x000D_
rk ae9679b1-67c9-4db6-8187-0641b0e929d2-0000_x000D_
I0601 23:53:23.488337  2748 master.cpp:1156] Master terminating_x000D_
I0601 23:53:23.492337  2728 hierarchical.cpp:579] Removed agent ae9679b1-67c9-4db6-8187-0641b0e929d2-S0_x000D_
I0601 23:53:23.530340  1512 cluster.cpp:162] Creating default 'local' authorizer_x000D_
I0601 23:53:23.544342  2728 master.cpp:436] Master f07f4fdd-cd91-4d62-bf33-169b20d02020 (ip-172-20-128-1.ec2.internal) started on 172.20.128.1:51241_x000D_
I0601 23:53:23.545341  2728 master.cpp:438] Flags at startup: --acls="""" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate_agents=""false"" --authenticate_frameworks=""false"" --authenticate_http_frameworks=""true"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""C:\temp\FWZORI\credentials"" --filter_gpu_resources=""true"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_unreachable_tasks_per_framework=""1000"" --port=""5050"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""in_memory"" --registry_fetch_timeout=""1mins"" --registry_gc_interval=""15mins"" --registry_max_agent_age=""2weeks"" --registry_max_agent_count=""102400"" --registry_store_timeout=""100secs"" --registry_strict=""false"" --root_submissions=""true"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/webui"" --work_dir=""C:\temp\FWZORI\master"" --zk_session_timeout=""10secs""_x000D_
I0601 23:53:23.550338  2728 master.cpp:515] Master only allowing authenticated HTTP frameworks to register_x000D_
I0601 23:53:23.550338  2728 credentials.hpp:37] Loading credentials for authentication from 'C:\temp\FWZORI\credentials'_x000D_
I0601 23:53:23.552338  2728 http.cpp:975] Creating default 'basic' HTTP authenticator for realm 'mesos-master-readonly'_x000D_
I0601 23:53:23.553339  2728 http.cpp:975] Creating default 'basic' HTTP authenticator for realm 'mesos-master-readwrite'_x000D_
I0601 23:53:23.554340  2728 http.cpp:975] Creating default 'basic' HTTP authenticator for realm 'mesos-master-scheduler'_x000D_
I0601 23:53:23.555341  2728 master.cpp:640] Authorization enabled_x000D_
I0601 23:53:23.570340  2124 master.cpp:2159] Elected as the leading master!_x000D_
I0601 23:53:23.570340  2124 master.cpp:1698] Recovering from registrar_x000D_
I0601 23:53:23.573341  1920 registrar.cpp:389] Successfully fetched the registry (0B) in 0ns_x000D_
I0601 23:53:23.573341  1920 registrar.cpp:493] Applied 1 operations in 0ns; attempting to update the registry_x000D_
I0601 23:53:23.575342  1920 registrar.cpp:550] Successfully updated the registry in 0ns_x000D_
I0601 23:53:23.576344  1920 registrar.cpp:422] Successfully recovered registrar_x000D_
I0601 23:53:23.577342  2728 master.cpp:1797] Recovered 0 agents from the registry (167B); allowing 10mins for agents to re-register_x000D_
I0601 23:53:23.595341  1512 containerizer.cpp:230] Using isolation: windows/cpu,filesystem/windows,environment_secret_x000D_
I0601 23:53:23.596343  1512 provisioner.cpp:255] Using default backend 'copy'_x000D_
I0601 23:53:23.626343  3976 slave.cpp:248] Mesos agent started on (133)@172.20.128.1:51241_x000D_
I0601 23:53:23.627342  3976 slave.cpp:249] Flags at startup: --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""C:\temp\kglZbS\store\appc"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""true"" --authenticatee=""crammd5"" --authentication_backoff_factor=""1secs"" --authorizer=""local"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""//./pipe/docker_engine"" --docker_stop_timeout=""0ns"" --docker_store_dir=""C:\temp\kglZbS\store\docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_reregistration_timeout=""15secs"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""C:\temp\kglZbS\fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_command_executor=""false"" --http_credentials=""C:\temp\kglZbS\http_credentials"" --http_heartbeat_interval=""30secs"" --initialize_driver_logging=""true"" --isolation=""windows/cpu"" --launcher=""windows"" --launcher_dir=""C:\Users\Administrator\workspace\mesos\Mesos_CI-build\FLAG\Plain\label\mesos-ec2-windows\mesos\build\src"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_executors_per_framework=""150"" --oversubscribed_resources_interval=""15secs"" --port=""5051"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]"" --runtime_dir=""C:\temp\kglZbS"" --sandbox_directory=""C:\mesos\sandbox"" --strict=""true"" --version=""false"" --work_dir=""C:\temp\b1wVnd""_x000D_
I0601 23:53:23.632310  3976 credentials.hpp:37] Loading credentials for authentication from 'C:\temp\kglZbS\http_credentials'_x000D_
I0601 23:53:23.634342  3976 http.cpp:975] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-readonly'_x000D_
I0601 23:53:23.635347  3976 http.cpp:975] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-readwrite'_x000D_
I0601 23:53:23.640344  3976 slave.cpp:552] Agent resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]_x000D_
I0601 23:53:23.641345  3976 slave.cpp:560] Agent attributes: [  ]_x000D_
I0601 23:53:23.641345  3976 slave.cpp:565] Agent hostname: ip-172-20-128-1.ec2.internal_x000D_
I0601 23:53:23.641345  2124 status_update_manager.cpp:177] Pausing sending status updates_x000D_
I0601 23:53:23.643345  1512 sched.cpp:232] Version: 1.4.0_x000D_
I0601 23:53:23.645345  1920 sched.cpp:336] New master detected at master@172.20.128.1:51241_x000D_
I0601 23:53:23.646344  1920 sched.cpp:365] Authentication is not available on this platform. Attempting to register without authentication_x000D_
I0601 23:53:23.647344  4168 master.cpp:2811] Received SUBSCRIBE call for framework 'default' at scheduler-895fb702-afb9-42fe-8802-83c47f72432e@172.20.128.1:51241_x000D_
I0601 23:53:23.647344  4168 master.cpp:2195] Authorizing framework principal 'test-principal' to receive offers for roles '{ * }'_x000D_
I0601 23:53:23.648345  2728 state.cpp:62] Recovering state from 'C:\temp\b1wVnd\meta'_x000D_
I0601 23:53:23.649345  4168 master.cpp:2811] Received SUBSCRIBE call for framework 'default' at scheduler-895fb702-afb9-42fe-8802-83c47f72432e@172.20.128.1:51241_x000D_
I0601 23:53:23.649345  4168 master.cpp:2195] Authorizing framework principal 'test-principal' to receive offers for roles '{ * }'_x000D_
I0601 23:53:23.649345  3976 status_update_manager.cpp:203] Recovering status update manager_x000D_
I0601 23:53:23.650308  4168 master.cpp:2888] Subscribing framework default with checkpointing enabled and capabilities [  ]_x000D_
I0601 23:53:23.650308  4496 containerizer.cpp:582] Recovering containerizer_x000D_
I0601 23:53:23.652308  4168 master.cpp:2888] Subscribing framework default with checkpointing enabled and capabilities [  ]_x000D_
I0601 23:53:23.652308  1920 sched.cpp:759] Framework registered with f07f4fdd-cd91-4d62-bf33-169b20d02020-0000_x000D_
I0601 23:53:23.652308  4168 master.cpp:2898] Framework f07f4fdd-cd91-4d62-bf33-169b20d02020-0000 (default) at scheduler-895fb702-afb9-42fe-8802-83c47f72432e@172.20.128.1:51241 already subscribed, resending acknowledgement_x000D_
I0601 23:53:23.653309  3976 hierarchical.cpp:294] Added framework f07f4fdd-cd91-4d62-bf33-169b20d02020-0000_x000D_
I0601 23:53:23.658309  1920 provisioner.cpp:416] Provisioner recovery complete_x000D_
I0601 23:53:23.659309  2748 slave.cpp:6119] Finished recovery_x000D_
I0601 23:53:23.662308  4496 status_update_manager.cpp:177] Pausing sending status updates_x000D_
I0601 23:53:23.662308  2748 slave.cpp:945] New master detected at master@172.20.128.1:51241_x000D_
I0601 23:53:23.663310  2748 slave.cpp:969] No credentials provided. Attempting to register without authentication_x000D_
I0601 23:53:23.663310  2748 slave.cpp:980] Detecting new master_x000D_
I0601 23:53:23.664309  3976 master.cpp:5425] Received register agent message from slave(133)@172.20.128.1:51241 (ip-172-20-128-1.ec2.internal)_x000D_
I0601 23:53:23.665309  3976 master.cpp:3657] Authorizing agent without a principal_x000D_
I0601 23:53:23.666309  2124 master.cpp:5564] Registering agent at slave(133)@172.20.128.1:51241 (ip-172-20-128-1.ec2.internal) with id f07f4fdd-cd91-4d62-bf33-169b20d02020-S0_x000D_
I0601 23:53:23.668309  3976 registrar.cpp:493] Applied 1 operations in 0ns; attempting to update the registry_x000D_
I0601 23:53:23.670310  3976 registrar.cpp:550] Successfully updated the registry in 0ns_x000D_
I0601 23:53:23.674311  1920 slave.cpp:1148] Registered with master master@172.20.128.1:51241; given agent ID f07f4fdd-cd91-4d62-bf33-169b20d02020-S0_x000D_
I0601 23:53:23.674311  4168 master.cpp:5642] Registered agent f07f4fdd-cd91-4d62-bf33-169b20d02020-S0 at slave(133)@172.20.128.1:51241 (ip-172-20-128-1.ec2.internal) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]_x000D_
I0601 23:53:23.675309  3976 status_update_manager.cpp:184] Resuming sending status updates_x000D_
I0601 23:53:23.676309  2728 hierarchical.cpp:546] Added agent f07f4fdd-cd91-4d62-bf33-169b20d02020-S0 (ip-172-20-128-1.ec2.internal) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: {})_x000D_
I0601 23:53:23.681309  1920 slave.cpp:1206] Forwarding total oversubscribed resources {}_x000D_
I0601 23:53:23.682309  2124 master.cpp:6295] Received update of agent f07f4fdd-cd91-4d62-bf33-169b20d02020-S0 at slave(133)@172.20.128.1:51241 (ip-172-20-128-1.ec2.internal) with total oversubscribed resources {}_x000D_
I0601 23:53:23.686309  2124 master.cpp:7252] Sending 1 offers to framework f07f4fdd-cd91-4d62-bf33-169b20d02020-0000 (default) at scheduler-895fb702-afb9-42fe-8802-83c47f72432e@172.20.128.1:51241_x000D_
I0601 23:53:23.694308  2728 master.cpp:3872] Processing ACCEPT call for offers: [ f07f4fdd-cd91-4d62-bf33-169b20d02020-O0 ] on agent f07f4fdd-cd91-4d62-bf33-169b20d02020-S0 at slave(133)@172.20.128.1:51241 (ip-172-20-128-1.ec2.internal) for framework f07f4fdd-cd91-4d62-bf33-169b20d02020-0000 (default) at scheduler-895fb702-afb9-42fe-8802-83c47f72432e@172.20.128.1:51241_x000D_
I0601 23:53:23.694308  2728 master.cpp:3424] Authorizing framework principal 'test-principal' to launch task 792f1a13-d0ee-4e98-a4c8-82b9849adfbe_x000D_
I0601 23:53:23.705309  2728 master.cpp:9265] Adding task 792f1a13-d0ee-4e98-a4c8-82b9849adfbe with resources cpus(*)(allocated: *):2; mem(*)(allocated: *):1024; disk(*)(allocated: *):1024; ports(*)(allocated: *):[31000-32000] on agent f07f4fdd-cd91-4d62-bf33-169b20d02020-S0 at slave(133)@172.20.128.1:51241 (ip-172-20-128-1.ec2.internal)_x000D_
I0601 23:53:23.707309  2728 master.cpp:4527] Launching task 792f1a13-d0ee-4e98-a4c8-82b9849adfbe of framework f07f4fdd-cd91-4d62-bf33-169b20d02020-0000 (default) at scheduler-895fb702-afb9-42fe-8802-83c47f72432e@172.20.128.1:51241 with resources cpus(*)(allocated: *):2; mem(*)(allocated: *):1024; disk(*)(allocated: *):1024; ports(*)(allocated: *):[31000-32000] on agent f07f4fdd-cd91-4d62-bf33-169b20d02020-S0 at slave(133)@172.20.128.1:51241 (ip-172-20-128-1.ec2.internal)_x000D_
I0601 23:53:23.710311  4496 slave.cpp:1632] Got assigned task '792f1a13-d0ee-4e98-a4c8-82b9849adfbe' for framework f07f4fdd-cd91-4d62-bf33-169b20d02020-0000_x000D_
I0601 23:53:23.721310  3976 hierarchical.cpp:871] Updated allocation of framework f07f4fdd-cd91-4d62-bf33-169b20d02020-0000 on agent f07f4fdd-cd91-4d62-bf33-169b20d02020-S0 from cpus(*)(allocated: *):2; mem(*)(allocated: *):1024; disk(*)(allocated: *):1024; ports(*)(allocated: *):[31000-32000] to cpus(*)(allocated: *):2; mem(*)(allocated: *):1024; disk(*)(allocated: *):1024; ports(*)(allocated: *):[31000-32000]_x000D_
I0601 23:53:23.723309  4496 slave.cpp:1913] Authorizing task '792f1a13-d0ee-4e98-a4c8-82b9849adfbe' for framework f07f4fdd-cd91-4d62-bf33-169b20d02020-0000_x000D_
I0601 23:53:23.727311  4496 slave.cpp:2100] Launching task '792f1a13-d0ee-4e98-a4c8-82b9849adfbe' for framework f07f4fdd-cd91-4d62-bf33-169b20d02020-0000_x000D_
I0601 23:53:23.738310  4496 slave.cpp:7078] Launching executor '792f1a13-d0ee-4e98-a4c8-82b9849adfbe' of framework f07f4fdd-cd91-4d62-bf33-169b20d02020-0000 with resources cpus(*)(allocated: *):0.1; mem(*)(allocated: *):32 in work directory 'C:\temp\b1wVnd\slaves\f07f4fdd-cd91-4d62-bf33-169b20d02020-S0\frameworks\f07f4fdd-cd91-4d62-bf33-169b20d02020-0000\executors\792f1a13-d0ee-4e98-a4c8-82b9849adfbe\runs\1cbce8a6-ae59-484f-b898-e2ea6396d2a9'_x000D_
I0601 23:53:23.741310  4496 slave.cpp:2795] Launching container 1cbce8a6-ae59-484f-b898-e2ea6396d2a9 for executor '792f1a13-d0ee-4e98-a4c8-82b9849adfbe' of framework f07f4fdd-cd91-4d62-bf33-169b20d02020-0000_x000D_
I0601 23:53:23.743311  3976 containerizer.cpp:1056] Starting container 1cbce8a6-ae59-484f-b898-e2ea6396d2a9_x000D_
I0601 23:53:23.750272  4496 slave.cpp:2329] Queued task '792f1a13-d0ee-4e98-a4c8-82b9849adfbe' for executor '792f1a13-d0ee-4e98-a4c8-82b9849adfbe' of framework f07f4fdd-cd91-4d62-bf33-169b20d02020-0000_x000D_
I0601 23:53:23.797314  1856 launcher.cppReceived SUBSCRIBED event_x000D_
Subscribed executor on ip-172-20-128-1.ec2.internal_x000D_
Received LAUNCH event_x000D_
Starting task 792f1a13-d0ee-4e98-a4c8-82b9849adfbe_x000D_
Running 'C:\Users\Administrator\workspace\mesos\Mesos_CI-build\FLAG\Plain\label\mesos-ec2-windows\mesos\build\src\mesos-containerizer.exe launch <POSSIBLY-SENSITIVE-DATA>'_x000D_
Forked command at 1180_x000D_
:140] Forked child with pid '4980' for container '1cbce8a6-ae59-484f-b898-e2ea6396d2a9'_x000D_
I0601 23:53:23.798315  1856 containerizer.cpp:1722] Checkpointing container's forked pid 4980 to 'C:\temp\b1wVnd\meta\slaves\f07f4fdd-cd91-4d62-bf33-169b20d02020-S0\frameworks\f07f4fdd-cd91-4d62-bf33-169b20d02020-0000\executors\792f1a13-d0ee-4e98-a4c8-82b9849adfbe\runs\1cbce8a6-ae59-484f-b898-e2ea6396d2a9\pids\forked.pid'_x000D_
I0601 23:53:24.029322  1856 slave.cpp:3825] Got registration for executor '792f1a13-d0ee-4e98-a4c8-82b9849adfbe' of framework f07f4fdd-cd91-4d62-bf33-169b20d02020-0000 from executor(1)@172.20.128.1:51637_x000D_
I0601 23:53:24.043324  2124 slave.cpp:2542] Sending queued task '792f1a13-d0ee-4e98-a4c8-82b9849adfbe' to executor '792f1a13-d0ee-4e98-a4c8-82b9849adfbe' of framework f07f4fdd-cd91-4d62-bf33-169b20d02020-0000 at executor(1)@172.20.128.1:51637_x000D_
I0601 23:53:24.185328  2728 slave.cpp:4295] Handling status update TASK_RUNNING (UUID: d814fd3c-25f8-4307-a8a7-3235177322b9) for task 792f1a13-d0ee-4e98-a4c8-82b9849adfbe of framework f07f4fdd-cd91-4d62-bf33-169b20d02020-0000 from executor(1)@172.20.128.1:51637_x000D_
I0601 23:53:24.192329  2124 status_update_manager.cpp:323] Received status update TASK_RUNNING (UUID: d814fd3c-25f8-4307-a8a7-3235177322b9) for task 792f1a13-d0ee-4e98-a4c8-82b9849adfbe of framework f07f4fdd-cd91-4d62-bf33-169b20d02020-0000_x000D_
I0601 23:53:24.195328  2124 status_update_manager.cpp:834] Checkpointing UPDATE for status update TASK_RUNNING (UUID: d814fd3c-25f8-4307-a8a7-3235177322b9) for task 792f1a13-d0ee-4e98-a4c8-82b9849adfbe of framework f07f4fdd-cd91-4d62-bf33-169b20d02020-0000_x000D_
I0601 23:53:24.197327  2728 slave.cpp:4735] Forwarding the update TASK_RUNNING (UUID: d814fd3c-25f8-4307-a8a7-3235177322b9) for task 792f1a13-d0ee-4e98-a4c8-82b9849adfbe of framework f07f4fdd-cd91-4d62-bf33-169b20d02020-0000 to master@172.20.128.1:51241_x000D_
I0601 23:53:24.199362  4496 master.cpp:6440] Status update TASK_RUNNING (UUID: d814fd3c-25f8-4307-a8a7-3235177322b9) for task 792f1a13-d0ee-4e98-a4c8-82b9849adfbe of framework f07f4fdd-cd91-4d62-bf33-169b20d02020-0000 from agent f07f4fdd-cd91-4d62-bf33-169b20d02020-S0 at slave(133)@172.20.128.1:51241 (ip-172-20-128-1.ec2.internal)_x000D_
I0601 23:53:24.198328  2728 slave.cpp:4645] Sending acknowledgement for status update TASK_RUNNING (UUID: d814fd3c-25f8-4307-a8a7-3235177322b9) for task 792f1a13-d0ee-4e98-a4c8-82b9849adfbe of framework f07f4fdd-cd91-4d62-bf33-169b20d02020-0000 to executor(1)@172.20.128.1:51637_x000D_
I0601 23:53:24.199362  4496 master.cpp:6502] Forwarding status update TASK_RUNNING (UUID: d814fd3c-25f8-4307-a8a7-3235177322b9) for task 792f1a13-d0ee-4e98-a4c8-82b9849adfbe of framework f07f4fdd-cd91-4d62-bf33-169b20d02020-0000_x000D_
I0601 23:53:24.200330  4496 master.cpp:8507] Updating the state of task 792f1a13-d0ee-4e98-a4c8-82b9849adfbe of framework f07f4fdd-cd91-4d62-bf33-169b20d02020-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)_x000D_
I0601 23:53:24.207330  4496 master.cpp:5190] Processing ACKNOWLEDGE call d814fd3c-25f8-4307-a8a7-3235177322b9 for task 792f1a13-d0ee-4e98-a4c8-82b9849adfbe of framework f07f4fdd-cd91-4d62-bf33-169b20d02020-0000 (default) at scheduler-895fb702-afb9-42fe-8802-83c47f72432e@172.20.128.1:51241 on agent f07f4fdd-cd91-4d62-bf33-169b20d02020-S0_x000D_
I0601 23:53:24.208281  4168 status_update_manager.cpp:395] Received status update acknowledgement (UUID: d814fd3c-25f8-4307-a8a7-3235177322b9) for task 792f1a13-d0ee-4e98-a4c8-82b9849adfbe of framework f07f4fdd-cd91-4d62-bf33-169b20d02020-0000_x000D_
I0601 23:53:24.209329  4168 status_update_manager.cpp:834] Checkpointing ACK for status update TASK_RUNNING (UUID: d814fd3c-25f8-4307-a8a7-3235177322b9) for task 792f1a13-d0ee-4e98-a4c8-82b9849adfbe of framework f07f4fdd-cd91-4d62-bf33-169b20d02020-0000_x000D_
I0601 23:53:24.211320  4496 slave.cpp:817] Agent terminating_x000D_
I0601 23:53:24.212329  4168 master.cpp:1314] Agent f07f4fdd-cd91-4d62-bf33-169b20d02020-S0 at slave(133)@172.20.128.1:51241 (ip-172-20-128-1.ec2.internal) disconnected_x000D_
I0601 23:53:24.212329  4168 master.cpp:3195] Disconnecting agent f07f4fdd-cd91-4d62-bf33-169b20d02020-S0 at slave(133)@172.20.128.1:51241 (ip-172-20-128-1.ec2.internal)_x000D_
I0601 23:53:24.212329  4168 master.cpp:3214] Deactivating agent f07f4fdd-cd91-4d62-bf33-169b20d02020-S0 at slave(133)@172.20.128.1:51241 (ip-172-20-128-1.ec2.internal)_x000D_
I0601 23:53:24.213330  1920 hierarchical.cpp:674] Agent f07f4fdd-cd91-4d62-bf33-169b20d02020-S0 deactivated_x000D_
I0601 23:53:24.214330  1512 containerizer.cpp:230] Using isolation: windows/cpu,filesystem/windows,environment_secret_x000D_
I0601 23:53:24.215328  1512 provisioner.cpp:255] Using default backend 'copy'_x000D_
I0601 23:53:24.254333  2728 slave.cpp:248] Mesos agent started on (134)@172.20.128.1:51241_x000D_
I0601 23:53:24.254333  2728 slave.cpp:249] Flags at startup: --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""C:\temp\kglZbS\store\appc"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""true"" --authenticatee=""crammd5"" --authentication_backoff_factor=""1secs"" --authorizer=""local"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""//./pipe/docker_engine"" --docker_stop_timeout=""0ns"" --docker_store_dir=""C:\temp\kglZbS\store\docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_reregistration_timeout=""15secs"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""C:\temp\kglZbS\fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_command_executor=""false"" --http_credentials=""C:\temp\kglZbS\http_credentials"" --http_heartbeat_interval=""30secs"" --initialize_driver_logging=""true"" --isolation=""windows/cpu"" --launcher=""windows"" --launcher_dir=""C:\Users\Administrator\workspace\mesos\Mesos_CI-build\FLAG\Plain\label\mesos-ec2-windows\mesos\build\src"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_executors_per_framework=""150"" --oversubscribed_resources_interval=""15secs"" --port=""5051"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]"" --runtime_dir=""C:\temp\kglZbS"" --sandbox_directory=""C:\mesos\sandbox"" --strict=""true"" --version=""false"" --work_dir=""C:\temp\b1wVnd""_x000D_
I0601 23:53:24.258334  2728 credentials.hpp:37] Loading credentials for authentication from 'C:\temp\kglZbS\http_credentials'_x000D_
I0601 23:53:24.261334  2728 http.cpp:975] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-readonly'_x000D_
I0601 23:53:24.262333  2728 http.cpp:975] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-readwrite'_x000D_
I0601 23:53:24.269332  2728 slave.cpp:552] Agent resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]_x000D_
I0601 23:53:24.269332  2728 slave.cpp:560] Agent attributes: [  ]_x000D_
I0601 23:53:24.269332  2728 slave.cpp:565] Agent hostname: ip-172-20-128-1.ec2.internal_x000D_
I0601 23:53:24.270332  2748 status_update_manager.cpp:177] Pausing sending status updates_x000D_
I0601 23:53:24.279335  4496 state.cpp:62] Recovering state from 'C:\temp\b1wVnd\meta'_x000D_
I0601 23:53:24.280333  4496 state.cpp:710] No committed checkpointed resources found at 'C:\temp\b1wVnd\meta\resources\resources.info'_x000D_
E0601 23:53:24.308333  3976 slave.cpp:6110] EXIT with status 1: Failed to perform recovery: Incompatible agent info detected._x000D_
------------------------------------------------------------_x000D_
Old agent info:_x000D_
hostname: ""ip-172-20-128-1.ec2.internal""_x000D_
resources {_x000D_
  name: ""cpus""_x000D_
  type: SCALAR_x000D_
  scalar {_x000D_
    value: 2_x000D_
  }_x000D_
  role: ""*""_x000D_
}_x000D_
resources {_x000D_
  name: ""mem""_x000D_
  type: SCALAR_x000D_
  scalar {_x000D_
    value: 1024_x000D_
  }_x000D_
  role: ""*""_x000D_
}_x000D_
resources {_x000D_
  name: ""disk""_x000D_
  type: SCALAR_x000D_
  scalar {_x000D_
    value: 1024_x000D_
  }_x000D_
  role: ""*""_x000D_
}_x000D_
resources {_x000D_
  name: ""ports""_x000D_
  type: RANGES_x000D_
  ranges {_x000D_
    range {_x000D_
      begin: 31000_x000D_
      end: 32000_x000D_
    }_x000D_
  }_x000D_
  role: ""*""_x000D_
}_x000D_
id {_x000D_
  value: ""f07f4fdd-cd91-4d62-bf33-169b20d02020-S0""_x000D_
}_x000D_
checkpoint: true_x000D_
port: 51241_x000D_
_x000D_
------------------------------------------------------------_x000D_
New agent info:_x000D_
hostname: ""ip-172-20-128-1.ec2.internal""_x000D_
resources {_x000D_
  name: ""cpus""_x000D_
  type: SCALAR_x000D_
  scalar {_x000D_
    value: 2_x000D_
  }_x000D_
  role: ""*""_x000D_
}_x000D_
resources {_x000D_
  name: ""mem""_x000D_
  type: SCALAR_x000D_
  scalar {_x000D_
    value: 1024_x000D_
  }_x000D_
  role: ""*""_x000D_
}_x000D_
resources {_x000D_
  name: ""disk""_x000D_
  type: SCALAR_x000D_
  scalar {_x000D_
    value: 1024_x000D_
  }_x000D_
  role: ""*""_x000D_
}_x000D_
resources {_x000D_
  name: ""ports""_x000D_
  type: RANGES_x000D_
  ranges {_x000D_
    range {_x000D_
      begin: 31000_x000D_
      end: 32000_x000D_
    }_x000D_
  }_x000D_
  role: ""*""_x000D_
}_x000D_
id {_x000D_
  value: ""latest""_x000D_
}_x000D_
checkpoint: true_x000D_
port: 51241_x000D_
_x000D_
------------------------------------------------------------_x000D_
To remedy this do as follows:_x000D_
Step 1: rm -f C:\temp\b1wVnd\meta\slaves\latest_x000D_
        This ensures agent doesn't recover old live executors._x000D_
Step 2: Restart the agent._x000D_
{code}",1.0,1.4.0,0.5,0.0,0.0,0.0,0.0,0.0,0.8,0.19298245614035087,0.09999999999999999,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.9541284403669724
Bug,Make use of cout/cerr and glog consistent.,"Some parts of mesos use glog before initialization of glog, hence messages via glog might not end up in a logdir:
bq. WARNING: Logging before InitGoogleLogging() is written to STDERR

The solution might be:
{{cout/cerr}} should be used before logging initialization.
{{glog}} should be used after logging initialization.
 
Usually, main function has initialization pattern like:
# load = flags.load(argc, argv) // Load flags from command line.
# Check if flags are correct, otherwise print error message to cerr and then exit.
# Check if user passed --help flag to print help message to cout and then exit.
# Parsing and setup of environment variables. If this fails, EXIT macro is used to print error message via glog.
# process::initialize()
# logging::initialize()
 
Steps 2 and 3 should use {{cout/cerr}} to eliminate any extra information generated by glog like current time, date and log level.

It would be preferable to move step 6 between steps 3 and 4 safely, because {{logging::initialize()}} doesn’t depend on {{process::initialize()}}.
In addition, initialization of glog should be added, where it's necessary.",3.0,0,0.5,0.25037707390648567,0.3333333333333333,0.5,0.14285714285714285,0.5,0.0,0.2631578947368421,0.29285714285714287,0.1923076923076923,0.2424242424242424,0.2424242424242424,0.0
Improvement,Add executor reconnection retry logic to the agent,"Currently, the agent sends a single {{ReconnectExecutorMessage}} to PID-based executors during recovery. It would be more robust to have the agent retry these messages until {{executor_reregister_timeout}} has elapsed.",3.0,0,0.5,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Improvement,Add an agent flag for executor re-registration timeout.,"Currently, the executor re-register timeout is hard-coded at 2 seconds. It would be beneficial to allow operators to specify this value.",1.0,0,0.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Bug,OversubscriptionTest.RescindRevocableOfferWithIncreasedRevocable is flaky.,"{noformat}_x000D_
[ RUN      ] OversubscriptionTest.RescindRevocableOfferWithIncreasedRevocable_x000D_
I0517 10:43:58.154139 2927604672 cluster.cpp:162] Creating default 'local' authorizer_x000D_
I0517 10:43:58.155712 260517888 master.cpp:436] Master a70cd84f-96ed-417f-8285-04416cf4ecb5 (neils-macbook-pro.local) started on 169.254.161.216:51870_x000D_
I0517 10:43:58.155740 260517888 master.cpp:438] Flags at startup: --acls="""" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate_agents=""true"" --authenticate_frameworks=""true"" --authenticate_http_frameworks=""true"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/private/var/folders/g7/cj4h93hx15d_5195_2436lc00000gn/T/C5v4kE/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_unreachable_tasks_per_framework=""1000"" --port=""5050"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""in_memory"" --registry_fetch_timeout=""1mins"" --registry_gc_interval=""15mins"" --registry_max_agent_age=""2weeks"" --registry_max_agent_count=""102400"" --registry_store_timeout=""100secs"" --registry_strict=""false"" --root_submissions=""true"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/private/var/folders/g7/cj4h93hx15d_5195_2436lc00000gn/T/C5v4kE/master"" --zk_session_timeout=""10secs""_x000D_
I0517 10:43:58.155948 260517888 master.cpp:488] Master only allowing authenticated frameworks to register_x000D_
I0517 10:43:58.155958 260517888 master.cpp:502] Master only allowing authenticated agents to register_x000D_
I0517 10:43:58.155963 260517888 master.cpp:515] Master only allowing authenticated HTTP frameworks to register_x000D_
I0517 10:43:58.155968 260517888 credentials.hpp:37] Loading credentials for authentication from '/private/var/folders/g7/cj4h93hx15d_5195_2436lc00000gn/T/C5v4kE/credentials'_x000D_
I0517 10:43:58.156102 260517888 master.cpp:560] Using default 'crammd5' authenticator_x000D_
I0517 10:43:58.156154 260517888 http.cpp:975] Creating default 'basic' HTTP authenticator for realm 'mesos-master-readonly'_x000D_
I0517 10:43:58.156276 260517888 http.cpp:975] Creating default 'basic' HTTP authenticator for realm 'mesos-master-readwrite'_x000D_
I0517 10:43:58.156409 260517888 http.cpp:975] Creating default 'basic' HTTP authenticator for realm 'mesos-master-scheduler'_x000D_
I0517 10:43:58.156517 260517888 master.cpp:640] Authorization enabled_x000D_
I0517 10:43:58.157871 263200768 master.cpp:2161] Elected as the leading master!_x000D_
I0517 10:43:58.157883 263200768 master.cpp:1700] Recovering from registrar_x000D_
I0517 10:43:58.158254 261591040 registrar.cpp:389] Successfully fetched the registry (0B) in 0ns_x000D_
I0517 10:43:58.158299 261591040 registrar.cpp:493] Applied 1 operations in 14us; attempting to update the registry_x000D_
I0517 10:43:58.158640 261591040 registrar.cpp:550] Successfully updated the registry in 0ns_x000D_
I0517 10:43:58.158766 261591040 registrar.cpp:422] Successfully recovered registrar_x000D_
I0517 10:43:58.158968 259444736 master.cpp:1799] Recovered 0 agents from the registry (164B); allowing 10mins for agents to re-register_x000D_
I0517 10:43:58.162422 2927604672 containerizer.cpp:221] Using isolation: posix/cpu,posix/mem,filesystem/posix_x000D_
I0517 10:43:58.162828 2927604672 provisioner.cpp:249] Using default backend 'copy'_x000D_
I0517 10:43:58.163873 2927604672 cluster.cpp:448] Creating default 'local' authorizer_x000D_
I0517 10:43:58.164876 262127616 slave.cpp:225] Mesos agent started on (7)@169.254.161.216:51870_x000D_
I0517 10:43:58.164902 262127616 slave.cpp:226] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/var/folders/g7/cj4h93hx15d_5195_2436lc00000gn/T/mesos/store/appc"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""true"" --authenticatee=""crammd5"" --authentication_backoff_factor=""1secs"" --authorizer=""local"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/var/folders/g7/cj4h93hx15d_5195_2436lc00000gn/T/OversubscriptionTest_RescindRevocableOfferWithIncreasedRevocable_TTBI1M/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/var/folders/g7/cj4h93hx15d_5195_2436lc00000gn/T/mesos/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/var/folders/g7/cj4h93hx15d_5195_2436lc00000gn/T/OversubscriptionTest_RescindRevocableOfferWithIncreasedRevocable_TTBI1M/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_command_executor=""false"" --http_credentials=""/var/folders/g7/cj4h93hx15d_5195_2436lc00000gn/T/OversubscriptionTest_RescindRevocableOfferWithIncreasedRevocable_TTBI1M/http_credentials"" --http_heartbeat_interval=""30secs"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher=""posix"" --launcher_dir=""/Users/neilc/ms/build-mesos/src"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_executors_per_framework=""150"" --oversubscribed_resources_interval=""15secs"" --port=""5051"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]"" --runtime_dir=""/var/folders/g7/cj4h93hx15d_5195_2436lc00000gn/T/OversubscriptionTest_RescindRevocableOfferWithIncreasedRevocable_TTBI1M"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --version=""false"" --work_dir=""/var/folders/g7/cj4h93hx15d_5195_2436lc00000gn/T/OversubscriptionTest_RescindRevocableOfferWithIncreasedRevocable_MuoZUz""_x000D_
I0517 10:43:58.165099 262127616 credentials.hpp:86] Loading credential for authentication from '/var/folders/g7/cj4h93hx15d_5195_2436lc00000gn/T/OversubscriptionTest_RescindRevocableOfferWithIncreasedRevocable_TTBI1M/credential'_x000D_
I0517 10:43:58.165222 262127616 slave.cpp:258] Agent using credential for: test-principal_x000D_
I0517 10:43:58.165240 262127616 credentials.hpp:37] Loading credentials for authentication from '/var/folders/g7/cj4h93hx15d_5195_2436lc00000gn/T/OversubscriptionTest_RescindRevocableOfferWithIncreasedRevocable_TTBI1M/http_credentials'_x000D_
I0517 10:43:58.165406 262127616 http.cpp:975] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-readonly'_x000D_
I0517 10:43:58.165530 262127616 http.cpp:975] Creating default 'basic' HTTP authenticator for realm 'mesos-agent-readwrite'_x000D_
I0517 10:43:58.166236 262127616 slave.cpp:529] Agent resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]_x000D_
I0517 10:43:58.166291 262127616 slave.cpp:537] Agent attributes: [  ]_x000D_
I0517 10:43:58.166301 262127616 slave.cpp:542] Agent hostname: neils-macbook-pro.local_x000D_
I0517 10:43:58.166380 261054464 status_update_manager.cpp:177] Pausing sending status updates_x000D_
I0517 10:43:58.166661 2927604672 sched.cpp:232] Version: 1.4.0_x000D_
I0517 10:43:58.166970 259981312 sched.cpp:336] New master detected at master@169.254.161.216:51870_x000D_
I0517 10:43:58.166982 263200768 state.cpp:62] Recovering state from '/var/folders/g7/cj4h93hx15d_5195_2436lc00000gn/T/OversubscriptionTest_RescindRevocableOfferWithIncreasedRevocable_MuoZUz/meta'_x000D_
I0517 10:43:58.167007 259981312 sched.cpp:407] Authenticating with master master@169.254.161.216:51870_x000D_
I0517 10:43:58.167017 259981312 sched.cpp:414] Using default CRAM-MD5 authenticatee_x000D_
I0517 10:43:58.167127 259444736 authenticatee.cpp:121] Creating new client SASL connection_x000D_
I0517 10:43:58.167276 261054464 status_update_manager.cpp:203] Recovering status update manager_x000D_
I0517 10:43:58.167337 260517888 master.cpp:7475] Authenticating scheduler-08ffd6fe-afc5-4847-8bc9-5ed484aea3a0@169.254.161.216:51870_x000D_
I0517 10:43:58.167469 259444736 containerizer.cpp:608] Recovering containerizer_x000D_
I0517 10:43:58.167549 263200768 authenticator.cpp:98] Creating new server SASL connection_x000D_
I0517 10:43:58.167659 259981312 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5_x000D_
I0517 10:43:58.167681 259981312 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'_x000D_
I0517 10:43:58.167770 261591040 authenticator.cpp:204] Received SASL authentication start_x000D_
I0517 10:43:58.167809 261591040 authenticator.cpp:326] Authentication requires more steps_x000D_
I0517 10:43:58.167899 262127616 authenticatee.cpp:259] Received SASL authentication step_x000D_
I0517 10:43:58.168004 262664192 authenticator.cpp:232] Received SASL authentication step_x000D_
I0517 10:43:58.168036 262664192 authenticator.cpp:318] Authentication success_x000D_
I0517 10:43:58.168140 259444736 authenticatee.cpp:299] Authentication success_x000D_
I0517 10:43:58.168159 260517888 master.cpp:7505] Successfully authenticated principal 'test-principal' at scheduler-08ffd6fe-afc5-4847-8bc9-5ed484aea3a0@169.254.161.216:51870_x000D_
I0517 10:43:58.168407 263200768 sched.cpp:513] Successfully authenticated with master master@169.254.161.216:51870_x000D_
I0517 10:43:58.168596 261054464 master.cpp:2813] Received SUBSCRIBE call for framework 'default' at scheduler-08ffd6fe-afc5-4847-8bc9-5ed484aea3a0@169.254.161.216:51870_x000D_
I0517 10:43:58.168611 261054464 master.cpp:2197] Authorizing framework principal 'test-principal' to receive offers for roles '{ * }'_x000D_
I0517 10:43:58.168609 261591040 provisioner.cpp:410] Provisioner recovery complete_x000D_
I0517 10:43:58.168901 259981312 master.cpp:2813] Received SUBSCRIBE call for framework 'default' at scheduler-08ffd6fe-afc5-4847-8bc9-5ed484aea3a0@169.254.161.216:51870_x000D_
I0517 10:43:58.168915 259981312 master.cpp:2197] Authorizing framework principal 'test-principal' to receive offers for roles '{ * }'_x000D_
I0517 10:43:58.168923 261054464 slave.cpp:5974] Finished recovery_x000D_
I0517 10:43:58.169025 259981312 master.cpp:2890] Subscribing framework default with checkpointing disabled and capabilities [ REVOCABLE_RESOURCES ]_x000D_
I0517 10:43:58.169322 262664192 hierarchical.cpp:273] Added framework a70cd84f-96ed-417f-8285-04416cf4ecb5-0000_x000D_
I0517 10:43:58.169355 261591040 sched.cpp:759] Framework registered with a70cd84f-96ed-417f-8285-04416cf4ecb5-0000_x000D_
I0517 10:43:58.169375 259981312 master.cpp:2890] Subscribing framework default with checkpointing disabled and capabilities [ REVOCABLE_RESOURCES ]_x000D_
I0517 10:43:58.169392 259981312 master.cpp:2900] Framework a70cd84f-96ed-417f-8285-04416cf4ecb5-0000 (default) at scheduler-08ffd6fe-afc5-4847-8bc9-5ed484aea3a0@169.254.161.216:51870 already subscribed, resending acknowledgement_x000D_
I0517 10:43:58.169566 261054464 status_update_manager.cpp:177] Pausing sending status updates_x000D_
I0517 10:43:58.169572 260517888 slave.cpp:922] New master detected at master@169.254.161.216:51870_x000D_
I0517 10:43:58.169620 260517888 slave.cpp:957] Detecting new master_x000D_
I0517 10:43:58.169721 260517888 slave.cpp:984] Authenticating with master master@169.254.161.216:51870_x000D_
I0517 10:43:58.169747 260517888 slave.cpp:995] Using default CRAM-MD5 authenticatee_x000D_
I0517 10:43:58.169876 259981312 authenticatee.cpp:121] Creating new client SASL connection_x000D_
I0517 10:43:58.169983 262664192 master.cpp:7475] Authenticating slave(7)@169.254.161.216:51870_x000D_
I0517 10:43:58.170220 260517888 authenticator.cpp:98] Creating new server SASL connection_x000D_
I0517 10:43:58.170321 261054464 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5_x000D_
I0517 10:43:58.170338 261054464 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'_x000D_
I0517 10:43:58.170450 262664192 authenticator.cpp:204] Received SASL authentication start_x000D_
I0517 10:43:58.170491 262664192 authenticator.cpp:326] Authentication requires more steps_x000D_
I0517 10:43:58.170567 262664192 authenticatee.cpp:259] Received SASL authentication step_x000D_
I0517 10:43:58.170634 260517888 authenticator.cpp:232] Received SASL authentication step_x000D_
I0517 10:43:58.170662 260517888 authenticator.cpp:318] Authentication success_x000D_
I0517 10:43:58.170758 261591040 authenticatee.cpp:299] Authentication success_x000D_
I0517 10:43:58.170771 261054464 master.cpp:7505] Successfully authenticated principal 'test-principal' at slave(7)@169.254.161.216:51870_x000D_
I0517 10:43:58.171092 262127616 slave.cpp:1079] Successfully authenticated with master master@169.254.161.216:51870_x000D_
I0517 10:43:58.171214 259981312 master.cpp:5429] Received register agent message from slave(7)@169.254.161.216:51870 (neils-macbook-pro.local)_x000D_
I0517 10:43:58.171231 259981312 master.cpp:3659] Authorizing agent with principal 'test-principal'_x000D_
I0517 10:43:58.171458 263200768 master.cpp:5564] Registering agent at slave(7)@169.254.161.216:51870 (neils-macbook-pro.local) with id a70cd84f-96ed-417f-8285-04416cf4ecb5-S0_x000D_
I0517 10:43:58.171792 261054464 registrar.cpp:493] Applied 1 operations in 81us; attempting to update the registry_x000D_
I0517 10:43:58.172471 261054464 registrar.cpp:550] Successfully updated the registry in 0ns_x000D_
I0517 10:43:58.172878 259444736 master.cpp:5639] Registered agent a70cd84f-96ed-417f-8285-04416cf4ecb5-S0 at slave(7)@169.254.161.216:51870 (neils-macbook-pro.local) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]_x000D_
I0517 10:43:58.172924 262664192 slave.cpp:1125] Registered with master master@169.254.161.216:51870; given agent ID a70cd84f-96ed-417f-8285-04416cf4ecb5-S0_x000D_
I0517 10:43:58.173074 261054464 status_update_manager.cpp:184] Resuming sending status updates_x000D_
I0517 10:43:58.173394 259981312 hierarchical.cpp:525] Added agent a70cd84f-96ed-417f-8285-04416cf4ecb5-S0 (neils-macbook-pro.local) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: {})_x000D_
I0517 10:43:58.174643 261591040 master.cpp:7305] Sending 1 offers to framework a70cd84f-96ed-417f-8285-04416cf4ecb5-0000 (default) at scheduler-08ffd6fe-afc5-4847-8bc9-5ed484aea3a0@169.254.161.216:51870_x000D_
I0517 10:43:58.175052 259444736 slave.cpp:6205] Forwarding total oversubscribed resources cpus(*){REV}:1_x000D_
I0517 10:43:58.175165 259981312 master.cpp:6324] Received update of agent a70cd84f-96ed-417f-8285-04416cf4ecb5-S0 at slave(7)@169.254.161.216:51870 (neils-macbook-pro.local) with total oversubscribed resources cpus(*){REV}:1_x000D_
I0517 10:43:58.175364 262664192 hierarchical.cpp:620] Agent a70cd84f-96ed-417f-8285-04416cf4ecb5-S0 (neils-macbook-pro.local) updated with oversubscribed resources cpus(*){REV}:1 (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]; cpus(*){REV}:1, allocated: cpus(*)(allocated: *):2; mem(*)(allocated: *):1024; disk(*)(allocated: *):1024; ports(*)(allocated: *):[31000-32000])_x000D_
I0517 10:43:58.175796 261054464 master.cpp:7305] Sending 1 offers to framework a70cd84f-96ed-417f-8285-04416cf4ecb5-0000 (default) at scheduler-08ffd6fe-afc5-4847-8bc9-5ed484aea3a0@169.254.161.216:51870_x000D_
I0517 10:43:58.176827 259444736 slave.cpp:6205] Forwarding total oversubscribed resources cpus(*){REV}:3_x000D_
I0517 10:43:58.177014 260517888 master.cpp:6324] Received update of agent a70cd84f-96ed-417f-8285-04416cf4ecb5-S0 at slave(7)@169.254.161.216:51870 (neils-macbook-pro.local) with total oversubscribed resources cpus(*){REV}:3_x000D_
I0517 10:43:58.177314 263200768 hierarchical.cpp:620] Agent a70cd84f-96ed-417f-8285-04416cf4ecb5-S0 (neils-macbook-pro.local) updated with oversubscribed resources cpus(*){REV}:3 (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]; cpus(*){REV}:3, allocated: cpus(*)(allocated: *):2; mem(*)(allocated: *):1024; disk(*)(allocated: *):1024; ports(*)(allocated: *):[31000-32000]; cpus(*)(allocated: *){REV}:1)_x000D_
I0517 10:43:58.177306 260517888 master.cpp:6344] Removing offer a70cd84f-96ed-417f-8285-04416cf4ecb5-O1 with revocable resources cpus(*)(allocated: *){REV}:1 on agent a70cd84f-96ed-417f-8285-04416cf4ecb5-S0 at slave(7)@169.254.161.216:51870 (neils-macbook-pro.local)_x000D_
I0517 10:43:58.177935 260517888 master.cpp:7305] Sending 1 offers to framework a70cd84f-96ed-417f-8285-04416cf4ecb5-0000 (default) at scheduler-08ffd6fe-afc5-4847-8bc9-5ed484aea3a0@169.254.161.216:51870_x000D_
I0517 10:43:58.178577 259444736 master.cpp:7305] Sending 1 offers to framework a70cd84f-96ed-417f-8285-04416cf4ecb5-0000 (default) at scheduler-08ffd6fe-afc5-4847-8bc9-5ed484aea3a0@169.254.161.216:51870_x000D_
../../mesos/src/tests/oversubscription_tests.cpp:552: Failure_x000D_
      Expected: allocatedResources(resources2, framework.role())_x000D_
      Which is: { cpus(*)(allocated: *){REV}:3 }_x000D_
To be equal to: resources3_x000D_
      Which is: { cpus(*)(allocated: *){REV}:2 }_x000D_
*** Aborted at 1495043038 (unix time) try ""date -d @1495043038"" if you are using GNU date ***_x000D_
PC: @        0x109064010 testing::UnitTest::AddTestPartResult()_x000D_
*** SIGSEGV (@0x0) received by PID 68930 (TID 0x7fffae7fb3c0) stack trace: ***_x000D_
    @     0x7fffa5af0b3a _sigtramp_x000D_
    @               0x10 (unknown)_x000D_
    @        0x109063817 testing::internal::AssertHelper::operator=()_x000D_
    @        0x108431e2f mesos::internal::tests::OversubscriptionTest_RescindRevocableOfferWithIncreasedRevocable_Test::TestBody()_x000D_
    @        0x1090d2a0a testing::internal::HandleSehExceptionsInMethodIfSupported<>()_x000D_
    @        0x10907b697 testing::internal::HandleExceptionsInMethodIfSupported<>()_x000D_
    @        0x10907b5d5 testing::Test::Run()_x000D_
    @        0x10907d328 testing::TestInfo::Run()_x000D_
    @        0x10907e897 testing::TestCase::Run()_x000D_
    @        0x10908e85c testing::internal::UnitTestImpl::RunAllTests()_x000D_
    @        0x1090d4cca testing::internal::HandleSehExceptionsInMethodIfSupported<>()_x000D_
    @        0x10908e287 testing::internal::HandleExceptionsInMethodIfSupported<>()_x000D_
    @        0x10908e158 testing::UnitTest::Run()_x000D_
    @        0x107e80971 RUN_ALL_TESTS()_x000D_
    @        0x107e7c4b1 main_x000D_
    @     0x7fffa58e1235 start_x000D_
    @                0x5 (unknown)_x000D_
zsh: segmentation fault  ./src/mesos-tests  --gtest_repeat=1000 --gtest_break_on_failure --verbose_x000D_
{noformat}_x000D_
",1.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.48717948717948717,0.4303030303030303,0.4303030303030303,0.0
Bug,"Build error on Windows when using ""int"" for a file descriptor","There is a build error for mesos-tests in src/tests/check_tests.cpp on Windows associated with the use of an ""int"" file descriptor:

C:\mesos\mesos\src\tests\check_tests.cpp(1890): error C2440: 'initializing': cannot convert from 'Try<std::array<os::WindowsFD,2>,Error>' to 'Try<std::array<int,2>,Error>' [C:\mesos\mesos\build\src\tests\mesos-tests.vcxproj]",1.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.01282051282051282,0.0,0.0,0.0
Bug,HierarchicalAllocatorTest.NestedRoleQuota is flaky,"{noformat}
10:25:30 [ RUN      ] HierarchicalAllocatorTest.NestedRoleQuota
10:25:30 I0503 03:25:30.335433 1601536 hierarchical.cpp:158] Initialized hierarchical allocator process
10:25:30 I0503 03:25:30.335777 1601536 hierarchical.cpp:273] Added framework framework1
10:25:30 I0503 03:25:30.335902 1601536 hierarchical.cpp:1286] Set quota cpus(*):2; mem(*):1024 for role 'a/b'
10:25:30 I0503 03:25:30.335971 1601536 hierarchical.cpp:1850] No allocations performed
10:25:30 I0503 03:25:30.335984 1601536 hierarchical.cpp:1940] No inverse offers to send out!
10:25:30 I0503 03:25:30.336010 1601536 hierarchical.cpp:1434] Performed allocation for 0 agents in 73us
10:25:30 I0503 03:25:30.336104 1601536 hierarchical.cpp:525] Added agent agent1 (agent1) with cpus(*):1; mem(*):512 (allocated: {})
10:25:30 I0503 03:25:30.336408 1601536 hierarchical.cpp:1940] No inverse offers to send out!
10:25:30 I0503 03:25:30.336423 1601536 hierarchical.cpp:1434] Performed allocation for 1 agents in 287us
10:25:30 I0503 03:25:30.336890 3211264 hierarchical.cpp:1114] Recovered cpus(*)(allocated: a/b):1; mem(*)(allocated: a/b):512 (total: cpus(*):1; mem(*):512, allocated: {}) on agent agent1 from framework framework1
10:25:30 I0503 03:25:30.336913 3211264 hierarchical.cpp:1151] Framework framework1 filtered agent agent1 for 10secs
10:25:30 I0503 03:25:30.337071 3211264 hierarchical.cpp:273] Added framework framework2
10:25:30 I0503 03:25:30.337180 3211264 hierarchical.cpp:2084] Filtered offer with cpus(*):1; mem(*):512 on agent agent1 for role a/b of framework framework1
10:25:30 I0503 03:25:30.337229 3211264 hierarchical.cpp:1850] No allocations performed
10:25:30 I0503 03:25:30.337249 3211264 hierarchical.cpp:1940] No inverse offers to send out!
10:25:30 I0503 03:25:30.337263 3211264 hierarchical.cpp:1434] Performed allocation for 1 agents in 150us
10:25:30 I0503 03:25:30.337530 1601536 hierarchical.cpp:273] Added framework framework3
10:25:30 I0503 03:25:30.337641 1601536 hierarchical.cpp:2084] Filtered offer with cpus(*):1; mem(*):512 on agent agent1 for role a/b of framework framework1
10:25:30 I0503 03:25:30.337684 1601536 hierarchical.cpp:1850] No allocations performed
10:25:30 I0503 03:25:30.337699 1601536 hierarchical.cpp:1940] No inverse offers to send out!
10:25:30 I0503 03:25:30.337713 1601536 hierarchical.cpp:1434] Performed allocation for 1 agents in 160us
10:25:30 I0503 03:25:30.337847 1601536 hierarchical.cpp:1286] Set quota cpus(*):1; mem(*):512 for role 'a/b/d'
10:25:45 ../../src/tests/hierarchical_allocator_tests.cpp:4498: Failure
10:25:45 Failed to wait 15secs for allocation
10:25:45 [  FAILED  ] HierarchicalAllocatorTest.NestedRoleQuota (15020 ms)
{noformat}
",2.0,0,0.0,1.0,0.0,0.0,0.0,0.0,0.8,0.5438596491228069,0.39999999999999997,0.48717948717948717,0.4303030303030303,0.4303030303030303,0.0
Task,Add authorization for the MARK_AGENT_GONE call.,We need to add the relevant AuthZ needed for the MARK_AGENT_GONE call. Note that AuthN would be added as part of the implementation of the API handler itself.,3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3846153846153846,0.4181818181818182,0.4121212121212121,0.0
Task,Add authorization for the MARK_AGENT_GONE call.,We need to add the relevant AuthZ needed for the MARK_AGENT_GONE call. Note that AuthN would be added as part of the implementation of the API handler itself.,3.0,0,0.0,0.5701357466063348,0.0,0.0,0.0,0.0,0.0,0.6842105263157894,0.5642857142857143,0.3846153846153846,0.4181818181818182,0.4121212121212121,0.0
Task,Implement the API handler on the master for marking agents as gone.,We need to implement the v1 Operator API call on the master that stores the agent gone information in the registry and returns a 200 {{OK}} response if successful. The handler would also be responsible for transitioning all active tasks to {{TASK_GONE}} and sending status updates for them to the framework.,3.0,0,0.0,0.5701357466063348,0.0,0.0,0.0,0.0,0.0,0.6842105263157894,0.5642857142857143,0.3846153846153846,0.4181818181818182,0.4121212121212121,0.0
Task,Add the MARK_AGENT_GONE call to the Operator v1 API protos.,We need to add the relevant call to the v1 Operator API protos to mark an agent as GONE. The actual handler implementation on the master would be done in a separate ticket.,2.0,0,0.0,0.5701357466063348,0.0,0.0,0.0,0.0,0.0,0.6842105263157894,0.5642857142857143,0.3846153846153846,0.4181818181818182,0.4121212121212121,0.0
Bug,Double free or corruption when using parallel test runner,"I observed the following when using the parallel test runner:

{noformat}
/home/bmahler/git/mesos/build/../support/mesos-gtest-runner.py --sequential=*ROOT_* ./mesos-tests
..
*** Error in `/home/bmahler/git/mesos/build/src/.libs/mesos-tests': double free or corruption (out): 0x00007fa818001310 ***
======= Backtrace: =========
/usr/lib64/libc.so.6(+0x7c503)[0x7fa87f27e503]
/usr/lib64/libsasl2.so.3(+0x866d)[0x7fa880f0d66d]
/usr/lib64/libsasl2.so.3(sasl_dispose+0x3b)[0x7fa880f1075b]
/home/bmahler/git/mesos/build/src/.libs/libmesos-1.3.0.so(_ZN5mesos8internal8cram_md527CRAMMD5AuthenticateeProcessD1Ev+0x5d)[0x7fa88708f67d]
/home/bmahler/git/mesos/build/src/.libs/libmesos-1.3.0.so(_ZN5mesos8internal8cram_md527CRAMMD5AuthenticateeProcessD0Ev+0x18)[0x7fa88708f734]
/home/bmahler/git/mesos/build/src/.libs/libmesos-1.3.0.so(_ZN5mesos8internal8cram_md520CRAMMD5AuthenticateeD1Ev+0xfb)[0x7fa88708a065]
/home/bmahler/git/mesos/build/src/.libs/libmesos-1.3.0.so(_ZN5mesos8internal8cram_md520CRAMMD5AuthenticateeD0Ev+0x18)[0x7fa88708a0b4]
/home/bmahler/git/mesos/build/src/.libs/libmesos-1.3.0.so(_ZN5mesos8internal5slave5Slave13_authenticateEv+0x67)[0x7fa8879ff579]
/home/bmahler/git/mesos/build/src/.libs/libmesos-1.3.0.so(_ZZN7process8dispatchIN5mesos8internal5slave5SlaveEEEvRKNS_3PIDIT_EEMS6_FvvEENKUlPNS_11ProcessBaseEE_clESD_+0xe2)[0x7fa887a60b7a]
/home/bmahler/git/mesos/build/src/.libs/libmesos-1.3.0.so(_ZNSt17_Function_handlerIFvPN7process11ProcessBaseEEZNS0_8dispatchIN5mesos8internal5slave5SlaveEEEvRKNS0_3PIDIT_EEMSA_FvvEEUlS2_E_E9_M_invokeERKSt9_Any_dataS2_+0x37)[0x7fa887aa0efe]
/home/bmahler/git/mesos/build/src/.libs/libmesos-1.3.0.so(_ZNKSt8functionIFvPN7process11ProcessBaseEEEclES2_+0x49)[0x7fa8888d1177]
/home/bmahler/git/mesos/build/src/.libs/libmesos-1.3.0.so(_ZN7process11ProcessBase5visitERKNS_13DispatchEventE+0x2f)[0x7fa8888b5063]
/home/bmahler/git/mesos/build/src/.libs/libmesos-1.3.0.so(_ZNK7process13DispatchEvent5visitEPNS_12EventVisitorE+0x2e)[0x7fa8888c0422]
/home/bmahler/git/mesos/build/src/.libs/mesos-tests(_ZN7process11ProcessBase5serveERKNS_5EventE+0x2e)[0xb088c8]
/home/bmahler/git/mesos/build/src/.libs/libmesos-1.3.0.so(_ZN7process14ProcessManager6resumeEPNS_11ProcessBaseE+0x525)[0x7fa8888b10d5]
/home/bmahler/git/mesos/build/src/.libs/libmesos-1.3.0.so(+0x5f1a880)[0x7fa8888ad880]
/home/bmahler/git/mesos/build/src/.libs/libmesos-1.3.0.so(+0x5f2ca8a)[0x7fa8888bfa8a]
/home/bmahler/git/mesos/build/src/.libs/libmesos-1.3.0.so(+0x5f2c9ce)[0x7fa8888bf9ce]
/home/bmahler/git/mesos/build/src/.libs/libmesos-1.3.0.so(+0x5f2c958)[0x7fa8888bf958]
/usr/lib64/libstdc++.so.6(+0xb5230)[0x7fa87fb90230]
/usr/lib64/libpthread.so.0(+0x7dc5)[0x7fa88040ddc5]
/usr/lib64/libc.so.6(clone+0x6d)[0x7fa87f2f973d]
{noformat}

Not sure how reproducible this is, appears to occur in the authentication path of the agent.",2.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.0
Task,Set working directory in DEBUG containers.,Currently working directory is not set for DEBUG containers. The most reasonable value seems to be parent's working directory.,3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.717948717948718,0.8242424242424242,0.7212121212121212,0.0
Task,Add authorization to master's operator maintenance API in v0 and v1,"None of the maintenance primitives in either API v0 or API v1 have any kind of authorization, which allows any user with valid credentials to do things such as shutting down a machine, schedule time off on an agent, modify maintenance schedule, etc.

The authorization support needs to be added to the v0 endpoints:

* {{/master/machine/up}}
* {{/master/machine/down}}
* {{/master/maintenance/schedule}}
* {{/master/maintenance/status}}

as well as to the v1 calls:

* {{GET_MAINTENANCE_STATUS}}
* {{GET_MAINTENANCE_SCHEDULE}}
* {{UPDATE_MAINTENANCE_SCHEDULE}}
* {{START_MAINTENANCE}}
* {{STOP_MAINTENANCE}}",3.0,0,0.0,0.004524886877828055,0.0,0.0,0.0,0.0,0.2,0.12280701754385964,0.24285714285714285,0.10256410256410256,0.11515151515151516,0.11515151515151516,0.0
Task,Add authentication to the checker and health checker libraries,The health checker library in {{src/checks/health_checker.cpp}} must be updated to authenticate with the agent's HTTP operator API.,2.0,0,0.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Bug,Remove thread_local workaround on OSX,"{{include/stout/thread_local.hpp}} in stout contains the following comment:

{noformat}
// A wrapper around the thread local storage attribute. The default
// clang on OSX does not support the c++11 standard `thread_local`
// intentionally until a higher performance implementation is
// released. See https://devforums.apple.com/message/1079348#1079348
// Until then, we use `__thread` on OSX instead.
// We required that THREAD_LOCAL is only used with POD types as this
// is a requirement of `__thread`.
{noformat}

As of XCode 8, this workaround should no longer be necessary, because Apple's clang supports {{thread_local}} natively -- see http://stackoverflow.com/a/29929949/5327044",2.0,0,0.0,1.0,0.0,0.0,0.0,0.0,0.8,0.5438596491228069,0.39999999999999997,0.48717948717948717,0.4303030303030303,0.4303030303030303,0.0
Bug,MasterAPITest.GetRoles is flaky on machines with non-C locale.,"{{MasterAPITest.GetRoles}} test sets role weight to a real number using {{.}} as a decimal mark. This however is not correct on machines with non-standard locale, because weight parsing code relies on locale: [https://github.com/apache/mesos/blob/7f04cf886fc2ed59414bf0056a2f351959a2d1f8/src/master/master.cpp#L727-L750]. This leads to test failures: [https://pastebin.com/sQR2Tr2Q].

There are several solutions here.

h4. 1. Change parsing code to be locale-agnostic.
This seems to be the most robust solution. However, the {{--weights}} flag is deprecated and will probably be removed soon, together with the parsing code. 

h4. 2. Fix call sites in our tests to ensure decimal mark is locale dependent.
This seems like a reasonable solution, but I'd argue we can do even better.

h4. 3. Use locale-agnostic format for doubles in tests.
Instead of saying {{""2.5""}} we can say {{""25e-1""}} which is locale agnostic.",1.0,"1.0.2,1.1.1,1.2.0",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.717948717948718,0.8242424242424242,0.7212121212121212,0.9266972477064219
Bug,Compile error with recent glibc,"{noformat}
../../mesos/src/slave/containerizer/mesos/isolators/gpu/isolator.cpp:152:13: error: In the GNU C Library, ""major"" is defined
 by <sys/sysmacros.h>. For historical compatibility, it is
 currently defined by <sys/types.h> as well, but we plan to
 remove this soon. To use ""major"", include <sys/sysmacros.h>
 directly. If you did not intend to use a system-defined macro
 ""major"", you should undefine it after including <sys/types.h>. [-Werror]
   entry.selector.major = major(device.get());
             ^~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
{noformat}

Observed on recent Arch Linux, GCC 6.3.1, glibc 2.25-1.",1.0,0,0.0,1.0,0.0,0.0,0.0,0.0,0.8,0.5438596491228069,0.39999999999999997,0.48717948717948717,0.4303030303030303,0.4303030303030303,0.0
Improvement,Upgrade vendored GMock / GTest,"We currently vendor gmock 1.7.0. The latest upstream version of gmock is 1.8.0, which fixes at least one annoying warning (MESOS-6539).",3.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.2,0.2631578947368421,0.36428571428571427,0.48717948717948717,0.4303030303030303,0.4303030303030303,0.0
Improvement,Command checks via agent pollute agent logs.,"Command checks via agent leverage debug container API of the agent to start checks. Each such invocation triggers a bunch of logs on the agent, because the API was not originally designed with periodic invocations in mind. We should find a way to avoid excessive logging on the agent.",3.0,0,0.5,0.25037707390648567,0.3333333333333333,0.5,0.14285714285714285,0.5,0.0,0.2631578947368421,0.29285714285714287,0.717948717948718,0.8242424242424242,0.7212121212121212,0.0
Improvement,Set MESOS_SANDBOX in debug containers.,"Currently {{MESOS_SANDBOX}} is not set for debug containers, see [https://github.com/apache/mesos/blob/7f04cf886fc2ed59414bf0056a2f351959a2d1f8/src/slave/containerizer/mesos/containerizer.cpp#L1392-L1407]. The most reasonable value seems to be task's sandbox.",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.717948717948718,0.8242424242424242,0.7212121212121212,0.0
Documentation,"Document Mesos ""check"" feature.",This should include framework authors recommendations about how and when to use general checks as well as comparison with health checks.,3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.717948717948718,0.8242424242424242,0.7212121212121212,0.0
Task,Add authorization to agent executor API,"The agent's {{/executor}} endpoint must be updated to accomplish simple implicit authorization of executor actions. This is analogous to the way the master's {{/scheduler}} endpoint handler verifies the framework's authenticated principal, effectively performing implicit authorization.",3.0,0,0.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Task,Authorize offer operations for converting disk resources,"All offer operations are authorized, hence authorization logic has to be added to new offer operations as well.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.2631578947368421,0.36428571428571427,0.28205128205128205,0.296969696969697,0.296969696969697,0.0
Task,Remove deprecated ACL `ShutdownFramework`,"The ACLs {{ShutdownFramework}}  was marked for deprecation more than six months ago, where it was replaced for the more convenient {{TeardownFramework}} ACL. The deprecation cycle for this action is finally due",1.0,0,0.0,0.004524886877828055,0.0,0.0,0.0,0.0,0.2,0.12280701754385964,0.24285714285714285,0.10256410256410256,0.11515151515151516,0.11515151515151516,0.0
Task,Fetcher should not depend on SlaveID.,"Currently, various Fetcher interfaces depends on SlaveID, which is an unnecessary coupling. For instance:
{code}
Try<Nothing> Fetcher::recover(const SlaveID& slaveId, const Flags& flags);

Future<Nothing> Fetcher::fetch(
    const ContainerID& containerId,
    const CommandInfo& commandInfo,
    const string& sandboxDirectory,
    const Option<string>& user,
    const SlaveID& slaveId,
    const Flags& flags);
{code}

Looks like the only reason we need a SlaveID is because we need to calculate the fetcher cache directory based on that. We should calculate the fetcher cache directory in the caller and pass that directory to Fetcher.",3.0,0,0.5,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.7948717948717948,1.0,1.0,0.0
Task,Implement a plugin to list container's on a given agent.,We need the CLI to support a `list` command to enumerate the containers running on a given agent. To achieve this we will need to implement a container plugin that will be implement this list method.,3.0,0,0.5,0.25037707390648567,0.3333333333333333,0.5,0.14285714285714285,0.5,0.0,0.2631578947368421,0.29285714285714287,0.1282051282051282,0.3090909090909091,0.3090909090909091,0.0
Task,Allow Mesos CLI to take masters IP,Allow the Mesos CLI to take master IPs. This will allow the CLI to send HTTP requests to one master in a cluster with one or multiple ones.,2.0,0,0.5,0.25037707390648567,0.3333333333333333,0.5,0.14285714285714285,0.5,0.0,0.2631578947368421,0.29285714285714287,0.1282051282051282,0.3090909090909091,0.3090909090909091,0.0
Task,Add ability to initialize a test cluster for Mesos CLI unit-test infrastructure,Similar to the Mesos unit-tests we need to have the ability to bring up a test cluster against which we can run the python unit-tests for Mesos CLI.,2.0,0,0.5,0.25037707390648567,0.3333333333333333,0.5,0.14285714285714285,0.5,0.0,0.2631578947368421,0.29285714285714287,0.1282051282051282,0.3090909090909091,0.3090909090909091,0.0
Task,Create a table abstraction for the Mesos CLI,Add a an abstraction for printing and formatting tables in the Mesos CLI. This will very useful for all commands that need to print their output in a table format.,2.0,0,0.5,0.25037707390648567,0.3333333333333333,0.5,0.14285714285714285,0.5,0.0,0.2631578947368421,0.29285714285714287,0.1282051282051282,0.3090909090909091,0.3090909090909091,0.0
Improvement,General checker does not support TCP checks.,"The general checker introduced in MESOS-6906 does not support TCP checks. This is a disadvantage, because the health checker does support them.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.717948717948718,0.8242424242424242,0.7212121212121212,0.0
Bug,Containerizer startup may cause sensitive data to leak into sandbox logs.,"The task sandbox logging does show the callup for the containerizer launch with all of its flags.
This is not safe when assuming that we may not want to leak sensitive data into the sandbox logging.

Example:
{noformat}
Received SUBSCRIBED event
Subscribed executor on lobomacpro2.fritz.box
Received LAUNCH event
Starting task test
/Users/till/Development/mesos-private/build/src/mesos-containerizer launch --help=""false"" --launch_info=""{""command"":{""environment"":{""variables"":[{""name"":""key1"",""type"":""VALUE"",""value"":""value1""}]},""shell"":true,""value"":""sleep 1000""},""environment"":{""variables"":[{""name"":""BIN_SH"",""type"":""VALUE"",""value"":""xpg4""},{""name"":""DUALCASE"",""type"":""VALUE"",""value"":""1""},{""name"":""DYLD_LIBRARY_PATH"",""type"":""VALUE"",""value"":""\/Users\/till\/Development\/mesos-private\/build\/src\/.libs""},{""name"":""LIBPROCESS_PORT"",""type"":""VALUE"",""value"":""0""},{""name"":""MESOS_AGENT_ENDPOINT"",""type"":""VALUE"",""value"":""192.168.178.20:5051""},{""name"":""MESOS_CHECKPOINT"",""type"":""VALUE"",""value"":""0""},{""name"":""MESOS_DIRECTORY"",""type"":""VALUE"",""value"":""\/tmp\/mesos\/slaves\/816619b6-f5ce-42d6-ad6b-2ef2001adc0a-S0\/frameworks\/4c8a82d4-8a5b-47f5-a660-5fef15da71a5-0000\/executors\/test\/runs\/b4bd0251-b42a-4ab3-9f02-60ede75bf3b1""},{""name"":""MESOS_EXECUTOR_ID"",""type"":""VALUE"",""value"":""test""},{""name"":""MESOS_EXECUTOR_SHUTDOWN_GRACE_PERIOD"",""type"":""VALUE"",""value"":""5secs""},{""name"":""MESOS_FRAMEWORK_ID"",""type"":""VALUE"",""value"":""4c8a82d4-8a5b-47f5-a660-5fef15da71a5-0000""},{""name"":""MESOS_HTTP_COMMAND_EXECUTOR"",""type"":""VALUE"",""value"":""0""},{""name"":""MESOS_SANDBOX"",""type"":""VALUE"",""value"":""\/tmp\/mesos\/slaves\/816619b6-f5ce-42d6-ad6b-2ef2001adc0a-S0\/frameworks\/4c8a82d4-8a5b-47f5-a660-5fef15da71a5-0000\/executors\/test\/runs\/b4bd0251-b42a-4ab3-9f02-60ede75bf3b1""},{""name"":""MESOS_SLAVE_ID"",""type"":""VALUE"",""value"":""816619b6-f5ce-42d6-ad6b-2ef2001adc0a-S0""},{""name"":""MESOS_SLAVE_PID"",""type"":""VALUE"",""value"":""slave(1)@192.168.178.20:5051""},{""name"":""PATH"",""type"":""VALUE"",""value"":""\/usr\/local\/sbin:\/usr\/local\/bin:\/usr\/sbin:\/usr\/bin:\/sbin:\/bin""},{""name"":""PWD"",""type"":""VALUE"",""value"":""\/private\/tmp\/mesos\/slaves\/816619b6-f5ce-42d6-ad6b-2ef2001adc0a-S0\/frameworks\/4c8a82d4-8a5b-47f5-a660-5fef15da71a5-0000\/executors\/test\/runs\/b4bd0251-b42a-4ab3-9f02-60ede75bf3b1""},{""name"":""SHLVL"",""type"":""VALUE"",""value"":""0""},{""name"":""__CF_USER_TEXT_ENCODING"",""type"":""VALUE"",""value"":""0x1F5:0x0:0x0""},{""name"":""key1"",""type"":""VALUE"",""value"":""value1""},{""name"":""key1"",""type"":""VALUE"",""value"":""value1""}]}}""
Forked command at 16329
{noformat}",3.0,1.2.0,0.0,0.3740573152337858,0.0,0.0,0.0,0.2,0.4,0.43859649122807015,0.2714285714285714,0.3717948717948718,0.24848484848484845,0.24848484848484845,0.9357798165137614
Bug,Possibly duplicate environment variables should not leak values to the sandbox.,"When looking into MESOS-7263, the following also came up.

{noformat}
./src/mesos-execute --name=""test"" --env='{""key1"":""value1""}' --command='sleep 1000' --master=127.0.0.1:5050
{noformat}

Within the contents of `stdout`:
{noformat}
Overwriting environment variable 'key1', original: 'value1', new: 'value1'
{noformat}

There seems no obvious need to warn the user as the value is identical.",1.0,1.2.0,0.0,0.3740573152337858,0.0,0.0,0.0,0.2,0.4,0.43859649122807015,0.2714285714285714,0.3717948717948718,0.24848484848484845,0.24848484848484845,0.9357798165137614
Bug,User supplied task environment variables cause warnings in sandbox stdout.,"The default executor causes task/command environment variables to get duplicated internally, causing warnings in the resulting sandbox {{stdout}}.

{noformat}
$ ./src/mesos-execute --name=""test"" --env='{""key1"":""value1""}' --command='sleep 1000' --master=127.0.0.1:5050
{noformat}

Result in {{stdout}} of the sandbox:
{noformat}
Overwriting environment variable 'key1', original: 'value1', new: 'value1'
{noformat}
",3.0,1.2.0,0.0,0.3740573152337858,0.0,0.0,0.0,0.2,0.4,0.43859649122807015,0.2714285714285714,0.3717948717948718,0.24848484848484845,0.24848484848484845,0.9357798165137614
Bug,Need to fix resource check in long-lived framework,"The multi-role changes in Mesos changed the implementation of `Resources::contains`.

This results in the search for a given resource to be performed only for unallocated resources.
For allocated resources the search is actually performed only for a given role. 

Due to this change the resource check in both the long-lived framework are failing leading to these frameworks not launching any tasks. 

The fix would be to unallocate all resources in a given offer and than do the `contains` check.",2.0,0,0.5,0.6726998491704375,0.0,0.0,0.0,0.1,0.6000000000000001,0.2807017543859649,0.29285714285714287,0.1282051282051282,0.3090909090909091,0.3090909090909091,0.0
Documentation,Add documentation for AGENT_ADDED/AGENT_REMOVED events.,"We need to add documentation to the existing Mesos Operator API docs for the newly added {{AGENT_ADDED}}/{{AGENT_REMOVED}} events. The protobuf definition for the events can be found here:
https://github.com/apache/mesos/blob/master/include/mesos/v1/master/master.proto",1.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02857142857142857,0.3846153846153846,0.4181818181818182,0.4121212121212121,0.0
Improvement,Add support to auto-load /dev/nvidia-uvm in the GPU isolator,"Loading /dev/nvidia-uvm (and installing a script to make sure it loads on reboot) is not technically part of the official Nvidia driver installation process. The rationale being that CUDA applications typically load this device on-demand if they need it. Unfortunately, it can't load it if mesos hasn't made it available to the container running the CUDA application though.

We should add support to have the mesos agent auto-load this device when running the GPU isolator.",2.0,1.2.0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.9357798165137614
Bug,Tasks launched via the default executor cannot access disk resource volumes.,"Currently, when a task in a task group tries to access a volume specified in disk resources (e.g., persistent volumes), it doesn't have access to them since they are mounted in the root container (executor). This happens due to there being no mechanism to specify resources for child containers yet. Hence, by default any resources (e.g., disk) are added to the root container.

A possible solution can be to set up the mapping manually by the default executor using the {{SANDBOX_PATH}} volume source type giving child containers access to the volume mounted in the parent container. This is at best a workaround and the ideal solution would be tackled as part of MESOS-7207.",3.0,0,0.0,0.5701357466063348,0.0,0.0,0.0,0.0,0.0,0.6842105263157894,0.5642857142857143,0.3846153846153846,0.4181818181818182,0.4121212121212121,0.0
Bug,HTTP health check doesn't work when mesos runs with --docker_mesos_image,"When running mesos-slave with option ""docker_mesos_image"" like:
{code}
--master=zk://standalone:2181/mesos  --containerizers=docker,mesos  --executor_registration_timeout=5mins  --hostname=standalone  --ip=0.0.0.0  --docker_stop_timeout=5secs  --gc_delay=1days  --docker_socket=/var/run/docker.sock  --no-systemd_enable_support  --work_dir=/tmp/mesos  --docker_mesos_image=panteras/paas-in-a-box:0.4.0
{code}

from the container that was started with option ""pid: host"" like:
{code}
  net:        host
  privileged: true
  pid:        host
{code}

and example marathon job, that use MESOS_HTTP checks like:
{code}
{
 ""id"": ""python-example-stable"",
 ""cmd"": ""python3 -m http.server 8080"",
 ""mem"": 16,
 ""cpus"": 0.1,
 ""instances"": 2,
 ""container"": {
   ""type"": ""DOCKER"",
   ""docker"": {
     ""image"": ""python:alpine"",
     ""network"": ""BRIDGE"",
     ""portMappings"": [
        { ""containerPort"": 8080, ""hostPort"": 0, ""protocol"": ""tcp"" }
     ]
   }
 },
 ""env"": {
   ""SERVICE_NAME"" : ""python""
 },
 ""healthChecks"": [
   {
     ""path"": ""/"",
     ""portIndex"": 0,
     ""protocol"": ""MESOS_HTTP"",
     ""gracePeriodSeconds"": 30,
     ""intervalSeconds"": 10,
     ""timeoutSeconds"": 30,
     ""maxConsecutiveFailures"": 3
   }
 ]
}
{code}

I see the errors like:
{code}
F0306 07:41:58.844293    35 health_checker.cpp:94] Failed to enter the net namespace of task (pid: '13527'): Pid 13527 does not exist
*** Check failure stack trace: ***
    @     0x7f51770b0c1d  google::LogMessage::Fail()
    @     0x7f51770b29d0  google::LogMessage::SendToLog()
    @     0x7f51770b0803  google::LogMessage::Flush()
    @     0x7f51770b33f9  google::LogMessageFatal::~LogMessageFatal()
    @     0x7f517647ce46  _ZNSt17_Function_handlerIFivEZN5mesos8internal6health14cloneWithSetnsERKSt8functionIS0_E6OptionIiERKSt6vectorINSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEESaISG_EEEUlvE_E9_M_invokeERKSt9_Any_data
    @     0x7f517647bf2b  mesos::internal::health::cloneWithSetns()
    @     0x7f517648374b  std::_Function_handler<>::_M_invoke()
    @     0x7f5177068167  process::internal::cloneChild()
    @     0x7f5177065c32  process::subprocess()
    @     0x7f5176481a9d  mesos::internal::health::HealthCheckerProcess::_httpHealthCheck()
    @     0x7f51764831f7  mesos::internal::health::HealthCheckerProcess::_healthCheck()
    @     0x7f517701f38c  process::ProcessBase::visit()
    @     0x7f517702c8b3  process::ProcessManager::resume()
    @     0x7f517702fb77  _ZNSt6thread5_ImplISt12_Bind_simpleIFZN7process14ProcessManager12init_threadsEvEUt_vEEE6_M_runEv
    @     0x7f51754ddc80  (unknown)
    @     0x7f5174cf06ba  start_thread
    @     0x7f5174a2682d  (unknown)
I0306 07:41:59.077986     9 health_checker.cpp:199] Ignoring failure as health check still in grace period
{code}

Looks like option docker_mesos_image makes, that newly started mesos job is not using ""pid host"" option same as mother container was started, but has his own PID namespace (so it doesn't matter if mother container was started with ""pid host"" or not it will never be able to find PID)",3.0,"1.1.0,1.1.1,1.2.0",0.5,0.0030165912518853697,0.3333333333333333,0.5,0.14285714285714285,0.0,0.0,0.017543859649122806,0.007142857142857143,0.01282051282051282,0.0,0.0,0.9296941896024465
Bug,Requesting tiny amount of CPU crashes master.,"If a task is submitted with a tiny CPU request e.g. 0.0004, then when it completes the master crashes due to a CHECK failure:
{noformat}
F0302 10:48:26.654909 15391 sorter.cpp:291] Check failed: allocations[name].resources[slaveId].contains(resources) 
{noformat}

I can reproduce this with the following command:
{noformat}
mesos-execute --command='sleep 5' --master=$MASTER --name=crashtest --resources='cpus:0.0004;mem:128'
{noformat}

If I replace 0.0004 with 0.001 the issue no longer occurs.",3.0,"1.1.0,1.2.0",0.5,1.0,0.0,0.0,0.0,0.0,0.8,0.5438596491228069,0.39999999999999997,0.01282051282051282,0.0,0.0,0.9311926605504588
Bug,Agent should validate that the nested container ID does not exceed certain length.,"This is related to MESOS-691.

Since nested container ID is generated by the executor, the agent should verify that the length of it does not exceed certain length.",3.0,"1.1.0,1.2.0",0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.017543859649122806,0.007142857142857143,0.7948717948717948,1.0,1.0,0.9311926605504588
Improvement,Rename 'AuthenticationResult',"The naming of {{process::http::authentication::AuthenticationResult}} is redundant. It would be more concise to use {{process::http::authentication::Result}} instead.

Note that this is a breaking change for authentication modules, so it should be advertised prominently.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Bug,Wrap IOSwitchboard.connect() in a dispatch,"Since the IOSwitchboard is implemented as a MesosIsolatorProcess, most
of its API calls are automatically dispatched onto its underlying
process by an Isolator wrapper. However, the IOSwitchboard also
includes an additional connect() call which is not accessed through
the Isolator wrapper. As such, we need to wrap it in a dispatch call
manually.",1.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Bug,Custom executors cannot use any reserved resources.,"A custom executor or the built-in default executor cannot launch a task if they use reserved resources as part of {{ExecutorInfo}}. This mostly happens due to the fact that we don't unallocate the {{Resource}} when comparing it with the checkpointed resources on the agent:

{code}
  Resources checkpointedExecutorResources =
    Resources(executorInfo.resources()).filter(needCheckpointing);
{code}

The fix can be as simple as changing this to:
{code}
  Resources checkpointedExecutorResources =
    unallocated(executorInfo.resources()).filter(needCheckpointing);
{code}",2.0,0,0.0,0.5701357466063348,0.0,0.0,0.0,0.0,0.0,0.6842105263157894,0.5642857142857143,0.3846153846153846,0.4181818181818182,0.4121212121212121,0.0
Improvement,Replace monadic type get() functions with operator*,"In MESOS-2757 we introduced {{T* operator->}} for {{Option}}, {{Future}} and {{Try}}. This provided a convenient short-hand for existing member functions {{T& get}} providing identical functionality._x000D_
_x000D_
To finalize the work of MESOS-2757 we should replace the existing {{T& get()}} member functions with functions {{T& operator*}}._x000D_
_x000D_
This is desirable as having both {{operator->}} and {{get}} in the code base at the same time lures developers into using the old-style {{get}} instead of {{operator->}} where it is not needed, e.g.,_x000D_
{code}_x000D_
m.get().fun();_x000D_
{code}_x000D_
instead of_x000D_
{code}_x000D_
m->fun();_x000D_
{code}_x000D_
_x000D_
We still require the functionality of {{get}} to directly access the contained value, but the current API unnecessarily conflates two (at least from a usage perspective) unrelated aspects; in these instances, we should use an {{operator*}} instead,_x000D_
{code}_x000D_
void f(const T&);_x000D_
    _x000D_
Try<T> m = ..;_x000D_
_x000D_
f(*m); // instead of: f(m.get());_x000D_
{code}_x000D_
_x000D_
Using {{operator*}} in these instances makes it much less likely that users would use it in instances when they wanted to call functions of the wrapped value, i.e.,_x000D_
{code}_x000D_
m->fun();_x000D_
{code}_x000D_
appears more natural than_x000D_
{code}_x000D_
(*m).fun();_x000D_
{code}_x000D_
    _x000D_
Note that this proposed change is in line with the interface of {{std::optional}}. Also, {{std::shared_ptr}}'s {{get}} is a useful function and implements an unrelated interface: it surfaces the wrapped pointer as opposed to its {{operator*}} which dereferences the wrapped pointer. Similarly, our current {{get}} also produce values, and are unrelated to {{std::shared_ptr}}'s {{get}}.",3.0,0,0.5,0.0,0.6666666666666666,1.0,0.5714285714285714,0.0,0.0,0.5438596491228069,0.5357142857142857,1.0,0.8121212121212121,0.8121212121212121,0.0
Bug,Crash when sending a SIGUSR1 signal to the agent.,"Looks like sending a {{SIGUSR1}} to the agent crashes it. This is a regression and used to work fine in the 1.1 release. Note that the agent does unregisters with the master and the crash happens after that.

Steps to reproduce:
- Start the agent.
- Send it a {{SIGUSR1}} signal.

The agent should crash with a stack trace similar to this:
{noformat}
I0209 16:19:46.210819 31977472 slave.cpp:851] Received SIGUSR1 signal from user gmann; unregistering and shutting down
I0209 16:19:46.210960 31977472 slave.cpp:803] Agent terminating
*** Aborted at 1486685986 (unix time) try ""date -d @1486685986"" if you are using GNU date ***
PC: @     0x7fffbc4904fc _pthread_key_global_init
*** SIGSEGV (@0x38) received by PID 88894 (TID 0x7fffc50c83c0) stack trace: ***
    @     0x7fffbc488bba _sigtramp
    @     0x7fe8a5d03f38 (unknown)
    @        0x10b6d67d9 _ZZ11synchronizeINSt3__115recursive_mutexEE12SynchronizedIT_EPS3_ENKUlPS1_E_clES6_
    @        0x10b6d67b8 _ZZ11synchronizeINSt3__115recursive_mutexEE12SynchronizedIT_EPS3_ENUlPS1_E_8__invokeES6_
    @        0x10b6d6889 Synchronized<>::Synchronized()
    @        0x10b6d678d Synchronized<>::Synchronized()
    @        0x10b6a708a synchronize<>()
    @        0x10e2f148d process::ProcessManager::wait()
    @        0x10e2e9a78 process::wait()
    @        0x10b30614f process::wait()
    @        0x10c9619dc mesos::internal::slave::StatusUpdateManager::~StatusUpdateManager()
    @        0x10c961a55 mesos::internal::slave::StatusUpdateManager::~StatusUpdateManager()
    @        0x10b1ab035 main
    @     0x7fffbc27b255 start
[1]    88894 segmentation fault  bin/mesos-agent.sh —master=127.0.0.1:5050
{noformat}",2.0,1.2.0,0.0,0.5701357466063348,0.0,0.0,0.0,0.0,0.0,0.6842105263157894,0.5642857142857143,0.3846153846153846,0.4181818181818182,0.4121212121212121,0.9357798165137614
Bug,The linux filesystem isolator should set mode and ownership for host volumes.,"If the host path is a relative path, the linux filesystem isolator should set the mode and ownership for this host volume since it allows non-root user to write to the volume. Note that this is the case of sharing the host fileysystem (without rootfs).",2.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.017543859649122806,0.007142857142857143,0.6794871794871795,0.5939393939393939,0.5939393939393939,0.0
Bug,Consider using the relink functionality of libprocess in the executor driver.,"As outlined in the root cause analysis for MESOS-5332, it is possible for a iptables firewall to terminate an idle connection after a timeout. (the default is 5 days). Once this happens, the executor driver is not notified of the disconnection. It keeps on thinking that it is still connected with the agent.

When the agent process is restarted, the executor still tries to re-use the old broken connection to send the re-register message to the agent. This is when it eventually realizes that the connection is broken (due to the nature of TCP) and calls the {{exited}} callback and commits suicide in 15 minutes upon the recovery timeout.

To offset this, an executor should always {{relink}} when it receives a reconnect request from the agent.",2.0,"1.0.2,1.1.0",0.0,0.5701357466063348,0.0,0.0,0.0,0.0,0.0,0.6842105263157894,0.5642857142857143,0.3846153846153846,0.4181818181818182,0.4121212121212121,0.9221100917431193
Bug,IOSwitchboard FDs leaked when containerizer launch fails -- leads to deadlock,"If the containizer launch path fails before actually
launching the container, the FDs allocated to the container by the
IOSwitchboard isolator are leaked. This leads to deadlock in
the destroy path because the IOSwitchboard does not shutdown until the
FDs it allocates to the container have been closed. Since the
switchboard doesn't shutdown, the future returned by its 'cleanup()'
function is never satisfied. 

We need a general purpose method for closing the IOSwitchboard FDs when failing in the launch path.",2.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Task,Add implicit executor authorization to local authorizer,"The local authorizer should be updated to perform implicit authorization of executor actions. When executors authenticate using a default executor secret, the authorizer will receive an authorization {{Subject}} which contains claims, but no principal. In this case, implicit authorization should be performed. Implicit authorization rules should enforce that an executor can perform actions on itself; i.e., subscribe as itself, send messages as itself, launch nested containers within itself.",3.0,0,0.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Task,Update the authorizer interface for executor authentication,"The authorizer interface must be updated to accommodate changes introduced by the implementation of executor authentication:
* The {{authorization::Subject}} message must be extended to include the {{claims}} from a {{Principal}}
* The local authorizer must be updated to accommodate this interface change",2.0,0,0.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Task,Add authorization actions for V1 executor calls,"Authorization actions should be added for the V1 executor calls:
* Subscribe
* Update
* Message",2.0,0,0.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Task,Add an '--executor_secret_key' flag to the agent,"A new {{\-\-executor_secret_key}} flag should be added to the agent to allow the operator to specify a secret file to be loaded into the default executor JWT authenticator and SecretGenerator modules. This secret will be used to generate default executor secrets when {{\-\-generate_executor_secrets}} is set, and will be used to verify those secrets when {{\-\-authenticate_http_executors}} is set.",1.0,0,0.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Task,Add a 'secret' field to the 'Environment' message,A new field of type {{Secret}} should be added to the {{Environment}} message to enable the inclusion of secrets in executor and task environments.,1.0,0,0.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Bug,Quota not recovered from registry in empty cluster.,"When a quota was set and the master is restarted, removal of the quota reliably leads to a {{CHECK}} failure for me.

Start a master:
{code}
$ mesos-master --work_dir=work_dir
{code}

Set a quota. This creates an implicit role.
{code}
$ cat quota.json
{
    ""role"": ""role2"",
    ""force"": true,
    ""guarantee"": [
        {
            ""name"": ""cpus"",
            ""type"": ""SCALAR"",
            ""scalar"": { ""value"": 1 }
        }
    ]
}

$ cat quota.json| http POST :5050/quota
HTTP/1.1 200 OK
Content-Length: 0
Date: Thu, 26 Jan 2017 12:33:38 GMT

$ http GET :5050/quota
HTTP/1.1 200 OK
Content-Length: 108
Content-Type: application/json
Date: Thu, 26 Jan 2017 12:33:56 GMT

{
    ""infos"": [
        {
            ""guarantee"": [
                {
                    ""name"": ""cpus"",
                    ""role"": ""*"",
                    ""scalar"": {
                        ""value"": 1.0
                    },
                    ""type"": ""SCALAR""
                }
            ],
            ""role"": ""role2""
        }
    ]
}

$ http GET :5050/roles
HTTP/1.1 200 OK
Content-Length: 106
Content-Type: application/json
Date: Thu, 26 Jan 2017 12:34:10 GMT

{
    ""roles"": [
        {
            ""frameworks"": [],
            ""name"": ""role2"",
            ""resources"": {
                ""cpus"": 0,
                ""disk"": 0,
                ""gpus"": 0,
                ""mem"": 0
            },
            ""weight"": 1.0
        }
    ]
}
{code}

Restart the master process using the same {{work_dir}} and attempt to delete the quota after the master is started. The {{DELETE}} succeeds with an {{OK}}.
{code}
$ http DELETE :5050/quota/role2
HTTP/1.1 200 OK
Content-Length: 0
Date: Thu, 26 Jan 2017 12:36:04 GMT
{code}

After handling the request, the master hits a {{CHECK}} failure and is aborted.
{code}
$ mesos-master --work_dir=work_dir
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0126 13:34:57.528599 3145483200 main.cpp:278] Build: 2017-01-23 07:57:34 by bbannier
I0126 13:34:57.529131 3145483200 main.cpp:279] Version: 1.2.0
I0126 13:34:57.529139 3145483200 main.cpp:286] Git SHA: dd07d025d40975ec660ed17031d95ec0dba842d2
[warn] kq_init: detected broken kqueue; not using.: No such process
I0126 13:34:57.758896 3145483200 main.cpp:385] Using 'HierarchicalDRF' allocator
I0126 13:34:57.764276 3145483200 replica.cpp:778] Replica recovered with log positions 3 -> 4 with 0 holes and 0 unlearned
I0126 13:34:57.765278 256114688 recover.cpp:451] Starting replica recovery
I0126 13:34:57.765547 256114688 recover.cpp:477] Replica is in VOTING status
I0126 13:34:57.795964 257187840 master.cpp:383] Master 569073cc-1195-45e9-b0d4-e2e1bf0d13d5 (172.18.9.56) started on 172.18.9.56:5050
I0126 13:34:57.796023 257187840 master.cpp:385] Flags at startup: --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate_agents=""false"" --authenticate_frameworks=""false"" --authenticate_http_frameworks=""false"" --authenticate_http_readonly=""false"" --authenticate_http_readwrite=""false"" --authenticators=""crammd5"" --authorizers=""local"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_unreachable_tasks_per_framework=""1000"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_gc_interval=""15mins"" --registry_max_agent_age=""2weeks"" --registry_max_agent_count=""102400"" --registry_store_timeout=""20secs"" --registry_strict=""false"" --root_submissions=""true"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""work_dir"" --zk_session_timeout=""10secs""
I0126 13:34:57.796478 257187840 master.cpp:437] Master allowing unauthenticated frameworks to register
I0126 13:34:57.796507 257187840 master.cpp:451] Master allowing unauthenticated agents to register
I0126 13:34:57.796517 257187840 master.cpp:465] Master allowing HTTP frameworks to register without authentication
I0126 13:34:57.796540 257187840 master.cpp:507] Using default 'crammd5' authenticator
W0126 13:34:57.796573 257187840 authenticator.cpp:512] No credentials provided, authentication requests will be refused
I0126 13:34:57.796584 257187840 authenticator.cpp:519] Initializing server SASL
I0126 13:34:57.825337 255578112 master.cpp:2121] Elected as the leading master!
I0126 13:34:57.825362 255578112 master.cpp:1643] Recovering from registrar
I0126 13:34:57.825736 255578112 log.cpp:553] Attempting to start the writer
I0126 13:34:57.826889 258260992 replica.cpp:495] Replica received implicit promise request from __req_res__(1)@172.18.9.56:5050 with proposal 2
I0126 13:34:57.828855 258260992 replica.cpp:344] Persisted promised to 2
I0126 13:34:57.829273 258260992 coordinator.cpp:238] Coordinator attempting to fill missing positions
I0126 13:34:57.829375 259334144 log.cpp:569] Writer started with ending position 4
I0126 13:34:57.830878 257187840 registrar.cpp:362] Successfully fetched the registry (159B) in 5.427968ms
I0126 13:34:57.831029 257187840 registrar.cpp:461] Applied 1 operations in 24us; attempting to update the registry
I0126 13:34:57.836194 259334144 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 5
I0126 13:34:57.836676 257724416 replica.cpp:539] Replica received write request for position 5 from __req_res__(2)@172.18.9.56:5050
I0126 13:34:57.837102 255578112 replica.cpp:693] Replica received learned notice for position 5 from @0.0.0.0:0
I0126 13:34:57.837745 257187840 registrar.cpp:506] Successfully updated the registry in 6.685184ms
I0126 13:34:57.837806 257187840 registrar.cpp:392] Successfully recovered registrar
I0126 13:34:57.837924 255578112 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 6
I0126 13:34:57.838132 256651264 master.cpp:1759] Recovered 0 agents from the registry (159B); allowing 10mins for agents to re-register
I0126 13:34:57.838312 257187840 replica.cpp:539] Replica received write request for position 6 from __req_res__(3)@172.18.9.56:5050
I0126 13:34:57.838692 256651264 replica.cpp:693] Replica received learned notice for position 6 from @0.0.0.0:0
I0126 13:36:04.887257 256114688 http.cpp:420] HTTP DELETE for /master/quota/role2 from 127.0.0.1:51458 with User-Agent='HTTPie/0.9.8'
I0126 13:36:04.887512 255578112 registrar.cpp:461] Applied 1 operations in 42us; attempting to update the registry
I0126 13:36:04.892643 255578112 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 7
I0126 13:36:04.893127 258797568 replica.cpp:539] Replica received write request for position 7 from __req_res__(4)@172.18.9.56:5050
I0126 13:36:04.895309 257187840 replica.cpp:693] Replica received learned notice for position 7 from @0.0.0.0:0
I0126 13:36:04.895814 258260992 registrar.cpp:506] Successfully updated the registry in 8.2688ms
F0126 13:36:04.895956 256114688 hierarchical.cpp:1180] Check failed: quotas.contains(role)
*** Check failure stack trace: ***
I0126 13:36:04.895961 255578112 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 8
I0126 13:36:04.896437 257187840 replica.cpp:539] Replica received write request for position 8 from __req_res__(5)@172.18.9.56:5050
I0126 13:36:04.896908 259334144 replica.cpp:693] Replica received learned notice for position 8 from @0.0.0.0:0
    @        0x10b5e52aa  google::LogMessage::Fail()
E0126 13:36:04.905042 259870720 process.cpp:2419] Failed to shutdown socket with fd 11: Socket is not connected
    @        0x10b5e282c  google::LogMessage::SendToLog()
    @        0x10b5e3959  google::LogMessage::Flush()
    @        0x10b5ee159  google::LogMessageFatal::~LogMessageFatal()
    @        0x10b5e5795  google::LogMessageFatal::~LogMessageFatal()
    @        0x1089e8d17  mesos::internal::master::allocator::internal::HierarchicalAllocatorProcess::removeQuota()
    @        0x107ebbc13  _ZZN7process8dispatchIN5mesos8internal6master9allocator21MesosAllocatorProcessERKNSt3__112basic_stringIcNS6_11char_traitsIcEENS6_9allocatorIcEEEESC_EEvRKNS_3PIDIT_EEMSG_FvT0_ET1_ENKUlPNS_11ProcessBaseEE_clESP_
    @        0x107ebbab0  _ZNSt3__128__invoke_void_return_wrapperIvE6__callIJRZN7process8dispatchIN5mesos8internal6master9allocator21MesosAllocatorProcessERKNS_12basic_stringIcNS_11char_traitsIcEENS_9allocatorIcEEEESF_EEvRKNS3_3PIDIT_EEMSJ_FvT0_ET1_EUlPNS3_11ProcessBaseEE_SS_EEEvDpOT_
    @        0x107ebb7b9  _ZNSt3__110__function6__funcIZN7process8dispatchIN5mesos8internal6master9allocator21MesosAllocatorProcessERKNS_12basic_stringIcNS_11char_traitsIcEENS_9allocatorIcEEEESE_EEvRKNS2_3PIDIT_EEMSI_FvT0_ET1_EUlPNS2_11ProcessBaseEE_NSC_ISS_EEFvSR_EEclEOSR_
    @        0x10b38ba27  std::__1::function<>::operator()()
    @        0x10b38b96c  process::ProcessBase::visit()
    @        0x10b40415e  process::DispatchEvent::visit()
    @        0x107665171  process::ProcessBase::serve()
    @        0x10b385c07  process::ProcessManager::resume()
    @        0x10b47db90  process::ProcessManager::init_threads()::$_0::operator()()
    @        0x10b47d7e0  _ZNSt3__114__thread_proxyINS_5tupleIJNS_10unique_ptrINS_15__thread_structENS_14default_deleteIS3_EEEEZN7process14ProcessManager12init_threadsEvE3$_0EEEEEPvSB_
    @     0x7fffb2b8eaab  _pthread_body
    @     0x7fffb2b8e9f7  _pthread_start
    @     0x7fffb2b8e1fd  thread_start
[2]    59343 abort      mesos-master --work_dir=work_dir
{code}",3.0,"1.0.2,1.1.0",0.5,1.0,0.0,0.0,0.0,0.0,0.8,0.5438596491228069,0.39999999999999997,1.0,0.8121212121212121,0.8121212121212121,0.9221100917431193
Documentation,Add executor authentication documentation,"Documentation should be added regarding executor authentication. This will include updating:
1) the configuration docs to include new agent flags
2) the authentication documentation
3) the authorization documentation
4) the upgrade documentation
5) the CHANGELOG",3.0,0,0.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Task,Add the SecretGenerator module interface,A new {{SecretGenerator}} module interface will be added to permit the agent to generate default executor credentials.,2.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.2,0.2631578947368421,0.36428571428571427,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Task,Add a 'Secret' protobuf message,A {{Secret}} protobuf message should be added to serve as a generic message for sending credentials and other secrets throughout Mesos.,2.0,0,0.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Bug,Change `Environment.Variable.Value` from required to optional,"To prepare for future work which will enable the modular fetching of secrets, we should change the {{Environment.Variable.Value}} field from {{required}} to {{optional}}. This way, the field can be left empty and filled in by a secret fetching module.",2.0,0,0.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Bug,Docker executor segfaults in ~MesosExecutorDriver(),"With the current Mesos master state (commit 42e515bc5c175a318e914d34473016feda4db6ff), the Docker executor segfaults during shutdown. 

Steps to reproduce:

1) Start master:
{code}
$ ./bin/mesos-master.sh --ip=127.0.0.1 --work_dir=/tmp/jp/mesos
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0125 13:41:15.963775 14744 main.cpp:278] Build: 2017-01-25 13:37:42 by jp
I0125 13:41:15.963868 14744 main.cpp:279] Version: 1.2.0
I0125 13:41:15.963877 14744 main.cpp:286] Git SHA: 42e515bc5c175a318e914d34473016feda4db6ff
{code}
(note that building it at 13:37 is not part of the repro)

2) Start agent:
{code}
$ ./bin/mesos-slave.sh --containerizers=mesos,docker --master=127.0.0.1:5050 --work_dir=/tmp/jp/mesos
{code}

3) Run {{mesos-execute}} with the Docker containerizer:
{code}
$ ./src/mesos-execute --master=127.0.0.1:5050 --name=testcommand --containerizer=docker --docker_image=debian --command=env
I0125 13:43:59.704973 14951 scheduler.cpp:184] Version: 1.2.0
I0125 13:43:59.706425 14952 scheduler.cpp:470] New master detected at master@127.0.0.1:5050
Subscribed with ID 57596743-06f4-45f1-a975-348cf70589b1-0000
Submitted task 'testcommand' to agent '57596743-06f4-45f1-a975-348cf70589b1-S0'
Received status update TASK_RUNNING for task 'testcommand'
  source: SOURCE_EXECUTOR
Received status update TASK_FINISHED for task 'testcommand'
  message: 'Container exited with status 0'
  source: SOURCE_EXECUTOR
{code}

Relevant agent output that shows the executor segfault:
{code}
[...]
I0125 13:44:16.249191 14823 slave.cpp:4328] Got exited event for executor(1)@192.99.40.208:33529
I0125 13:44:16.347095 14830 docker.cpp:2358] Executor for container 396282a9-7bf0-48ee-ba07-3ff2ca801d53 has exited
I0125 13:44:16.347127 14830 docker.cpp:2052] Destroying container 396282a9-7bf0-48ee-ba07-3ff2ca801d53
I0125 13:44:16.347439 14830 docker.cpp:2179] Running docker stop on container 396282a9-7bf0-48ee-ba07-3ff2ca801d53
I0125 13:44:16.349215 14826 slave.cpp:4691] Executor 'testcommand' of framework 57596743-06f4-45f1-a975-348cf70589b1-0000 terminated with signal Segmentation fault (core dumped)
[...]
{code}

The complete task stderr:
{code}
$ cat /tmp/jp/mesos/slaves/57596743-06f4-45f1-a975-348cf70589b1-S0/frameworks/57596743-06f4-45f1-a975-348cf70589b1-0000/executors/testcommand/runs/latest/stderr 
I0125 13:44:12.850073 15030 exec.cpp:162] Version: 1.2.0
I0125 13:44:12.864229 15050 exec.cpp:237] Executor registered on agent 57596743-06f4-45f1-a975-348cf70589b1-S0
I0125 13:44:12.865842 15054 docker.cpp:850] Running docker -H unix:///var/run/docker.sock run --cpu-shares 1024 --memory 134217728 --env-file /tmp/xFZ8G9 -v /tmp/jp/mesos/slaves/57596743-06f4-45f1-a975-348cf70589b1-S0/frameworks/57596743-06f4-45f1-a975-348cf70589b1-0000/executors/testcommand/runs/396282a9-7bf0-48ee-ba07-3ff2ca801d53:/mnt/mesos/sandbox --net host --entrypoint /bin/sh --name mesos-57596743-06f4-45f1-a975-348cf70589b1-S0.396282a9-7bf0-48ee-ba07-3ff2ca801d53 debian -c env
I0125 13:44:15.248721 15064 exec.cpp:410] Executor asked to shutdown
*** Aborted at 1485369856 (unix time) try ""date -d @1485369856"" if you are using GNU date ***
PC: @     0x7fb38f153dd0 (unknown)
*** SIGSEGV (@0x68) received by PID 15030 (TID 0x7fb3961a88c0) from PID 104; stack trace: ***
    @     0x7fb38f15b5c0 (unknown)
    @     0x7fb38f153dd0 (unknown)
    @     0x7fb39332c607 __gthread_mutex_lock()
    @     0x7fb39332c657 __gthread_recursive_mutex_lock()
    @     0x7fb39332edca std::recursive_mutex::lock()
    @     0x7fb393337bd8 _ZZ11synchronizeISt15recursive_mutexE12SynchronizedIT_EPS2_ENKUlPS0_E_clES5_
    @     0x7fb393337bf8 _ZZ11synchronizeISt15recursive_mutexE12SynchronizedIT_EPS2_ENUlPS0_E_4_FUNES5_
    @     0x7fb39333ba6b Synchronized<>::Synchronized()
    @     0x7fb393337cac synchronize<>()
    @     0x7fb39492f15c process::ProcessManager::wait()
    @     0x7fb3949353f0 process::wait()
    @     0x55fd63f31fe5 process::wait()
    @     0x7fb39332ce3c mesos::MesosExecutorDriver::~MesosExecutorDriver()
    @     0x55fd63f2bd86 main
    @     0x7fb38e4fc401 __libc_start_main
    @     0x55fd63f2ab5a _start
{code}



",1.0,0,0.5,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.02564102564102564,0.006060606060606061,0.006060606060606061,0.0
Bug,Fix BOOST random generator initialization on Windows,"seed_rng::seed_rng does not produced the expected result in Windows since is using `/dev/urandom` file.  

0:005> k
 # Child-SP          RetAddr           Call Site
00 00000049`22dfc108 00007ff6`5193822f kernel32!CreateFileW
...
0e 00000049`22dfc660 00007ff6`502228fd mesos_agent!boost::uuids::detail::seed_rng::seed_rng+0x3d [d:\repositories\mesoswin\build\3rdparty\boost-1.53.0\src\boost-1.53.0\boost\uuid\seed_rng.hpp @ 80]
0f 00000049`22dfc690 00007ff6`502591e3 mesos_agent!boost::uuids::detail::seed<boost::random::mersenne_twister_engine<unsigned int,32,624,397,31,2567483615,11,4294967295,7,2636928640,15,4022730752,18,1812433253> >+0x4d [d:\repositories\mesoswin\build\3rdparty\boost-1.53.0\src\boost-1.53.0\boost\uuid\seed_rng.hpp @ 246]
10 00000049`22dfc790 00007ff6`50395518 mesos_agent!boost::uuids::basic_random_generator<boost::random::mersenne_twister_engine<unsigned int,32,624,397,31,2567483615,11,4294967295,7,2636928640,15,4022730752,18,1812433253> >::basic_random_generator<boost::random::mersenne_twister_engine<unsigned int,32,624,397,31,2567483615,11,4294967295,7,2636928640,15,4022730752,18,1812433253> >+0xd3 [d:\repositories\mesoswin\build\3rdparty\boost-1.53.0\src\boost-1.53.0\boost\uuid\random_generator.hpp @ 50]
11 00000049`22dfc800 00007ff6`500ad140 mesos_agent!id::UUID::random+0x78 [d:\repositories\mesoswin\3rdparty\stout\include\stout\uuid.hpp @ 49]
12 00000049`22dfc870 00007ff6`5007ff55 mesos_agent!mesos::internal::slave::Framework::launchExecutor+0x70 [d:\repositories\mesoswin\src\slave\slave.cpp @ 6301]
13 00000049`22dfd520 00007ff6`502a0a35 mesos_agent!mesos::internal::slave::Slave::_run+0x2455 [d:\repositories\mesoswin\src\slave\slave.cpp @ 1990]
...
0:005> du @rcx
000001d7`cc55fb60  ""/dev/urandom""
",1.0,0,0.5,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.01282051282051282,0.0303030303030303,0.0303030303030303,0.0
Task,"Separate the mesos-containerizer binary into a static binary, which only depends on stout","The {{mesos-containerizer}} binary currently has [three commands|https://github.com/apache/mesos/blob/6cf3a94a52e87a593c9cba373bf433cfc4178639/src/slave/containerizer/mesos/main.cpp#L46-L48]:

* [MesosContainerizerLaunch|https://github.com/apache/mesos/blob/6cf3a94a52e87a593c9cba373bf433cfc4178639/src/slave/containerizer/mesos/launch.cpp]
* [MesosContainerizerMount|https://github.com/apache/mesos/blob/6cf3a94a52e87a593c9cba373bf433cfc4178639/src/slave/containerizer/mesos/mount.cpp]
* [NetworkCniIsolatorSetup|https://github.com/apache/mesos/blob/6cf3a94a52e87a593c9cba373bf433cfc4178639/src/slave/containerizer/mesos/isolators/network/cni/cni.cpp#L1776-L1997]

These commands are all heavily dependent on stout, and have no need to be linked to libprocess.  In fact, adding an erroneous call to {{process::initialize}} (either explicitly, or by accidentally using a libprocess method) will break {{mesos-containerizer}} can cause several Mesos containerizer tests to fail.  (The tasks fail to launch, saying {{Failed to synchronize with agent (it's probably exited)}}).

Because this binary only depends on stout, we can separate it from the other source files and make this a static binary.
",3.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.8,0.19298245614035087,0.09999999999999999,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.0
Bug,Docker containerizer: mangled environment when env value contains LF byte.,"Consider this Marathon app definition:

{code}
{
  ""id"": ""/testapp"",
  ""cmd"": ""env && tail -f /dev/null"",
  ""env"":{
    ""TESTVAR"":""line1\nline2""
  },
  ""cpus"": 0.1,
  ""mem"": 10,
  ""instances"": 1,
  ""container"": {
    ""type"": ""DOCKER"",
    ""docker"": {
      ""image"": ""alpine""
    }
  }
}
{code}

The JSON-encoded newline in the value of the {{TESTVAR}} environment variable leads to a corrupted task environment. What follows is a subset of the resulting task environment (as printed via {{env}}, i.e. in key=value notation):

{code}
line2=
TESTVAR=line1
{code}

That is, the trailing part of the intended value ended up being interpreted as variable name, and only the leading part of the intended value was used as actual value for {{TESTVAR}}.

Common application scenarios that would badly break with that involve pretty-printed JSON documents or YAML documents passed along via the environment.

Following the code and information flow led to the conclusion that Docker's {{--env-file}} command line interface is the weak point in the flow. It is currently used in Mesos' Docker containerizer for passing the environment to the container:

{code}
  argv.push_back(""--env-file"");
  argv.push_back(environmentFile);
{code}

(Ref: [code|https://github.com/apache/mesos/blob/c0aee8cc10b1d1f4b2db5ff12b771372fdd5b1f3/src/docker/docker.cpp#L584])


Docker's {{--env-file}} argument behavior is documented via

{quote}
The --env-file flag takes a filename as an argument
and expects each line to be in the VAR=VAL format,
{quote}
(Ref: https://docs.docker.com/engine/reference/commandline/run/)

That is, Docker identifies individual environment variable key/value pair definitions based on newline bytes in that file which explains the observed environment variable value fragmentation. Notably, Docker does not provide a mechanism for escaping newline bytes in the values specified in this environment file.

I think it is important to understand that Docker's {{--env-file}} mechanism is ill-posed in the sense that it is not capable of transmitting the whole range of environment variable values allowed by POSIX. That's what the Single UNIX Specification, Version 3 has to say about environment variable values:

{quote}
the value shall be composed of characters from the
portable character set (except NUL and as indicated below). 
{quote}
(Ref: http://pubs.opengroup.org/onlinepubs/009695399/basedefs/xbd_chap08.html)

About ""The portable character set"": http://pubs.opengroup.org/onlinepubs/009695399/basedefs/xbd_chap06.html#tagtcjh_3

It includes (among others) the LF byte. Understandably, the current Docker {{--env-file}} behavior will not change, so this is not an issue that can be deferred to Docker: https://github.com/docker/docker/issues/12997

Notably, the {{--env-file}} method for communicating environment variables to Docker containers was just recently introduced to Mesos as of https://issues.apache.org/jira/browse/MESOS-6566, for not leaking secrets through the process listing. Previously, we specified env key/value pairs on the command line which leaked secrets to the process list and probably also did not support the full range of valid environment variable values.

We need a solution that
1) does not leak sensitive values (i.e. is compliant with MESOS-6566).
2) allows for passing arbitrary environment variable values.

It seems that Docker's {{--env}} method can be used for that. It can be used to define _just the names of the environment variables_ to-be-passed-along, in which case the docker binary will read the corresponding values from its own environment, which we can clearly prepare appropriately when we invoke the corresponding child process. This method would still leak environment variable _names_ to the process listing, but (especially if documented) this should be fine.",3.0,0,0.5,0.3740573152337858,0.0,0.0,0.0,0.2,0.4,0.43859649122807015,0.2714285714285714,0.02564102564102564,0.006060606060606061,0.006060606060606061,0.0
Bug,SchedulerTest.MasterFailover is flaky,"This was observed in a CentOS 7 VM, with libevent and SSL enabled:_x000D_
{code}_x000D_
W0118 22:38:33.789465  3407 scheduler.cpp:513] Dropping SUBSCRIBE: Scheduler is in state DISCONNECTED_x000D_
I0118 22:38:33.811820  3408 scheduler.cpp:361] Connected with the master at http://127.0.0.1:43211/master/api/v1/scheduler_x000D_
../../src/tests/scheduler_tests.cpp:315: Failure_x000D_
Mock function called more times than expected - returning directly._x000D_
    Function call: connected(0x7fff97227550)_x000D_
         Expected: to be called once_x000D_
           Actual: called twice - over-saturated and active_x000D_
{code}_x000D_
_x000D_
Find attached the entire log from a failed run.",2.0,0,0.5,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Bug,"Libprocess reinitialization is flaky, can segfault.","This was observed on ASF CI. Based on the placement of the stacktrace, the segfault seems to occur during libprocess reinitialization, when {{process::initialize}} is called:
{code}
[----------] 4 tests from Encryption/NetSocketTest
[ RUN      ] Encryption/NetSocketTest.EOFBeforeRecv/0
I0117 15:18:35.320691 27596 openssl.cpp:419] CA file path is unspecified! NOTE: Set CA file path with LIBPROCESS_SSL_CA_FILE=<filepath>
I0117 15:18:35.320714 27596 openssl.cpp:424] CA directory path unspecified! NOTE: Set CA directory path with LIBPROCESS_SSL_CA_DIR=<dirpath>
I0117 15:18:35.320719 27596 openssl.cpp:429] Will not verify peer certificate!
NOTE: Set LIBPROCESS_SSL_VERIFY_CERT=1 to enable peer certificate verification
I0117 15:18:35.320726 27596 openssl.cpp:435] Will only verify peer certificate if presented!
NOTE: Set LIBPROCESS_SSL_REQUIRE_CERT=1 to require peer certificate verification
I0117 15:18:35.335141 27596 process.cpp:1234] libprocess is initialized on 172.17.0.3:46415 with 16 worker threads
[       OK ] Encryption/NetSocketTest.EOFBeforeRecv/0 (422 ms)
[ RUN      ] Encryption/NetSocketTest.EOFBeforeRecv/1
I0117 15:18:35.390697 27596 process.cpp:1234] libprocess is initialized on 172.17.0.3:39822 with 16 worker threads
[       OK ] Encryption/NetSocketTest.EOFBeforeRecv/1 (6 ms)
[ RUN      ] Encryption/NetSocketTest.EOFAfterRecv/0
I0117 15:18:35.998528 27596 openssl.cpp:419] CA file path is unspecified! NOTE: Set CA file path with LIBPROCESS_SSL_CA_FILE=<filepath>
I0117 15:18:35.998559 27596 openssl.cpp:424] CA directory path unspecified! NOTE: Set CA directory path with LIBPROCESS_SSL_CA_DIR=<dirpath>
I0117 15:18:35.998566 27596 openssl.cpp:429] Will not verify peer certificate!
NOTE: Set LIBPROCESS_SSL_VERIFY_CERT=1 to enable peer certificate verification
I0117 15:18:35.998572 27596 openssl.cpp:435] Will only verify peer certificate if presented!
NOTE: Set LIBPROCESS_SSL_REQUIRE_CERT=1 to require peer certificate verification
I0117 15:18:36.010643 27596 process.cpp:1234] libprocess is initialized on 172.17.0.3:47429 with 16 worker threads
[       OK ] Encryption/NetSocketTest.EOFAfterRecv/0 (664 ms)
[ RUN      ] Encryption/NetSocketTest.EOFAfterRecv/1
I0117 15:18:36.079453 27596 process.cpp:1234] libprocess is initialized on 172.17.0.3:38149 with 16 worker threads
[       OK ] Encryption/NetSocketTest.EOFAfterRecv/1 (19 ms)
*** Aborted at 1484666316 (unix time) try ""date -d @1484666316"" if you are using GNU date ***
PC: @     0x7f7643ad7c56 __memcpy_ssse3_back
*** SIGSEGV (@0x57c10f8) received by PID 27596 (TID 0x7f76393c2700) from PID 92016888; stack trace: ***
    @     0x7f7644ba0370 (unknown)
    @     0x7f7643ad7c56 __memcpy_ssse3_back
    @     0x7f76443248e0 (unknown)
    @     0x7f7644324f8c (unknown)
    @           0x422a4d process::UPID::UPID()
I0117 15:18:36.090376 27596 process.cpp:1234] libprocess is initialized on 172.17.0.3:43835 with 16 worker threads
[----------] 4 tests from Encryption/NetSocketTest (1116 ms total)

[----------] 6 tests from SSLVerifyIPAdd/SSLTest
[ RUN      ] SSLVerifyIPAdd/SSLTest.BasicSameProcess/0
    @           0x8ae4a8 process::DispatchEvent::DispatchEvent()
    @           0x8a6a5e process::internal::dispatch()
    @           0x8c0b44 process::dispatch<>()
    @           0x8a598a process::ProcessBase::route()
    @           0x98be53 process::ProcessBase::route<>()
    @           0x988096 process::Help::initialize()
    @           0x89ef2a process::ProcessManager::resume()
    @           0x89b976 _ZZN7process14ProcessManager12init_threadsEvENKUt_clEv
    @           0x8adb3c _ZNSt12_Bind_simpleIFZN7process14ProcessManager12init_threadsEvEUt_vEE9_M_invokeIIEEEvSt12_Index_tupleIIXspT_EEE
    @           0x8ada80 _ZNSt12_Bind_simpleIFZN7process14ProcessManager12init_threadsEvEUt_vEEclEv
    @           0x8ada0a _ZNSt6thread5_ImplISt12_Bind_simpleIFZN7process14ProcessManager12init_threadsEvEUt_vEEE6_M_runEv
    @     0x7f764431b230 (unknown)
    @     0x7f7644b98dc5 start_thread
    @     0x7f7643a8473d __clone
make[7]: *** [check-local] Segmentation fault
{code}",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Bug,SlaveRecoveryTest/0.RegisterDisconnectedSlave test is flaky,"Observed this on ASF CI:

{code}
[ RUN      ] SlaveRecoveryTest/0.RegisterDisconnectedSlave
I0111 18:52:55.691341 27071 cluster.cpp:160] Creating default 'local' authorizer
I0111 18:52:55.693492 27076 master.cpp:383] Master f193ab26-63f6-4284-9de0-cd89bb73adb4 (b09aa0a37318) started on 172.17.0.3:42061
I0111 18:52:55.693522 27076 master.cpp:385] Flags at startup: --acls="""" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""1secs"" --allocat
or=""HierarchicalDRF"" --authenticate_agents=""false"" --authenticate_frameworks=""true"" --authenticate_http_frameworks=""true"" --authenticate_http_readonly=""true"" --authenticate_http
_readwrite=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/e83N9C/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --htt
p_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_ag
ent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""in_memory"" --r
egistry_fetch_timeout=""1mins"" --registry_gc_interval=""15mins"" --registry_max_agent_age=""2weeks"" --registry_max_agent_count=""102400"" --registry_store_timeout=""100secs"" --registry
_strict=""false"" --root_submissions=""true"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/e83N9C/master"" --zk_session_timeout=""
10secs""
I0111 18:52:55.693800 27076 master.cpp:435] Master only allowing authenticated frameworks to register
I0111 18:52:55.693811 27076 master.cpp:451] Master allowing unauthenticated agents to register
I0111 18:52:55.693819 27076 master.cpp:462] Master only allowing authenticated HTTP frameworks to register
I0111 18:52:55.693825 27076 credentials.hpp:37] Loading credentials for authentication from '/tmp/e83N9C/credentials'
I0111 18:52:55.693991 27076 master.cpp:507] Using default 'crammd5' authenticator
I0111 18:52:55.694067 27076 http.cpp:922] Using default 'basic' HTTP authenticator for realm 'mesos-master-readonly'
I0111 18:52:55.694119 27076 http.cpp:922] Using default 'basic' HTTP authenticator for realm 'mesos-master-readwrite'
I0111 18:52:55.694142 27076 http.cpp:922] Using default 'basic' HTTP authenticator for realm 'mesos-master-scheduler'
I0111 18:52:55.694156 27076 master.cpp:587] Authorization enabled
I0111 18:52:55.694834 27083 hierarchical.cpp:149] Initialized hierarchical allocator process
I0111 18:52:55.694885 27083 whitelist_watcher.cpp:77] No whitelist given
I0111 18:52:55.695083 27076 master.cpp:2119] Elected as the leading master!
I0111 18:52:55.695096 27076 master.cpp:1641] Recovering from registrar
I0111 18:52:55.695166 27076 registrar.cpp:329] Recovering registrar
I0111 18:52:55.695739 27076 registrar.cpp:362] Successfully fetched the registry (0B) in 551936ns
I0111 18:52:55.695775 27076 registrar.cpp:461] Applied 1 operations in 4972ns; attempting to update the registry
I0111 18:52:55.696069 27076 registrar.cpp:506] Successfully updated the registry in 273920ns
I0111 18:52:55.696121 27076 registrar.cpp:392] Successfully recovered registrar
I0111 18:52:55.696553 27079 hierarchical.cpp:176] Skipping recovery of hierarchical allocator: nothing to recover
I0111 18:52:55.696570 27073 master.cpp:1757] Recovered 0 agents from the registry (129B); allowing 10mins for agents to re-register
I0111 18:52:55.698655 27071 containerizer.cpp:220] Using isolation: posix/cpu,posix/mem,filesystem/posix,network/cni
W0111 18:52:55.699059 27071 backend.cpp:76] Failed to create 'aufs' backend: AufsBackend requires root privileges, but is running as user mesos
W0111 18:52:55.699148 27071 backend.cpp:76] Failed to create 'bind' backend: BindBackend requires root privileges
I0111 18:52:55.701257 27071 cluster.cpp:446] Creating default 'local' authorizer
I0111 18:52:55.702049 27077 slave.cpp:209] Mesos agent started on (485)@172.17.0.3:42061
I0111 18:52:55.702069 27077 slave.cpp:210] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http_readonly=""true"" --authenticate_http_readwrite=""true"" --authenticatee=""crammd5"" --authentication_backoff_factor=""1secs"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/SlaveRecoveryTest_0_RegisterDisconnectedSlave_IX53a9/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/SlaveRecoveryTest_0_RegisterDisconnectedSlave_IX53a9/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_command_executor=""false"" --http_credentials=""/tmp/SlaveRecoveryTest_0_RegisterDisconnectedSlave_IX53a9/http_credentials"" --http_heartbeat_interval=""30secs"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher=""posix"" --launcher_dir=""/mesos/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_executors_per_framework=""150"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --runtime_dir=""/tmp/SlaveRecoveryTest_0_RegisterDisconnectedSlave_IX53a9"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/SlaveRecoveryTest_0_RegisterDisconnectedSlave_XwgqaT""
I0111 18:52:55.702533 27077 credentials.hpp:86] Loading credential for authentication from '/tmp/SlaveRecoveryTest_0_RegisterDisconnectedSlave_IX53a9/credential'
I0111 18:52:55.702687 27077 slave.cpp:352] Agent using credential for: test-principal
I0111 18:52:55.702721 27077 credentials.hpp:37] Loading credentials for authentication from '/tmp/SlaveRecoveryTest_0_RegisterDisconnectedSlave_IX53a9/http_credentials'
I0111 18:52:55.702865 27077 http.cpp:922] Using default 'basic' HTTP authenticator for realm 'mesos-agent-readonly'
I0111 18:52:55.703058 27077 http.cpp:922] Using default 'basic' HTTP authenticator for realm 'mesos-agent-readwrite'
I0111 18:52:55.703541 27077 slave.cpp:539] Agent resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0111 18:52:55.703575 27077 slave.cpp:547] Agent attributes: [  ]
I0111 18:52:55.703583 27077 slave.cpp:552] Agent hostname: b09aa0a37318
I0111 18:52:55.704030 27086 state.cpp:60] Recovering state from '/tmp/SlaveRecoveryTest_0_RegisterDisconnectedSlave_XwgqaT/meta'
I0111 18:52:55.704193 27086 status_update_manager.cpp:203] Recovering status update manager
I0111 18:52:55.704581 27086 containerizer.cpp:599] Recovering containerizer
I0111 18:52:55.705227 27080 provisioner.cpp:251] Provisioner recovery complete
I0111 18:52:55.705379 27086 slave.cpp:5421] Finished recovery
I0111 18:52:55.705912 27086 slave.cpp:5595] Querying resource estimator for oversubscribable resources
I0111 18:52:55.706006 27086 slave.cpp:5609] Received oversubscribable resources {} from the resource estimator
I0111 18:52:55.706210 27081 status_update_manager.cpp:177] Pausing sending status updates
I0111 18:52:55.706233 27078 slave.cpp:924] New master detected at master@172.17.0.3:42061
I0111 18:52:55.706285 27078 slave.cpp:959] Detecting new master
I0111 18:52:55.715513 27078 slave.cpp:986] Authenticating with master master@172.17.0.3:42061
I0111 18:52:55.715556 27078 slave.cpp:997] Using default CRAM-MD5 authenticatee
I0111 18:52:55.715667 27077 authenticatee.cpp:121] Creating new client SASL connection
I0111 18:52:55.715894 27081 master.cpp:6835] Authenticating slave(485)@172.17.0.3:42061
I0111 18:52:55.715944 27077 authenticator.cpp:414] Starting authentication session for crammd5-authenticatee(986)@172.17.0.3:42061
I0111 18:52:55.716037 27075 authenticator.cpp:98] Creating new server SASL connection
I0111 18:52:55.716234 27080 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
I0111 18:52:55.716277 27080 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
I0111 18:52:55.716333 27080 authenticator.cpp:204] Received SASL authentication start
I0111 18:52:55.716477 27080 authenticator.cpp:326] Authentication requires more steps
I0111 18:52:55.716605 27080 authenticatee.cpp:259] Received SASL authentication step
I0111 18:52:55.716790 27080 authenticator.cpp:232] Received SASL authentication step
I0111 18:52:55.716907 27080 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'b09aa0a37318' server FQDN: 'b09aa0a37318' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0111 18:52:55.717041 27080 auxprop.cpp:181] Looking up auxiliary property '*userPassword'
I0111 18:52:55.717185 27080 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0111 18:52:55.717207 27080 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'b09aa0a37318' server FQDN: 'b09aa0a37318' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0111 18:52:55.717247 27080 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0111 18:52:55.717257 27080 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0111 18:52:55.717296 27080 authenticator.cpp:318] Authentication success
I0111 18:52:55.717373 27080 authenticatee.cpp:299] Authentication success
I0111 18:52:55.717404 27086 master.cpp:6865] Successfully authenticated principal 'test-principal' at slave(485)@172.17.0.3:42061
I0111 18:52:55.717432 27081 authenticator.cpp:432] Authentication session cleanup for crammd5-authenticatee(986)@172.17.0.3:42061
I0111 18:52:55.717664 27080 slave.cpp:1081] Successfully authenticated with master master@172.17.0.3:42061
I0111 18:52:55.717756 27080 slave.cpp:1503] Will retry registration in 17.356946ms if necessary
I0111 18:52:55.717931 27081 master.cpp:5234] Registering agent at slave(485)@172.17.0.3:42061 (b09aa0a37318) with id f193ab26-63f6-4284-9de0-cd89bb73adb4-S0
I0111 18:52:55.718101 27077 registrar.cpp:461] Applied 1 operations in 20803ns; attempting to update the registry
I0111 18:52:55.718472 27076 registrar.cpp:506] Successfully updated the registry in 338176ns
I0111 18:52:55.719049 27081 slave.cpp:4285] Received ping from slave-observer(462)@172.17.0.3:42061
I0111 18:52:55.719120 27081 slave.cpp:1127] Registered with master master@172.17.0.3:42061; given agent ID f193ab26-63f6-4284-9de0-cd89bb73adb4-S0
I0111 18:52:55.719139 27081 fetcher.cpp:90] Clearing fetcher cache
I0111 18:52:55.719128 27084 master.cpp:5305] Registered agent f193ab26-63f6-4284-9de0-cd89bb73adb4-S0 at slave(485)@172.17.0.3:42061 (b09aa0a37318) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0111 18:52:55.719342 27084 hierarchical.cpp:491] Added agent f193ab26-63f6-4284-9de0-cd89bb73adb4-S0 (b09aa0a37318) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: {})
I0111 18:52:55.719408 27084 hierarchical.cpp:1690] No allocations performed
I0111 18:52:55.719430 27084 hierarchical.cpp:1315] Performed allocation for agent f193ab26-63f6-4284-9de0-cd89bb73adb4-S0 in 61136ns
I0111 18:52:55.719466 27081 slave.cpp:1155] Checkpointing SlaveInfo to '/tmp/SlaveRecoveryTest_0_RegisterDisconnectedSlave_XwgqaT/meta/slaves/f193ab26-63f6-4284-9de0-cd89bb73adb4-S0/slave.info'
I0111 18:52:55.719480 27084 status_update_manager.cpp:184] Resuming sending status updates
I0111 18:52:55.719810 27081 slave.cpp:1193] Forwarding total oversubscribed resources {}
I0111 18:52:55.719862 27081 master.cpp:5712] Received update of agent f193ab26-63f6-4284-9de0-cd89bb73adb4-S0 at slave(485)@172.17.0.3:42061 (b09aa0a37318) with total oversubscribed resources {}
I0111 18:52:55.720095 27087 hierarchical.cpp:561] Agent f193ab26-63f6-4284-9de0-cd89bb73adb4-S0 (b09aa0a37318) updated with oversubscribed resources {} (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: {})
I0111 18:52:55.720407 27087 hierarchical.cpp:1690] No allocations performed
I0111 18:52:55.720433 27087 hierarchical.cpp:1315] Performed allocation for agent f193ab26-63f6-4284-9de0-cd89bb73adb4-S0 in 161873ns
I0111 18:52:55.721096 27071 sched.cpp:232] Version: 1.2.0
I0111 18:52:55.721359 27079 sched.cpp:336] New master detected at master@172.17.0.3:42061
I0111 18:52:55.721395 27079 sched.cpp:407] Authenticating with master master@172.17.0.3:42061
I0111 18:52:55.721410 27079 sched.cpp:414] Using default CRAM-MD5 authenticatee
I0111 18:52:55.721606 27072 authenticatee.cpp:121] Creating new client SASL connection
I0111 18:52:55.721803 27072 master.cpp:6835] Authenticating scheduler-491fec7d-d55e-48e3-9ca3-fce068ce2dce@172.17.0.3:42061
I0111 18:52:55.722048 27072 authenticator.cpp:414] Starting authentication session for crammd5-authenticatee(987)@172.17.0.3:42061
I0111 18:52:55.722317 27072 authenticator.cpp:98] Creating new server SASL connection
I0111 18:52:55.722518 27072 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
I0111 18:52:55.722548 27072 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
I0111 18:52:55.722594 27072 authenticator.cpp:204] Received SASL authentication start
I0111 18:52:55.722645 27072 authenticator.cpp:326] Authentication requires more steps
I0111 18:52:55.722689 27072 authenticatee.cpp:259] Received SASL authentication step
I0111 18:52:55.722745 27072 authenticator.cpp:232] Received SASL authentication step
I0111 18:52:55.722771 27072 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'b09aa0a37318' server FQDN: 'b09aa0a37318' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0111 18:52:55.722784 27072 auxprop.cpp:181] Looking up auxiliary property '*userPassword'
I0111 18:52:55.722797 27072 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0111 18:52:55.722815 27072 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: 'b09aa0a37318' server FQDN: 'b09aa0a37318' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0111 18:52:55.722826 27072 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0111 18:52:55.722832 27072 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0111 18:52:55.722847 27072 authenticator.cpp:318] Authentication success
I0111 18:52:55.722895 27072 authenticatee.cpp:299] Authentication success
I0111 18:52:55.722937 27072 master.cpp:6865] Successfully authenticated principal 'test-principal' at scheduler-491fec7d-d55e-48e3-9ca3-fce068ce2dce@172.17.0.3:42061
I0111 18:52:55.722971 27072 authenticator.cpp:432] Authentication session cleanup for crammd5-authenticatee(987)@172.17.0.3:42061
I0111 18:52:55.723062 27072 sched.cpp:513] Successfully authenticated with master master@172.17.0.3:42061
I0111 18:52:55.723081 27072 sched.cpp:836] Sending SUBSCRIBE call to master@172.17.0.3:42061
I0111 18:52:55.723125 27072 sched.cpp:869] Will retry registration in 1.630125153secs if necessary
I0111 18:52:55.723235 27072 master.cpp:2707] Received SUBSCRIBE call for framework 'default' at scheduler-491fec7d-d55e-48e3-9ca3-fce068ce2dce@172.17.0.3:42061
I0111 18:52:55.723264 27072 master.cpp:2155] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0111 18:52:55.723398 27072 master.cpp:2783] Subscribing framework default with checkpointing enabled and capabilities [  ]
I0111 18:52:55.723603 27072 hierarchical.cpp:277] Added framework f193ab26-63f6-4284-9de0-cd89bb73adb4-0000
I0111 18:52:55.723865 27072 hierarchical.cpp:1785] No inverse offers to send out!
I0111 18:52:55.723897 27072 hierarchical.cpp:1292] Performed allocation for 1 agents in 273576ns
I0111 18:52:55.724014 27072 sched.cpp:759] Framework registered with f193ab26-63f6-4284-9de0-cd89bb73adb4-0000
I0111 18:52:55.724050 27072 sched.cpp:773] Scheduler::registered took 15750ns
I0111 18:52:55.724215 27072 master.cpp:6664] Sending 1 offers to framework f193ab26-63f6-4284-9de0-cd89bb73adb4-0000 (default) at scheduler-491fec7d-d55e-48e3-9ca3-fce068ce2dce@172.17.0.3:42061
I0111 18:52:55.724386 27072 sched.cpp:933] Scheduler::resourceOffers took 36299ns
I0111 18:52:55.725075 27085 master.cpp:3662] Processing ACCEPT call for offers: [ f193ab26-63f6-4284-9de0-cd89bb73adb4-O0 ] on agent f193ab26-63f6-4284-9de0-cd89bb73adb4-S0 at slave(485)@172.17.0.3:42061 (b09aa0a37318) for framework f193ab26-63f6-4284-9de0-cd89bb73adb4-0000 (default) at scheduler-491fec7d-d55e-48e3-9ca3-fce068ce2dce@172.17.0.3:42061
I0111 18:52:55.725113 27085 master.cpp:3249] Authorizing framework principal 'test-principal' to launch task b969d109-e0fb-4308-9290-92b22ae8c4b7
I0111 18:52:55.725637 27085 master.cpp:8581] Adding task b969d109-e0fb-4308-9290-92b22ae8c4b7 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on agent f193ab26-63f6-4284-9de0-cd89bb73adb4-S0 at slave(485)@172.17.0.3:42061 (b09aa0a37318)
I0111 18:52:55.725724 27085 master.cpp:4313] Launching task b969d109-e0fb-4308-9290-92b22ae8c4b7 of framework f193ab26-63f6-4284-9de0-cd89bb73adb4-0000 (default) at scheduler-491fec7d-d55e-48e3-9ca3-fce068ce2dce@172.17.0.3:42061 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on agent f193ab26-63f6-4284-9de0-cd89bb73adb4-S0 at slave(485)@172.17.0.3:42061 (b09aa0a37318)
I0111 18:52:55.726084 27085 slave.cpp:1571] Got assigned task 'b969d109-e0fb-4308-9290-92b22ae8c4b7' for framework f193ab26-63f6-4284-9de0-cd89bb73adb4-0000
I0111 18:52:55.726239 27085 slave.cpp:6262] Checkpointing FrameworkInfo to '/tmp/SlaveRecoveryTest_0_RegisterDisconnectedSlave_XwgqaT/meta/slaves/f193ab26-63f6-4284-9de0-cd89bb73adb4-S0/frameworks/f193ab26-63f6-4284-9de0-cd89bb73adb4-0000/framework.info'
I0111 18:52:55.726835 27085 slave.cpp:6273] Checkpointing framework pid 'scheduler-491fec7d-d55e-48e3-9ca3-fce068ce2dce@172.17.0.3:42061' to '/tmp/SlaveRecoveryTest_0_RegisterDisconnectedSlave_XwgqaT/meta/slaves/f193ab26-63f6-4284-9de0-cd89bb73adb4-S0/frameworks/f193ab26-63f6-4284-9de0-cd89bb73adb4-0000/framework.pid'
I0111 18:52:55.727361 27085 slave.cpp:1731] Launching task 'b969d109-e0fb-4308-9290-92b22ae8c4b7' for framework f193ab26-63f6-4284-9de0-cd89bb73adb4-0000
I0111 18:52:55.727908 27085 paths.cpp:530] Trying to chown '/tmp/SlaveRecoveryTest_0_RegisterDisconnectedSlave_XwgqaT/slaves/f193ab26-63f6-4284-9de0-cd89bb73adb4-S0/frameworks/f193ab26-63f6-4284-9de0-cd89bb73adb4-0000/executors/b969d109-e0fb-4308-9290-92b22ae8c4b7/runs/5ca261a9-7655-4473-ad23-4638c856c858' to user 'mesos'
I0111 18:52:55.732523 27085 slave.cpp:6739] Checkpointing ExecutorInfo to '/tmp/SlaveRecoveryTest_0_RegisterDisconnectedSlave_XwgqaT/meta/slaves/f193ab26-63f6-4284-9de0-cd89bb73adb4-S0/frameworks/f193ab26-63f6-4284-9de0-cd89bb73adb4-0000/executors/b969d109-e0fb-4308-9290-92b22ae8c4b7/executor.info'
I0111 18:52:55.733202 27085 slave.cpp:6348] Launching executor 'b969d109-e0fb-4308-9290-92b22ae8c4b7' of framework f193ab26-63f6-4284-9de0-cd89bb73adb4-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/tmp/SlaveRecoveryTest_0_RegisterDisconnectedSlave_XwgqaT/slaves/f193ab26-63f6-4284-9de0-cd89bb73adb4-S0/frameworks/f193ab26-63f6-4284-9de0-cd89bb73adb4-0000/executors/b969d109-e0fb-4308-9290-92b22ae8c4b7/runs/5ca261a9-7655-4473-ad23-4638c856c858'
I0111 18:52:55.733749 27077 containerizer.cpp:991] Starting container 5ca261a9-7655-4473-ad23-4638c856c858 for executor 'b969d109-e0fb-4308-9290-92b22ae8c4b7' of framework f193ab26-63f6-4284-9de0-cd89bb73adb4-0000
I0111 18:52:55.733780 27085 slave.cpp:6762] Checkpointing TaskInfo to '/tmp/SlaveRecoveryTest_0_RegisterDisconnectedSlave_XwgqaT/meta/slaves/f193ab26-63f6-4284-9de0-cd89bb73adb4-S0/frameworks/f193ab26-63f6-4284-9de0-cd89bb73adb4-0000/executors/b969d109-e0fb-4308-9290-92b22ae8c4b7/runs/5ca261a9-7655-4473-ad23-4638c856c858/tasks/b969d109-e0fb-4308-9290-92b22ae8c4b7/task.info'
I0111 18:52:55.734252 27085 slave.cpp:2053] Queued task 'b969d109-e0fb-4308-9290-92b22ae8c4b7' for executor 'b969d109-e0fb-4308-9290-92b22ae8c4b7' of framework f193ab26-63f6-4284-9de0-cd89bb73adb4-0000
I0111 18:52:55.734329 27085 slave.cpp:877] Successfully attached file '/tmp/SlaveRecoveryTest_0_RegisterDisconnectedSlave_XwgqaT/slaves/f193ab26-63f6-4284-9de0-cd89bb73adb4-S0/frameworks/f193ab26-63f6-4284-9de0-cd89bb73adb4-0000/executors/b969d109-e0fb-4308-9290-92b22ae8c4b7/runs/5ca261a9-7655-4473-ad23-4638c856c858'
I0111 18:52:55.735308 27081 containerizer.cpp:1540] Launching 'mesos-containerizer' with flags '--help=""false"" --launch_info=""{""command"":{""arguments"":[""mesos-executor"",""--launcher_dir=\/mesos\/build\/src""],""shell"":false,""value"":""\/mesos\/build\/src\/mesos-executor""},""environment"":{""variables"":[{""name"":""LIBPROCESS_PORT"",""value"":""0""},{""name"":""MESOS_AGENT_ENDPOINT"",""value"":""172.17.0.3:42061""},{""name"":""MESOS_CHECKPOINT"",""value"":""1""},{""name"":""MESOS_DIRECTORY"",""value"":""\/tmp\/SlaveRecoveryTest_0_RegisterDisconnectedSlave_XwgqaT\/slaves\/f193ab26-63f6-4284-9de0-cd89bb73adb4-S0\/frameworks\/f193ab26-63f6-4284-9de0-cd89bb73adb4-0000\/executors\/b969d109-e0fb-4308-9290-92b22ae8c4b7\/runs\/5ca261a9-7655-4473-ad23-4638c856c858""},{""name"":""MESOS_EXECUTOR_ID"",""value"":""b969d109-e0fb-4308-9290-92b22ae8c4b7""},{""name"":""MESOS_EXECUTOR_SHUTDOWN_GRACE_PERIOD"",""value"":""5secs""},{""name"":""MESOS_FRAMEWORK_ID"",""value"":""f193ab26-63f6-4284-9de0-cd89bb73adb4-0000""},{""name"":""MESOS_HTTP_COMMAND_EXECUTOR"",""value"":""0""},{""name"":""MESOS_RECOVERY_TIMEOUT"",""value"":""15mins""},{""name"":""MESOS_SLAVE_ID"",""value"":""f193ab26-63f6-4284-9de0-cd89bb73adb4-S0""},{""name"":""MESOS_SLAVE_PID"",""value"":""slave(485)@172.17.0.3:42061""},{""name"":""MESOS_SUBSCRIPTION_BACKOFF_MAX"",""value"":""2secs""},{""name"":""MESOS_SANDBOX"",""value"":""\/tmp\/SlaveRecoveryTest_0_RegisterDisconnectedSlave_XwgqaT\/slaves\/f193ab26-63f6-4284-9de0-cd89bb73adb4-S0\/frameworks\/f193ab26-63f6-4284-9de0-cd89bb73adb4-0000\/executors\/b969d109-e0fb-4308-9290-92b22ae8c4b7\/runs\/5ca261a9-7655-4473-ad23-4638c856c858""}]},""user"":""mesos"",""working_directory"":""\/tmp\/SlaveRecoveryTest_0_RegisterDisconnectedSlave_XwgqaT\/slaves\/f193ab26-63f6-4284-9de0-cd89bb73adb4-S0\/frameworks\/f193ab26-63f6-4284-9de0-cd89bb73adb4-0000\/executors\/b969d109-e0fb-4308-9290-92b22ae8c4b7\/runs\/5ca261a9-7655-4473-ad23-4638c856c858""}"" --pipe_read=""7"" --pipe_write=""8"" --runtime_directory=""/tmp/SlaveRecoveryTest_0_RegisterDisconnectedSlave_IX53a9/containers/5ca261a9-7655-4473-ad23-4638c856c858"" --unshare_namespace_mnt=""false""'
I0111 18:52:55.736647 27081 launcher.cpp:133] Forked child with pid '30914' for container '5ca261a9-7655-4473-ad23-4638c856c858'
I0111 18:52:55.736830 27081 containerizer.cpp:1639] Checkpointing container's forked pid 30914 to '/tmp/SlaveRecoveryTest_0_RegisterDisconnectedSlave_XwgqaT/meta/slaves/f193ab26-63f6-4284-9de0-cd89bb73adb4-S0/frameworks/f193ab26-63f6-4284-9de0-cd89bb73adb4-0000/executors/b969d109-e0fb-4308-9290-92b22ae8c4b7/runs/5ca261a9-7655-4473-ad23-4638c856c858/pids/forked.pid'
I0111 18:52:55.738760 27080 fetcher.cpp:349] Starting to fetch URIs for container: 5ca261a9-7655-4473-ad23-4638c856c858, directory: /tmp/SlaveRecoveryTest_0_RegisterDisconnectedSlave_XwgqaT/slaves/f193ab26-63f6-4284-9de0-cd89bb73adb4-S0/frameworks/f193ab26-63f6-4284-9de0-cd89bb73adb4-0000/executors/b969d109-e0fb-4308-9290-92b22ae8c4b7/runs/5ca261a9-7655-4473-ad23-4638c856c858
I0111 18:52:55.828845 30921 exec.cpp:162] Version: 1.2.0
I0111 18:52:55.833731 27078 slave.cpp:3322] Got registration for executor 'b969d109-e0fb-4308-9290-92b22ae8c4b7' of framework f193ab26-63f6-4284-9de0-cd89bb73adb4-0000 from executor(1)@172.17.0.3:44874
I0111 18:52:55.833977 27078 slave.cpp:3408] Checkpointing executor pid 'executor(1)@172.17.0.3:44874' to '/tmp/SlaveRecoveryTest_0_RegisterDisconnectedSlave_XwgqaT/meta/slaves/f193ab26-63f6-4284-9de0-cd89bb73adb4-S0/frameworks/f193ab26-63f6-4284-9de0-cd89bb73adb4-0000/executors/b969d109-e0fb-4308-9290-92b22ae8c4b7/runs/5ca261a9-7655-4473-ad23-4638c856c858/pids/libprocess.pid'
I0111 18:52:55.835109 30923 exec.cpp:237] Executor registered on agent f193ab26-63f6-4284-9de0-cd89bb73adb4-S0
I0111 18:52:55.835211 27078 slave.cpp:2267] Sending queued task 'b969d109-e0fb-4308-9290-92b22ae8c4b7' to executor 'b969d109-e0fb-4308-9290-92b22ae8c4b7' of framework f193ab26-63f6-4284-9de0-cd89bb73adb4-0000 at executor(1)@172.17.0.3:44874
Received SUBSCRIBED event
Subscribed executor on b09aa0a37318
Received LAUNCH event
Starting task b969d109-e0fb-4308-9290-92b22ae8c4b7
/mesos/build/src/mesos-containerizer launch --help=""false"" --launch_info=""{""command"":{""shell"":true,""value"":""sleep 1000""}}"" --unshare_namespace_mnt=""false""
Forked command at 30933
I0111 18:52:55.842718 27082 slave.cpp:3754] Handling status update TASK_RUNNING (UUID: 6fbee882-8439-4c01-91b5-49b7955349f8) for task b969d109-e0fb-4308-9290-92b22ae8c4b7 of framework f193ab26-63f6-4284-9de0-cd89bb73adb4-0000 from executor(1)@172.17.0.3:44874
I0111 18:52:55.843504 27073 status_update_manager.cpp:323] Received status update TASK_RUNNING (UUID: 6fbee882-8439-4c01-91b5-49b7955349f8) for task b969d109-e0fb-4308-9290-92b22ae8c4b7 of framework f193ab26-63f6-4284-9de0-cd89bb73adb4-0000
I0111 18:52:55.843534 27073 status_update_manager.cpp:500] Creating StatusUpdate stream for task b969d109-e0fb-4308-9290-92b22ae8c4b7 of framework f193ab26-63f6-4284-9de0-cd89bb73adb4-0000
I0111 18:52:55.843977 27073 status_update_manager.cpp:832] Checkpointing UPDATE for status update TASK_RUNNING (UUID: 6fbee882-8439-4c01-91b5-49b7955349f8) for task b969d109-e0fb-4308-9290-92b22ae8c4b7 of framework f193ab26-63f6-4284-9de0-cd89bb73adb4-0000
I0111 18:52:55.844115 27073 status_update_manager.cpp:377] Forwarding update TASK_RUNNING (UUID: 6fbee882-8439-4c01-91b5-49b7955349f8) for task b969d109-e0fb-4308-9290-92b22ae8c4b7 of framework f193ab26-63f6-4284-9de0-cd89bb73adb4-0000 to the agent
I0111 18:52:55.844358 27073 slave.cpp:4195] Forwarding the update TASK_RUNNING (UUID: 6fbee882-8439-4c01-91b5-49b7955349f8) for task b969d109-e0fb-4308-9290-92b22ae8c4b7 of framework f193ab26-63f6-4284-9de0-cd89bb73adb4-0000 to master@172.17.0.3:42061
I0111 18:52:55.844611 27073 slave.cpp:4089] Status update manager successfully handled status update TASK_RUNNING (UUID: 6fbee882-8439-4c01-91b5-49b7955349f8) for task b969d109-e0fb-4308-9290-92b22ae8c4b7 of framework f193ab26-63f6-4284-9de0-cd89bb73adb4-0000
I0111 18:52:55.844825 27073 slave.cpp:4105] Sending acknowledgement for status update TASK_RUNNING (UUID: 6fbee882-8439-4c01-91b5-49b7955349f8) for task b969d109-e0fb-4308-9290-92b22ae8c4b7 of framework f193ab26-63f6-4284-9de0-cd89bb73adb4-0000 to executor(1)@172.17.0.3:44874
I0111 18:52:55.844782 27086 master.cpp:5848] Status update TASK_RUNNING (UUID: 6fbee882-8439-4c01-91b5-49b7955349f8) for task b969d109-e0fb-4308-9290-92b22ae8c4b7 of framework f193ab26-63f6-4284-9de0-cd89bb73adb4-0000 from agent f193ab26-63f6-4284-9de0-cd89bb73adb4-S0 at slave(485)@172.17.0.3:42061 (b09aa0a37318)
I0111 18:52:55.844988 27086 master.cpp:5910] Forwarding status update TASK_RUNNING (UUID: 6fbee882-8439-4c01-91b5-49b7955349f8) for task b969d109-e0fb-4308-9290-92b22ae8c4b7 of framework f193ab26-63f6-4284-9de0-cd89bb73adb4-0000
I0111 18:52:55.845077 27086 master.cpp:7953] Updating the state of task b969d109-e0fb-4308-9290-92b22ae8c4b7 of framework f193ab26-63f6-4284-9de0-cd89bb73adb4-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I0111 18:52:55.845196 27086 sched.cpp:1041] Scheduler::statusUpdate took 24432ns
I0111 18:52:55.845324 27086 master.cpp:4950] Processing ACKNOWLEDGE call 6fbee882-8439-4c01-91b5-49b7955349f8 for task b969d109-e0fb-4308-9290-92b22ae8c4b7 of framework f193ab26-63f6-4284-9de0-cd89bb73adb4-0000 (default) at scheduler-491fec7d-d55e-48e3-9ca3-fce068ce2dce@172.17.0.3:42061 on agent f193ab26-63f6-4284-9de0-cd89bb73adb4-S0
I0111 18:52:55.845535 27086 status_update_manager.cpp:395] Received status update acknowledgement (UUID: 6fbee882-8439-4c01-91b5-49b7955349f8) for task b969d109-e0fb-4308-9290-92b22ae8c4b7 of framework f193ab26-63f6-4284-9de0-cd89bb73adb4-0000
I0111 18:52:55.845602 27086 status_update_manager.cpp:832] Checkpointing ACK for status update TASK_RUNNING (UUID: 6fbee882-8439-4c01-91b5-49b7955349f8) for task b969d109-e0fb-4308-9290-92b22ae8c4b7 of framework f193ab26-63f6-4284-9de0-cd89bb73adb4-0000
I0111 18:52:55.845835 27086 slave.cpp:3042] Status update manager successfully handled status update acknowledgement (UUID: 6fbee882-8439-4c01-91b5-49b7955349f8) for task b969d109-e0fb-4308-9290-92b22ae8c4b7 of framework f193ab26-63f6-4284-9de0-cd89bb73adb4-0000
I0111 18:52:55.846096 27075 containerizer.cpp:2118] Destroying container 5ca261a9-7655-4473-ad23-4638c856c858 in RUNNING state
I0111 18:52:55.846189 27075 launcher.cpp:149] Asked to destroy container 5ca261a9-7655-4473-ad23-4638c856c858
I0111 18:52:55.853996 27075 slave.cpp:4327] Got exited event for executor(1)@172.17.0.3:44874
I0111 18:52:55.882633 27079 containerizer.cpp:2481] Container 5ca261a9-7655-4473-ad23-4638c856c858 has exited
I0111 18:52:55.883587 27079 provisioner.cpp:322] Ignoring destroy request for unknown container 5ca261a9-7655-4473-ad23-4638c856c858
I0111 18:52:55.884263 27080 slave.cpp:4690] Executor 'b969d109-e0fb-4308-9290-92b22ae8c4b7' of framework f193ab26-63f6-4284-9de0-cd89bb73adb4-0000 terminated with signal Killed
I0111 18:52:55.884340 27080 slave.cpp:3754] Handling status update TASK_FAILED (UUID: a641ba05-c0cc-4d51-9bc7-5d8dae71afce) for task b969d109-e0fb-4308-9290-92b22ae8c4b7 of framework f193ab26-63f6-4284-9de0-cd89bb73adb4-0000 from @0.0.0.0:0
W0111 18:52:55.884745 27083 containerizer.cpp:1933] Ignoring update for unknown container 5ca261a9-7655-4473-ad23-4638c856c858
I0111 18:52:55.885037 27074 status_update_manager.cpp:323] Received status update TASK_FAILED (UUID: a641ba05-c0cc-4d51-9bc7-5d8dae71afce) for task b969d109-e0fb-4308-9290-92b22ae8c4b7 of framework f193ab26-63f6-4284-9de0-cd89bb73adb4-0000
I0111 18:52:55.885092 27074 status_update_manager.cpp:832] Checkpointing UPDATE for status update TASK_FAILED (UUID: a641ba05-c0cc-4d51-9bc7-5d8dae71afce) for task b969d109-e0fb-4308-9290-92b22ae8c4b7 of framework f193ab26-63f6-4284-9de0-cd89bb73adb4-0000
I0111 18:52:55.885257 27074 status_update_manager.cpp:377] Forwarding update TASK_FAILED (UUID: a641ba05-c0cc-4d51-9bc7-5d8dae71afce) for task b969d109-e0fb-4308-9290-92b22ae8c4b7 of framework f193ab26-63f6-4284-9de0-cd89bb73adb4-0000 to the agent
I0111 18:52:55.885406 27082 slave.cpp:4195] Forwarding the update TASK_FAILED (UUID: a641ba05-c0cc-4d51-9bc7-5d8dae71afce) for task b969d109-e0fb-4308-9290-92b22ae8c4b7 of framework f193ab26-63f6-4284-9de0-cd89bb73adb4-0000 to master@172.17.0.3:42061
I0111 18:52:55.885622 27082 slave.cpp:796] Agent terminating
I0111 18:52:55.885812 27074 master.cpp:5848] Status update TASK_FAILED (UUID: a641ba05-c0cc-4d51-9bc7-5d8dae71afce) for task b969d109-e0fb-4308-9290-92b22ae8c4b7 of framework f193ab26-63f6-4284-9de0-cd89bb73adb4-0000 from agent f193ab26-63f6-4284-9de0-cd89bb73adb4-S0 at slave(485)@172.17.0.3:42061 (b09aa0a37318)
I0111 18:52:55.885972 27074 master.cpp:5910] Forwarding status update TASK_FAILED (UUID: a641ba05-c0cc-4d51-9bc7-5d8dae71afce) for task b969d109-e0fb-4308-9290-92b22ae8c4b7 of framework f193ab26-63f6-4284-9de0-cd89bb73adb4-0000
I0111 18:52:55.886055 27074 master.cpp:7953] Updating the state of task b969d109-e0fb-4308-9290-92b22ae8c4b7 of framework f193ab26-63f6-4284-9de0-cd89bb73adb4-0000 (latest state: TASK_FAILED, status update state: TASK_FAILED)
/mesos/src/t",2.0,0,0.5,1.0,0.0,0.0,0.0,0.0,0.8,0.5438596491228069,0.39999999999999997,0.8846153846153846,0.9878787878787878,0.9878787878787878,0.0
Bug,Zero health check timeout is interpreted literally.,"Currently zero health check timeout is interpreted literally, which is not very helpful since a health check does not even get a chance to finish. We suggest to fixe this behaviour by interpreting zero as {{Duration::max()}} effectively rendering the timeout infinite.",1.0,"1.0.2,1.1.0",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.717948717948718,0.8242424242424242,0.7212121212121212,0.9221100917431193
Task,Add authorization tests for debug API handlers,Should test authz of all 3 debug calls.,3.0,0,0.5,0.004524886877828055,0.0,0.0,0.0,0.0,0.2,0.12280701754385964,0.24285714285714285,0.8846153846153846,0.9878787878787878,0.9878787878787878,0.0
Bug,Transition Windows away from `os::killtree`.,"Windows does not have as robust a notion of a process hierarchy as Unix, and thus functions like `os::killtree` will always have critical limitations and semantic mismatches between Unix and Windows.

We should transition away from this function when we can, and replace it with something similar to how we kill a cgroup.",3.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.8,0.19298245614035087,0.09999999999999999,0.14102564102564102,0.1515151515151515,0.16969696969696968,0.0
Bug,FaultToleranceTest.FrameworkReregister is flaky,"Observed on internal CI:

{noformat}
[21:27:38] :     [Step 11/11] /mnt/teamcity/work/4240ba9ddd0997c3/src/tests/fault_tolerance_tests.cpp:892: Failure
[21:27:38] :     [Step 11/11] Value of: framework.values[""registered_time""].as().as()
[21:27:38] :     [Step 11/11]   Actual: 1482442093
[21:27:38] :     [Step 11/11] Expected: static_cast(registerTime.secs())
[21:27:38] :     [Step 11/11] Which is: 1482442094
{noformat}

Looks like another instance of MESOS-4695.",3.0,0,0.0,1.0,0.0,0.0,0.0,0.0,0.8,0.5438596491228069,0.39999999999999997,0.48717948717948717,0.4303030303030303,0.4303030303030303,0.0
Bug,OsTest.User fails on recent Arch Linux.,"{noformat}
[ RUN      ] OsTest.User
../../../mesos/3rdparty/stout/tests/os_tests.cpp:683: Failure
Value of: os::getuid(UUID::random().toString()).isNone()
  Actual: false
Expected: true
../../../mesos/3rdparty/stout/tests/os_tests.cpp:684: Failure
Value of: os::getgid(UUID::random().toString()).isNone()
  Actual: false
Expected: true
[  FAILED  ] OsTest.User (12 ms)
{noformat}

Appeared relatively recently (last two weeks). Cause appears to be that {{getpwnam\_r}} now returns {{EINVAL}} for an invalid input, which {{os::getuid()}} and {{os::getgid()}} are not prepared to handle.",3.0,0,0.0,1.0,0.0,0.0,0.0,0.0,0.8,0.5438596491228069,0.39999999999999997,0.48717948717948717,0.4303030303030303,0.4303030303030303,0.0
Documentation,"Update the addition, deletion and modification logic of CNI configuration files.",We need update the CNI documentation to highlight that we can add/delete and modify CNI networks on the fly without the need for agent restart.,1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2807017543859649,0.39285714285714285,0.1282051282051282,0.3090909090909091,0.3090909090909091,0.0
Bug,Check unreachable task cache for task ID collisions on launch,"As discussed in MESOS-6785, it is possible to crash the master by launching a task that reuses the ID of an unreachable/partitioned task. A complete solution to this problem will be quite involved, but an incremental improvement is easy: when we see a task launch operation, reject the launch attempt if the task ID collides with an ID in the per-framework {{unreachableTasks}} cache. This doesn't catch all situations in which IDs are reused, but it is better than nothing.",2.0,0,0.0,1.0,0.0,0.0,0.0,0.0,0.8,0.5438596491228069,0.39999999999999997,0.48717948717948717,0.4303030303030303,0.4303030303030303,0.0
Bug,SSL socket can lose bytes in the case of EOF,"During recent work on SSL-enabled tests in libprocess (MESOS-5966), we discovered a bug in {{LibeventSSLSocketImpl}}, wherein the socket can either fail to receive an EOF, or lose data when an EOF is received.

The {{LibeventSSLSocketImpl::event_callback(short events)}} method immediately sets any pending {{RecvRequest}}'s promise to zero upon receipt of an EOF. However, at the time the promise is set, there may actually be data waiting to be read by libevent. Upon receipt of an EOF, we should attempt to read the socket's bufferevent first to ensure that we aren't losing any data previously received by the socket.",3.0,0,0.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Bug,Wrong task started time in webui,"Reported by [~janisz]
{quote}
Hi

When task has enabled Mesos healthcheck start time in UI can show wrong
time. This happens because UI assumes that first status is task started
[0]. This is not always true because Mesos keeps only recent tasks statuses
[1] so when healthcheck updates tasks status it can override task start
time displayed in webui.

Best
Tomek

[0]
https://github.com/apache/mesos/blob/master/src/webui/master/static/js/controllers.js#L140
[1]
https://github.com/apache/mesos/blob/f2adc8a95afda943f6a10e771aad64300da19047/src/common/protobuf_utils.cpp#L263-L265
{quote}",1.0,0,0.5,0.0,0.3333333333333333,0.5,0.14285714285714285,0.0,0.2,0.5087719298245614,0.4357142857142857,0.05128205128205128,0.03636363636363637,0.03636363636363637,0.0
Bug,SSL socket's 'shutdown()' method is broken,"We recently uncovered two issues with the {{LibeventSSLSocketImpl::shutdown}} method:
* The introduction of a shutdown method parameter with [this commit|https://reviews.apache.org/r/54113/] means that the implementation's method is no longer overriding the default implementation. In addition to fixing the implementation method's signature, we should add the {{override}} specifier to all of our socket implementations' methods to ensure that this doesn't happen in the future.
* The {{LibeventSSLSocketImpl::shutdown}} function does not actually shutdown the SSL socket. The proper function to shutdown an SSL socket is {{SSL_shutdown}}, which is called in the implementation's destructor. We should move this into {{shutdown()}} so that by the time that method returns, the socket has actually been shutdown.",2.0,0,0.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Bug,CHECK failure on duplicate task IDs,"The master crashes with a CHECK failure in the following scenario:

# Framework launches task X on agent A1. The framework may or may not be partition-aware; let's assume it is not partition-aware.
# A1 becomes partitioned from the master.
# Framework launches task X on agent A2.
# Master fails over.
# Agents A1 and A2 both re-register with the master. Because the master has failed over, the task on A1 is _not_ terminated (""non-strict registry semantics"").

This results in two running tasks with the same ID, which causes a master {{CHECK}} failure among other badness:

{noformat}
master.hpp:2299] Check failed: !tasks.contains(task->task_id()) Duplicate task b88153a2-571a-41e7-9e9b-c297fef4f3cd of framework eaef1879-8cc9-412f-928d-86c9925a7abb-0000
{noformat}",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.48717948717948717,0.4303030303030303,0.4303030303030303,0.0
Bug,IOSwitchboardTest.KillSwitchboardContainerDestroyed is flaky,"{noformat}
[ RUN      ] IOSwitchboardTest.KillSwitchboardContainerDestroyed
I1212 13:57:02.641043  2211 containerizer.cpp:220] Using isolation: posix/cpu,filesystem/posix,network/cni
W1212 13:57:02.641438  2211 backend.cpp:76] Failed to create 'overlay' backend: OverlayBackend requires root privileges, but is running as user nrc
W1212 13:57:02.641559  2211 backend.cpp:76] Failed to create 'bind' backend: BindBackend requires root privileges
I1212 13:57:02.642822  2268 containerizer.cpp:594] Recovering containerizer
I1212 13:57:02.643975  2253 provisioner.cpp:253] Provisioner recovery complete
I1212 13:57:02.644953  2255 containerizer.cpp:986] Starting container 09e87380-00ab-4987-83c9-fa1c5d86717f for executor 'executor' of framework
I1212 13:57:02.647004  2245 switchboard.cpp:430] Allocated pseudo terminal '/dev/pts/54' for container 09e87380-00ab-4987-83c9-fa1c5d86717f
I1212 13:57:02.652305  2245 switchboard.cpp:596] Created I/O switchboard server (pid: 2705) listening on socket file '/tmp/mesos-io-switchboard-b4af1c92-6633-44f3-9d35-e0e36edaf70a' for container 09e87380-00ab-4987-83c9-fa1c5d86717f
I1212 13:57:02.655513  2267 launcher.cpp:133] Forked child with pid '2706' for container '09e87380-00ab-4987-83c9-fa1c5d86717f'
I1212 13:57:02.655732  2267 containerizer.cpp:1621] Checkpointing container's forked pid 2706 to '/tmp/IOSwitchboardTest_KillSwitchboardContainerDestroyed_Me5CRx/meta/slaves/frameworks/executors/executor/runs/09e87380-00ab-4987-83c9-fa1c5d86717f/pids/forked.pid'
I1212 13:57:02.726306  2265 containerizer.cpp:2463] Container 09e87380-00ab-4987-83c9-fa1c5d86717f has exited
I1212 13:57:02.726352  2265 containerizer.cpp:2100] Destroying container 09e87380-00ab-4987-83c9-fa1c5d86717f in RUNNING state
E1212 13:57:02.726495  2243 switchboard.cpp:861] Unexpected termination of I/O switchboard server: 'IOSwitchboard' exited with signal: Killed for container 09e87380-00ab-4987-83c9-fa1c5d86717f
I1212 13:57:02.726563  2265 launcher.cpp:149] Asked to destroy container 09e87380-00ab-4987-83c9-fa1c5d86717f
E1212 13:57:02.783607  2228 switchboard.cpp:799] Failed to remove unix domain socket file '/tmp/mesos-io-switchboard-b4af1c92-6633-44f3-9d35-e0e36edaf70a' for container '09e87380-00ab-4987-83c9-fa1c5d86717f': No such file or directory
../../mesos/src/tests/containerizer/io_switchboard_tests.cpp:661: Failure
Value of: wait.get()->reasons().size() == 1
  Actual: false
Expected: true
*** Aborted at 1481579822 (unix time) try ""date -d @1481579822"" if you are using GNU date ***
PC: @          0x1bf16d0 testing::UnitTest::AddTestPartResult()
*** SIGSEGV (@0x0) received by PID 2211 (TID 0x7faed7d078c0) from PID 0; stack trace: ***
    @     0x7faecf855100 (unknown)
    @          0x1bf16d0 testing::UnitTest::AddTestPartResult()
    @          0x1be6247 testing::internal::AssertHelper::operator=()
    @          0x19ed751 mesos::internal::tests::IOSwitchboardTest_KillSwitchboardContainerDestroyed_Test::TestBody()
    @          0x1c0ed8c testing::internal::HandleSehExceptionsInMethodIfSupported<>()
    @          0x1c09e74 testing::internal::HandleExceptionsInMethodIfSupported<>()
    @          0x1beb505 testing::Test::Run()
    @          0x1bebc88 testing::TestInfo::Run()
    @          0x1bec2ce testing::TestCase::Run()
    @          0x1bf2ba8 testing::internal::UnitTestImpl::RunAllTests()
    @          0x1c0f9b1 testing::internal::HandleSehExceptionsInMethodIfSupported<>()
    @          0x1c0a9f2 testing::internal::HandleExceptionsInMethodIfSupported<>()
    @          0x1bf18ee testing::UnitTest::Run()
    @          0x11bc9e3 RUN_ALL_TESTS()
    @          0x11bc599 main
    @     0x7faece663b15 __libc_start_main
    @           0xa9c219 (unknown)
{noformat}
",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.48717948717948717,0.4303030303030303,0.4303030303030303,0.0
Improvement,Inherit Environment from parent container when launching DEBUG container.,"Right now whenever we enter a DEBUG container we have a fresh environment. For a better user experience, we should have the DEBUG container inherit the environment set up in its parent container image spec (if there is one). ",3.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.3205128205128205,0.6,0.6,0.0
Bug,The 'http::connect(address)' always uses the DEFAULT_KIND() of socket even if SSL is undesired.,"    The 'http::connect(address)' variant of 'http::connect()' doesn't
    currently support SSL. However, when SSL is enabled, the default for
    all 'Socket::create()' calls is to use the 'DEFAULT_KIND()' of socket
    which is set to SSL. This causes problems with 'connect()' becuuse it
    will create a socket of 'kind' SSL without a way to override it.
",1.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Improvement,Add heartbeats to both input/output connections in IOSwitchboard,"Some networks will kill idle connections if no data is transfered over them within a set amount of time. For example, using AWS's Elastic Load Balancer (ELB), the default time to kill a connection is only 60s! Because of this, we need a way to send application level heartbeats to keep these connections alive for any long running LAUNCH_NESTED_CONTAINER_SESSION, ATTACH_CONTAINER_INPUT and ATTACH_CONTAINER_OUTPUT calls. 

We should serve these heartbeats from the IOSwitchboard server rather than the agent handlers since the agent essentially acts as a proxy and the heartbeats should originate from the actual server being communicated with.",2.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Bug,IOSwitchboard doesn't properly flush data on ATTACH_CONTAINER_OUTPUT,"Currently we are doing a close on the write end of all connection pipes when we exit the switchboard, but we don't wait until the read is flushed before exiting. This can cause some data to get dropped since the process may exit before the reader is flushed.  The current code is:
{noformat}
void IOSwitchboardServerProcess::finalize()                                   
{ 
  foreach (HttpConnection& connection, outputConnections) {                   
    connection.close();                                              
  }                                                                           
  
  if (failure.isSome()) {
    promise.fail(failure->message);                                           
  } else {
    promise.set(Nothing());                                                   
  }                                                                           
} 
{noformat}

We should change it to:
{noformat}
void IOSwitchboardServerProcess::finalize()                                   
{ 
  foreach (HttpConnection& connection, outputConnections) {                   
    connection.close();
    connection.closed().await();                                              
  }                                                                           
  
  if (failure.isSome()) {
    promise.fail(failure->message);                                           
  } else {
    promise.set(Nothing());                                                   
  }                                                                           
} 
{noformat}",1.0,0,0.5,0.5701357466063348,0.0,0.0,0.0,0.0,0.0,0.6842105263157894,0.5642857142857143,0.3205128205128205,0.6,0.6,0.0
Bug,The agent should synchronize with the IOSwitchboard to determine when it is ready to accept incoming connections.,"Currently, the agent has no way of knowing when the IOSwitchboard has started up and is ready to listen for incoming connections. We should add support to synchronize between them so the agent can figure this out.

The implementation should not block the launch path of the container, but rather incoming connections through the IOSwitchboards {{connect()}} call.",2.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Bug,Check that `PreferredToolArchitecture` is set to `x64` on Windows before building,"If this variable is not set before we build, it will cause the linker to occasionally hang forever, due to a MSVC toolchain bug in the linker.

We should make this easy on developers and check for them. If the variable is not set, we should display an error message explaining.",2.0,0,0.0,0.44494720965309204,0.0,0.0,0.0,0.0,0.0,0.14035087719298245,0.15,0.14102564102564102,0.1515151515151515,0.16969696969696968,0.0
Improvement,"Unify ""active"" and ""state""/""connected"" fields in Master::Framework","Rather than tracking whether a framework is ""active"" separately from whether it is ""connected"", we should consider using a single ""state"" variable to track the current state of the framework (connected-and-active, connected-and-inactive, disconnected, etc.)",2.0,0,0.0,1.0,0.0,0.0,0.0,0.0,0.8,0.5438596491228069,0.39999999999999997,0.48717948717948717,0.4303030303030303,0.4303030303030303,0.0
Bug,Should destroy DEBUG containers on agent recovery.,We need to add support to destroy DEBUG containers on agent recovery. Right now these containers will stick around forever (or until they run to completion).,3.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Bug,Port `slave_recovery_tests.cpp`,https://reviews.apache.org/r/65408/,3.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.8,0.19298245614035087,0.09999999999999999,0.14102564102564102,0.1515151515151515,0.16969696969696968,0.0
Bug,Remove of unix domain socket path in IOSwitchboard::cleanup,We currently leak all of the unix domain socket files created by the switchboard in the `/tmp` directory. We need to clean them up properly.,1.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Bug,IOSwitchboard should recover spawned server pid on agent restarts,"We need to do proper recovery of the io switchboard server pid across agent restarts. As of now, if the agent restarts there is now way to recover this pid.",2.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Improvement,Update 'io::redirect()' to take an optional vector of callback hooks.,"These callback hooks should be invoked before passing any data read from
the 'from' file descriptor on to the 'to' file descriptor.",1.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Task,Add some benchmark test for quota allocation,"Comparing to non-quota allocation, current quota allocation involves a separate allocation stage and additional tracking such as headroom and role consumed quota. Thus quota allocation performance could be drastically different (probably slower) than non-quota allocation. A dedicated benchmark for quota allocation is necessary.",3.0,0,0.5,0.0,0.6666666666666666,1.0,0.5714285714285714,0.0,0.0,0.5438596491228069,0.5357142857142857,0.10256410256410256,0.06666666666666667,0.06666666666666667,0.0
Improvement,Support `foreachpair` for LinkedHashMap,{{LinkedHashMap}} does not support iteration via {{foreachpair}}; it should.,3.0,0,0.0,1.0,0.0,0.0,0.0,0.0,0.8,0.5438596491228069,0.39999999999999997,0.48717948717948717,0.4303030303030303,0.4303030303030303,0.0
Bug,SSL downgrade path will CHECK-fail when using both temporary and persistent sockets,"The code path for downgrading sockets from SSL to non-SSL includes this code:
{code}
    // If this address is a temporary link.
    if (temps.count(addresses[to_fd]) > 0) {
      temps[addresses[to_fd]] = to_fd;
      // No need to erase as we're changing the value, not the key.
    }

    // If this address is a persistent link.
    if (persists.count(addresses[to_fd]) > 0) {
      persists[addresses[to_fd]] = to_fd;
      // No need to erase as we're changing the value, not the key.
    }
{code}
https://github.com/apache/mesos/blob/1.1.x/3rdparty/libprocess/src/process.cpp#L2311-L2321

It is possible for libprocess to hold both temporary and persistent sockets to the same address.  This can happen when a message is first sent ({{ProcessBase::send}}), and then a link is established ({{ProcessBase::link}}).  When the target of the message/link is a non-SSL socket, both temporary and persistent sockets go through the downgrade path.

If a temporary socket is present while a persistent socket is being created, the above code will remap both temporary and persistent sockets to the same address (it should only remap the persistent socket).  This leads to some CHECK failures if those sockets are used or closed later:
* {code}
    bool persist = persists.count(address) > 0;
    bool temp = temps.count(address) > 0;
    if (persist || temp) {
      int s = persist ? persists[address] : temps[address];
      CHECK(sockets.count(s) > 0);
socket = sockets.at(s);
{code}
https://github.com/apache/mesos/blob/1.1.x/3rdparty/libprocess/src/process.cpp#L1942
* {code}
        if (dispose.count(s) > 0) {
          // This is either a temporary socket we created or it's a
          // socket that we were receiving data from and possibly
          // sending HTTP responses back on. Clean up either way.
          if (addresses.count(s) > 0) {
            const Address& address = addresses[s];
            CHECK(temps.count(address) > 0 && temps[address] == s);
temps.erase(address);
{code}
https://github.com/apache/mesos/blob/1.1.x/3rdparty/libprocess/src/process.cpp#L2044",3.0,"0.28.2,1.0.2,1.1.0",0.0,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.7004281345565748
Bug,Error: dereferencing type-punned pointer will break strict-aliasing rules.,"Trying to update the mesos package to 1.1.0 in Fedora.  Getting:_x000D_
{noformat}_x000D_
libtool: compile:  g++ -DPACKAGE_NAME=\""mesos\"" -DPACKAGE_TARNAME=\""mesos\"" -DPACKAGE_VERSION=\""1.1.0\"" ""-DPACKAGE_STRING=\""mesos 1.1.0\"""" -DPACKAGE_BUGREPORT=\""\"" -DPACKAGE_URL=\""\"" -DPACKAGE=\""mesos\"" -DVERSION=\""1.1.0\"" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_DLFCN_H=1 -DLT_OBJDIR=\"".libs/\"" -DHAVE_CXX11=1 -DHAVE_PTHREAD_PRIO_INHERIT=1 -DHAVE_PTHREAD=1 -DHAVE_LIBZ=1 -DHAVE_FTS_H=1 -DHAVE_APR_POOLS_H=1 -DHAVE_LIBAPR_1=1 -DHAVE_BOOST_VERSION_HPP=1 -DHAVE_LIBCURL=1 -DHAVE_ELFIO_ELFIO_HPP=1 -DHAVE_GLOG_LOGGING_H=1 -DHAVE_HTTP_PARSER_H=1 -DMESOS_HAS_JAVA=1 -DHAVE_LEVELDB_DB_H=1 -DHAVE_LIBNL_3=1 -DHAVE_LIBNL_ROUTE_3=1 -DHAVE_LIBNL_IDIAG_3=1 -DWITH_NETWORK_ISOLATOR=1 -DHAVE_GOOGLE_PROTOBUF_MESSAGE_H=1 -DHAVE_EV_H=1 -DHAVE_PICOJSON_H=1 -DHAVE_LIBSASL2=1 -DHAVE_SVN_VERSION_H=1 -DHAVE_LIBSVN_SUBR_1=1 -DHAVE_SVN_DELTA_H=1 -DHAVE_LIBSVN_DELTA_1=1 -DHAVE_LIBZ=1 -DHAVE_ZOOKEEPER_H=1 -DHAVE_PYTHON=\""2.7\"" -DMESOS_HAS_PYTHON=1 -I. -Wall -Werror -Wsign-compare -DLIBDIR=\""/usr/lib64\"" -DPKGLIBEXECDIR=\""/usr/libexec/mesos\"" -DPKGDATADIR=\""/usr/share/mesos\"" -DPKGMODULEDIR=\""/usr/lib64/mesos/modules\"" -I../include -I../include -I../include/mesos -DPICOJSON_USE_INT64 -D__STDC_FORMAT_MACROS -I../3rdparty/libprocess/include -I../3rdparty/nvml-352.79 -I../3rdparty/stout/include -DHAS_AUTHENTICATION=1 -Iyes/include -I/usr/include/subversion-1 -Iyes/include -Iyes/include -Iyes/include/libnl3 -Iyes/include -I/ -Iyes/include -I/usr/include/apr-1 -I/usr/include/apr-1.0 -I/builddir/build/BUILD/mesos-1.1.0/libev-4.15/include -isystem yes/include -Iyes/include -I/usr/src/gmock -I/usr/src/gmock/include -I/usr/src/gmock/src -I/usr/src/gmock/gtest -I/usr/src/gmock/gtest/include -I/usr/src/gmock/gtest/src -Iyes/include -Iyes/include -I/usr/include -I/builddir/build/BUILD/mesos-1.1.0/libev4.15/include -Iyes/include -I/usr/include -I/usr/include/zookeeper -pthread -O2 -g -pipe -Wall -Werror=format-security -Wp,-D_FORTIFY_SOURCE=2 -fexceptions -fstack-protector-strong --param=ssp-buffer-size=4 -grecord-gcc-switches -specs=/usr/lib/rpm/redhat/redhat-hardened-cc1 -m64 -mtune=generic -DEV_CHILD_ENABLE=0 -I/builddir/build/BUILD/mesos-1.1.0/libev-4.15 -Wno-unused-local-typedefs -Wno-maybe-uninitialized -std=c++11 -c health-check/health_checker.cpp  -fPIC -DPIC -o health-check/.libs/libmesos_no_3rdparty_la-health_checker.o_x000D_
In file included from health-check/health_checker.cpp:51:0:_x000D_
./linux/ns.hpp: In function 'Try<int> ns::clone(pid_t, int, const std::function<int()>&, int)':_x000D_
./linux/ns.hpp:480:69: error: dereferencing type-punned pointer will break strict-aliasing rules [-Werror=strict-aliasing]_x000D_
     pid_t pid = ((struct ucred*) CMSG_DATA(CMSG_FIRSTHDR(&message)))->pid;_x000D_
                                                                     ^~_x000D_
./linux/ns.hpp: In lambda function:_x000D_
./linux/ns.hpp:581:59: error: dereferencing type-punned pointer will break strict-aliasing rules [-Werror=strict-aliasing]_x000D_
       ((struct ucred*) CMSG_DATA(CMSG_FIRSTHDR(&message)))->pid = ::getpid();_x000D_
                                                           ^~_x000D_
./linux/ns.hpp:582:59: error: dereferencing type-punned pointer will break strict-aliasing rules [-Werror=strict-aliasing]_x000D_
       ((struct ucred*) CMSG_DATA(CMSG_FIRSTHDR(&message)))->uid = ::getuid();_x000D_
                                                           ^~_x000D_
./linux/ns.hpp:583:59: error: dereferencing type-punned pointer will break strict-aliasing rules [-Werror=strict-aliasing]_x000D_
       ((struct ucred*) CMSG_DATA(CMSG_FIRSTHDR(&message)))->gid = ::getgid();_x000D_
                                                           ^~_x000D_
cc1plus: all warnings being treated as errors_x000D_
make[2]: *** [Makefile:6655: health-check/libmesos_no_3rdparty_la-health_checker.lo] Error 1_x000D_
{noformat}_x000D_
",3.0,"1.1.0,1.2.3,1.3.1,1.4.1",0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.01282051282051282,0.0,0.0,0.9404816513761468
Improvement,Actively Scan for CNI Configurations,"Mesos-Agent currently loads the CNI configs into memory at startup. After this point, new configurations that are added will remain unknown to the Mesos Agent process until it is restarted.

This ticket is to request that the Mesos Agent process can the CNI config directory each time it is networking a task, so that modifying, adding, and removing networks will not require a slave reboot.",3.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.2807017543859649,0.39285714285714285,0.01282051282051282,0.01212121212121212,0.01212121212121212,0.0
Task,Update the Containerizer to handle attachInput and attachOutput calls.,"With the per-container I/O switchboard we are adding, the containerizer should be responsible for both launching the I/O switchboard process, as well as allowing external components to interface with it.

",2.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Task,"Add special case for entering the ""mount"" namespace of a parent container","Currently, tasks launched with the command executor have a hierarchy of processes inside their container that looks as follows:

{noformat}
| - mesos-containerizer launch
|   | - mesos-executor
|   |   | - task process
{noformat}

However, the only pid from this hierarchy of processes that the agent is aware of is the the pid for the top-level {{mesos-containerizer launch}} binary.

If all of these binaries were part of the same set of namespaces, then this would be sufficient to discover the namespaces of the {{task process}} (we could simply inspect the namespaces of the {{mesos-containerizer launch}} pid and know they were the same for the {{task process}}.

This is true for most of the namespaces that each of these processes exist in. However, the {{mnt}} namespace of the two may differ. That is, the {{mesos-containerizer launch}} binary is always in the same {{mnt}} namespace as the host, while the {{task process}} binary may be in its own {{mnt}} namespace if file system isolation is turned on and it has a new rootfs provisioned for it (e.g. a docker image was provided for it).

This has not been a problem until now because we never wanted to simply _enter_ the {{mnt}} namespace of a container before. Even with nested containers for pods, we always create a new {{mnt}} namespace branched off the host {{mnt}} namespace (in order to support the injection of host-mounted volumes).

However, with the new debugging support we are adding, we need a way of entering the {{mnt}} namespace of a parent container instead of cloning a new one.

Since we only have access to the {{pid}} of the container's init process, we can simply enter all namespaces associated with that pid except the {{mnt}} namespace. For the {{mnt}} namespace, we need to special case it to walk the process hierarchy until we find the first process in a different {{mnt}} namespace and enter that one instead. If none are found, simply enter the {{mnt}} namespace of the ""init"" process.

This is a dirty dirty hack, but should be sufficient for now.

Eventually we want to completely eliminate the command executor in favor of the ""pod"" (i.e. ""default"") executor, which doesn't have this problem at all.",2.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Improvement,Add support for incremental gzip decompression.,We currently only support compressing and decompressing based on the entire input being available at once. We can add a {{gzip::Decompressor}} to support incremental decompression.,3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.0
Bug,Memory leak in the libprocess request decoder.,"The libprocess decoder can leak a {{Request}} object in cases when a client disconnects while the request is in progress. In such cases, the decoder's destructor won't delete the active {{Request}} object that it had allocated on the heap.

https://github.com/apache/mesos/blob/master/3rdparty/libprocess/src/decoder.hpp#L271",2.0,0,0.0,0.5701357466063348,0.0,0.0,0.0,0.0,0.0,0.6842105263157894,0.5642857142857143,0.3846153846153846,0.4181818181818182,0.4121212121212121,0.0
Bug,"_version uses incorrect MESOS_{MAJOR,MINOR,PATCH}_VERSION in libmesos java binding.","When the macros were re-assigned they were not flushed fully through the codebase:
https://github.com/apache/mesos/commit/6bc6a40a54491cfd733263cd3962e490b0b4bdbb",1.0,1.1.0,0.0,0.42533936651583715,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3142857142857143,0.24358974358974358,0.22424242424242424,0.22424242424242424,0.926605504587156
Bug,Java Scheduler Adapter does not surface MasterInfo.,"The HTTP adapter does not surface the {{MasterInfo}}. This makes it not compatible with the V0 API where the {{registered}} and {{reregistered}} calls provided the MasterInfo to the framework.
cc [~vinodkone]",2.0,1.1.0,0.5,0.5701357466063348,0.0,0.0,0.0,0.0,0.0,0.6842105263157894,0.5642857142857143,0.24358974358974358,0.22424242424242424,0.22424242424242424,0.926605504587156
Task,Build a standalone python client for connecting to our Mock HTTP Server that implements the new Debug APIs,"This client prototype should have a similar CLI to what we eventually want to build into the Mesos or DC/OS CLI.

{noformat}
Streaming HTTP Client

Usage:
  client task exec [--tty] [--interactive] <task-id> <cmd> [<args>...]
  client task attach [--tty] [--interactive] <task-id>

Options:
  --tty          Allocate a tty on the server before
                 attaching to the container.
  --interactive  Connect the stdin of the client to
                 the stdin of the container.
{noformat}",3.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.014285714285714285,0.3205128205128205,0.6,0.6,0.0
Task,Build a Mock HTTP Server that implements the new Debugging API calls,"The mock server should simply launch a process to run whatever command is passed to it, rather than attempt to launch an actual nested container in mesos. However, it should do everything necessary to deal with attaching a {{pty}}  / redirecting {{stdin/stdout/stderr}} properly.",3.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.014285714285714285,0.3205128205128205,0.6,0.6,0.0
Task,Update Mesos logger components to handle redirection of stdin,Currently the Mesos loggers only handle redirection of {{stdout/stderr}}. We need to update them to also handle redirection of {{stdin}}.,3.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Task,Build a Container I/O Switchboard,"In order to facilitate attach operations for a running container, we plan to introduce a new component into Mesos known as an “I/O switchboard”. The goal of this switchboard is to allow external components to *dynamically* interpose on the {{stdin}}, {{stdout}} and {{stderr}} of the init process of a running Mesos container. It will be implemented as a per-container, stand-alone process launched by the mesos containerizer at the time a container is first launched.

Each per-container switchboard will be responsible for the following:
 * Accepting a single dynamic request to register an fd for streaming data to the {{stdin}} of a container’s init process.
 * Accepting *multiple* dynamic requests to register fds for streaming data from the {{stdout}} and {{stderr}} of a container’s init process to those fds.
 * Allocating a pty for the new process (if requested), and directing data through the master fd of the pty as necessary.
 * Passing the *actual* set of file descriptors that should be dup’d onto the {{stdin}}, {{stdout}} and {{stderr}} of a container’s init process back to the containerizer. 

The idea being that the switchboard will maintain three asynchronous loops (one each for {{stdin}}, {{stdout}} and {{stderr}}) that constantly pipe data to/from a container’s init process to/from all of the file descriptors that have been dynamically registered with it.",3.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Task,Add a task_id -> container_id mapping in state.json,"Currently, there is no way to get the {{container-id}} of a task from hitting the mesos master alone.  You must first hit the master to get the {{task_id -> agent_id}} and {{task_id -> executor_id}} mappings, then hit the corresponding agent with {{agent_id}} to get the {{executor_id -> container_id}} mapping.

It would simplify things alot if the {{container_id}} information was immediately available in the {{/tasks}} and {{/state}} endpoints of the master itself.",2.0,0,0.5,1.0,0.0,0.0,0.0,0.0,0.8,0.5438596491228069,0.39999999999999997,0.3205128205128205,0.6,0.6,0.0
Task,Add fine grained control of which namespaces a nested container should inherit (or not).,"We need finer grained control of which namespaces / cgroups a nested container should inherit or not.

Right now, there are some implicit assumptions about which cgroups we enter and which namespaces we inherit when we launch a nested container. For example, under the current semantics, a nested container will always get a new pid namespace but inherit the network namespace from its parent. Moreover, nested containers will always inherit all of the cgroups from their parent (except the freezer cgroup), with no possiblity of choosing any different configuration.

My current thinking is to pass the set of isolators to {{containerizer->launch()} that we would like to have invoked as part of launching a new container. Only if that isolator is enabled (via the agent flags) AND it is passed in via {{launch()}, will it be used to isolate the new container (note that both cgroup isolation as well as namespace membership also implemented using isolators).  This is a sort of a whitelist approach, where we have to know the full set of isolators we want our container launched with ahead of time.

Alternatively, we could consider passing in the set of isolators that we would like *disabled* instead.  This way we could blacklist certain isolators from kicking in, even if they have been enabled via the agent flags.

In both approaches, one major caveat of this is that it will have to become part of the top-level containerizer API, but it is specific only to the universal containerizer. Maybe this is OK as we phase out the docker containerizer anyway.

I am leaning towards the blacklist approach at the moment...",2.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Task,Add support for port-mapping in `mesos-execute`,Add support to specify port-mappings for a container in mesos-execute.,1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2807017543859649,0.39285714285714285,0.1282051282051282,0.3090909090909091,0.3090909090909091,0.0
Bug,"The python linter doesn't rebuild the virtual environment before linting when ""pip-requirements.txt"" has changed","We need to detect if ""pip-requirements.txt"" changes and rebuild the virtual environment if it has.",2.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Documentation,Add documentation for CNI port-mapper plugin.,Need to add the CNI port-mapper plugin to the CNI documentation within Mesos.,1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2807017543859649,0.39285714285714285,0.1282051282051282,0.3090909090909091,0.3090909090909091,0.0
Improvement,HealthChecker sends updates to executor via libprocess messaging.,"Currently {{HealthChecker}} sends status updates via libprocess messaging to the executor's UPID. This seems unnecessary after refactoring health checker into the library: a simple callback will do. Moreover, not requiring executor's {{UPID}} will simplify creating a mocked {{HealthChecker}}.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.717948717948718,0.8242424242424242,0.7212121212121212,0.0
Improvement,Ensure Python support scripts are linted,"Currently {{support/mesos-style.py}} does not lint files under {{support/}}. This is mostly due to the fact that these scripts are too inconsistent style-wise that they wouldn't even pass the linter now.

We should clean up all Python scripts under {{support/}} so they pass the Python linter, and activate that directory in the linter for future additions. ",3.0,0,0.5,0.25037707390648567,0.3333333333333333,0.5,0.14285714285714285,0.5,0.0,0.2631578947368421,0.29285714285714287,1.0,0.8121212121212121,0.8121212121212121,0.0
Bug,Report new PARTITION_AWARE task statuses in HTTP endpoints,"At a minimum, the {{/state-summary}} endpoint needs to be updated.",1.0,0,0.0,1.0,0.0,0.0,0.0,0.0,0.8,0.5438596491228069,0.39999999999999997,0.48717948717948717,0.4303030303030303,0.4303030303030303,0.0
Improvement,Add a column for FrameworkID when displaying tasks in the WebUI,"The Mesos Web UI home page shows a list of active/completed/orphan tasks tasks like this:
|| ID || Name || State || Started || Host || ||
| 1 | My ambiguously named task | RUNNING | 1 minute ago | 10.10.0.1 | Sandbox |
| 1 | My ambiguously named task | RUNNING | 1 minute ago | 10.10.0.1 | Sandbox |
| 2 | My ambiguously named task | RUNNING | 1 minute ago | 10.10.0.1 | Sandbox |

When you start multiple frameworks, the task IDs and names show in the UI may be ambiguous, requiring extra clicks/investigation to disambiguate.  

In the above case, to disambiguate between the two tasks with ID {{1}}, the user would need to navigate to each sandbox and check the associated frameworkID in the {{/browse}} view.  

We could add a column showing the {{FrameworkID}} next to each task:
|| Framework || ID || Name || State || Started || Host || ||
| 179b5436-30ec-45e9-b324-fa5c5a1dd756-0000 | 1 | My ambiguously named task | RUNNING | 1 minute ago | 10.10.0.1 | Sandbox |
| 179b5436-30ec-45e9-b324-fa5c5a1dd756-0001 | 1 | My ambiguously named task | RUNNING | 1 minute ago | 10.10.0.1 | Sandbox |
| 179b5436-30ec-45e9-b324-fa5c5a1dd756-0001 | 2 | My ambiguously named task | RUNNING | 1 minute ago | 10.10.0.1 | Sandbox |

The {{FrameworkID}} s could be links to the associated framework
{code}
<a href=""{{'#/frameworks/' + framework.id}}"" title=""{{framework.id}}"">
  {{framework.id | truncateMesosID}}
</a>
{code}
-----
This involves additions to three tables:
https://github.com/apache/mesos/blob/1.0.x/src/webui/master/static/home.html#L152-L157
https://github.com/apache/mesos/blob/1.0.x/src/webui/master/static/home.html#L199-L205
https://github.com/apache/mesos/blob/1.0.x/src/webui/master/static/home.html#L246-L252
",1.0,0,0.5,0.006033182503770739,1.0,1.0,0.42857142857142855,0.0,0.2,0.0,0.007142857142857143,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.0
Improvement,Enable partition-awareness in mesos-execute,Helpful for testing.,2.0,0,0.0,1.0,0.0,0.0,0.0,0.0,0.8,0.5438596491228069,0.39999999999999997,0.48717948717948717,0.4303030303030303,0.4303030303030303,0.0
Task,Allow `network/cni` isolator to take a search path for CNI plugins instead of single directory,"Currently the `network/cni` isolator expects a single directory with the `--network_cni_plugins_dir` . This is very limiting because this forces the operator to put all the CNI plugins in the same directory. 

With Mesos port-mapper CNI plugin this would also imply that the operator would have to move this plugin from the Mesos installation directory to a directory specified in the `--network_cni_plugins_dir`. 

To simplify the operators experience it would make sense for the `--network_cni_plugins_dir` flag to take in set of directories instead of single directory. The `network/cni` isolator can then search this set of directories to find the CNI plugin.",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2807017543859649,0.39285714285714285,0.1282051282051282,0.3090909090909091,0.3090909090909091,0.0
Bug,CNI should not use `ifconfig` in executors `pre_exec_command`,"Currently the `network/cni` isolator sets up the `pre_exec_command` for executors when a container needs to be launched on a non-host network. The `pre_exec_command` is `ifconfig lo up`. This is done to primarily bring loopback up in the new network namespace.

Setting up the `pre_exec_command` to bring loopback up is problematic since the executors PATH variable is generally very limited (doesn't contain all path that the agents PATH variable has due to security concerns). 

Therefore instead of running `ifconfig lo up` in the `pre_exec_command` we should run it in `NetworkCniIsolatorSetup` subcommand, which runs with the same PATH variable as the agent.",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2807017543859649,0.39285714285714285,0.1282051282051282,0.3090909090909091,0.3090909090909091,0.0
Bug,Agent fails to kill empty parent container,"I launched a pod using Marathon, which led to the launching of a task group on a Mesos agent. The pod spec was flawed, so this led to Marathon repeatedly re-launching multiple instances of the task group. After this went on for a few minutes, I told Marathon to scale the app to 0 instances, which sends {{TASK_KILLED}} for one task in each task group. After this, the Mesos agent reports {{TASK_KILLED}} status updates for all 3 tasks in the pod, but hitting the {{/containers}} endpoint on the agent reveals that the executor container for this task group is still running.

Here is the task group launching on the agent:
{code}
slave.cpp:1696] Launching task group containing tasks [ test-pod.instance-bd0f7a5b-8c02-11e6-ad52-6eec1b96a601.healthTask1, test-pod.instance-bd0f7a5b-8c02-11e6-ad52-6eec1b96a601.healthTask2, test-pod.instance-bd0f7a5b-8c02-11e6-ad52-6eec1b96a601.clientTask ] for framework 42838ca8-8d6b-475b-9b3b-59f3cd0d6834-0000
{code}
and here is the executor container starting:
{code}
mesos-agent[2994]: I1006 20:23:27.407563  3094 containerizer.cpp:965] Starting container bf38ff09-3da1-487a-8926-1f4cc88bce32 for executor 'instance-test-pod.bd0f7a5b-8c02-11e6-ad52-6eec1b96a601' of framework 42838ca8-8d6b-475b-9b3b-59f3cd0d6834-0000
{code}
and here is the output showing the {{TASK_KILLED}} updates for one task group:
{code}
mesos-agent[2994]: I1006 20:23:28.728224  3097 slave.cpp:2283] Asked to kill task test-pod.instance-bd0f7a5b-8c02-11e6-ad52-6eec1b96a601.healthTask1 of framework 42838ca8-8d6b-475b-9b3b-59f3cd0d6834-0000
mesos-agent[2994]: W1006 20:23:28.728304  3097 slave.cpp:2364] Transitioning the state of task test-pod.instance-bd0f7a5b-8c02-11e6-ad52-6eec1b96a601.healthTask1 of framework 42838ca8-8d6b-475b-9b3b-59f3cd0d6834-0000 to TASK_KILLED because the executor is not registered
mesos-agent[2994]: I1006 20:23:28.728659  3097 slave.cpp:3609] Handling status update TASK_KILLED (UUID: 1cb8197a-7829-4a05-9cb1-14ba97519c42) for task test-pod.instance-bd0f7a5b-8c02-11e6-ad52-6eec1b96a601.healthTask1 of framework 42838ca8-8d6b-475b-9b3b-59f3cd0d6834-0000 from @0.0.0.0:0
mesos-agent[2994]: I1006 20:23:28.728817  3097 slave.cpp:3609] Handling status update TASK_KILLED (UUID: e377e9fb-6466-4ce5-b32a-37d840b9f87c) for task test-pod.instance-bd0f7a5b-8c02-11e6-ad52-6eec1b96a601.healthTask2 of framework 42838ca8-8d6b-475b-9b3b-59f3cd0d6834-0000 from @0.0.0.0:0
mesos-agent[2994]: I1006 20:23:28.728912  3097 slave.cpp:3609] Handling status update TASK_KILLED (UUID: 24d44b25-ea52-43a1-afdb-6c04389879d2) for task test-pod.instance-bd0f7a5b-8c02-11e6-ad52-6eec1b96a601.clientTask of framework 42838ca8-8d6b-475b-9b3b-59f3cd0d6834-0000 from @0.0.0.0:0
{code}
however, if we grep the log for the executor's ID, the last line mentioning it is:
{code}
slave.cpp:3080] Creating a marker file for HTTP based executor 'instance-test-pod.bd0f7a5b-8c02-11e6-ad52-6eec1b96a601' of framework 42838ca8-8d6b-475b-9b3b-59f3cd0d6834-0000 (via HTTP) at path '/var/lib/mesos/slave/meta/slaves/42838ca8-8d6b-475b-9b3b-59f3cd0d6834-S0/frameworks/42838ca8-8d6b-475b-9b3b-59f3cd0d6834-0000/executors/instance-test-pod.bd0f7a5b-8c02-11e6-ad52-6eec1b96a601/runs/bf38ff09-3da1-487a-8926-1f4cc88bce32/http.marker'
{code}
so it seems the executor never exited. If we hit the agent's {{/containers}} endpoint, we get output which includes this executor container:
{code}
{
    ""container_id"": ""bf38ff09-3da1-487a-8926-1f4cc88bce32"",
    ""executor_id"": ""instance-test-pod.bd0f7a5b-8c02-11e6-ad52-6eec1b96a601"",
    ""executor_name"": """",
    ""framework_id"": ""42838ca8-8d6b-475b-9b3b-59f3cd0d6834-0000"",
    ""source"": """",
    ""statistics"": {
      ""cpus_limit"": 0.1,
      ""cpus_nr_periods"": 17,
      ""cpus_nr_throttled"": 11,
      ""cpus_system_time_secs"": 0.02,
      ""cpus_throttled_time_secs"": 0.784142448,
      ""cpus_user_time_secs"": 0.09,
      ""disk_limit_bytes"": 10485760,
      ""disk_used_bytes"": 20480,
      ""mem_anon_bytes"": 11337728,
      ""mem_cache_bytes"": 0,
      ""mem_critical_pressure_counter"": 0,
      ""mem_file_bytes"": 0,
      ""mem_limit_bytes"": 33554432,
      ""mem_low_pressure_counter"": 0,
      ""mem_mapped_file_bytes"": 0,
      ""mem_medium_pressure_counter"": 0,
      ""mem_rss_bytes"": 11337728,
      ""mem_swap_bytes"": 0,
      ""mem_total_bytes"": 12013568,
      ""mem_unevictable_bytes"": 0,
      ""timestamp"": 1475792290.12373
    },
    ""status"": {
      ""executor_pid"": 9068,
      ""network_infos"": [
        {
          ""ip_addresses"": [
            {
              ""ip_address"": ""9.0.1.34"",
              ""protocol"": ""IPv4""
            }
          ],
          ""labels"": {},
          ""name"": ""dcos"",
          ""port_mappings"": [
            {
              ""container_port"": 8080,
              ""host_port"": 24758,
              ""protocol"": ""tcp""
            },
            {
              ""container_port"": 8081,
              ""host_port"": 24759,
              ""protocol"": ""tcp""
            }
          ]
        }
      ]
    }
  },
{code}
and looking through the output of {{ps}} on the agent, indeed we can locate the executor process:
{code}
$ ps aux | grep 9068
root      9068  0.0  0.1  96076 25380 ?        Ss   20:23   0:00 mesos-containerizer launch --command={""arguments"":[""mesos-default-executor""],""shell"":false,""user"":""root"",""value"":""\/opt\/mesosphere\/packages\/mesos--3f320308144e6b5acb70fc89bc7ea13c70302de0\/libexec\/mesos\/mesos-default-executor""} --help=false --pipe_read=49 --pipe_write=50 --pre_exec_commands=[{""arguments"":[""mesos-containerizer"",""mount"",""--help=false"",""--operation=make-rslave"",""--path=\/""],""shell"":false,""value"":""\/opt\/mesosphere\/packages\/mesos--3f320308144e6b5acb70fc89bc7ea13c70302de0\/libexec\/mesos\/mesos-containerizer""},{""shell"":true,""value"":""ifconfig lo up""}] --runtime_directory=/var/run/mesos/containers/bf38ff09-3da1-487a-8926-1f4cc88bce32 --unshare_namespace_mnt=false --user=root --working_directory=/var/lib/mesos/slave/slaves/42838ca8-8d6b-475b-9b3b-59f3cd0d6834-S0/frameworks/42838ca8-8d6b-475b-9b3b-59f3cd0d6834-0000/executors/instance-test-pod.bd0f7a5b-8c02-11e6-ad52-6eec1b96a601/runs/bf38ff09-3da1-487a-8926-1f4cc88bce32
core     20406  0.0  0.0   6764  1020 pts/1    S+   23:05   0:00 grep --colour=auto 9068
$ sudo cat /proc/9068/task/9068/children
9330
$ ps aux | grep 9330
root      9330  0.0  0.2 498040 32944 ?        Sl   20:23   0:00 mesos-default-executor
root     19330  0.0  0.0      0     0 ?        S    22:49   0:00 [kworker/0:2]
core     20573  0.0  0.0   6764   888 pts/1    S+   23:07   0:00 grep --colour=auto 9330
{code}
Looking at the executor's logs, we find stdout is:
{code}
Executing pre-exec command '{""arguments"":[""mesos-containerizer"",""mount"",""--help=false"",""--operation=make-rslave"",""--path=\/""],""shell"":false,""value"":""\/opt\/mesosphere\/packages\/mesos--3f320308144e6b5acb70fc89bc7ea13c70302de0\/libexec\/mesos\/mesos-containerizer""}'
Executing pre-exec command '{""shell"":true,""value"":""ifconfig lo up""}'
{code}
and stderr is:
{code}
I1006 20:23:28.817401  9330 executor.cpp:189] Version: 1.1.0
I1006 20:23:28.906349  9352 default_executor.cpp:123] Received SUBSCRIBED event
I1006 20:23:28.908797  9352 default_executor.cpp:127] Subscribed executor on 10.0.0.133
{code}
With this short executor log, it seems possible that the agent received the {{TASK_KILLED}} before any tasks were sent to the executor, and the agent removed those tasks from its data structures without terminating the parent container. ",3.0,0,0.5,0.5701357466063348,0.0,0.0,0.0,0.0,0.0,0.6842105263157894,0.5642857142857143,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Bug,CHECK failure in HierarchicalAllocatorTest.NoDoubleAccounting,"Observed in internal CI:

{noformat}
[15:52:21] :     [Step 10/10] [ RUN      ] HierarchicalAllocatorTest.NoDoubleAccounting
[15:52:21]W:     [Step 10/10] I1006 15:52:21.813817 23713 hierarchical.cpp:275] Added framework framework1
[15:52:21]W:     [Step 10/10] I1006 15:52:21.814100 23713 hierarchical.cpp:1694] No allocations performed
[15:52:21]W:     [Step 10/10] I1006 15:52:21.814102 23712 process.cpp:3377] Handling HTTP event for process 'metrics' with path: '/metrics/snapshot'
[15:52:21]W:     [Step 10/10] I1006 15:52:21.814121 23713 hierarchical.cpp:1789] No inverse offers to send out!
[15:52:21]W:     [Step 10/10] I1006 15:52:21.814146 23713 hierarchical.cpp:1286] Performed allocation for 0 agents in 52445ns
[15:52:21]W:     [Step 10/10] I1006 15:52:21.814206 23713 hierarchical.cpp:485] Added agent agent1 (agent1) with cpus(*):1 (allocated: cpus(*):1)
[15:52:21]W:     [Step 10/10] I1006 15:52:21.814237 23713 hierarchical.cpp:1694] No allocations performed
[15:52:21]W:     [Step 10/10] I1006 15:52:21.814247 23713 hierarchical.cpp:1789] No inverse offers to send out!
[15:52:21]W:     [Step 10/10] I1006 15:52:21.814259 23713 hierarchical.cpp:1309] Performed allocation for agent agent1 in 33887ns
[15:52:21]W:     [Step 10/10] I1006 15:52:21.814294 23713 hierarchical.cpp:485] Added agent agent2 (agent2) with cpus(*):1 (allocated: cpus(*):1)
[15:52:21]W:     [Step 10/10] I1006 15:52:21.814332 23713 hierarchical.cpp:1694] No allocations performed
[15:52:21]W:     [Step 10/10] I1006 15:52:21.814342 23713 hierarchical.cpp:1789] No inverse offers to send out!
[15:52:21]W:     [Step 10/10] I1006 15:52:21.814349 23713 hierarchical.cpp:1309] Performed allocation for agent agent2 in 42682ns
[15:52:21]W:     [Step 10/10] I1006 15:52:21.814417 23713 hierarchical.cpp:275] Added framework framework2
[15:52:21]W:     [Step 10/10] I1006 15:52:21.814445 23713 hierarchical.cpp:1694] No allocations performed
[15:52:21]W:     [Step 10/10] I1006 15:52:21.814455 23713 hierarchical.cpp:1789] No inverse offers to send out!
[15:52:21]W:     [Step 10/10] I1006 15:52:21.814469 23713 hierarchical.cpp:1286] Performed allocation for 2 agents in 37976ns
[15:52:21]W:     [Step 10/10] F1006 15:52:21.824954 23692 json.hpp:334] Check failed: 'boost::get<T>(this)' Must be non NULL
[15:52:21]W:     [Step 10/10] *** Check failure stack trace: ***
[15:52:21]W:     [Step 10/10]     @     0x7fe953bbd71d  google::LogMessage::Fail()
[15:52:21]W:     [Step 10/10]     @     0x7fe953bbf55d  google::LogMessage::SendToLog()
[15:52:21]W:     [Step 10/10]     @     0x7fe953bbd30c  google::LogMessage::Flush()
[15:52:21]W:     [Step 10/10]     @     0x7fe953bbfe59  google::LogMessageFatal::~LogMessageFatal()
[15:52:21]W:     [Step 10/10]     @           0x7cc903  JSON::Value::as<>()
[15:52:21]W:     [Step 10/10]     @           0x8b633c  mesos::internal::tests::HierarchicalAllocatorTest_NoDoubleAccounting_Test::TestBody()
[15:52:21]W:     [Step 10/10]     @          0x129ce23  testing::internal::HandleExceptionsInMethodIfSupported<>()
[15:52:21]W:     [Step 10/10]     @          0x1292f07  testing::Test::Run()
[15:52:21]W:     [Step 10/10]     @          0x1292fae  testing::TestInfo::Run()
[15:52:21]W:     [Step 10/10]     @          0x12930b5  testing::TestCase::Run()
[15:52:21]W:     [Step 10/10]     @          0x1293368  testing::internal::UnitTestImpl::RunAllTests()
[15:52:21]W:     [Step 10/10]     @          0x1293624  testing::UnitTest::Run()
[15:52:21]W:     [Step 10/10]     @           0x507254  main
[15:52:21]W:     [Step 10/10]     @     0x7fe95122876d  (unknown)
[15:52:21]W:     [Step 10/10]     @           0x51e341  (unknown)
[15:52:21]W:     [Step 10/10] Aborted (core dumped)
[15:52:21]W:     [Step 10/10] Process exited with code 134
{noformat}
",1.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.48717948717948717,0.4303030303030303,0.4303030303030303,0.0
Task,Update CHANGELOG to mention addition of agent '--runtime_dir' flag.,"We recently introduced a new agent flag for {{\-\-runtime_dir}}. Unlike the {{\-\-work_dir}}, this directory is designed to hold the state of a running agent between subsequent agent-restarts (but not across host reboots).

By default, this flag is set to {{/var/run/mesos}} since this is a {{tempfs}} on linux that gets automatically cleaned up on reboot. When running as non-root we set the default to {{os::temp()/mesos/runtime}}.

We should call this out in the CHAGNELOG",1.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Improvement,Add authorization support for nested container calls,"We need to authorize {LAUNCH, KILL, WAIT}_NESTED_CONTAINER API calls.",3.0,0,0.5,0.004524886877828055,0.0,0.0,0.0,0.0,0.2,0.12280701754385964,0.24285714285714285,0.0,0.006060606060606061,0.006060606060606061,0.0
Improvement,Add authentication support to the default executor,"The V1 executors should be updated to authenticate with the agent when HTTP executor authentication is enabled. This will be hard-coded into the executor library for the MVP, and it can be refactored into an {{HttpAuthenticatee}} module later. The executor must:
* load a JWT from its environment, if present
* decorate its requests with an {{Authorization}} header containing the JWT",2.0,0,0.5,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.0,0.006060606060606061,0.006060606060606061,0.0
Bug,HealthCheckTest.HealthyTaskViaHTTPWithoutType fails on some distros.,"I see consistent failures of this test in the internal CI in *some* distros, specifically CentOS 6, Ubuntu 14, 15, 16. The source of the health check failure is always the same: {{curl}} cannot connect to the target:
{noformat}
Received task health update, healthy: false
W0929 17:22:05.270992  2730 health_checker.cpp:204] Health check failed 1 times consecutively: HTTP health check failed: curl returned exited with status 7: curl: (7) couldn't connect to host
I0929 17:22:05.273634 26850 slave.cpp:3609] Handling status update TASK_RUNNING (UUID: f5408ac9-f6ba-447f-b3d7-9dce44384ffe) for task aa0792d3-8d85-4c32-bd04-56a9b552ebda in health state unhealthy of framework 2e0e9ea1-0ae5-4f28-80bb-a9abc56c5a6f-0000 from executor(1)@172.30.2.20:58660
I0929 17:22:05.274178 26844 status_update_manager.cpp:323] Received status update TASK_RUNNING (UUID: f5408ac9-f6ba-447f-b3d7-9dce44384ffe) for task aa0792d3-8d85-4c32-bd04-56a9b552ebda in health state unhealthy of framework 2e0e9ea1-0ae5-4f28-80bb-a9abc56c5a6f-0000
I0929 17:22:05.274226 26844 status_update_manager.cpp:377] Forwarding update TASK_RUNNING (UUID: f5408ac9-f6ba-447f-b3d7-9dce44384ffe) for task aa0792d3-8d85-4c32-bd04-56a9b552ebda in health state unhealthy of framework 2e0e9ea1-0ae5-4f28-80bb-a9abc56c5a6f-0000 to the agent
I0929 17:22:05.274314 26845 slave.cpp:4026] Forwarding the update TASK_RUNNING (UUID: f5408ac9-f6ba-447f-b3d7-9dce44384ffe) for task aa0792d3-8d85-4c32-bd04-56a9b552ebda in health state unhealthy of framework 2e0e9ea1-0ae5-4f28-80bb-a9abc56c5a6f-0000 to master@172.30.2.20:38955
I0929 17:22:05.274415 26845 slave.cpp:3920] Status update manager successfully handled status update TASK_RUNNING (UUID: f5408ac9-f6ba-447f-b3d7-9dce44384ffe) for task aa0792d3-8d85-4c32-bd04-56a9b552ebda in health state unhealthy of framework 2e0e9ea1-0ae5-4f28-80bb-a9abc56c5a6f-0000
I0929 17:22:05.274436 26845 slave.cpp:3936] Sending acknowledgement for status update TASK_RUNNING (UUID: f5408ac9-f6ba-447f-b3d7-9dce44384ffe) for task aa0792d3-8d85-4c32-bd04-56a9b552ebda in health state unhealthy of framework 2e0e9ea1-0ae5-4f28-80bb-a9abc56c5a6f-0000 to executor(1)@172.30.2.20:58660
I0929 17:22:05.274534 26849 master.cpp:5661] Status update TASK_RUNNING (UUID: f5408ac9-f6ba-447f-b3d7-9dce44384ffe) for task aa0792d3-8d85-4c32-bd04-56a9b552ebda in health state unhealthy of framework 2e0e9ea1-0ae5-4f28-80bb-a9abc56c5a6f-0000 from agent 2e0e9ea1-0ae5-4f28-80bb-a9abc56c5a6f-S0 at slave(77)@172.30.2.20:38955 (ip-172-30-2-20.mesosphere.io)
../../src/tests/health_check_tests.cpp:1398: Failure
I0929 17:22:05.274567 26849 master.cpp:5723] Forwarding status update TASK_RUNNING (UUID: f5408ac9-f6ba-447f-b3d7-9dce44384ffe) for task aa0792d3-8d85-4c32-bd04-56a9b552ebda in health state unhealthy of framework 2e0e9ea1-0ae5-4f28-80bb-a9abc56c5a6f-0000
Value of: statusHealth.get().healthy()
  Actual: false
  Expected: true
I0929 17:22:05.274636 26849 master.cpp:7560] Updating the state of task aa0792d3-8d85-4c32-bd04-56a9b552ebda of framework 2e0e9ea1-0ae5-4f28-80bb-a9abc56c5a6f-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I0929 17:22:05.274829 26844 sched.cpp:1025] Scheduler::statusUpdate took 43297ns
Received SHUTDOWN event
{noformat}",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.717948717948718,0.8242424242424242,0.7212121212121212,0.0
Bug,Agent should not allow HTTP executors to re-subscribe before containerizer recovery is done.,"In the old API, agent will send a reconnect request to the executor and then the executor will register with the agent.

Now, in the new API, agent will allow an executor to re-subscribe before containerizer recovery is done. This is problematic because containerizer has no idea about the containers yet, calling containerizer->update will lead to a failure, causing the container being killed.

{noformat}
[04:04:11]W:	 [Step 10/10] I0929 04:04:11.693418 22646 containerizer.cpp:580] Recovering containerizer
[04:04:11]W:	 [Step 10/10] I0929 04:04:11.693444 22646 containerizer.cpp:636] Recovering container 568968cc-f41c-475a-bb2b-45d8babd853d for executor 'default' of framework 7e4c8518-cb45-4b09-9fa8-c029d56289e2-0000
[04:04:11]W:	 [Step 10/10] I0929 04:04:11.693445 22645 http.cpp:273] HTTP POST for /agent/api/v1/executor from 172.30.2.198:42683
[04:04:11]W:	 [Step 10/10] I0929 04:04:11.693567 22645 slave.cpp:3017] Received Subscribe request for HTTP executor 'default' of framework 7e4c8518-cb45-4b09-9fa8-c029d56289e2-0000 (via HTTP)
[04:04:11]W:	 [Step 10/10] I0929 04:04:11.693613 22645 slave.cpp:3080] Creating a marker file for HTTP based executor 'default' of framework 7e4c8518-cb45-4b09-9fa8-c029d56289e2-0000 (via HTTP) at path '/mnt/teamcity/temp/buildTmp/SlaveRecoveryTest_0_ROOT_CGROUPS_ReconnectDefaultExecutor_XpQvvJ/meta/slaves/7e4c8518-cb45-4b09-9fa8-c029d56289e2-S0/frameworks/7e4c8518-cb45-4b09-9fa8-c029d56289e2-0000/executors/default/runs/568968cc-f41c-475a-bb2b-45d8babd853d/http.marker'
[04:04:11]W:	 [Step 10/10] I0929 04:04:11.693733 22645 slave.cpp:3609] Handling status update TASK_RUNNING (UUID: 6cc3f9a7-d020-46f0-82c1-39fbb9d43786) for task db1f9b1b-75d2-4d96-831f-48d6f28301e8 of framework 7e4c8518-cb45-4b09-9fa8-c029d56289e2-0000
[04:04:11]W:	 [Step 10/10] I0929 04:04:11.693801 22645 slave.cpp:3609] Handling status update TASK_RUNNING (UUID: f80d217b-7844-4134-8cc8-db6998ac437e) for task 3a583cbb-8ea9-440a-864d-e68a23472368 of framework 7e4c8518-cb45-4b09-9fa8-c029d56289e2-0000
[04:04:11]W:	 [Step 10/10] E0929 04:04:11.694232 22648 slave.cpp:2055] Failed to update resources for container 568968cc-f41c-475a-bb2b-45d8babd853d of executor 'default' of framework 7e4c8518-cb45-4b09-9fa8-c029d56289e2-0000, destroying container: Collect failed: Unknown container
{noformat}",3.0,"1.0.0,1.0.1",0.5,0.5701357466063348,0.0,0.0,0.0,0.0,0.0,0.6842105263157894,0.5642857142857143,0.7948717948717948,1.0,1.0,0.9174770642201833
Task,Allow WebUI/other tools to access the task sandbox for a nested container.,"Currently, the agent does not expose any information about the {{ContainerID}} -> {{TaskID}} mapping. This makes it very hard for the WebUI/other tools to access construct the path to the task sandbox. As a workaround, one possible approach can be to create a symlink from {{tasks/TaskID}} -> {{containers/ContainerID}}. The WebUI can then construct the path to the task sandbox by following the symbolic link. Eventually, we would like to expose this information on the agent itself.",2.0,0,0.0,0.5701357466063348,0.0,0.0,0.0,0.0,0.0,0.6842105263157894,0.5642857142857143,0.3846153846153846,0.4181818181818182,0.4121212121212121,0.0
Bug,Default executor should kill all other tasks in a task group if any task exits with a non-zero exit status.,The default restart policy for a task group is to kill all active containers if any of the tasks terminates with a non-zero exit status code for now. The default executor needs to honor this default policy.,3.0,0,0.0,0.5701357466063348,0.0,0.0,0.0,0.0,0.0,0.6842105263157894,0.5642857142857143,0.3846153846153846,0.4181818181818182,0.4121212121212121,0.0
Bug,Libprocess links will not generate an ExitedEvent if the socket creation fails,"Noticed this while inspecting nearby code for potential races.

Normally, when a libprocess actor (the ""linkee"") links to a remote process, it does the following:
1) Create a socket.
2) Connect to the remote process (asynchronous).
3) Check the connection succeeded.

If (2) or (3) fail, the linkee will receive a {{ExitedEvent}}, which indicates that the link broke.  In case (1) fails, there is no {{ExitedEvent}}:
https://github.com/apache/mesos/blob/7c833abbec9c9e4eb51d67f7a8e7a8d0870825f8/3rdparty/libprocess/src/process.cpp#L1558-L1562",2.0,"0.27.3,0.28.2,1.0.1",0.0,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.47418960244648317
Bug,Driver based schedulers performing explicit acknowledgements cannot acknowledge updates from HTTP based executors.,"It seems that the agent code sets {{StatusUpdate}}->{{slave_id}} but does not set the {{TaskStatus}}->{{slave_id}} if it's not already set. On the driver, when we receive such a status update and if it has explicit ACK enabled, it would pass the {{TaskStatus}} to the scheduler. But, the scheduler has no way of acking this update due to {{slave_id}} not being present. Note that, implicit acknowledgements still work since they use the {{slave_id}} from {{StatusUpdate}}. Hence, we never noticed this in our tests as all of them use implicit acknowledgements on the driver.",3.0,0,0.0,0.5701357466063348,0.0,0.0,0.0,0.0,0.0,0.6842105263157894,0.5642857142857143,0.3846153846153846,0.4181818181818182,0.4121212121212121,0.0
Task,Add 'argv' variant of 'os::system',"The {{os::system()}} function always spawns whatever string you pass to is a a direct argument to {{sh -c '<arg_string>'}}. However, this can be problematic if you build {{<arg_string>}} from user supplied input and they have the opportunity to inject arbitrary commands at the end of it (e.g. by adding a ""; rm -rf"" as part of the last user supplied argument).

To counter this, we should introduce a variant of {{os::system()}} that takes a single command and a list of args (similar to the {{posix_spawn()}} function.",3.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Bug,Potential socket leak during Zookeeper network changes,"There is a potential leak when using the version of {{link}} with {{RemoteConnection::RECONNECT}}.  This was originally implemented to refresh links during master recovery. 

The leak occurs here:
https://github.com/apache/mesos/blob/5e23edd513caec51ce3e94b3d785d714052525e8/3rdparty/libprocess/src/process.cpp#L1592-L1597
^ The comment here is not correct, as that is *not* the last reference to the {{existing}} socket.

At this point, the {{existing}} socket may be a perfectly valid link.  Valid links will all have a reference inside a callback loop created here:
https://github.com/apache/mesos/blob/5e23edd513caec51ce3e94b3d785d714052525e8/3rdparty/libprocess/src/process.cpp#L1503-L1509

-----

We need to stop the callback loop but prevent any resulting {{ExitedEvents}} from being sent due to stopping the callback loop.  This means discarding the callback loop's future after we have called {{swap_implementing_socket}}.",3.0,"0.28.3,1.0.0",0.0,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.5872935779816514
Bug,Master CHECK fails during recovery while relinking to other masters,"Mesos Version: 1.0.1
OS: CoreOS 1068

{code}
Sep 22 20:05:17 node-44a84215535c mesos-master[104478]: I0922 20:05:17.948004 104495 manager.cpp:795] overlay-master in `RECOVERING` state . Hence, not sending an update to agentoverlay-agent@10.4.4.1:5051
Sep 22 20:05:17 node-44a84215535c mesos-master[104478]: F0922 20:05:17.948120 104529 process.cpp:2243] Check failed: sockets.count(from_fd) > 0
Sep 22 20:05:17 node-44a84215535c mesos-master[104478]: *** Check failure stack trace: ***
Sep 22 20:05:17 node-44a84215535c mesos-master[104478]:     @     0x7fc1908829fd  google::LogMessage::Fail()
Sep 22 20:05:17 node-44a84215535c mesos-master[104478]:     @     0x7fc19088482d  google::LogMessage::SendToLog()
Sep 22 20:05:17 node-44a84215535c mesos-master[104478]:     @     0x7fc1908825ec  google::LogMessage::Flush()
Sep 22 20:05:17 node-44a84215535c mesos-master[104478]:     @     0x7fc190885129  google::LogMessageFatal::~LogMessageFatal()
Sep 22 20:05:17 node-44a84215535c mesos-master[104478]:     @     0x7fc1908171dd  process::SocketManager::swap_implementing_socket()
Sep 22 20:05:17 node-44a84215535c mesos-master[104478]:     @     0x7fc19081aa90  process::SocketManager::link_connect()
Sep 22 20:05:17 node-44a84215535c mesos-master[104478]:     @     0x7fc1908227f9  _ZNSt17_Function_handlerIFvRKN7process6FutureI7NothingEEEZNKS3_5onAnyISt5_BindIFSt7_Mem_fnIMNS0_13SocketManagerEFvS5_NS0_7network6SocketERKNS0_4UPIDEEEPSA_St12_PlaceholderILi1EESC_SD_EEvEES5_OT_NS3_6PreferEEUlS5_E_E9_M_invokeERKSt9_Any_dataS5_
Sep 22 20:05:17 node-44a84215535c mesos-master[104478]:     @           0x41eb26  _ZN7process8internal3runISt8functionIFvRKNS_6FutureI7NothingEEEEJRS5_EEEvRKSt6vectorIT_SaISC_EEDpOT0_
Sep 22 20:05:17 node-44a84215535c mesos-master[104478]:     @           0x42a36f  process::Future<>::fail()
Sep 22 20:05:17 node-44a84215535c mesos-master[104478]:     @     0x7fc19085283c  process::network::LibeventSSLSocketImpl::event_callback()
Sep 22 20:05:17 node-44a84215535c mesos-master[104478]:     @     0x7fc190852f17  process::network::LibeventSSLSocketImpl::event_callback()
Sep 22 20:05:17 node-44a84215535c mesos-master[104478]:     @     0x7fc18d616631  bufferevent_run_deferred_callbacks_locked
Sep 22 20:05:17 node-44a84215535c mesos-master[104478]:     @     0x7fc18d60cc5d  event_base_loop
Sep 22 20:05:17 node-44a84215535c mesos-master[104478]:     @     0x7fc190865a1d  process::EventLoop::run()
Sep 22 20:05:17 node-44a84215535c mesos-master[104478]:     @     0x7fc18eeabd73  (unknown)
Sep 22 20:05:17 node-44a84215535c mesos-master[104478]:     @     0x7fc18e6a852c  (unknown)
Sep 22 20:05:17 node-44a84215535c mesos-master[104478]:     @     0x7fc18e3e61dd  (unknown)
Sep 22 20:05:18 node-44a84215535c systemd[1]: [0;1;39mdcos-mesos-master.service: Main process exited, code=killed, status=6/ABRT
{code}",2.0,"0.28.3,1.0.1",0.5,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.01282051282051282,0.0,0.0,0.5873394495412844
Task,Add support for health checks to the default executor.,"Currently, there is no health checking mechanism for the tasks in a task group. Ideally, we would like to re-use the existing health checking infrastructure and do health checking for all the tasks in a task group. If one of them, fails we should kill all the tasks in the task group (default policy). We would add support for specifying custom policies in the future.",3.0,0,0.0,0.5701357466063348,0.0,0.0,0.0,0.0,0.0,0.6842105263157894,0.5642857142857143,0.3846153846153846,0.4181818181818182,0.4121212121212121,0.0
Bug,Containers that use the Mesos containerizer but don't want to provision a container image fail to validate.,"Tasks using  features like volumes or CNI in their containers, have to define these in {{TaskInfo.container}}. When these tasks don't want/need to provision a container image, neither {{ContainerInfo.docker}} nor {{ContainerInfo.mesos}} will be set. Nevertheless, the container type in {{ContainerInfo.type}} needs to be set, because it is a required field.
In that case, the recently introduced validation rules in {{master/validation.cpp}} ({{validateContainerInfo}} will fail, which isn't expected.",1.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.28205128205128205,0.296969696969697,0.296969696969697,0.0
Bug,ContainerInfo is not validated.,"Currently Mesos does not validate {{ContainerInfo}} provided with {{TaskInfo}} or {{ExecutorInfo}}, hence invalid task configurations can be accepted.",3.0,1.0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.717948717948718,0.8242424242424242,0.7212121212121212,0.9174311926605504
Task,Make the `network/cni` isolator nesting aware,"In pods, child containers share the network and UTS namespace with the parent containers. This implies that during `prepare` and `isolate` the `network/cni` isolator needs to be aware the parent-child relationship between containers to make the following decisions:
a) During `prepare` a container should be allocated a new network namespace and UTS namespace only if the container is a top level container.
b) During `isolate` the network files (/etc/hosts, /etc/hostname, /etc/resolv.conf) should be created only for top level containers. The network files for child containers will just be symlinks to the parent containers network files.",3.0,1.1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2807017543859649,0.39285714285714285,0.1282051282051282,0.3090909090909091,0.3090909090909091,0.926605504587156
Bug,Clean up queued tasks if a task group is killed before launch.,"We are not properly cleaning up a queued task group when one of its task is killed i.e. https://github.com/apache/mesos/blob/aaa353acc515c0435a859113c9ee236247b51169/src/slave/slave.cpp#L6554 , we clean up the queued task but don't go around cleaning up the queued task group. Also, it would be great to add a test similar to we did for exercising the {{pending}} tasks workflow i.e. {{SlaveTest.KillTaskGroupBetweenRunTaskParts}} for {{queuedTasks}}.",2.0,0,0.0,0.5701357466063348,0.0,0.0,0.0,0.0,0.0,0.6842105263157894,0.5642857142857143,0.3846153846153846,0.4181818181818182,0.4121212121212121,0.0
Task,Populate `CommandInfo` correctly for default executors.,"As a follow up to MESOS-6076, we need to ensure that we populate {{CommandInfo}} correctly for the default executor. There are two approaches:

- We can populate the command info and then store it. When the agent (re-)registers with the master, we need to unset it.
- Call {{containerizer->launch}} with the modified {{ExecutorInfo}} for default executors and not store them. This has the advantage of getting rid of the book keeping code and getting rid of it when sending it to the master upon (re-)registration. However, we do send the {{ExecutorInfo}} to the executor as part of the {{SUBSCRIBED}} event. However, we don't see the default executor implementation trying to do something with this value.",2.0,0,0.0,0.5701357466063348,0.0,0.0,0.0,0.0,0.0,0.6842105263157894,0.5642857142857143,0.3846153846153846,0.4181818181818182,0.4121212121212121,0.0
Bug,Some tests do not properly set 'flags.launcher' with the correct value,"In some of our tests we manually create a 'PosixLauncher' rather than relying on the value of 'flags.launcher' to decide which type of launcher to create. Since calls to 'CreateSlaveFlags()' set 'flags.launcher' to 'linux' by default, there is a discrepency in what the flags say, and what actual launcher type we are creating.

We should fix this.",2.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Task,Make the disk usage isolator nesting-aware,"With the addition of task groups, the disk usage isolator must be updated. Since sub-container sandboxes are nested within the parent container's sandbox, the isolator must exclude these folders from its usage calculation when examining the parent container's disk usage.",3.0,0,0.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Bug,TCP health checks are not portable.,"MESOS-3567 introduced a dependency on ""bash"" for TCP health checks, which is undesirable. We should implement a portable solution for TCP health checks.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.717948717948718,0.8242424242424242,0.7212121212121212,0.0
Bug,Potential FD double close in libevent's implementation of `sendfile`.,"Repro copied from: https://reviews.apache.org/r/51509/

It is possible to make the master CHECK fail by repeatedly hitting the web UI and reloading the static assets:

1) Paste lots of text (16KB or more) of text into `src/webui/master/static/home.html`.  The more text, the more reliable the repro.

2) Start the master with SSL enabled:
{code}
LIBPROCESS_SSL_ENABLED=true LIBPROCESS_SSL_KEY_FILE=key.pem LIBPROCESS_SSL_CERT_FILE=cert.pem bin/mesos-master.sh --work_dir=/tmp/master
{code}

3) Run two instances of this python script repeatedly:
{code}
import socket
import ssl

s = ssl.wrap_socket(socket.socket())
s.connect((""localhost"", 5050))

s.sendall(""""""GET /static/home.html HTTP/1.1
User-Agent: foobar
Host: localhost:5050
Accept: */*
Connection: Keep-Alive

"""""")

# The HTTP part of the response
print s.recv(1000)
{code}

i.e. 
{code}
while python test.py; do :; done & while python test.py; do :; done
{code}",3.0,"0.27.3,0.28.2,1.0.1",0.0,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.47418960244648317
Bug,"Docker containerizer launch command may access a ""Container"" struct after it has been destroyed","Per a comment in this review: https://reviews.apache.org/r/51391/#review146735_x000D_
_x000D_
There are two places in the Docker containerizer where we copy a {{Container*}} by reference into a continuation:_x000D_
* https://github.com/apache/mesos/blob/71179708650642e145f75e250bf6d6b3eaabb6c5/src/slave/containerizer/docker.cpp#L1192_x000D_
* https://github.com/apache/mesos/blob/71179708650642e145f75e250bf6d6b3eaabb6c5/src/slave/containerizer/docker.cpp#L1283_x000D_
_x000D_
If the container is destroyed in the mean time, then we will potentially segfault here.",2.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.017543859649122806,0.007142857142857143,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.0
Task,Added a default (task group) executor.,"We would like to build a basic default pod executor that upon receiving a {{LAUNCH_GROUP}} event from the agent, sends a {{TASK_RUNNING}} status update. This would be a good building block for getting to a fully functional pod based default command executor.",3.0,0,0.0,0.5701357466063348,0.0,0.0,0.0,0.0,0.0,0.6842105263157894,0.5642857142857143,0.3846153846153846,0.4181818181818182,0.4121212121212121,0.0
Bug,Unable to launch containers on CNI networks on CoreOS,"CoreOS does not have an `/etc/hosts`. Currently, in the `network/cni` isolator, if we don't see a `/etc/hosts` on the host filesystem we don't bind mount the containers `hosts` file to this target for the `command executor`. On distros such as CoreOS this fails the container launch since the `libprocess` initialization of the `command executor` fails cause it can't resolve its `hostname`.

We should be creating the `/etc/hosts` and `/etc/hostname` files when they are absent on the host filesystem since creating these files should not affect name resolution on the host network namespace, and it will allow the `/etc/hosts` file to be bind mounted correctly and allow name resolution in the containers network namespace as well. ",1.0,1.0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2807017543859649,0.39285714285714285,0.1282051282051282,0.3090909090909091,0.3090909090909091,0.9174311926605504
Improvement,Add functions to the 'Launcher' abstraction to aid in checkpointing the exit status of containers.,"Currently, the launcher does not checkpoint the exit status of a container anywhere.  This will become especially important as we start to add nested container support. In preparation for adding this support, we should add a few helper functions to the Launcher to aid in determining the path where this exit status should be stored.",3.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Improvement,Add an agent flag for 'runtime_dir',"Currently, a number of agent components hard code a path
under '/var/run/mesos' in order to persist runtime information across
agent crashes. This path should be configurable.",2.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Improvement,Update elfio to version 3.2,Some patches I worked on got merged upstream and became part of elfio-3.2. We should upgrade our version bundled with Mesos to 3.2,1.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Task,unit-test for port-mapper CNI plugin,Write unit-tests for the port mapper plugin.,3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2807017543859649,0.39285714285714285,0.1282051282051282,0.3090909090909091,0.3090909090909091,0.0
Task,Introduce `PortMapping` protobuf.,Currently we have a `PortMapping` message defined for `DockerInfo`. This can be used only by the `DockerContainerizer`. We need to introduce a new Protobuf message in `NetworkInfo` which will allow frameworks to specify port mapping when using CNI with the `MesosContainerizer`.,1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2807017543859649,0.39285714285714285,0.1282051282051282,0.3090909090909091,0.3090909090909091,0.0
Task,Design for port-mapper CNI plugin,Create a design doc for port-mapper CNI plugin.,1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2807017543859649,0.39285714285714285,0.1282051282051282,0.3090909090909091,0.3090909090909091,0.0
Bug,Use readdir instead of readdir_r.,"{{readdir_r}} is deprecated in recent versions of glibc (https://sourceware.org/ml/libc-alpha/2016-02/msg00093.html). As a result, Mesos doesn't build on recent Arch Linux:

{noformat}
/bin/sh ../libtool  --tag=CXX   --mode=compile ccache g++ -DPACKAGE_NAME=\""mesos\"" -DPACKAGE_TARNAME=\""mesos\"" -DPACKAGE_VERSION=\""1.1.0\"" -DPACKAGE_STRING=\""mesos\ 1.1.0\"" -DPACKAGE_BUGREPORT=\""\"" -DPACKAGE_URL=\""\"" -DPACKAGE=\""mesos\"" -DVERSION=\""1.1.0\"" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_DLFCN_H=1 -DLT_OBJDIR=\"".libs/\"" -DHAVE_CXX11=1 -DHAVE_PTHREAD_PRIO_INHERIT=1 -DHAVE_PTHREAD=1 -DHAVE_LIBZ=1 -DHAVE_FTS_H=1 -DHAVE_APR_POOLS_H=1 -DHAVE_LIBAPR_1=1 -DHAVE_LIBCURL=1 -DMESOS_HAS_JAVA=1 -DHAVE_LIBSASL2=1 -DHAVE_SVN_VERSION_H=1 -DHAVE_LIBSVN_SUBR_1=1 -DHAVE_SVN_DELTA_H=1 -DHAVE_LIBSVN_DELTA_1=1 -DHAVE_LIBZ=1 -I. -I../../mesos/src   -Wall -Werror -Wsign-compare -DLIBDIR=\""/usr/local/lib\"" -DPKGLIBEXECDIR=\""/usr/local/libexec/mesos\"" -DPKGDATADIR=\""/usr/local/share/mesos\"" -DPKGMODULEDIR=\""/usr/local/lib/mesos/modules\"" -I../../mesos/include -I../include -I../include/mesos -DPICOJSON_USE_INT64 -D__STDC_FORMAT_MACROS -isystem ../3rdparty/boost-1.53.0 -I../3rdparty/elfio-3.1 -I../3rdparty/glog-0.3.3/src -I../3rdparty/leveldb-1.4/include -I../../mesos/3rdparty/libprocess/include -I../3rdparty/nvml-352.79 -I../3rdparty/picojson-1.3.0 -I../3rdparty/protobuf-2.6.1/src -I../../mesos/3rdparty/stout/include -I../3rdparty/zookeeper-3.4.8/src/c/include -I../3rdparty/zookeeper-3.4.8/src/c/generated -DHAS_AUTHENTICATION=1 -I/usr/include/subversion-1 -I/usr/include/apr-1 -I/usr/include/apr-1.0  -pthread -g1 -O0 -Wno-unused-local-typedefs -std=c++11 -MT appc/libmesos_no_3rdparty_la-spec.lo -MD -MP -MF appc/.deps/libmesos_no_3rdparty_la-spec.Tpo -c -o appc/libmesos_no_3rdparty_la-spec.lo `test -f 'appc/spec.cpp' || echo '../../mesos/src/'`appc/spec.cpp
libtool: compile:  ccache g++ -DPACKAGE_NAME=\""mesos\"" -DPACKAGE_TARNAME=\""mesos\"" -DPACKAGE_VERSION=\""1.1.0\"" ""-DPACKAGE_STRING=\""mesos 1.1.0\"""" -DPACKAGE_BUGREPORT=\""\"" -DPACKAGE_URL=\""\"" -DPACKAGE=\""mesos\"" -DVERSION=\""1.1.0\"" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_DLFCN_H=1 -DLT_OBJDIR=\"".libs/\"" -DHAVE_CXX11=1 -DHAVE_PTHREAD_PRIO_INHERIT=1 -DHAVE_PTHREAD=1 -DHAVE_LIBZ=1 -DHAVE_FTS_H=1 -DHAVE_APR_POOLS_H=1 -DHAVE_LIBAPR_1=1 -DHAVE_LIBCURL=1 -DMESOS_HAS_JAVA=1 -DHAVE_LIBSASL2=1 -DHAVE_SVN_VERSION_H=1 -DHAVE_LIBSVN_SUBR_1=1 -DHAVE_SVN_DELTA_H=1 -DHAVE_LIBSVN_DELTA_1=1 -DHAVE_LIBZ=1 -I. -I../../mesos/src -Wall -Werror -Wsign-compare -DLIBDIR=\""/usr/local/lib\"" -DPKGLIBEXECDIR=\""/usr/local/libexec/mesos\"" -DPKGDATADIR=\""/usr/local/share/mesos\"" -DPKGMODULEDIR=\""/usr/local/lib/mesos/modules\"" -I../../mesos/include -I../include -I../include/mesos -DPICOJSON_USE_INT64 -D__STDC_FORMAT_MACROS -isystem ../3rdparty/boost-1.53.0 -I../3rdparty/elfio-3.1 -I../3rdparty/glog-0.3.3/src -I../3rdparty/leveldb-1.4/include -I../../mesos/3rdparty/libprocess/include -I../3rdparty/nvml-352.79 -I../3rdparty/picojson-1.3.0 -I../3rdparty/protobuf-2.6.1/src -I../../mesos/3rdparty/stout/include -I../3rdparty/zookeeper-3.4.8/src/c/include -I../3rdparty/zookeeper-3.4.8/src/c/generated -DHAS_AUTHENTICATION=1 -I/usr/include/subversion-1 -I/usr/include/apr-1 -I/usr/include/apr-1.0 -pthread -g1 -O0 -Wno-unused-local-typedefs -std=c++11 -MT appc/libmesos_no_3rdparty_la-spec.lo -MD -MP -MF appc/.deps/libmesos_no_3rdparty_la-spec.Tpo -c ../../mesos/src/appc/spec.cpp  -fPIC -DPIC -o appc/.libs/libmesos_no_3rdparty_la-spec.o
In file included from ../../mesos/3rdparty/stout/include/stout/os.hpp:52:0,
                 from ../../mesos/src/appc/spec.cpp:17:
../../mesos/3rdparty/stout/include/stout/os/ls.hpp: In function ‘Try<std::__cxx11::list<std::__cxx11::basic_string<char> > > os::ls(const string&)’:
../../mesos/3rdparty/stout/include/stout/os/ls.hpp:56:19: error: ‘int readdir_r(DIR*, dirent*, dirent**)’ is deprecated [-Werror=deprecated-declarations]
   while ((error = readdir_r(dir, temp, &entry)) == 0 && entry != nullptr) {
                   ^~~~~~~~~
In file included from ../../mesos/3rdparty/stout/include/stout/os/ls.hpp:19:0,
                 from ../../mesos/3rdparty/stout/include/stout/os.hpp:52,
                 from ../../mesos/src/appc/spec.cpp:17:
/usr/include/dirent.h:183:12: note: declared here
 extern int readdir_r (DIR *__restrict __dirp,
            ^~~~~~~~~
In file included from ../../mesos/3rdparty/stout/include/stout/os.hpp:52:0,
                 from ../../mesos/src/appc/spec.cpp:17:
../../mesos/3rdparty/stout/include/stout/os/ls.hpp:56:46: error: ‘int readdir_r(DIR*, dirent*, dirent**)’ is deprecated [-Werror=deprecated-declarations]
   while ((error = readdir_r(dir, temp, &entry)) == 0 && entry != nullptr) {
                                              ^
In file included from ../../mesos/3rdparty/stout/include/stout/os/ls.hpp:19:0,
                 from ../../mesos/3rdparty/stout/include/stout/os.hpp:52,
                 from ../../mesos/src/appc/spec.cpp:17:
/usr/include/dirent.h:183:12: note: declared here
 extern int readdir_r (DIR *__restrict __dirp,
            ^~~~~~~~~
cc1plus: all warnings being treated as errors
{noformat}

Seems like {{readdir_r}} is deprecated; manpage suggests using {{readdir}} instead.",3.0,0,0.0,1.0,0.0,0.0,0.0,0.0,0.8,0.5438596491228069,0.39999999999999997,0.48717948717948717,0.4303030303030303,0.4303030303030303,0.0
Improvement,Abstract mesos-style.py to allow future linters to be added more easily.,"    Currently, mesos-style.py is just a collection of functions that
    check the style of relevant files in the mesos code base.  However,
    the script assumes that we always wanted to run cpplint over every
    file we are checking. Since we are planning on adding a python linter
    to the codebase soon, it makes sense to abstract the common
    functionality from this script into a class so that a cpp-based linter
    and a python-based linter can inherit the same set of common
    functionality.",2.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Bug,Protobuf JSON deserialisation does not accept numbers formated as strings,"Proto2 does not specify JSON mappings but [Proto3|https://developers.google.com/protocol-buffers/docs/proto3#json] does and it recommend to map 64bit numbers as a string. Unfortunately Mesos does not accepts strings in places of uint64 and return 400 Bad 
{quote}
Request error Failed to convert JSON into Call protobuf: Not expecting a JSON string for field 'value'.
{quote}
Is this by purpose or is this a bug?",1.0,1.0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.038461538461538464,0.01212121212121212,0.01212121212121212,0.9174311926605504
Bug,PollSocketImpl can write to a stale fd.,"When tracking down MESOS-5986 with [~greggomann] and [~anandmazumdar]. We were curious why PollSocketImpl avoids the same issue. It seems that PollSocketImpl has a similar race, however in the case of PollSocketImpl we will simply write to a stale file descriptor.

One example is {{PollSocketImpl::send(const char*, size_t)}}:

https://github.com/apache/mesos/blob/1.0.0/3rdparty/libprocess/src/poll_socket.cpp#L241-L245
{code}
Future<size_t> PollSocketImpl::send(const char* data, size_t size)
{
  return io::poll(get(), io::WRITE)
    .then(lambda::bind(&internal::socket_send_data, get(), data, size));
}

Future<size_t> socket_send_data(int s, const char* data, size_t size)
{
  CHECK(size > 0);

  while (true) {
    ssize_t length = send(s, data, size, MSG_NOSIGNAL);

#ifdef __WINDOWS__
    int error = WSAGetLastError();
#else
    int error = errno;
#endif // __WINDOWS__

    if (length < 0 && net::is_restartable_error(error)) {
      // Interrupted, try again now.
      continue;
    } else if (length < 0 && net::is_retryable_error(error)) {
      // Might block, try again later.
      return io::poll(s, io::WRITE)
        .then(lambda::bind(&internal::socket_send_data, s, data, size));
    } else if (length <= 0) {
      // Socket error or closed.
      if (length < 0) {
        const string error = os::strerror(errno);
        VLOG(1) << ""Socket error while sending: "" << error;
      } else {
        VLOG(1) << ""Socket closed while sending"";
      }
      if (length == 0) {
        return length;
      } else {
        return Failure(ErrnoError(""Socket send failed""));
      }
    } else {
      CHECK(length > 0);

      return length;
    }
  }
}
{code}

If the last reference to the {{Socket}} goes away before the {{socket_send_data}} loop completes, then we will write to a stale fd!

It turns out that we have avoided this issue because in libprocess we happen to keep a reference to the {{Socket}} around when sending:

https://github.com/apache/mesos/blob/1.0.0/3rdparty/libprocess/src/process.cpp#L1678-L1707
{code}
void send(Encoder* encoder, Socket socket)
{
  switch (encoder->kind()) {
    case Encoder::DATA: {
      size_t size;
      const char* data = static_cast<DataEncoder*>(encoder)->next(&size);
      socket.send(data, size)
        .onAny(lambda::bind(
            &internal::_send,
            lambda::_1,
            socket,
            encoder,
            size));
      break;
    }
    case Encoder::FILE: {
      off_t offset;
      size_t size;
      int fd = static_cast<FileEncoder*>(encoder)->next(&offset, &size);
      socket.sendfile(fd, offset, size)
        .onAny(lambda::bind(
            &internal::_send,
            lambda::_1,
            socket,
            encoder,
            size));
      break;
    }
  }
}
{code}

However, this may not be true in all call-sites going forward. Currently, it appears that http::Connection can trigger this bug.",3.0,0,0.5,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.0
Bug,SSL Socket CHECK can fail after socket receives EOF,"While writing a test for MESOS-3753, I encountered a bug where [this check|https://github.com/apache/mesos/blob/853821cafcca3550b9c7bdaba5262d73869e2ee1/3rdparty/libprocess/src/libevent_ssl_socket.cpp#L708] fails at the very end of the test body, while objects in the stack frame are being destroyed. After adding some debug logging output, I produced the following:
{code}
I0804 08:32:33.263211 273793024 libevent_ssl_socket.cpp:681] *** in send()17
I0804 08:32:33.263209 273256448 process.cpp:2970] Cleaning up __limiter__(3)@127.0.0.1:55688
I0804 08:32:33.263263 275939328 libevent_ssl_socket.cpp:152] *** in initialize(): 14
I0804 08:32:33.263206 272719872 process.cpp:2865] Resuming (61)@127.0.0.1:55688 at 2016-08-04 15:32:33.263261952+00:00
I0804 08:32:33.263327 275939328 libevent_ssl_socket.cpp:584] *** in recv()14
I0804 08:32:33.263337 272719872 hierarchical.cpp:571] Agent e2a49340-34ec-403f-a5a4-15e29c4a2434-S0 deactivated
I0804 08:32:33.263322 275402752 process.cpp:2865] Resuming help@127.0.0.1:55688 at 2016-08-04 15:32:33.263343104+00:00
I0804 08:32:33.263510 275939328 libevent_ssl_socket.cpp:322] *** in event_callback(bev)
I0804 08:32:33.263536 275939328 libevent_ssl_socket.cpp:353] *** in event_callback check for EOF/CONNECTED/ERROR: 19
I0804 08:32:33.263592 275939328 libevent_ssl_socket.cpp:159] *** in shutdown(): 19
I0804 08:32:33.263622 1985901312 process.cpp:3170] Donating thread to (87)@127.0.0.1:55688 while waiting
I0804 08:32:33.263639 274329600 process.cpp:2865] Resuming __http__(12)@127.0.0.1:55688 at 2016-08-04 15:32:33.263653888+00:00
I0804 08:32:33.263659 1985901312 process.cpp:2865] Resuming (87)@127.0.0.1:55688 at 2016-08-04 15:32:33.263671040+00:00
I0804 08:32:33.263730 1985901312 process.cpp:2970] Cleaning up (87)@127.0.0.1:55688
I0804 08:32:33.263741 275939328 libevent_ssl_socket.cpp:322] *** in event_callback(bev)
I0804 08:32:33.263736 274329600 process.cpp:2970] Cleaning up __http__(12)@127.0.0.1:55688
I0804 08:32:33.263778 275939328 libevent_ssl_socket.cpp:353] *** in event_callback check for EOF/CONNECTED/ERROR: 17
I0804 08:32:33.263818 275939328 libevent_ssl_socket.cpp:159] *** in shutdown(): 17
I0804 08:32:33.263839 272183296 process.cpp:2865] Resuming help@127.0.0.1:55688 at 2016-08-04 15:32:33.263857920+00:00
I0804 08:32:33.263933 273793024 process.cpp:2865] Resuming __gc__@127.0.0.1:55688 at 2016-08-04 15:32:33.263951104+00:00
I0804 08:32:33.264034 275939328 libevent_ssl_socket.cpp:681] *** in send()17
I0804 08:32:33.264020 272719872 process.cpp:2865] Resuming __http__(11)@127.0.0.1:55688 at 2016-08-04 15:32:33.264041984+00:00
I0804 08:32:33.264036 274329600 process.cpp:2865] Resuming status-update-manager(3)@127.0.0.1:55688 at 2016-08-04 15:32:33.264056064+00:00
I0804 08:32:33.264071 272719872 process.cpp:2970] Cleaning up __http__(11)@127.0.0.1:55688
I0804 08:32:33.264088 274329600 process.cpp:2970] Cleaning up status-update-manager(3)@127.0.0.1:55688
I0804 08:32:33.264086 275939328 libevent_ssl_socket.cpp:721] *** sending on socket: 17, data: 0

I0804 08:32:33.264112 272183296 process.cpp:2865] Resuming (89)@127.0.0.1:55688 at 2016-08-04 15:32:33.264126976+00:00
I0804 08:32:33.264118 275402752 process.cpp:2865] Resuming help@127.0.0.1:55688 at 2016-08-04 15:32:33.264144896+00:00
I0804 08:32:33.264149 272183296 process.cpp:2970] Cleaning up (89)@127.0.0.1:55688
I0804 08:32:33.264202 275939328 libevent_ssl_socket.cpp:281] *** in send_callback(bev)
I0804 08:32:33.264400 273793024 process.cpp:3170] Donating thread to (86)@127.0.0.1:55688 while waiting
I0804 08:32:33.264413 273256448 process.cpp:2865] Resuming (76)@127.0.0.1:55688 at 2016-08-04 15:32:33.264428032+00:00
I0804 08:32:33.296268 275939328 libevent_ssl_socket.cpp:300] *** in send_callback(): 17
I0804 08:32:33.296419 273256448 process.cpp:2970] Cleaning up (76)@127.0.0.1:55688
I0804 08:32:33.296357 273793024 process.cpp:2865] Resuming (86)@127.0.0.1:55688 at 2016-08-04 15:32:33.296414976+00:00
I0804 08:32:33.296464 273793024 process.cpp:2970] Cleaning up (86)@127.0.0.1:55688
I0804 08:32:33.296497 275939328 libevent_ssl_socket.cpp:104] *** releasing SSL socket
I0804 08:32:33.296517 275939328 libevent_ssl_socket.cpp:106] *** released SSL socket: 19
I0804 08:32:33.296515 274329600 process.cpp:2865] Resuming help@127.0.0.1:55688 at 2016-08-04 15:32:33.296532992+00:00
I0804 08:32:33.296550 275939328 libevent_ssl_socket.cpp:721] *** sending on socket: 17, data: 0

I0804 08:32:33.296583 273793024 process.cpp:2865] Resuming (77)@127.0.0.1:55688 at 2016-08-04 15:32:33.296616960+00:00
F0804 08:32:33.296623 275939328 libevent_ssl_socket.cpp:723] Check failed: 'self->send_request.get()' Must be non NULL
*** Check failure stack trace: ***
{code}

The {{in send()17}} line indicates the beginning of {{send()}} for the SSL socket using FD 17. {{in shutdown(): 17}} indicates the beginning of {{shutdown()}} for the same socket, while {{sending on socket: 17}} indicates the execution of the lambda from {{send()}} on the event loop. Since {{shutdown()}} was called in between the call to {{send()}} and the execution of its lambda, it looks like the {{Socket}} was destroyed before the lambda could run. It's unclear why this would happen, since {{send()}}'s lambda captures a shared copy of the socket's {{this}} pointer in order to keep it alive.",3.0,1.0.0,0.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.9174311926605504
Bug,NvidiaVolume errors out if any binary is missing,"We currently error out if a binary we were trying to add to the volume is not found on the host filesystem. However, these are not the semantics that we want. By design, we list all the binaries that *may* exist on the filesystem that we want to put in the volume, not all of the binaries that *must* exist. We should simply skip any unfound binaries and move on to the next one instead of erroring out.",2.0,1.0.0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.9174311926605504
Task,Remove HTTP_PARSER_VERSION_MAJOR < 2 code in decoder.,https://reviews.apache.org/r/50683,1.0,0,0.0,0.42533936651583715,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3142857142857143,0.24358974358974358,0.22424242424242424,0.22424242424242424,0.0
Bug,Linux 'MountInfoTable' entries not sorted as expected,"Many places in the codebase assume that the mountinfo table is sorted according to the order: {{parent mount point < child mount point}}.

However, in some cases this may not be true if (for example), a parent mount point (say {{/}}) is remounted to add some extra flags to it.  When this happens, the remounted file system will appear in the mount table at the point where it was remounted.

We actually encountered this problem in the wild for the case of {{/}} being remounted after {{/run}} was mounted -- causing problems in the {{NvidiaVolume}} which assumes the {{parent  < child}} ordering.",3.0,1.0.0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.9174311926605504
Task,Add libprocess HTTP tests with SSL support,Libprocess contains SSL unit tests which test our SSL support using simple sockets. We should add tests which also make use of libprocess's various HTTP classes and helpers in a variety of SSL configurations.,3.0,0,0.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Bug,All non-root tests fail on GPU machine,"A recent addition to ensure that {{NvidiaVolume::create()}} ran as root broke all non-root tests on GPU machines. The reason is that we unconditionally create this volume so long as we detect {{nvml.isAvailable()}} which will fail now that we are only allowed to create this volume if we have root permissions.

We should fix this by adding the proper conditions to determine when / if we should create this volume based on some combination of {{\-\-containerizer}} and {{\-\-isolation}} flags.",2.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Bug,Reviewbot failing due to python files not being cleaned up after distclean,"This is on ASF CI. https://builds.apache.org/job/mesos-reviewbot/14573/consoleFull

{code}
find python -name ""build"" -o -name ""dist"" -o -name ""*.pyc""	\
	  -o -name ""*.egg-info"" -exec rm -rf '{}' \+
test -z ""libmesos_no_3rdparty.la libbuild.la liblog.la libstate.la libjava.la libexamplemodule.la libtestallocator.la libtestanonymous.la libtestauthentication.la libtestauthorizer.la libtestcontainer_logger.la libtesthook.la libtesthttpauthenticator.la libtestisolator.la libtestmastercontender.la libtestmasterdetector.la libtestqos_controller.la libtestresource_estimator.la"" || rm -f libmesos_no_3rdparty.la libbuild.la liblog.la libstate.la libjava.la libexamplemodule.la libtestallocator.la libtestanonymous.la libtestauthentication.la libtestauthorizer.la libtestcontainer_logger.la libtesthook.la libtesthttpauthenticator.la libtestisolator.la libtestmastercontender.la libtestmasterdetector.la libtestqos_controller.la libtestresource_estimator.la
test -z ""liblogrotate_container_logger.la libfixed_resource_estimator.la libload_qos_controller.la "" || rm -f liblogrotate_container_logger.la libfixed_resource_estimator.la libload_qos_controller.la 
 rm -f mesos-fetcher mesos-executor mesos-containerizer mesos-logrotate-logger mesos-health-check mesos-usage mesos-docker-executor
 rm -f mesos-agent mesos-master mesos-slave
rm -f ./so_locations
rm -f *.o
rm -f *.lo
rm -f ../include/mesos/*.o
rm -f ./so_locations
rm -f *.tab.c
test -z """" || rm -f 
rm -f TAGS ID GTAGS GRTAGS GSYMS GPATH tags
rm -f ./so_locations
rm -f ../include/mesos/*.lo
test . = ""../../src"" || test -z """" || rm -f 
rm -f ../include/mesos/.deps/.dirstamp
rm -f ../include/mesos/agent/*.o
rm -f ../include/mesos/agent/*.lo
rm -f ../include/mesos/.dirstamp
rm -f ../include/mesos/allocator/*.o
rm -f ../include/mesos/agent/.deps/.dirstamp
rm -f ../include/mesos/allocator/*.lo
rm -f ../include/mesos/agent/.dirstamp
rm -f ../include/mesos/appc/*.o
rm -f ../include/mesos/allocator/.deps/.dirstamp
rm -f ../include/mesos/appc/*.lo
rm -f ../include/mesos/allocator/.dirstamp
rm -f ../include/mesos/authentication/*.o
rm -f ../include/mesos/appc/.deps/.dirstamp
rm -f ../include/mesos/authentication/*.lo
rm -f ../include/mesos/appc/.dirstamp
rm -f ../include/mesos/authorizer/*.o
rm -f ../include/mesos/authentication/.deps/.dirstamp
rm -f ../include/mesos/authorizer/*.lo
rm -f ../include/mesos/authentication/.dirstamp
rm -f ../include/mesos/containerizer/*.o
rm -f ../include/mesos/authorizer/.deps/.dirstamp
rm -f ../include/mesos/containerizer/*.lo
rm -f ../include/mesos/authorizer/.dirstamp
rm -f ../include/mesos/docker/*.o
rm -f ../include/mesos/containerizer/.deps/.dirstamp
rm -f ../include/mesos/docker/*.lo
rm: cannot remove 'python/cli/build': Is a directory
rm: cannot remove 'python/executor/build': Is a directory
rm: cannot remove 'python/interface/build': Is a directory
rm: cannot remove 'python/native/build': Is a directory
rm: cannot remove 'python/scheduler/build': Is a directory
rm -f ../include/mesos/containerizer/.dirstamp
make[2]: [clean-generic] Error 1 (ignored)
rm -f ../include/mesos/executor/*.o
rm -f ../include/mesos/docker/.deps/.dirstamp
rm -f ../include/mesos/executor/*.lo
rm -f ../include/mesos/docker/.dirstamp
rm -f ../include/mesos/fetcher/*.o
rm -f ../include/mesos/executor/.deps/.dirstamp
rm -f ../include/mesos/fetcher/*.lo
rm -f ../include/mesos/executor/.dirstamp
rm -f ../include/mesos/maintenance/*.o
rm -f ../include/mesos/fetcher/.deps/.dirstamp
rm -f ../include/mesos/maintenance/*.lo
rm -f ../include/mesos/fetcher/.dirstamp
rm -f ../include/mesos/master/*.o
rm -f ../include/mesos/maintenance/.deps/.dirstamp
rm -f ../include/mesos/master/*.lo
rm -f ../include/mesos/module/*.o
rm -f ../include/mesos/maintenance/.dirstamp
rm -f ../include/mesos/module/*.lo
rm -f ../include/mesos/master/.deps/.dirstamp
rm -f ../include/mesos/master/.dirstamp
rm -f ../include/mesos/quota/*.o
rm -f ../include/mesos/module/.deps/.dirstamp
rm -f ../include/mesos/quota/*.lo
rm -f ../include/mesos/module/.dirstamp
rm -f ../include/mesos/scheduler/*.o
rm -f ../include/mesos/quota/.deps/.dirstamp
rm -f ../include/mesos/scheduler/*.lo
rm -f ../include/mesos/quota/.dirstamp
rm -f ../include/mesos/slave/*.o
rm -f ../include/mesos/scheduler/.deps/.dirstamp
rm -f ../include/mesos/scheduler/.dirstamp
rm -f ../include/mesos/slave/*.lo
rm -f ../include/mesos/slave/.deps/.dirstamp
rm -f ../include/mesos/state/*.o
rm -f ../include/mesos/slave/.dirstamp
rm -f ../include/mesos/state/*.lo
rm -f ../include/mesos/state/.deps/.dirstamp
rm -f ../include/mesos/uri/*.o
rm -f ../include/mesos/state/.dirstamp
rm -f ../include/mesos/uri/*.lo
rm -f ../include/mesos/uri/.deps/.dirstamp
rm -f ../include/mesos/v1/*.o
rm -f ../include/mesos/uri/.dirstamp
rm -f ../include/mesos/v1/*.lo
rm -f ../include/mesos/v1/.deps/.dirstamp
rm -f ../include/mesos/v1/agent/*.o
rm -f ../include/mesos/v1/.dirstamp
rm -f ../include/mesos/v1/agent/*.lo
rm -f ../include/mesos/v1/agent/.deps/.dirstamp
rm -f ../include/mesos/v1/allocator/*.o
rm -f ../include/mesos/v1/agent/.dirstamp
rm -f ../include/mesos/v1/allocator/*.lo
rm -f ../include/mesos/v1/allocator/.deps/.dirstamp
rm -f examples/java/*.class
rm -f ../include/mesos/v1/executor/*.o
rm -f ../include/mesos/v1/allocator/.dirstamp
rm -f ../include/mesos/v1/executor/.deps/.dirstamp
rm -f ../include/mesos/v1/executor/*.lo
rm -f ../include/mesos/v1/executor/.dirstamp
rm -f java/jni/org_apache_mesos*.h
rm -f ../include/mesos/v1/maintenance/*.o
rm -f ../include/mesos/v1/maintenance/*.lo
rm -f ../include/mesos/v1/maintenance/.deps/.dirstamp
rm -f ../include/mesos/v1/master/*.o
rm -f ../include/mesos/v1/maintenance/.dirstamp
rm -f ../include/mesos/v1/master/*.lo
rm -f ../include/mesos/v1/master/.deps/.dirstamp
rm -f ../include/mesos/v1/master/.dirstamp
rm -f ../include/mesos/v1/quota/*.o
rm -f ../include/mesos/v1/quota/.deps/.dirstamp
rm -f ../include/mesos/v1/quota/*.lo
rm -f ../include/mesos/v1/quota/.dirstamp
rm -f ../include/mesos/v1/scheduler/*.o
rm -f ../include/mesos/v1/scheduler/*.lo
rm -f ../include/mesos/v1/scheduler/.deps/.dirstamp
rm -f appc/*.o
rm -f ../include/mesos/v1/scheduler/.dirstamp
rm -f appc/*.lo
rm -f authentication/cram_md5/*.o
rm -f appc/.deps/.dirstamp
rm -f authentication/cram_md5/*.lo
rm -f appc/.dirstamp
rm -f authentication/http/*.o
rm -f authentication/cram_md5/.deps/.dirstamp
rm -f authentication/http/*.lo
rm -f authentication/cram_md5/.dirstamp
rm -f authorizer/*.o
rm -f authorizer/*.lo
rm -f authentication/http/.deps/.dirstamp
rm -f authorizer/local/*.o
rm -f authentication/http/.dirstamp
rm -f authorizer/local/*.lo
rm -f authorizer/.deps/.dirstamp
rm -f cli/*.o
rm -f authorizer/.dirstamp
rm -f authorizer/local/.deps/.dirstamp
rm -f common/*.o
rm -f authorizer/local/.dirstamp
rm -f common/*.lo
rm -f cli/.deps/.dirstamp
rm -f docker/*.o
rm -f cli/.dirstamp
rm -f common/.deps/.dirstamp
rm -f docker/*.lo
rm -f common/.dirstamp
rm -f examples/*.o
rm -f docker/.deps/.dirstamp
rm -f docker/.dirstamp
rm -f examples/.deps/.dirstamp
rm -f examples/.dirstamp
rm -f examples/*.lo
rm -f exec/.deps/.dirstamp
rm -f exec/*.o
rm -f exec/.dirstamp
rm -f executor/.deps/.dirstamp
rm -f exec/*.lo
rm -f executor/.dirstamp
rm -f executor/*.o
rm -f files/.deps/.dirstamp
rm -f executor/*.lo
rm -f files/.dirstamp
rm -f files/*.o
rm -f hdfs/.deps/.dirstamp
rm -f files/*.lo
rm -f hdfs/.dirstamp
rm -f health-check/.deps/.dirstamp
rm -f health-check/.dirstamp
rm -f hdfs/*.o
rm -f hook/.deps/.dirstamp
rm -f hdfs/*.lo
rm -f hook/.dirstamp
rm -f health-check/*.o
rm -f internal/.deps/.dirstamp
rm -f health-check/*.lo
rm -f internal/.dirstamp
rm -f hook/*.o
rm -f java/jni/.deps/.dirstamp
rm -f hook/*.lo
rm -f java/jni/.dirstamp
rm -f jvm/.deps/.dirstamp
rm -f internal/*.o
rm -f jvm/.dirstamp
rm -f internal/*.lo
rm -f jvm/org/apache/.deps/.dirstamp
rm -f jvm/org/apache/.dirstamp
rm -f java/jni/*.o
rm -f launcher/.deps/.dirstamp
rm -f java/jni/*.lo
rm -f launcher/.dirstamp
rm -f jvm/*.o
rm -f launcher/posix/.deps/.dirstamp
rm -f jvm/*.lo
rm -f launcher/posix/.dirstamp
rm -f jvm/org/apache/*.o
rm -f linux/.deps/.dirstamp
rm -f jvm/org/apache/*.lo
rm -f linux/.dirstamp
rm -f launcher/*.o
rm -f linux/routing/.deps/.dirstamp
rm -f linux/routing/.dirstamp
rm -f linux/routing/diagnosis/.deps/.dirstamp
rm -f launcher/posix/*.o
rm -f linux/*.o
rm -f linux/routing/diagnosis/.dirstamp
rm -f linux/*.lo
rm -f linux/routing/filter/.deps/.dirstamp
rm -f linux/routing/*.o
rm -f linux/routing/filter/.dirstamp
rm -f linux/routing/*.lo
rm -f linux/routing/link/.deps/.dirstamp
rm -f linux/routing/link/.dirstamp
rm -f linux/routing/diagnosis/*.o
rm -f linux/routing/queueing/.deps/.dirstamp
rm -f linux/routing/diagnosis/*.lo
rm -f linux/routing/queueing/.dirstamp
rm -rf ../include/mesos/.libs ../include/mesos/_libs
rm -f linux/routing/filter/*.o
rm -f local/.deps/.dirstamp
rm -f linux/routing/filter/*.lo
rm -rf ../include/mesos/agent/.libs ../include/mesos/agent/_libs
rm -f local/.dirstamp
rm -f linux/routing/link/*.o
rm -rf ../include/mesos/allocator/.libs ../include/mesos/allocator/_libs
rm -f log/.deps/.dirstamp
rm -f linux/routing/link/*.lo
rm -rf ../include/mesos/appc/.libs ../include/mesos/appc/_libs
rm -f log/.dirstamp
rm -f linux/routing/queueing/*.o
rm -rf ../include/mesos/authentication/.libs ../include/mesos/authentication/_libs
rm -f log/tool/.deps/.dirstamp
rm -f linux/routing/queueing/*.lo
rm -rf ../include/mesos/authorizer/.libs ../include/mesos/authorizer/_libs
rm -f log/tool/.dirstamp
rm -f local/*.o
rm -rf ../include/mesos/containerizer/.libs ../include/mesos/containerizer/_libs
rm -f logging/.deps/.dirstamp
rm -rf ../include/mesos/docker/.libs ../include/mesos/docker/_libs
rm -f local/*.lo
rm -f logging/.dirstamp
rm -f log/*.o
rm -rf ../include/mesos/executor/.libs ../include/mesos/executor/_libs
rm -f master/.deps/.dirstamp
rm -f log/*.lo
rm -rf ../include/mesos/fetcher/.libs ../include/mesos/fetcher/_libs
rm -f master/.dirstamp
rm -f log/tool/*.o
rm -rf ../include/mesos/maintenance/.libs ../include/mesos/maintenance/_libs
rm -f master/allocator/.deps/.dirstamp
rm -f log/tool/*.lo
rm -rf ../include/mesos/master/.libs ../include/mesos/master/_libs
rm -f master/allocator/.dirstamp
rm -f logging/*.o
rm -f master/allocator/mesos/.deps/.dirstamp
rm -rf ../include/mesos/module/.libs ../include/mesos/module/_libs
rm -f logging/*.lo
rm -f master/allocator/mesos/.dirstamp
rm -rf ../include/mesos/quota/.libs ../include/mesos/quota/_libs
rm -f master/*.o
rm -f master/allocator/sorter/drf/.deps/.dirstamp
rm -rf ../include/mesos/scheduler/.libs ../include/mesos/scheduler/_libs
rm -f master/*.lo
rm -f master/allocator/sorter/drf/.dirstamp
rm -f master/contender/.deps/.dirstamp
rm -rf ../include/mesos/slave/.libs ../include/mesos/slave/_libs
rm -f master/allocator/*.o
rm -f master/contender/.dirstamp
rm -rf ../include/mesos/state/.libs ../include/mesos/state/_libs
rm -f master/allocator/*.lo
rm -f master/detector/.deps/.dirstamp
rm -rf ../include/mesos/uri/.libs ../include/mesos/uri/_libs
rm -f master/allocator/mesos/*.o
rm -f master/detector/.dirstamp
rm -rf ../include/mesos/v1/.libs ../include/mesos/v1/_libs
rm -f master/allocator/mesos/*.lo
rm -rf ../include/mesos/v1/agent/.libs ../include/mesos/v1/agent/_libs
rm -f messages/.deps/.dirstamp
rm -f master/allocator/sorter/drf/*.o
rm -f messages/.dirstamp
rm -rf ../include/mesos/v1/allocator/.libs ../include/mesos/v1/allocator/_libs
rm -f master/allocator/sorter/drf/*.lo
rm -f module/.deps/.dirstamp
rm -rf ../include/mesos/v1/executor/.libs ../include/mesos/v1/executor/_libs
rm -f master/contender/*.o
rm -rf ../include/mesos/v1/maintenance/.libs ../include/mesos/v1/maintenance/_libs
rm -f module/.dirstamp
rm -f master/contender/*.lo
rm -f sched/.deps/.dirstamp
rm -rf ../include/mesos/v1/master/.libs ../include/mesos/v1/master/_libs
rm -f master/detector/*.o
rm -rf ../include/mesos/v1/quota/.libs ../include/mesos/v1/quota/_libs
rm -f sched/.dirstamp
rm -f master/detector/*.lo
rm -rf ../include/mesos/v1/scheduler/.libs ../include/mesos/v1/scheduler/_libs
rm -f scheduler/.deps/.dirstamp
rm -f messages/*.o
rm -f scheduler/.dirstamp
rm -f messages/*.lo
rm -rf appc/.libs appc/_libs
rm -f slave/.deps/.dirstamp
rm -f module/*.o
rm -rf authentication/cram_md5/.libs authentication/cram_md5/_libs
rm -f slave/.dirstamp
rm -f module/*.lo
rm -f slave/container_loggers/.deps/.dirstamp
rm -f sched/*.o
rm -rf authentication/http/.libs authentication/http/_libs
rm -f slave/container_loggers/.dirstamp
rm -f sched/*.lo
rm -f slave/containerizer/.deps/.dirstamp
rm -rf authorizer/.libs authorizer/_libs
rm -f scheduler/*.o
rm -f slave/containerizer/.dirstamp
rm -rf authorizer/local/.libs authorizer/local/_libs
rm -f scheduler/*.lo
rm -f slave/containerizer/mesos/.deps/.dirstamp
rm -rf common/.libs common/_libs
rm -f slave/*.o
rm -f slave/containerizer/mesos/.dirstamp
rm -f slave/containerizer/mesos/isolators/appc/.deps/.dirstamp
rm -f slave/*.lo
rm -rf docker/.libs docker/_libs
rm -f slave/containerizer/mesos/isolators/appc/.dirstamp
rm -f slave/container_loggers/*.o
rm -f slave/containerizer/mesos/isolators/cgroups/.deps/.dirstamp
rm -rf examples/.libs examples/_libs
rm -f slave/container_loggers/*.lo
rm -f slave/containerizer/mesos/isolators/cgroups/.dirstamp
rm -f slave/containerizer/*.o
rm -f slave/containerizer/mesos/isolators/docker/.deps/.dirstamp
rm -rf exec/.libs exec/_libs
rm -f slave/containerizer/*.lo
rm -f slave/containerizer/mesos/isolators/docker/.dirstamp
rm -rf executor/.libs executor/_libs
rm -f slave/containerizer/mesos/isolators/docker/volume/.deps/.dirstamp
rm -f slave/containerizer/mesos/*.o
rm -f slave/containerizer/mesos/isolators/docker/volume/.dirstamp
rm -f slave/containerizer/mesos/*.lo
rm -f slave/containerizer/mesos/isolators/filesystem/.deps/.dirstamp
rm -rf files/.libs files/_libs
rm -f slave/containerizer/mesos/isolators/appc/*.o
rm -f slave/containerizer/mesos/isolators/filesystem/.dirstamp
rm -rf hdfs/.libs hdfs/_libs
rm -f slave/containerizer/mesos/isolators/gpu/.deps/.dirstamp
rm -f slave/containerizer/mesos/isolators/appc/*.lo
rm -f slave/containerizer/mesos/isolators/cgroups/*.o
rm -rf health-check/.libs health-check/_libs
rm -f slave/containerizer/mesos/isolators/gpu/.dirstamp
rm -f slave/containerizer/mesos/isolators/cgroups/*.lo
rm -rf hook/.libs hook/_libs
rm -f slave/containerizer/mesos/isolators/namespaces/.deps/.dirstamp
rm -f slave/containerizer/mesos/isolators/namespaces/.dirstamp
rm -f slave/containerizer/mesos/isolators/docker/*.o
rm -rf internal/.libs internal/_libs
rm -f slave/containerizer/mesos/isolators/network/.deps/.dirstamp
rm -f slave/containerizer/mesos/isolators/docker/*.lo
rm -rf java/jni/.libs java/jni/_libs
rm -f slave/containerizer/mesos/isolators/network/.dirstamp
rm -f slave/containerizer/mesos/isolators/docker/volume/*.o
rm -f slave/containerizer/mesos/isolators/network/cni/.deps/.dirstamp
rm -f slave/containerizer/mesos/isolators/docker/volume/*.lo
rm -f slave/containerizer/mesos/isolators/network/cni/.dirstamp
rm -rf jvm/.libs jvm/_libs
rm -f slave/containerizer/mesos/isolators/filesystem/*.o
rm -f slave/containerizer/mesos/isolators/posix/.deps/.dirstamp
rm -rf jvm/org/apache/.libs jvm/org/apache/_libs
rm -f slave/containerizer/mesos/isolators/filesystem/*.lo
rm -f slave/containerizer/mesos/isolators/posix/.dirstamp
rm -rf linux/.libs linux/_libs
rm -f slave/containerizer/mesos/isolators/gpu/*.o
rm -f slave/containerizer/mesos/isolators/xfs/.deps/.dirstamp
rm -f slave/containerizer/mesos/isolators/xfs/.dirstamp
rm -f slave/containerizer/mesos/isolators/gpu/*.lo
rm -rf linux/routing/.libs linux/routing/_libs
rm -f slave/containerizer/mesos/provisioner/.deps/.dirstamp
rm -f slave/containerizer/mesos/isolators/namespaces/*.o
rm -rf linux/routing/diagnosis/.libs linux/routing/diagnosis/_libs
rm -f slave/containerizer/mesos/provisioner/.dirstamp
rm -rf linux/routing/filter/.libs linux/routing/filter/_libs
rm -f slave/containerizer/mesos/isolators/namespaces/*.lo
rm -f slave/containerizer/mesos/provisioner/appc/.deps/.dirstamp
rm -rf linux/routing/link/.libs linux/routing/link/_libs
rm -f slave/containerizer/mesos/isolators/network/*.o
rm -f slave/containerizer/mesos/provisioner/appc/.dirstamp
rm -f slave/containerizer/mesos/isolators/network/*.lo
rm -f slave/containerizer/mesos/provisioner/backends/.deps/.dirstamp
rm -rf linux/routing/queueing/.libs linux/routing/queueing/_libs
rm -f slave/containerizer/mesos/isolators/network/cni/*.o
rm -f slave/containerizer/mesos/provisioner/backends/.dirstamp
rm -rf local/.libs local/_libs
rm -f slave/containerizer/mesos/provisioner/docker/.deps/.dirstamp
rm -f slave/containerizer/mesos/isolators/network/cni/*.lo
rm -rf log/.libs log/_libs
rm -f slave/containerizer/mesos/provisioner/docker/.dirstamp
rm -f slave/containerizer/mesos/isolators/posix/*.o
rm -f slave/qos_controllers/.deps/.dirstamp
rm -f slave/containerizer/mesos/isolators/posix/*.lo
rm -f slave/qos_controllers/.dirstamp
rm -f slave/containerizer/mesos/isolators/xfs/*.o
rm -f slave/resource_estimators/.deps/.dirstamp
rm -f slave/containerizer/mesos/isolators/xfs/*.lo
rm -f slave/resource_estimators/.dirstamp
rm -f slave/containerizer/mesos/provisioner/*.o
rm -f state/.deps/.dirstamp
rm -rf log/tool/.libs log/tool/_libs
rm -f state/.dirstamp
rm -f slave/containerizer/mesos/provisioner/*.lo
rm -f tests/.deps/.dirstamp
rm -rf logging/.libs logging/_libs
rm -f slave/containerizer/mesos/provisioner/appc/*.o
rm -f tests/.dirstamp
rm -rf master/.libs master/_libs
rm -f slave/containerizer/mesos/provisioner/appc/*.lo
rm -f tests/common/.deps/.dirstamp
rm -f slave/containerizer/mesos/provisioner/backends/*.o
rm -f tests/common/.dirstamp
rm -f slave/containerizer/mesos/provisioner/backends/*.lo
rm -f tests/containerizer/.deps/.dirstamp
rm -f slave/containerizer/mesos/provisioner/docker/*.o
rm -f tests/containerizer/.dirstamp
rm -f slave/containerizer/mesos/provisioner/docker/*.lo
rm -f uri/.deps/.dirstamp
rm -f uri/.dirstamp
rm -f slave/qos_controllers/*.o
rm -f uri/fetchers/.deps/.dirstamp
rm -f uri/fetchers/.dirstamp
rm -f slave/qos_controllers/*.lo
rm -f usage/.deps/.dirstamp
rm -f slave/resource_estimators/*.o
rm -rf master/allocator/.libs master/allocator/_libs
rm -f usage/.dirstamp
rm -f slave/resource_estimators/*.lo
rm -rf master/allocator/mesos/.libs master/allocator/mesos/_libs
rm -f state/*.o
rm -f v1/.deps/.dirstamp
rm -rf master/allocator/sorter/drf/.libs master/allocator/sorter/drf/_libs
rm -f state/*.lo
rm -f v1/.dirstamp
rm -f version/.deps/.dirstamp
rm -f tests/*.o
rm -rf master/contender/.libs master/contender/_libs
rm -f version/.dirstamp
rm -rf master/detector/.libs master/detector/_libs
rm -f watcher/.deps/.dirstamp
rm -f watcher/.dirstamp
rm -rf messages/.libs messages/_libs
rm -f zookeeper/.deps/.dirstamp
rm -rf module/.libs module/_libs
rm -f zookeeper/.dirstamp
rm -rf sched/.libs sched/_libs
rm -rf scheduler/.libs scheduler/_libs
rm -rf slave/.libs slave/_libs
rm -rf slave/container_loggers/.libs slave/container_loggers/_libs
rm -rf slave/containerizer/.libs slave/containerizer/_libs
rm -rf slave/containerizer/mesos/.libs slave/containerizer/mesos/_libs
rm -rf slave/containerizer/mesos/isolators/appc/.libs slave/containerizer/mesos/isolators/appc/_libs
rm -rf slave/containerizer/mesos/isolators/cgroups/.libs slave/containerizer/mesos/isolators/cgroups/_libs
rm -rf slave/containerizer/mesos/isolators/docker/.libs slave/containerizer/mesos/isolators/docker/_libs
rm -rf slave/containerizer/mesos/isolators/docker/volume/.libs slave/containerizer/mesos/isolators/docker/volume/_libs
rm -rf slave/containerizer/mesos/isolators/filesystem/.libs slave/containerizer/mesos/isolators/filesystem/_libs
rm -rf slave/containerizer/mesos/isolators/gpu/.libs slave/containerizer/mesos/isolators/gpu/_libs
rm -rf slave/containerizer/mesos/isolators/namespaces/.libs slave/containerizer/mesos/isolators/namespaces/_libs
rm -rf slave/containerizer/mesos/isolators/network/.libs slave/containerizer/mesos/isolators/network/_libs
rm -rf slave/containerizer/mesos/isolators/network/cni/.libs slave/containerizer/mesos/isolators/network/cni/_libs
rm -rf slave/containerizer/mesos/isolators/posix/.libs slave/containerizer/mesos/isolators/posix/_libs
rm -rf slave/containerizer/mesos/isolators/xfs/.libs slave/containerizer/mesos/isolators/xfs/_libs
rm -rf slave/containerizer/mesos/provisioner/.libs slave/containerizer/mesos/provisioner/_libs
rm -rf slave/containerizer/mesos/provisioner/appc/.libs slave/containerizer/mesos/provisioner/appc/_libs
rm -f tests/common/*.o
rm -rf slave/containerizer/mesos/provisioner/backends/.libs slave/containerizer/mesos/provisioner/backends/_libs
rm -f tests/containerizer/*.o
rm -rf slave/containerizer/mesos/provisioner/docker/.libs slave/containerizer/mesos/provisioner/docker/_libs
rm -rf slave/qos_controllers/.libs slave/qos_controllers/_libs
rm -rf slave/resource_estimators/.libs slave/resource_estimators/_libs
rm -rf state/.libs state/_libs
rm -rf uri/.libs uri/_libs
rm -rf uri/fetchers/.libs uri/fetchers/_libs
rm -rf usage/.libs usage/_libs
rm -f uri/*.o
rm -f uri/*.lo
rm -rf v1/.libs v1/_libs
rm -f uri/fetchers/*.o
rm -rf version/.libs version/_libs
rm -f uri/fetchers/*.lo
rm -rf watcher/.libs watcher/_libs
rm -f usage/*.o
rm -rf zookeeper/.libs zookeeper/_libs
rm -f usage/*.lo
rm -f v1/*.o
rm -f v1/*.lo
rm -f version/*.o
rm -f version/*.lo
rm -f watcher/*.o
rm -f watcher/*.lo
rm -f zookeeper/*.o
rm -f zookeeper/*.lo
rm -rf ../include/mesos/.deps ../include/mesos/agent/.deps ../include/mesos/allocator/.deps ../include/mesos/appc/.deps ../include/mesos/authentication/.deps ../include/mesos/authorizer/.deps ../include/mesos/containerizer/.deps ../include/mesos/docker/.deps ../include/mesos/executor/.deps ../include/mesos/fetcher/.deps ../include/mesos/maintenance/.deps ../include/mesos/master/.deps ../include/mesos/module/.deps ../include/mesos/quota/.deps ../include/mesos/scheduler/.deps ../include/mesos/slave/.deps ../include/mesos/state/.deps ../include/mesos/uri/.deps ../include/mesos/v1/.deps ../include/mesos/v1/agent/.deps ../include/mesos/v1/allocator/.deps ../include/mesos/v1/executor/.deps ../include/mesos/v1/maintenance/.deps ../include/mesos/v1/master/.deps ../include/mesos/v1/quota/.deps ../include/mesos/v1/scheduler/.deps appc/.deps authentication/cram_md5/.deps authentication/http/.deps authorizer/.deps authorizer/local/.deps cli/.deps common/.deps docker/.deps examples/.deps exec/.deps executor/.deps files/.deps hdfs/.deps health-check/.deps hook/.deps internal/.deps java/jni/.deps jvm/.deps jvm/org/apache/.deps launcher/.deps launcher/posix/.deps linux/.deps linux/routing/.deps linux/routing/diagnosis/.deps linux/routing/filter/.deps linux/routing/link/.deps linux/routing/queueing/.deps local/.deps log/.deps log/tool/.deps logging/.deps master/.deps master/allocator/.deps master/allocator/mesos/.deps master/allocator/sorter/drf/.deps master/contender/.deps master/detector/.deps messages/.deps module/.deps sched/.deps scheduler/.deps slave/.deps slave/container_loggers/.deps slave/containerizer/.deps slave/containerizer/mesos/.deps slave/containerizer/mesos/isolators/appc/.deps slave/containerizer/mesos/isolators/cgroups/.deps slave/containerizer/mesos/isolators/docker/.deps slave/containerizer/mesos/isolators/docker/volume/.deps slave/containerizer/mesos/isolators/filesystem/.deps slave/containerizer/mesos/isolators/gpu/.deps slave/containerizer/mesos/isolators/namespaces/.deps slave/containerizer/mesos/isolators/network/.deps slave/containerizer/mesos/isolators/network/cni/.deps slave/containerizer/mesos/isolators/posix/.deps slave/containerizer/mesos/isolators/xfs/.deps slave/containerizer/mesos/provisioner/.deps slave/containerizer/mesos/provisioner/appc/.deps slave/containerizer/mesos/provisioner/backends/.deps slave/containerizer/mesos/provisioner/docker/.deps slave/qos_controllers/.deps slave/resource_estimators/.deps state/.deps tests/.deps tests/common/.deps tests/containerizer/.deps uri/.deps uri/fetchers/.deps usage/.deps v1/.deps version/.deps watcher/.deps zookeeper/.deps
rm -f Makefile
make[2]: Leaving directory `/mesos/mesos-1.1.0/_build/src'
rm -f config.status config.cache config.log configure.lineno config.status.lineno
rm -f Makefile
ERROR: files left in build directory after distclean:
./src/python/executor/build/temp.linux-x86_64-2.7/src/mesos/executor/module.o
./src/python/executor/build/temp.linux-x86_64-2.7/src/mesos/executor/mesos_executor_driver_impl.o
./src/python/executor/build/temp.linux-x86_64-2.7/src/mesos/executor/proxy_executor.o
./src/python/executor/build/lib.linux-x86_64-2.7/mesos/executor/_executor.so
./src/python/executor/build/lib.linux-x86_64-2.7/mesos/executor/__init__.py
./src/python/executor/build/lib.linux-x86_64-2.7/mesos/__init__.py
./src/python/executor/ext_modules.pyc
./src/python/scheduler/build/temp.linux-x86_64-2.7/src/mesos/scheduler/module.o
./src/python/scheduler/build/temp.linux-x86_64-2.7/src/mesos/scheduler/mesos_scheduler_driver_impl.o
./src/python/scheduler/build/temp.linux-x86_64-2.7/src/mesos/scheduler/proxy_scheduler.o
./src/python/scheduler/build/lib.linux-x86_64-2.7/mesos/scheduler/_scheduler.so
./src/python/scheduler/build/lib.linux-x86_64-2.7/mesos/scheduler/__init__.py
./src/python/scheduler/build/lib.linux-x86_64-2.7/mesos/__init__.py
./src/python/scheduler/ext_modules.pyc
./src/python/build/lib.linux-x86_64-2.7/mesos/__init__.py
./src/python/cli/build/lib.linux-x86_64-2.7/mesos/http.py
./src/python/cli/build/lib.linux-x86_64-2.7/mesos/cli.py
./src/python/cli/build/lib.linux-x86_64-2.7/mesos/futures.py
./src/python/cli/build/lib.linux-x86_64-2.7/mesos/__init__.py
./src/python/interface/build/lib.linux-x86_64-2.7/mesos/__init__.py
./src/python/interface/build/lib.linux-x86_64-2.7/mesos/interface/containerizer_pb2.py
./src/python/interface/build/lib.linux-x86_64-2.7/mesos/interface/mesos_pb2.py
./src/python/interface/build/lib.linux-x86_64-2.7/mesos/interface/__init__.py
./src/python/native/build/lib.linux-x86_64-2.7/mesos/__init__.py
./src/python/native/build/lib.linux-x86_64-2.7/mesos/native/__init__.py
make[1]: Leaving directory `/mesos/mesos-1.1.0/_build'
make[1]: *** [distcleancheck] Error 1
make: *** [distcheck] Error 1
+ docker rmi mesos-1470073345-14436

{code}",2.0,1.1.0,0.5,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.8846153846153846,0.9878787878787878,0.9878787878787878,0.926605504587156
Bug,NvidiaVolume::create() should check for root before creating volume,"Without root, we cannot create the nvidia volume in {{/var/run/mesos}} or mount a {{tmpfs}} in cases where we need to override the {{noexec}} on the current file system.",2.0,1.0.0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.9174311926605504
Improvement,Remove `O_SYNC` from StatusUpdateManager logs,"Currently the {{StatusUpdateManager}} uses {{O_SYNC}} to flush status updates to disk. 

We don't need to use {{O_SYNC}} because we only read this file if the host did not crash. {{os::write}} success implies the kernel will have flushed our data to the page cache. This is sufficient for the recovery scenarios we use this data for.",1.0,0,0.0,0.42533936651583715,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3142857142857143,0.24358974358974358,0.22424242424242424,0.22424242424242424,0.0
Bug,Incremental http parsing of URLs leads to decoder error,"When requests arrive to the decoder in pieces (e.g. {{mes}} followed by a separate chunk of {{os.apache.org}}) the http parser is not able to handle this case if the split is within the URL component.

This causes the decoder to error out, and can lead to connection invalidation.

The scheduler driver is susceptible to this.",3.0,1.0.0,0.0,0.42533936651583715,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3142857142857143,0.24358974358974358,0.22424242424242424,0.22424242424242424,0.9174311926605504
Bug,Orphan tasks can show up as running after they have finished.,"On my cluster I have 111 Orphan Tasks of which some are RUNNING some are FINISHED and some are FAILED. When I open the task details for a FINISHED tasks the following page shows a state of TASK_FINISHED and likewise when I open a FAILED task the details page shows TASK_FAILED.

However when I open the details for the RUNNING tasks they all have a task state of TASK_FINISHED. None of them is in state TASK_RUNNING.
",3.0,1.0.0,0.5,0.5701357466063348,0.0,0.0,0.0,0.0,0.0,0.6842105263157894,0.5642857142857143,0.05128205128205128,0.01818181818181818,0.01818181818181818,0.9174311926605504
Bug,Agent's '--version' flag doesn't work,"With the removal of the agent's default {{work_dir}}, the {{--version}} flag no longer works. Instead, the agent complains about the lack of a {{work_dir}} and prints the usage instructions.",1.0,1.0.0,0.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.9174311926605504
Bug,"Ubuntu 14.04 LTS GPU Isolator ""/run"" directory is noexec","In Ubuntu 14.04 LTS the mount for /run directory is noexec.  It affect the {{/var/run/mesos/isolators/gpu/nvidia_352.63/bin}} directory which mesos GPU isolators depended on.

{{bill@billz:/var/run$ mount | grep noexec
proc on /proc type proc (rw,noexec,nosuid,nodev)
sysfs on /sys type sysfs (rw,noexec,nosuid,nodev)
devpts on /dev/pts type devpts (rw,noexec,nosuid,gid=5,mode=0620)
tmpfs on /run type tmpfs (rw,noexec,nosuid,size=10%,mode=0755)}}

The /var/run is link to /run:
{{bill@billz:/var$ ll
total 52
drwxr-xr-x 13 root root     4096 May  5 20:00 ./
drwxr-xr-x 27 root root     4096 Jul 14 17:29 ../
lrwxrwxrwx  1 root root        9 May  5 19:50 lock -> /run/lock/
drwxrwxr-x 19 root syslog   4096 Jul 28 08:00 log/
drwxr-xr-x  2 root root     4096 Aug  4  2015 opt/
lrwxrwxrwx  1 root root        4 May  5 19:50 run -> /run/}}

Current the work around is mount without noexec:
{{sudo mount -o remount,exec /run}}",3.0,1.0.0,0.5,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.01282051282051282,0.0,0.0,0.9174311926605504
Bug,"Stout ""OsTest.User"" test can fail on some systems","Libc call {{getgrouplist}} doesn't return the {{gid}} list in a sorted manner (in my case, it's returning ""471 100"") ... whereas {{id -G}} return a sorted list (""100 471"" in my case) causing the validation inside the loop to fail.

We should sort both lists before comparing the values.",2.0,0,0.5,0.0,0.6666666666666666,1.0,0.2857142857142857,0.0,0.0,0.017543859649122806,0.007142857142857143,0.28205128205128205,0.29090909090909095,0.28484848484848485,0.0
Bug,ExamplesTest.DiskFullFramework fails on Arch,"This test fails consistently on recent Arch linux, running in a VM.",1.0,0,0.5,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.48717948717948717,0.4303030303030303,0.4303030303030303,0.0
Bug,Process routes implementation seems to drop routes on Windows.,"In several libprocess tests, the routes set up by process.cpp seem to be getting mangled/dropped on Windows. Specifically:_x000D_
_x000D_
* HTTPTest.Endpoints help has the route '/help/(14)', but '/help/(14)/body' fails; on HTTPTest.EndpointsHelpRemoval, the former can be removed, but when the latter is attempted, it fails._x000D_
* HTTPTest.NestedGet will properly generate the route '/a/b/c', but '/a/b' is missing._x000D_
* ProcessTest.FirewallDisabledPaths, FIrewallUninstall seem to fail to create firewall rules.",1.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.8,0.19298245614035087,0.09999999999999999,0.11538461538461538,0.05454545454545455,0.16969696969696968,0.0
Improvement,Make the command executor unversioned,"Currently, the command executor in {{src/launcher/executor.cpp}} is in the {{v1}} namespace. As referenced in the versioning design doc, we had agreed to keep the mesos internal code in the unversioned namespace and use {{evolve/devolve}} helpers for requests/responses. 

Following this pattern, we should bring the command executor in the {{mesos::internal}} namespace.",2.0,0,0.0,0.5701357466063348,0.0,0.0,0.0,0.0,0.0,0.6842105263157894,0.5642857142857143,0.3846153846153846,0.4181818181818182,0.4121212121212121,0.0
Bug,/help endpoint does not set Content-Type to HTML,"This change added a default {{Content-Type}} to all responses:
https://github.com/apache/mesos/commit/b2c5d91addbae609af3791f128c53fb3a26c7d53

Unfortunately, this changed the {{/help}} endpoint from no {{Content-Type}} to {{text/plain}}.  For a browser to render this page correctly, we need an HTML content type.",1.0,1.0.0,0.0,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.9174311926605504
Bug,`os::cloexec` does not exist on Windows,"`os::cloexec` does not work on Windows. It will never work at the OS level. Because of this, there are likely many important and hard-to-detect bugs hanging around the agent._x000D_
_x000D_
This is extremely important to fix. Some possible solutions to investigate (some of which are _extremely_ risky):_x000D_
_x000D_
* Abstract out file descriptors into a class, implement cloexec in that class on Windows (since we can't rely on the OS to do it)._x000D_
* Refactor all the code that relies on `os::cloexec` to not rely on it._x000D_
_x000D_
Of the two, the first seems less risky in the short term, because the cloexec code only affects Windows. Depending on the semantics of the implementation of the `FileDescriptor` class, it is possible that this is riskier to Windows in the longer term, as the semantics of `cloexec` may have subtle difference between Linux and Windows.",2.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.8,0.19298245614035087,0.09999999999999999,0.11538461538461538,0.05454545454545455,0.16969696969696968,0.0
Bug,cgroups/net_cls isolator causing agent recovery issues,"We run with 'cgroups/net_cls' in our isolator list, and when we restart any agent process in a cluster running an experimental custom isolator as well, the agents are unable to recover from checkpoint, because net_cls reports that unknown orphan containers have duplicate net_cls handles.

While this is a problem that needs to be solved (probably by fixing our custom isolator), it's also a problem that the net_cls isolator fails recovery just for duplicate handles in cgroups that it is literally about to unconditionally destroy during recovery. Can this be fixed?",1.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.2807017543859649,0.39285714285714285,0.02564102564102564,0.006060606060606061,0.006060606060606061,0.0
Bug,Strict/RegistrarTest.UpdateQuota/0 is flaky,"Observed on ASF CI (https://builds.apache.org/job/Mesos/BUILDTOOL=autotools,COMPILER=clang,CONFIGURATION=--verbose%20--enable-libevent%20--enable-ssl,ENVIRONMENT=GLOG_v=1%20MESOS_VERBOSE=1,OS=ubuntu:14.04,label_exp=(docker%7C%7CHadoop)&&(!ubuntu-us1)&&(!ubuntu-6)/2539/consoleFull). Log file is attached. Note that this might have been uncovered due to the recent removal of {{os::sleep}} from {{Clock::settle}}.",3.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.48717948717948717,0.4303030303030303,0.4303030303030303,0.0
Bug,Logrotate ContainerLogger module does not rotate logs when run as root with `--switch_user`.,"The logrotate ContainerLogger module runs as the agent's user.  In most cases, this is {{root}}.

When {{logrotate}} is run as root, there is an additional check the configuration files must pass (because a root {{logrotate}} needs to be secured against non-root modifications to the configuration):
https://github.com/logrotate/logrotate/blob/fe80cb51a2571ca35b1a7c8ba0695db5a68feaba/config.c#L807-L815

Log rotation will fail under the following scenario:
1) The agent is run with {{--switch_user}} (default: true)
2) A task is launched with a non-root user specified
3) The logrotate module spawns a few companion processes (as root) and this creates the {{stdout}}, {{stderr}}, {{stdout.logrotate.conf}}, and {{stderr.logrotate.conf}} files (as root).  This step races with the next step.
4) The Mesos containerizer and Fetcher will {{chown}} the task's sandbox to the non-root user.  Including the files just created.
5) When {{logrotate}} is run, it will skip any non-root configuration files.  This means the files are not rotated.

----

Fix: The logrotate module's companion processes should call {{setuid}} and {{setgid}}.",3.0,"0.27.0,0.28.0,1.0.0",0.5,0.004524886877828055,0.0,0.0,0.0,0.0,0.0,0.017543859649122806,0.007142857142857143,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.4740061162079511
Task,Create a 'Disk (not) full' example framework,"We need example frameworks for verifying the correct behavior of posix/disk isolator when the disk quota enforcement is in place. One framework for verifying that disk quota enforcement is working and that container gets terminated when it goes beyond disk quota, and another one for verifying that container does not get killed if it stays within its disk quota bounds. 
",3.0,0,0.0,0.07088989441930618,0.0,0.0,0.0,0.0,0.2,0.14035087719298245,0.10714285714285714,0.11538461538461538,0.11515151515151516,0.11515151515151516,0.0
Bug,CMake build needs to generate protobufs before building libmesos,"The existing CMake lists place protobufs at the same level as other Mesos sources:
https://github.com/apache/mesos/blob/c4cecf9c279c5206faaf996fef0b1810b490b329/src/CMakeLists.txt#L415

This is incorrect, as protobuf changes need to be regenerated before we can build against them.

Note: in the autotools build, this is done by compiling protobufs into {{libmesos}}, which then builds {{libmesos_no_3rdparty}}:
https://github.com/apache/mesos/blob/c4cecf9c279c5206faaf996fef0b1810b490b329/src/Makefile.am#L1304-L1305",2.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.017543859649122806,0.014285714285714285,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.0
Task,Add a test that runs the 'mesos-local' binary,"The balloon framework test runs the Mesos master and agent binaries, but we don't seem to have any tests which run the {{mesos-local}} binary at the moment. Such a test should be added, or one of the existing example framework tests could be modified to accomplish this.",2.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Bug,Docker health checks are malformed.,"When wrapping the health check command into {{docker exec}}, docker executor erroneously forms the health check command itself. Here is an excerpt from an executor log:
{noformat}
Launching health check process: docker exec mesos-2070f452-2120-45ad-a8d2-a339d234da41-S0.c27d1b78-d4aa-424b-91fa-1e91576db9b5 sh -c "" true "" /opt/mesosphere/packages/mesos--59d45b30116143cb8d9995ca26f9dec5e93dc710/libexec/mesos/mesos-health-check --executor=(1)@10.0.1.41:40651 --health_check_json={""command"":{""shell"":true,""value"":""docker exec mesos-2070f452-2120-45ad-a8d2-a339d234da41-S0.c27d1b78-d4aa-424b-91fa-1e91576db9b5 sh -c \"" true \""""},""consecutive_failures"":1,""delay_seconds"":0.0,""grace_period_seconds"":10.0,""interval_seconds"":5.0,""timeout_seconds"":10.0} --task_id=testhc.db69c60b-4a75-11e6-b9b0-c254ada9b06d
{noformat}",1.0,1.0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.717948717948718,0.8242424242424242,0.7212121212121212,0.9174311926605504
Bug,The fetcher can access any local file as root,"The Mesos fetcher currently runs as root and does a blind cp+chown of any file:// URI into the task's sandbox, to be owned by the task user. Even if frameworks are restricted from running tasks as root, it seems they can still access root-protected files in this way. We should secure the fetcher so that it has the filesystem permissions of the user its associated task is being run as. One option would be to run the fetcher as the same user that the task will run as.",3.0,0,0.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Bug,PersistentVolumeEndpointsTest.OfferCreateThenEndpointRemove test is flaky,"Observed on ASF CI: https://builds.apache.org/job/Mesos/BUILDTOOL=autotools,COMPILER=gcc,CONFIGURATION=--verbose,ENVIRONMENT=GLOG_v=1%20MESOS_VERBOSE=1,OS=ubuntu%3A14.04,label_exp=(docker%7C%7CHadoop)&&(!ubuntu-us1)&&(!ubuntu-6)/2497/changes

{code}
[ RUN      ] PersistentVolumeEndpointsTest.OfferCreateThenEndpointRemove
I0713 18:43:55.968503 28220 cluster.cpp:155] Creating default 'local' authorizer
I0713 18:43:56.082345 28220 leveldb.cpp:174] Opened db in 113.403661ms
I0713 18:43:56.131445 28220 leveldb.cpp:181] Compacted db in 49.034774ms
I0713 18:43:56.131533 28220 leveldb.cpp:196] Created db iterator in 28012ns
I0713 18:43:56.131552 28220 leveldb.cpp:202] Seeked to beginning of db in 3046ns
I0713 18:43:56.131564 28220 leveldb.cpp:271] Iterated through 0 keys in the db in 255ns
I0713 18:43:56.131614 28220 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0713 18:43:56.134064 28246 recover.cpp:451] Starting replica recovery
I0713 18:43:56.134627 28246 recover.cpp:477] Replica is in EMPTY status
I0713 18:43:56.136396 28252 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (9553)@172.17.0.8:35418
I0713 18:43:56.136759 28252 recover.cpp:197] Received a recover response from a replica in EMPTY status
I0713 18:43:56.137676 28246 recover.cpp:568] Updating replica status to STARTING
I0713 18:43:56.148720 28242 master.cpp:382] Master 2258d072-b0c9-4c40-874c-6cf933ee345a (500c3e866abe) started on 172.17.0.8:35418
I0713 18:43:56.148759 28242 master.cpp:384] Flags at startup: --acls="""" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""50ms"" --allocator=""HierarchicalDRF"" --authenticate_agents=""true"" --authenticate_frameworks=""true"" --authenticate_http=""true"" --authenticate_http_frameworks=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/LrwRl4/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --roles=""role1"" --root_submissions=""true"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-1.1.0/_inst/share/mesos/webui"" --work_dir=""/tmp/LrwRl4/master"" --zk_session_timeout=""10secs""
I0713 18:43:56.149247 28242 master.cpp:434] Master only allowing authenticated frameworks to register
I0713 18:43:56.149265 28242 master.cpp:448] Master only allowing authenticated agents to register
I0713 18:43:56.149273 28242 master.cpp:461] Master only allowing authenticated HTTP frameworks to register
I0713 18:43:56.149283 28242 credentials.hpp:37] Loading credentials for authentication from '/tmp/LrwRl4/credentials'
I0713 18:43:56.149780 28242 master.cpp:506] Using default 'crammd5' authenticator
I0713 18:43:56.149940 28242 master.cpp:578] Using default 'basic' HTTP authenticator
I0713 18:43:56.150091 28242 master.cpp:658] Using default 'basic' HTTP framework authenticator
I0713 18:43:56.150209 28242 master.cpp:705] Authorization enabled
W0713 18:43:56.150233 28242 master.cpp:768] The '--roles' flag is deprecated. This flag will be removed in the future. See the Mesos 0.27 upgrade notes for more information
I0713 18:43:56.150760 28240 hierarchical.cpp:151] Initialized hierarchical allocator process
I0713 18:43:56.151018 28249 whitelist_watcher.cpp:77] No whitelist given
I0713 18:43:56.155668 28242 master.cpp:1973] The newly elected leader is master@172.17.0.8:35418 with id 2258d072-b0c9-4c40-874c-6cf933ee345a
I0713 18:43:56.155781 28242 master.cpp:1986] Elected as the leading master!
I0713 18:43:56.155848 28242 master.cpp:1673] Recovering from registrar
I0713 18:43:56.156065 28254 registrar.cpp:332] Recovering registrar
I0713 18:43:56.201568 28245 hierarchical.cpp:1537] No allocations performed
I0713 18:43:56.201666 28245 hierarchical.cpp:1172] Performed allocation for 0 agents in 167962ns
I0713 18:43:56.218626 28246 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 80.746657ms
I0713 18:43:56.218705 28246 replica.cpp:320] Persisted replica status to STARTING
I0713 18:43:56.219219 28246 recover.cpp:477] Replica is in STARTING status
I0713 18:43:56.221391 28246 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (9556)@172.17.0.8:35418
I0713 18:43:56.221869 28253 recover.cpp:197] Received a recover response from a replica in STARTING status
I0713 18:43:56.222760 28249 recover.cpp:568] Updating replica status to VOTING
I0713 18:43:56.252303 28254 hierarchical.cpp:1537] No allocations performed
I0713 18:43:56.252404 28254 hierarchical.cpp:1172] Performed allocation for 0 agents in 167038ns
I0713 18:43:56.270256 28249 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 47.316392ms
I0713 18:43:56.270387 28249 replica.cpp:320] Persisted replica status to VOTING
I0713 18:43:56.270700 28250 recover.cpp:582] Successfully joined the Paxos group
I0713 18:43:56.271121 28250 recover.cpp:466] Recover process terminated
I0713 18:43:56.271503 28248 log.cpp:553] Attempting to start the writer
I0713 18:43:56.273140 28240 replica.cpp:493] Replica received implicit promise request from (9557)@172.17.0.8:35418 with proposal 1
I0713 18:43:56.303086 28254 hierarchical.cpp:1537] No allocations performed
I0713 18:43:56.303175 28254 hierarchical.cpp:1172] Performed allocation for 0 agents in 155905ns
I0713 18:43:56.312978 28240 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 39.718643ms
I0713 18:43:56.313405 28240 replica.cpp:342] Persisted promised to 1
I0713 18:43:56.314775 28245 coordinator.cpp:238] Coordinator attempting to fill missing positions
I0713 18:43:56.316547 28250 replica.cpp:388] Replica received explicit promise request from (9558)@172.17.0.8:35418 for position 0 with proposal 2
I0713 18:43:56.354794 28239 hierarchical.cpp:1537] No allocations performed
I0713 18:43:56.354898 28239 hierarchical.cpp:1172] Performed allocation for 0 agents in 178033ns
I0713 18:43:56.363484 28250 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 46.846904ms
I0713 18:43:56.363585 28250 replica.cpp:712] Persisted action at 0
I0713 18:43:56.365622 28250 replica.cpp:537] Replica received write request for position 0 from (9559)@172.17.0.8:35418
I0713 18:43:56.365727 28250 leveldb.cpp:436] Reading position from leveldb took 45172ns
I0713 18:43:56.406314 28252 hierarchical.cpp:1537] No allocations performed
I0713 18:43:56.406421 28252 hierarchical.cpp:1172] Performed allocation for 0 agents in 177001ns
I0713 18:43:56.421867 28250 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 56.06514ms
I0713 18:43:56.421968 28250 replica.cpp:712] Persisted action at 0
I0713 18:43:56.423286 28254 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I0713 18:43:56.458665 28250 hierarchical.cpp:1537] No allocations performed
I0713 18:43:56.458799 28250 hierarchical.cpp:1172] Performed allocation for 0 agents in 250863ns
I0713 18:43:56.470486 28254 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 47.13918ms
I0713 18:43:56.470552 28254 replica.cpp:712] Persisted action at 0
I0713 18:43:56.470584 28254 replica.cpp:697] Replica learned NOP action at position 0
I0713 18:43:56.471782 28247 log.cpp:569] Writer started with ending position 0
I0713 18:43:56.475908 28253 leveldb.cpp:436] Reading position from leveldb took 79764ns
I0713 18:43:56.479058 28247 registrar.cpp:365] Successfully fetched the registry (0B) in 322.939904ms
I0713 18:43:56.479388 28247 registrar.cpp:464] Applied 1 operations in 50643ns; attempting to update the 'registry'
I0713 18:43:56.483093 28247 log.cpp:577] Attempting to append 168 bytes to the log
I0713 18:43:56.483269 28249 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I0713 18:43:56.484673 28245 replica.cpp:537] Replica received write request for position 1 from (9560)@172.17.0.8:35418
I0713 18:43:56.509866 28239 hierarchical.cpp:1537] No allocations performed
I0713 18:43:56.509959 28239 hierarchical.cpp:1172] Performed allocation for 0 agents in 157789ns
I0713 18:43:56.512147 28245 leveldb.cpp:341] Persisting action (187 bytes) to leveldb took 27.358967ms
I0713 18:43:56.512193 28245 replica.cpp:712] Persisted action at 1
I0713 18:43:56.513278 28250 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I0713 18:43:56.537894 28250 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 24.568093ms
I0713 18:43:56.537973 28250 replica.cpp:712] Persisted action at 1
I0713 18:43:56.538008 28250 replica.cpp:697] Replica learned APPEND action at position 1
I0713 18:43:56.539737 28252 registrar.cpp:509] Successfully updated the 'registry' in 60.26496ms
I0713 18:43:56.539949 28252 registrar.cpp:395] Successfully recovered registrar
I0713 18:43:56.540544 28252 master.cpp:1781] Recovered 0 agents from the Registry (129B) ; allowing 10mins for agents to re-register
I0713 18:43:56.540832 28250 hierarchical.cpp:178] Skipping recovery of hierarchical allocator: nothing to recover
I0713 18:43:56.541285 28251 log.cpp:596] Attempting to truncate the log to 1
I0713 18:43:56.541637 28248 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I0713 18:43:56.542763 28240 replica.cpp:537] Replica received write request for position 2 from (9561)@172.17.0.8:35418
I0713 18:43:56.571691 28240 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 28.798341ms
I0713 18:43:56.571889 28240 replica.cpp:712] Persisted action at 2
I0713 18:43:56.573218 28240 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I0713 18:43:56.620200 28240 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 46.927607ms
I0713 18:43:56.620338 28240 leveldb.cpp:399] Deleting ~1 keys from leveldb took 59898ns
I0713 18:43:56.620512 28240 replica.cpp:712] Persisted action at 2
I0713 18:43:56.620630 28240 replica.cpp:697] Replica learned TRUNCATE action at position 2
I0713 18:43:56.624091 28249 hierarchical.cpp:1537] No allocations performed
I0713 18:43:56.624169 28249 hierarchical.cpp:1172] Performed allocation for 0 agents in 140818ns
I0713 18:43:56.628180 28220 containerizer.cpp:196] Using isolation: posix/cpu,posix/mem,filesystem/posix,network/cni
W0713 18:43:56.629341 28220 backend.cpp:75] Failed to create 'aufs' backend: AufsBackend requires root privileges, but is running as user mesos
W0713 18:43:56.629616 28220 backend.cpp:75] Failed to create 'bind' backend: BindBackend requires root privileges
I0713 18:43:56.631988 28220 cluster.cpp:432] Creating default 'local' authorizer
I0713 18:43:56.635001 28243 slave.cpp:205] Agent started on 251)@172.17.0.8:35418
I0713 18:43:56.635308 28220 resources.cpp:572] Parsing resources as JSON failed: disk:512
Trying semicolon-delimited string format instead
I0713 18:43:56.635026 28243 slave.cpp:206] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""true"" --authenticatee=""crammd5"" --authentication_backoff_factor=""1secs"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/PersistentVolumeEndpointsTest_OfferCreateThenEndpointRemove_gqStXQ/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/PersistentVolumeEndpointsTest_OfferCreateThenEndpointRemove_gqStXQ/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_command_executor=""false"" --http_credentials=""/tmp/PersistentVolumeEndpointsTest_OfferCreateThenEndpointRemove_gqStXQ/http_credentials"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-1.1.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""disk(*):1024"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/PersistentVolumeEndpointsTest_OfferCreateThenEndpointRemove_gqStXQ""
I0713 18:43:56.635709 28243 credentials.hpp:86] Loading credential for authentication from '/tmp/PersistentVolumeEndpointsTest_OfferCreateThenEndpointRemove_gqStXQ/credential'
I0713 18:43:56.635892 28243 slave.cpp:343] Agent using credential for: test-principal
I0713 18:43:56.635924 28243 credentials.hpp:37] Loading credentials for authentication from '/tmp/PersistentVolumeEndpointsTest_OfferCreateThenEndpointRemove_gqStXQ/http_credentials'
I0713 18:43:56.636272 28243 slave.cpp:395] Using default 'basic' HTTP authenticator
I0713 18:43:56.636615 28243 resources.cpp:572] Parsing resources as JSON failed: disk(*):1024
Trying semicolon-delimited string format instead
I0713 18:43:56.636878 28243 resources.cpp:572] Parsing resources as JSON failed: disk(*):1024
Trying semicolon-delimited string format instead
I0713 18:43:56.637318 28243 slave.cpp:594] Agent resources: disk(*):1024; cpus(*):16; mem(*):47270; ports(*):[31000-32000]
I0713 18:43:56.637859 28243 slave.cpp:602] Agent attributes: [  ]
I0713 18:43:56.638073 28220 sched.cpp:226] Version: 1.1.0
I0713 18:43:56.638074 28243 slave.cpp:607] Agent hostname: 500c3e866abe
I0713 18:43:56.640148 28253 sched.cpp:330] New master detected at master@172.17.0.8:35418
I0713 18:43:56.640650 28253 sched.cpp:396] Authenticating with master master@172.17.0.8:35418
I0713 18:43:56.640738 28253 sched.cpp:403] Using default CRAM-MD5 authenticatee
I0713 18:43:56.640801 28239 state.cpp:57] Recovering state from '/tmp/PersistentVolumeEndpointsTest_OfferCreateThenEndpointRemove_gqStXQ/meta'
I0713 18:43:56.640976 28243 authenticatee.cpp:121] Creating new client SASL connection
I0713 18:43:56.641319 28253 status_update_manager.cpp:200] Recovering status update manager
I0713 18:43:56.641477 28243 master.cpp:6006] Authenticating scheduler-398078e0-6dae-4c02-8197-af69d9eb230a@172.17.0.8:35418
I0713 18:43:56.641636 28239 authenticator.cpp:414] Starting authentication session for crammd5_authenticatee(554)@172.17.0.8:35418
I0713 18:43:56.641542 28240 containerizer.cpp:522] Recovering containerizer
I0713 18:43:56.642201 28239 authenticator.cpp:98] Creating new server SASL connection
I0713 18:43:56.642602 28252 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
I0713 18:43:56.642634 28252 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
I0713 18:43:56.642714 28239 authenticator.cpp:204] Received SASL authentication start
I0713 18:43:56.642792 28239 authenticator.cpp:326] Authentication requires more steps
I0713 18:43:56.642882 28239 authenticatee.cpp:259] Received SASL authentication step
I0713 18:43:56.642978 28239 authenticator.cpp:232] Received SASL authentication step
I0713 18:43:56.643009 28239 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: '500c3e866abe' server FQDN: '500c3e866abe' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0713 18:43:56.643026 28239 auxprop.cpp:181] Looking up auxiliary property '*userPassword'
I0713 18:43:56.643064 28239 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0713 18:43:56.643091 28239 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: '500c3e866abe' server FQDN: '500c3e866abe' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0713 18:43:56.643107 28239 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0713 18:43:56.643117 28239 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0713 18:43:56.643136 28239 authenticator.cpp:318] Authentication success
I0713 18:43:56.643290 28239 authenticatee.cpp:299] Authentication success
I0713 18:43:56.643379 28239 master.cpp:6036] Successfully authenticated principal 'test-principal' at scheduler-398078e0-6dae-4c02-8197-af69d9eb230a@172.17.0.8:35418
I0713 18:43:56.643501 28244 authenticator.cpp:432] Authentication session cleanup for crammd5_authenticatee(554)@172.17.0.8:35418
I0713 18:43:56.643987 28244 sched.cpp:502] Successfully authenticated with master master@172.17.0.8:35418
I0713 18:43:56.644011 28244 sched.cpp:820] Sending SUBSCRIBE call to master@172.17.0.8:35418
I0713 18:43:56.644103 28244 sched.cpp:853] Will retry registration in 809.142674ms if necessary
I0713 18:43:56.644287 28244 master.cpp:2550] Received SUBSCRIBE call for framework 'default' at scheduler-398078e0-6dae-4c02-8197-af69d9eb230a@172.17.0.8:35418
I0713 18:43:56.644346 28244 master.cpp:2012] Authorizing framework principal 'test-principal' to receive offers for role 'role1'
I0713 18:43:56.644675 28249 provisioner.cpp:253] Provisioner recovery complete
I0713 18:43:56.645089 28245 master.cpp:2626] Subscribing framework default with checkpointing disabled and capabilities [  ]
I0713 18:43:56.645783 28249 hierarchical.cpp:271] Added framework 2258d072-b0c9-4c40-874c-6cf933ee345a-0000
I0713 18:43:56.645916 28249 hierarchical.cpp:1537] No allocations performed
I0713 18:43:56.646000 28249 hierarchical.cpp:1632] No inverse offers to send out!
I0713 18:43:56.646083 28248 sched.cpp:743] Framework registered with 2258d072-b0c9-4c40-874c-6cf933ee345a-0000
I0713 18:43:56.646116 28249 hierarchical.cpp:1172] Performed allocation for 0 agents in 249850ns
I0713 18:43:56.646163 28248 sched.cpp:757] Scheduler::registered took 21831ns
I0713 18:43:56.646317 28246 slave.cpp:4856] Finished recovery
I0713 18:43:56.663516 28246 slave.cpp:5028] Querying resource estimator for oversubscribable resources
I0713 18:43:56.664029 28254 status_update_manager.cpp:174] Pausing sending status updates
I0713 18:43:56.664043 28246 slave.cpp:969] New master detected at master@172.17.0.8:35418
I0713 18:43:56.664567 28246 slave.cpp:1028] Authenticating with master master@172.17.0.8:35418
I0713 18:43:56.665148 28246 slave.cpp:1039] Using default CRAM-MD5 authenticatee
I0713 18:43:56.665555 28246 slave.cpp:1001] Detecting new master
I0713 18:43:56.665590 28244 authenticatee.cpp:121] Creating new client SASL connection
I0713 18:43:56.665889 28246 slave.cpp:5042] Received oversubscribable resources  from the resource estimator
I0713 18:43:56.666071 28253 master.cpp:6006] Authenticating slave(251)@172.17.0.8:35418
I0713 18:43:56.666316 28244 authenticator.cpp:414] Starting authentication session for crammd5_authenticatee(555)@172.17.0.8:35418
I0713 18:43:56.666803 28244 authenticator.cpp:98] Creating new server SASL connection
I0713 18:43:56.667027 28244 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
I0713 18:43:56.667058 28244 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
I0713 18:43:56.667140 28244 authenticator.cpp:204] Received SASL authentication start
I0713 18:43:56.667201 28244 authenticator.cpp:326] Authentication requires more steps
I0713 18:43:56.667275 28244 authenticatee.cpp:259] Received SASL authentication step
I0713 18:43:56.667359 28244 authenticator.cpp:232] Received SASL authentication step
I0713 18:43:56.667390 28244 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: '500c3e866abe' server FQDN: '500c3e866abe' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0713 18:43:56.667404 28244 auxprop.cpp:181] Looking up auxiliary property '*userPassword'
I0713 18:43:56.667484 28244 auxprop.cpp:181] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0713 18:43:56.667508 28244 auxprop.cpp:109] Request to lookup properties for user: 'test-principal' realm: '500c3e866abe' server FQDN: '500c3e866abe' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0713 18:43:56.667521 28244 auxprop.cpp:131] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0713 18:43:56.667531 28244 auxprop.cpp:131] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0713 18:43:56.667551 28244 authenticator.cpp:318] Authentication success
I0713 18:43:56.667709 28244 authenticatee.cpp:299] Authentication success
I0713 18:43:56.667801 28244 master.cpp:6036] Successfully authenticated principal 'test-principal' at slave(251)@172.17.0.8:35418
I0713 18:43:56.667876 28244 authenticator.cpp:432] Authentication session cleanup for crammd5_authenticatee(555)@172.17.0.8:35418
I0713 18:43:56.668190 28244 slave.cpp:1123] Successfully authenticated with master master@172.17.0.8:35418
I0713 18:43:56.668352 28244 slave.cpp:1529] Will retry registration in 5.90776ms if necessary
I0713 18:43:56.668772 28251 master.cpp:4676] Registering agent at slave(251)@172.17.0.8:35418 (500c3e866abe) with id 2258d072-b0c9-4c40-874c-6cf933ee345a-S0
I0713 18:43:56.669507 28251 registrar.cpp:464] Applied 1 operations in 96357ns; attempting to update the 'registry'
I0713 18:43:56.670956 28253 log.cpp:577] Attempting to append 337 bytes to the log
I0713 18:43:56.671103 28251 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I0713 18:43:56.672089 28254 replica.cpp:537] Replica received write request for position 3 from (9577)@172.17.0.8:35418
I0713 18:43:56.674686 28250 hierarchical.cpp:1537] No allocations performed
I0713 18:43:56.675036 28250 hierarchical.cpp:1632] No inverse offers to send out!
I0713 18:43:56.675192 28250 hierarchical.cpp:1172] Performed allocation for 0 agents in 574337ns
I0713 18:43:56.674938 28242 slave.cpp:1529] Will retry registration in 31.506393ms if necessary
I0713 18:43:56.675686 28250 master.cpp:4664] Ignoring register agent message from slave(251)@172.17.0.8:35418 (500c3e866abe) as admission is already in progress
I0713 18:43:56.689913 28254 leveldb.cpp:341] Persisting action (356 bytes) to leveldb took 17.767596ms
I0713 18:43:56.690068 28254 replica.cpp:712] Persisted action at 3
I0713 18:43:56.691304 28254 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I0713 18:43:56.708274 28239 slave.cpp:1529] Will retry registration in 39.295397ms if necessary
I0713 18:43:56.708631 28245 master.cpp:4664] Ignoring register agent message from slave(251)@172.17.0.8:35418 (500c3e866abe) as admission is already in progress
I0713 18:43:56.726626 28239 hierarchical.cpp:1537] No allocations performed
I0713 18:43:56.726686 28239 hierarchical.cpp:1632] No inverse offers to send out!
I0713 18:43:56.726758 28239 hierarchical.cpp:1172] Performed allocation for 0 agents in 207473ns
I0713 18:43:56.731587 28254 leveldb.cpp:341] Persisting action (358 bytes) to leveldb took 40.15631ms
I0713 18:43:56.731680 28254 replica.cpp:712] Persisted action at 3
I0713 18:43:56.731724 28254 replica.cpp:697] Replica learned APPEND action at position 3
I0713 18:43:56.733752 28254 registrar.cpp:509] Successfully updated the 'registry' in 64.109056ms
I0713 18:43:56.734061 28240 log.cpp:596] Attempting to truncate the log to 3
I0713 18:43:56.734262 28240 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I0713 18:43:56.734968 28254 master.cpp:4745] Registered agent 2258d072-b0c9-4c40-874c-6cf933ee345a-S0 at slave(251)@172.17.0.8:35418 (500c3e866abe) with disk(*):1024; cpus(*):16; mem(*):47270; ports(*):[31000-32000]
I0713 18:43:56.735697 28252 hierarchical.cpp:478] Added agent 2258d072-b0c9-4c40-874c-6cf933ee345a-S0 (500c3e866abe) with disk(*):1024; cpus(*):16; mem(*):47270; ports(*):[31000-32000] (allocated: )
I0713 18:43:56.736590 28252 hierarchical.cpp:1632] No inverse offers to send out!
I0713 18:43:56.736665 28252 hierarchical.cpp:1195] Performed allocation for agent 2258d072-b0c9-4c40-874c-6cf933ee345a-S0 in 924479ns
I0713 18:43:56.736768 28252 slave.cpp:1169] Registered with master master@172.17.0.8:35418; given agent ID 2258d072-b0c9-4c40-874c-6cf933ee345a-S0
I0713 18:43:56.736790 28252 fetcher.cpp:86] Clearing fetcher cache
I0713 18:43:56.737208 28252 slave.cpp:1192] Checkpointing SlaveInfo to '/tmp/PersistentVolumeEndpointsTest_OfferCreateThenEndpointRemove_gqStXQ/meta/slaves/2258d072-b0c9-4c40-874c-6cf933ee345a-S0/slave.info'
I0713 18:43:56.737787 28249 master.cpp:5835] Sending 1 offers to framework 2258d072-b0c9-4c40-874c-6cf933ee345a-0000 (default) at scheduler-398078e0-6dae-4c02-8197-af69d9eb230a@172.17.0.8:35418
I0713 18:43:56.737946 28249 status_update_manager.cpp:181] Resuming sending status updates
I0713 18:43:56.738322 28249 sched.cpp:917] Scheduler::resourceOffers took 112243ns
I0713 18:43:56.739811 28252 slave.cpp:1229] Forwarding total oversubscribed resources 
I0713 18:43:56.740494 28252 slave.cpp:3760] Received ping from slave-observer(245)@172.17.0.8:35418
I0713 18:43:56.740677 28252 master.cpp:5128] Received update of agent 2258d072-b0c9-4c40-874c-6cf933ee345a-S0 at slave(251)@172.17.0.8:35418 (500c3e866abe) with total oversubscribed resources 
I0713 18:43:56.741009 28252 hierarchical.cpp:542] Agent 2258d072-b0c9-4c40-874c-6cf933ee345a-S0 (500c3e866abe) updated with oversubscribed resources  (total: disk(*):1024; cpus(*):16; mem(*):47270; ports(*):[31000-32000], allocated: disk(*):1024; cpus(*):16; mem(*):47270; ports(*):[31000-32000])
I0713 18:43:56.741189 28252 hierarchical.cpp:1537] No allocations performed
I0713 18:43:56.741228 28252 hierarchical.cpp:1632] No inverse offers to send out!
I0713 18:43:56.741289 28252 hierarchical.cpp:1195] Performed allocation for agent 2258d072-b0c9-4c40-874c-6cf933ee345a-S0 in 219942ns
I0713 18:43:56.742151 28240 replica.cpp:537] Replica received write request for position 4 from (9578)@172.17.0.8:35418
I0713 18:43:56.744851 28254 master.cpp:3468] Processing ACCEPT call for offers: [ 2258d072-b0c9-4c40-874c-6cf933ee345a-O0 ] on agent 2258d072-b0c9-4c40-874c-6cf933ee345a-S0 at slave(251)@172.17.0.8:35418 (500c3e866abe) for framework 2258d072-b0c9-4c40-874c-6cf933ee345a-0000 (default) at scheduler-398078e0-6dae-4c02-8197-af69d9eb230a@172.17.0.8:35418
I0713 18:43:56.744977 28254 master.cpp:3144] Authorizing principal 'test-principal' to reserve resources 'disk(role1, test-principal):512'
I0713 18:43:56.747194 28254 master.cpp:3695] Applying RESERVE operation for resources disk(role1, test-principal):512 from framework 2258d072-b0c9-4c40-874c-6cf933ee345a-0000 (default) at scheduler-398078e0-6dae-4c02-8197-af69d9eb230a@172.17.0.8:35418 to agent 2258d072-b0c9-4c40-874c-6cf933ee345a-S0 at slave(251)@172.17.0.8:35418 (500c3e866abe)
I0713 18:43:56.747602 28254 master.cpp:7098] Sending checkpointed resources disk(role1, test-principal):512 to agent 2258d072-b0c9-4c40-874c-6cf933ee345a-S0 at slave(251)@172.17.0.8:35418 (500c3e866abe)
I0713 18:43:56.748193 28245 slave.cpp:2600] Updated checkpointed resources from  to disk(role1, test-principal):512
I0713 18:43:56.750397 28239 hierarchical.cpp:683] Updated allocation of framework 2258d072-b0c9-4c40-874c-6cf933ee345a-0000 on agent 2258d072-b0c9-4c40-874c-6cf933ee345a-S0 from disk(*):1024; cpus(*):16; mem(*):47270; ports(*):[31000-32000] to disk(*):512; cpus(*):16; mem(*):47270; ports(*):[31000-32000]; disk(role1, test-principal):512
I0713 18:43:56.751693 28239 hierarchical.cpp:924] Recovered disk(*):512; cpus(*):16; mem(*):47270; ports(*):[31000-32000]; disk(role1, test-principal):512 (total: disk(*):512; cpus(*):16; mem(*):47270; ports(*):[31000-32000]; disk(role1, test-principal):512, allocated: ) on agent 2258d072-b0c9-4c40-874c-6cf933ee345a-S0 from framework 2258d072-b0c9-4c40-874c-6cf933ee345a-0000
I0713 18:43:56.777899 28243 hierarchical.cpp:1632] No inverse offers to send out!
I0713 18:43:56.777992 28243 hierarchical.cpp:1172] Performed allocation for 1 agents in 1.044349ms
I0713 18:43:56.778578 28243 master.cpp:5835] Sending 1 offers to framework 2258d072-b0c9-4c40-874c-6cf933ee345a-0000 (default) at scheduler-398078e0-6dae-4c02-8197-af69d9eb230a@172.17.0.8:35418
I0713 18:43:56.779105 28243 sched.cpp:917] Scheduler::resourceOffers took 122192ns
I0713 18:43:56.781126 28243 master.cpp:3468] Processing ACCEPT call for offers: [ 2258d072-b0c9-4c40-874c-6cf933ee345a-O1 ] on agent 2258d072-b0c9-4c40-874c-6cf933ee345a-S0 at slave(251)@172.17.0.8:35418 (500c3e866abe) for framework 2258d072-b0c9-4c40-874c-6cf933ee345a-0000 (default) at scheduler-398078e0-6dae-4c02-8197-af69d9eb230a@172.17.0.8:35418
I0713 18:43:56.781239 28243 master.cpp:3253] Authorizing principal 'test-principal' to create volumes
I0713 18:43:56.783177 28245 master.cpp:3801] Applying CREATE operation for volumes disk(role1, test-principal)[id1:volume_path]:1 from framework 2258d072-b0c9-4c40-874c-6cf933ee345a-0000 (default) at scheduler-398078e0-6dae-4c02-8197-af69d9eb230a@172.17.0.8:35418 to agent 2258d072-b0c9-4c40-874c-6cf933ee345a-S0 at slave(251)@172.17.0.8:35418 (500c3e866abe)
I0713 18:43:56.783653 28245 master.cpp:7098] Sending checkpointed resources disk(role1, test-principal):511; disk(role1, test-principal)[id1:volume_path]:1 to agent 2258d072-b0c9-4c40-874c-6cf933ee345a-S0 at slave(251)@172.17.0.8:35418 (500c3e866abe)
I0713 18:43:56.785676 28249 hierarchical.cpp:683] Updated allocation of framework 2258d072-b0c9-4c40-874c-6cf933ee345a-0000 on agent 2258d072-b0c9-4c40-874c-6cf933ee345a-S0 from disk(role1, test-principal):512; disk(*):512; cpus(*):16; mem(*):47270; ports(*):[31000-32000] to disk(role1, test-principal):511; disk(*):512; cpus(*):16; mem(*):47270; ports(*):[31000-32000]; disk(role1, test-principal)[id1:volume_path]:1
I0713 18:43:56.786803 28249 hierarchical.cpp:924] Recovered disk(role1, test-principal):511; disk(*):512; cpus(*):16; mem(*):47270; ports(*):[31000-32000]; disk(role1, test-principal)[id1:volume_path]:1 (total: disk(*):512; cpus(*):16; mem(*):47270; ports(*):[31000-32000]; disk(role1, test-principal):511; disk(role1, test-principal)[id1:volume_path]:1, allocated: ) on agent 2258d072-b0c9-4c40-874c-6cf933ee345a-S0 from framework 2258d072-b0c9-4c40-874c-6cf933ee345a-0000
I0713 18:43:56.790855 28241 slave.cpp:2600] Updated checkpointed resources from disk(role1, test-principal):512 to disk(role1, test-principal):511; disk(role1, test-principal)[id1:volume_path]:1
I0713 18:43:56.798692 28240 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 56.485928ms
I0713 18:43:56.798765 28240 replica.cpp:712] Persisted action at 4
I0713 18:43:56.800184 28240 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I0713 18:43:56.830730 28245 hierarchical.cpp:1632] No inverse offers to send out!
I0713 18:43:56.830854 28245 hierarchical.cpp:1172] Performed allocation for 1 agents in 1.388256ms
I0713 18:43:56.831647 28245 master.cpp:5835] Sending 1 offers to framework 2258d072-b0c9-4c40-874c-6cf933ee345a-0000 (default) at scheduler-398078e0-6dae-4c02-8197-af69d9eb230a@172.17.0.8:35418
I0713 18:43:56.832273 28245 sched.cpp:917] Scheduler::resourceOffers took 146725ns
I0713 18:43:56.841397 28245 process.cpp:3322] Handling HTTP event for process 'master' with path: '/master/destroy-volumes'
I0713 18:43:56.845082 28245 http.cpp:381] HTTP POST for /master/destroy-volumes from 172.17.0.8:57083
I0713 18:43:56.847327 28240 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 47.043061ms
I0713 18:43:56.847586 28240 leveldb.cpp:399] Deleting ~2 keys from leveldb took 196582ns
I0713 18:43:56.847704 28240 replica.cpp:712] Persisted action at 4
I0713 18:43:56.847744 28240 replica.cpp:697] Replica learned TRUNCATE action at po",1.0,1.0.0,0.5,1.0,0.0,0.0,0.0,0.0,0.8,0.5438596491228069,0.39999999999999997,0.8846153846153846,0.9878787878787878,0.9878787878787878,0.9174311926605504
Task,Add example framework for using inverse offers,We should have an example framework (in src/examples) demonstrating how to handle inverse offers. ,3.0,0,0.5,0.3740573152337858,0.0,0.0,0.0,0.2,0.4,0.43859649122807015,0.2714285714285714,0.11538461538461538,0.11515151515151516,0.11515151515151516,0.0
Improvement,Include disk source information in stringification,"Some frameworks (like kafka_mesos) ignore the Source field when trying to reserve an offered mount or path persistent volume; the resulting error message is bewildering:

{code:none}
Task uses more resources
cpus(*):4; mem(*):4096;     ports(*):[31000-31000]; disk(kafka, kafka)[kafka_0:data]:960679
than available
cpus(*):32; mem(*):256819;  ports(*):[31000-32000]; disk(kafka, kafka)[kafka_0:data]:960679;   disk(*):240169;
{code}

The stringification of disk resources should include source information.
",3.0,0.28.2,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.007142857142857143,0.02564102564102564,0.01212121212121212,0.01212121212121212,0.25706422018348624
Improvement,Add a build script for the Windows CI,"The ASF CI for Mesos runs a script that lives inside the Mesos codebase:
https://github.com/apache/mesos/blob/1cbfdc3c1e4b8498a67f8531ab264003c8c19fb1/support/docker_build.sh

ASF Infrastructure have set up a machine that we can use for building Mesos on Windows.  Considering the environment, we will need a separate script to build here.",3.0,0,0.0,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.0
Task,Port libprocess http_tests.cpp,"The test cases:_x000D_
 * HTTPTest, EndpointsHelp_x000D_
 * HTTPTest, EndpointsHelpRemoval_x000D_
 * HTTPTest, NestedGet_x000D_
 * -HTTPTest, QueryEncodeDecode-_x000D_
_x000D_
Are disabled in the Windows build because they fail.",1.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.8,0.19298245614035087,0.09999999999999999,0.11538461538461538,0.05454545454545455,0.16969696969696968,0.0
Bug,MasterAPITest.Subscribe is flaky,"This test seems to be flaky, although on Mac OS X and CentOS 7 the error a bit different.

On Mac OS X:
{noformat}[ RUN      ] ContentType/MasterAPITest.Subscribe/0
I0708 11:42:48.474665 1927435008 cluster.cpp:155] Creating default 'local' authorizer
I0708 11:42:48.480677 1927435008 leveldb.cpp:174] Opened db in 5727us
I0708 11:42:48.481494 1927435008 leveldb.cpp:181] Compacted db in 722us
I0708 11:42:48.481541 1927435008 leveldb.cpp:196] Created db iterator in 19us
I0708 11:42:48.481572 1927435008 leveldb.cpp:202] Seeked to beginning of db in 9us
I0708 11:42:48.481587 1927435008 leveldb.cpp:271] Iterated through 0 keys in the db in 7us
I0708 11:42:48.481617 1927435008 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0708 11:42:48.482030 350982144 recover.cpp:451] Starting replica recovery
I0708 11:42:48.482203 350982144 recover.cpp:477] Replica is in EMPTY status
I0708 11:42:48.484107 348299264 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (3780)@127.0.0.1:50325
I0708 11:42:48.484318 350982144 recover.cpp:197] Received a recover response from a replica in EMPTY status
I0708 11:42:48.484750 348835840 master.cpp:382] Master e055d60c-05ff-487e-82da-d0a43e52605c (localhost) started on 127.0.0.1:50325
I0708 11:42:48.484850 349908992 recover.cpp:568] Updating replica status to STARTING
I0708 11:42:48.484788 348835840 master.cpp:384] Flags at startup: --acls="""" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate_agents=""true"" --authenticate_frameworks=""true"" --authenticate_http=""true"" --authenticate_http_frameworks=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/private/tmp/Sn2Kf4/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/private/tmp/Sn2Kf4/master"" --zk_session_timeout=""10secs""
W0708 11:42:48.485263 348835840 master.cpp:387] 
**************************************************
Master bound to loopback interface! Cannot communicate with remote schedulers or agents. You might want to set '--ip' flag to a routable IP address.
**************************************************
I0708 11:42:48.485291 348835840 master.cpp:434] Master only allowing authenticated frameworks to register
I0708 11:42:48.485314 348835840 master.cpp:448] Master only allowing authenticated agents to register
I0708 11:42:48.485335 348835840 master.cpp:461] Master only allowing authenticated HTTP frameworks to register
I0708 11:42:48.485347 348835840 credentials.hpp:37] Loading credentials for authentication from '/private/tmp/Sn2Kf4/credentials'
I0708 11:42:48.485373 349372416 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 397us
I0708 11:42:48.485414 349372416 replica.cpp:320] Persisted replica status to STARTING
I0708 11:42:48.485608 350982144 recover.cpp:477] Replica is in STARTING status
I0708 11:42:48.485749 348835840 master.cpp:506] Using default 'crammd5' authenticator
I0708 11:42:48.485852 348835840 master.cpp:578] Using default 'basic' HTTP authenticator
I0708 11:42:48.486018 348835840 master.cpp:658] Using default 'basic' HTTP framework authenticator
I0708 11:42:48.486140 348835840 master.cpp:705] Authorization enabled
I0708 11:42:48.486486 350982144 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (3783)@127.0.0.1:50325
I0708 11:42:48.486758 352055296 recover.cpp:197] Received a recover response from a replica in STARTING status
I0708 11:42:48.487176 350982144 recover.cpp:568] Updating replica status to VOTING
I0708 11:42:48.487576 352055296 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 300us
I0708 11:42:48.487658 352055296 replica.cpp:320] Persisted replica status to VOTING
I0708 11:42:48.487736 350982144 recover.cpp:582] Successfully joined the Paxos group
I0708 11:42:48.487951 350982144 recover.cpp:466] Recover process terminated
I0708 11:42:48.489441 348835840 master.cpp:1973] The newly elected leader is master@127.0.0.1:50325 with id e055d60c-05ff-487e-82da-d0a43e52605c
I0708 11:42:48.489518 348835840 master.cpp:1986] Elected as the leading master!
I0708 11:42:48.489545 348835840 master.cpp:1673] Recovering from registrar
I0708 11:42:48.489637 350982144 registrar.cpp:332] Recovering registrar
I0708 11:42:48.490120 351518720 log.cpp:553] Attempting to start the writer
I0708 11:42:48.491161 350445568 replica.cpp:493] Replica received implicit promise request from (3784)@127.0.0.1:50325 with proposal 1
I0708 11:42:48.491461 350445568 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 252us
I0708 11:42:48.491528 350445568 replica.cpp:342] Persisted promised to 1
I0708 11:42:48.492337 348299264 coordinator.cpp:238] Coordinator attempting to fill missing positions
I0708 11:42:48.493482 349372416 replica.cpp:388] Replica received explicit promise request from (3785)@127.0.0.1:50325 for position 0 with proposal 2
I0708 11:42:48.493854 349372416 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 283us
I0708 11:42:48.493904 349372416 replica.cpp:712] Persisted action at 0
I0708 11:42:48.495302 348299264 replica.cpp:537] Replica received write request for position 0 from (3786)@127.0.0.1:50325
I0708 11:42:48.495455 348299264 leveldb.cpp:436] Reading position from leveldb took 45us
I0708 11:42:48.495761 348299264 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 261us
I0708 11:42:48.495803 348299264 replica.cpp:712] Persisted action at 0
I0708 11:42:48.496484 350445568 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I0708 11:42:48.496795 350445568 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 255us
I0708 11:42:48.496857 350445568 replica.cpp:712] Persisted action at 0
I0708 11:42:48.496896 350445568 replica.cpp:697] Replica learned NOP action at position 0
I0708 11:42:48.497445 350982144 log.cpp:569] Writer started with ending position 0
I0708 11:42:48.498523 350982144 leveldb.cpp:436] Reading position from leveldb took 80us
I0708 11:42:48.499307 349908992 registrar.cpp:365] Successfully fetched the registry (0B) in 9.63712ms
I0708 11:42:48.499464 349908992 registrar.cpp:464] Applied 1 operations in 36us; attempting to update the 'registry'
I0708 11:42:48.499953 351518720 log.cpp:577] Attempting to append 159 bytes to the log
I0708 11:42:48.500088 350982144 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I0708 11:42:48.500880 348299264 replica.cpp:537] Replica received write request for position 1 from (3787)@127.0.0.1:50325
I0708 11:42:48.501186 348299264 leveldb.cpp:341] Persisting action (178 bytes) to leveldb took 259us
I0708 11:42:48.501231 348299264 replica.cpp:712] Persisted action at 1
I0708 11:42:48.501786 351518720 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I0708 11:42:48.502118 351518720 leveldb.cpp:341] Persisting action (180 bytes) to leveldb took 311us
I0708 11:42:48.502260 351518720 replica.cpp:712] Persisted action at 1
I0708 11:42:48.502305 351518720 replica.cpp:697] Replica learned APPEND action at position 1
I0708 11:42:48.503475 349908992 registrar.cpp:509] Successfully updated the 'registry' in 3.944192ms
I0708 11:42:48.503909 349908992 registrar.cpp:395] Successfully recovered registrar
I0708 11:42:48.504003 350982144 log.cpp:596] Attempting to truncate the log to 1
I0708 11:42:48.504250 349372416 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I0708 11:42:48.504546 350445568 master.cpp:1781] Recovered 0 agents from the Registry (121B) ; allowing 10mins for agents to re-register
I0708 11:42:48.506022 352055296 replica.cpp:537] Replica received write request for position 2 from (3788)@127.0.0.1:50325
I0708 11:42:48.506479 352055296 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 320us
I0708 11:42:48.506513 352055296 replica.cpp:712] Persisted action at 2
I0708 11:42:48.506978 351518720 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I0708 11:42:48.507155 351518720 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 169us
I0708 11:42:48.507237 351518720 leveldb.cpp:399] Deleting ~1 keys from leveldb took 37us
I0708 11:42:48.507264 351518720 replica.cpp:712] Persisted action at 2
I0708 11:42:48.507285 351518720 replica.cpp:697] Replica learned TRUNCATE action at position 2
I0708 11:42:48.521363 1927435008 cluster.cpp:432] Creating default 'local' authorizer
I0708 11:42:48.522498 350982144 slave.cpp:205] Agent started on 119)@127.0.0.1:50325
I0708 11:42:48.522538 350982144 slave.cpp:206] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""true"" --authenticatee=""crammd5"" --authentication_backoff_factor=""1secs"" --authorizer=""local"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/var/folders/ny/tcvyblqj43s2gdh2_895v9nw0000gp/T/ContentType_MasterAPITest_Subscribe_0_VaPndX/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/var/folders/ny/tcvyblqj43s2gdh2_895v9nw0000gp/T/ContentType_MasterAPITest_Subscribe_0_VaPndX/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_command_executor=""false"" --http_credentials=""/var/folders/ny/tcvyblqj43s2gdh2_895v9nw0000gp/T/ContentType_MasterAPITest_Subscribe_0_VaPndX/http_credentials"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/Users/zhitao/Uber/sync/zhitao-mesos1.dev.uber.com/home/uber/mesos/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --version=""false"" --work_dir=""/var/folders/ny/tcvyblqj43s2gdh2_895v9nw0000gp/T/ContentType_MasterAPITest_Subscribe_0_VaPndX""
W0708 11:42:48.522903 350982144 slave.cpp:209] 
**************************************************
Agent bound to loopback interface! Cannot communicate with remote master(s). You might want to set '--ip' flag to a routable IP address.
**************************************************
I0708 11:42:48.522922 350982144 credentials.hpp:86] Loading credential for authentication from '/var/folders/ny/tcvyblqj43s2gdh2_895v9nw0000gp/T/ContentType_MasterAPITest_Subscribe_0_VaPndX/credential'
W0708 11:42:48.522965 1927435008 scheduler.cpp:157] 
**************************************************
Scheduler driver bound to loopback interface! Cannot communicate with remote master(s). You might want to set 'LIBPROCESS_IP' environment variable to use a routable IP address.
**************************************************
I0708 11:42:48.522992 1927435008 scheduler.cpp:172] Version: 1.0.0
I0708 11:42:48.523066 350982144 slave.cpp:343] Agent using credential for: test-principal
I0708 11:42:48.523092 350982144 credentials.hpp:37] Loading credentials for authentication from '/var/folders/ny/tcvyblqj43s2gdh2_895v9nw0000gp/T/ContentType_MasterAPITest_Subscribe_0_VaPndX/http_credentials'
I0708 11:42:48.523334 350982144 slave.cpp:395] Using default 'basic' HTTP authenticator
I0708 11:42:48.523973 352055296 scheduler.cpp:461] New master detected at master@127.0.0.1:50325
I0708 11:42:48.524050 350982144 slave.cpp:594] Agent resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0708 11:42:48.524196 350982144 slave.cpp:602] Agent attributes: [  ]
I0708 11:42:48.524224 350982144 slave.cpp:607] Agent hostname: localhost
I0708 11:42:48.525522 350445568 state.cpp:57] Recovering state from '/var/folders/ny/tcvyblqj43s2gdh2_895v9nw0000gp/T/ContentType_MasterAPITest_Subscribe_0_VaPndX/meta'
I0708 11:42:48.525853 350445568 status_update_manager.cpp:200] Recovering status update manager
I0708 11:42:48.526165 350445568 slave.cpp:4856] Finished recovery
I0708 11:42:48.527223 349372416 status_update_manager.cpp:174] Pausing sending status updates
I0708 11:42:48.527231 352055296 slave.cpp:969] New master detected at master@127.0.0.1:50325
I0708 11:42:48.527276 352055296 slave.cpp:1028] Authenticating with master master@127.0.0.1:50325
I0708 11:42:48.527328 352055296 slave.cpp:1039] Using default CRAM-MD5 authenticatee
I0708 11:42:48.527561 352055296 slave.cpp:1001] Detecting new master
I0708 11:42:48.527582 348299264 authenticatee.cpp:121] Creating new client SASL connection
I0708 11:42:48.528666 349908992 master.cpp:6006] Authenticating slave(119)@127.0.0.1:50325
I0708 11:42:48.528880 352055296 authenticator.cpp:98] Creating new server SASL connection
I0708 11:42:48.529089 350445568 http.cpp:381] HTTP POST for /master/api/v1/scheduler from 127.0.0.1:50918
I0708 11:42:48.529233 350445568 master.cpp:2272] Received subscription request for HTTP framework 'default'
I0708 11:42:48.529261 350445568 master.cpp:2012] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0708 11:42:48.529323 352055296 authenticatee.cpp:213] Received SASL authentication mechanisms: CRAM-MD5
I0708 11:42:48.529357 352055296 authenticatee.cpp:239] Attempting to authenticate with mechanism 'CRAM-MD5'
I0708 11:42:48.529417 352055296 authenticator.cpp:204] Received SASL authentication start
I0708 11:42:48.529503 352055296 authenticator.cpp:326] Authentication requires more steps
I0708 11:42:48.529561 352055296 master.cpp:2370] Subscribing framework 'default' with checkpointing disabled and capabilities [  ]
I0708 11:42:48.529721 349908992 authenticatee.cpp:259] Received SASL authentication step
I0708 11:42:48.530005 348835840 authenticator.cpp:232] Received SASL authentication step
I0708 11:42:48.530241 348835840 authenticator.cpp:318] Authentication success
I0708 11:42:48.530254 350445568 hierarchical.cpp:271] Added framework e055d60c-05ff-487e-82da-d0a43e52605c-0000
I0708 11:42:48.530900 349908992 authenticatee.cpp:299] Authentication success
I0708 11:42:48.531186 350982144 master.cpp:6036] Successfully authenticated principal 'test-principal' at slave(119)@127.0.0.1:50325
I0708 11:42:48.531657 348299264 slave.cpp:1123] Successfully authenticated with master master@127.0.0.1:50325
I0708 11:42:48.531935 349372416 master.cpp:4676] Registering agent at slave(119)@127.0.0.1:50325 (localhost) with id e055d60c-05ff-487e-82da-d0a43e52605c-S0
I0708 11:42:48.532304 349908992 registrar.cpp:464] Applied 1 operations in 55us; attempting to update the 'registry'
I0708 11:42:48.532908 348835840 log.cpp:577] Attempting to append 326 bytes to the log
I0708 11:42:48.533015 352055296 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I0708 11:42:48.533641 349372416 replica.cpp:537] Replica received write request for position 3 from (3798)@127.0.0.1:50325
I0708 11:42:48.533867 349372416 leveldb.cpp:341] Persisting action (345 bytes) to leveldb took 186us
I0708 11:42:48.533917 349372416 replica.cpp:712] Persisted action at 3
I0708 11:42:48.537066 349908992 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I0708 11:42:48.538169 349908992 leveldb.cpp:341] Persisting action (347 bytes) to leveldb took 914us
I0708 11:42:48.538226 349908992 replica.cpp:712] Persisted action at 3
I0708 11:42:48.538255 349908992 replica.cpp:697] Replica learned APPEND action at position 3
I0708 11:42:48.539247 352055296 registrar.cpp:509] Successfully updated the 'registry' in 6.895104ms
I0708 11:42:48.539302 348299264 log.cpp:596] Attempting to truncate the log to 3
I0708 11:42:48.539393 348299264 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I0708 11:42:48.539798 348835840 master.cpp:4745] Registered agent e055d60c-05ff-487e-82da-d0a43e52605c-S0 at slave(119)@127.0.0.1:50325 (localhost) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0708 11:42:48.539881 348299264 hierarchical.cpp:478] Added agent e055d60c-05ff-487e-82da-d0a43e52605c-S0 (localhost) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
I0708 11:42:48.539901 349908992 slave.cpp:1169] Registered with master master@127.0.0.1:50325; given agent ID e055d60c-05ff-487e-82da-d0a43e52605c-S0
I0708 11:42:48.540287 350445568 status_update_manager.cpp:181] Resuming sending status updates
I0708 11:42:48.540501 351518720 replica.cpp:537] Replica received write request for position 4 from (3799)@127.0.0.1:50325
I0708 11:42:48.540583 352055296 master.cpp:5835] Sending 1 offers to framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 (default)
I0708 11:42:48.540798 351518720 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 247us
I0708 11:42:48.540868 351518720 replica.cpp:712] Persisted action at 4
I0708 11:42:48.540895 349908992 slave.cpp:1229] Forwarding total oversubscribed resources 
I0708 11:42:48.541035 352055296 master.cpp:5128] Received update of agent e055d60c-05ff-487e-82da-d0a43e52605c-S0 at slave(119)@127.0.0.1:50325 (localhost) with total oversubscribed resources 
I0708 11:42:48.541291 349908992 hierarchical.cpp:542] Agent e055d60c-05ff-487e-82da-d0a43e52605c-S0 (localhost) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000])
I0708 11:42:48.541630 350982144 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I0708 11:42:48.541911 350982144 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 189us
I0708 11:42:48.541965 350982144 leveldb.cpp:399] Deleting ~2 keys from leveldb took 28us
I0708 11:42:48.541987 350982144 replica.cpp:712] Persisted action at 4
I0708 11:42:48.542006 350982144 replica.cpp:697] Replica learned TRUNCATE action at position 4
I0708 11:42:48.544836 352055296 http.cpp:381] HTTP POST for /master/api/v1 from 127.0.0.1:50920
I0708 11:42:48.544884 352055296 http.cpp:484] Processing call SUBSCRIBE
I0708 11:42:48.545382 352055296 master.cpp:7599] Added subscriber: a85e7341-ac15-4f18-9021-1a2efa326442 to the list of active subscribers
I0708 11:42:48.550048 348835840 http.cpp:381] HTTP POST for /master/api/v1/scheduler from 127.0.0.1:50919
I0708 11:42:48.550339 348835840 master.cpp:3468] Processing ACCEPT call for offers: [ e055d60c-05ff-487e-82da-d0a43e52605c-O0 ] on agent e055d60c-05ff-487e-82da-d0a43e52605c-S0 at slave(119)@127.0.0.1:50325 (localhost) for framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 (default)
I0708 11:42:48.550390 348835840 master.cpp:3106] Authorizing framework principal 'test-principal' to launch task d94e54c0-8c89-43bd-be2f-adeb8cf70cb1
W0708 11:42:48.551434 348835840 validation.cpp:650] Executor default for task d94e54c0-8c89-43bd-be2f-adeb8cf70cb1 uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases.
W0708 11:42:48.551477 348835840 validation.cpp:662] Executor default for task d94e54c0-8c89-43bd-be2f-adeb8cf70cb1 uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases.
I0708 11:42:48.551803 348835840 master.cpp:7565] Adding task d94e54c0-8c89-43bd-be2f-adeb8cf70cb1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on agent e055d60c-05ff-487e-82da-d0a43e52605c-S0 (localhost)
I0708 11:42:48.551949 348835840 master.cpp:3957] Launching task d94e54c0-8c89-43bd-be2f-adeb8cf70cb1 of framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 (default) with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on agent e055d60c-05ff-487e-82da-d0a43e52605c-S0 at slave(119)@127.0.0.1:50325 (localhost)
I0708 11:42:48.552151 352055296 slave.cpp:1569] Got assigned task d94e54c0-8c89-43bd-be2f-adeb8cf70cb1 for framework e055d60c-05ff-487e-82da-d0a43e52605c-0000
I0708 11:42:48.552592 352055296 slave.cpp:1688] Launching task d94e54c0-8c89-43bd-be2f-adeb8cf70cb1 for framework e055d60c-05ff-487e-82da-d0a43e52605c-0000
I0708 11:42:48.553282 352055296 paths.cpp:528] Trying to chown '/var/folders/ny/tcvyblqj43s2gdh2_895v9nw0000gp/T/ContentType_MasterAPITest_Subscribe_0_VaPndX/slaves/e055d60c-05ff-487e-82da-d0a43e52605c-S0/frameworks/e055d60c-05ff-487e-82da-d0a43e52605c-0000/executors/default/runs/62add906-b60f-43ec-ab06-0514a798de26' to user 'zhitao'
I0708 11:42:48.566201 352055296 slave.cpp:5748] Launching executor default of framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 with resources  in work directory '/var/folders/ny/tcvyblqj43s2gdh2_895v9nw0000gp/T/ContentType_MasterAPITest_Subscribe_0_VaPndX/slaves/e055d60c-05ff-487e-82da-d0a43e52605c-S0/frameworks/e055d60c-05ff-487e-82da-d0a43e52605c-0000/executors/default/runs/62add906-b60f-43ec-ab06-0514a798de26'
I0708 11:42:48.567876 352055296 executor.cpp:188] Version: 1.0.0
I0708 11:42:48.568428 352055296 slave.cpp:1914] Queuing task 'd94e54c0-8c89-43bd-be2f-adeb8cf70cb1' for executor 'default' of framework e055d60c-05ff-487e-82da-d0a43e52605c-0000
E0708 11:42:48.571115 352591872 process.cpp:2104] Failed to shutdown socket with fd 254: Socket is not connected
W0708 11:42:48.570768 352055296 executor.cpp:739] Dropping SUBSCRIBE: Executor is in state DISCONNECTED

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: disconnected(0x7fad21fcebf0)
Stack trace:
../../src/tests/api_tests.cpp:1537: Failure
Failed to wait 15secs for event
E0708 11:43:03.556205 352591872 process.cpp:2104] Failed to shutdown socket with fd 235: Socket is not connected
E0708 11:43:03.556584 352591872 process.cpp:2104] Failed to shutdown socket with fd 223: Socket is not connected
I0708 11:43:03.557134 349908992 master.cpp:1410] Framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 (default) disconnected
I0708 11:43:03.557176 349908992 master.cpp:2851] Disconnecting framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 (default)
I0708 11:43:03.557209 349908992 master.cpp:2875] Deactivating framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 (default)
I0708 11:43:03.557415 349908992 master.cpp:1423] Giving framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 (default) 0ns to failover
I0708 11:43:03.557456 348835840 hierarchical.cpp:382] Deactivated framework e055d60c-05ff-487e-82da-d0a43e52605c-0000
I0708 11:43:03.557878 350445568 master.cpp:5687] Framework failover timeout, removing framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 (default)
I0708 11:43:03.557945 350445568 master.cpp:6422] Removing framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 (default)
I0708 11:43:03.558076 352055296 slave.cpp:2292] Asked to shut down framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 by master@127.0.0.1:50325
I0708 11:43:03.558106 352055296 slave.cpp:2317] Shutting down framework e055d60c-05ff-487e-82da-d0a43e52605c-0000
I0708 11:43:03.558131 352055296 slave.cpp:4481] Shutting down executor 'default' of framework e055d60c-05ff-487e-82da-d0a43e52605c-0000
W0708 11:43:03.558147 352055296 slave.hpp:768] Unable to send event to executor 'default' of framework e055d60c-05ff-487e-82da-d0a43e52605c-0000: unknown connection type
I0708 11:43:03.558188 350445568 master.cpp:6959] Updating the state of task d94e54c0-8c89-43bd-be2f-adeb8cf70cb1 of framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 (latest state: TASK_KILLED, status update state: TASK_KILLED)
I0708 11:43:03.558507 350445568 master.cpp:7025] Removing task d94e54c0-8c89-43bd-be2f-adeb8cf70cb1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] of framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 on agent e055d60c-05ff-487e-82da-d0a43e52605c-S0 at slave(119)@127.0.0.1:50325 (localhost)
I0708 11:43:03.558709 350445568 master.cpp:7054] Removing executor 'default' with resources  of framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 on agent e055d60c-05ff-487e-82da-d0a43e52605c-S0 at slave(119)@127.0.0.1:50325 (localhost)
I0708 11:43:03.559051 349372416 hierarchical.cpp:333] Removed framework e055d60c-05ff-487e-82da-d0a43e52605c-0000
I0708 11:43:03.567955 350982144 slave.cpp:4163] Executor 'default' of framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 exited with status 0
I0708 11:43:03.568176 350982144 slave.cpp:4267] Cleaning up executor 'default' of framework e055d60c-05ff-487e-82da-d0a43e52605c-0000
W0708 11:43:03.568258 348299264 master.cpp:5369] Ignoring unknown exited executor 'default' of framework e055d60c-05ff-487e-82da-d0a43e52605c-0000 on agent e055d60c-05ff-487e-82da-d0a43e52605c-S0 at slave(119)@127.0.0.1:50325 (localhost)
I0708 11:43:03.568584 348299264 gc.cpp:55] Scheduling '/var/folders/ny/tcvyblqj43s2gdh2_895v9nw0000gp/T/ContentType_MasterAPITest_Subscribe_0_VaPndX/slaves/e055d60c-05ff-487e-82da-d0a43e52605c-S0/frameworks/e055d60c-05ff-487e-82da-d0a43e52605c-0000/executors/default/runs/62add906-b60f-43ec-ab06-0514a798de26' for gc 6.99999342143407days in the future
I0708 11:43:03.568864 350982144 slave.cpp:4355] Cleaning up framework e055d60c-05ff-487e-82da-d0a43e52605c-0000
I0708 11:43:03.568879 352055296 gc.cpp:55] Scheduling '/var/folders/ny/tcvyblqj43s2gdh2_895v9nw0000gp/T/ContentType_MasterAPITest_Subscribe_0_VaPndX/slaves/e055d60c-05ff-487e-82da-d0a43e52605c-S0/frameworks/e055d60c-05ff-487e-82da-d0a43e52605c-0000/executors/default' for gc 6.99999341739556days in the future
I0708 11:43:03.569056 350445568 status_update_manager.cpp:282] Closing status update streams for framework e055d60c-05ff-487e-82da-d0a43e52605c-0000
I0708 11:43:03.569247 350982144 slave.cpp:841] Agent terminating
I0708 11:43:03.569239 348835840 gc.cpp:55] Scheduling '/var/folders/ny/tcvyblqj43s2gdh2_895v9nw0000gp/T/ContentType_MasterAPITest_Subscribe_0_VaPndX/slaves/e055d60c-05ff-487e-82da-d0a43e52605c-S0/frameworks/e055d60c-05ff-487e-82da-d0a43e52605c-0000' for gc 6.99999341315852days in the future
I0708 11:43:03.569524 350982144 master.cpp:1371] Agent e055d60c-05ff-487e-82da-d0a43e52605c-S0 at slave(119)@127.0.0.1:50325 (localhost) disconnected
I0708 11:43:03.569577 350982144 master.cpp:2910] Disconnecting agent e055d60c-05ff-487e-82da-d0a43e52605c-S0 at slave(119)@127.0.0.1:50325 (localhost)
I0708 11:43:03.569767 350982144 master.cpp:2929] Deactivating agent e055d60c-05ff-487e-82da-d0a43e52605c-S0 at slave(119)@127.0.0.1:50325 (localhost)
I0708 11:43:03.570020 349372416 hierarchical.cpp:571] Agent e055d60c-05ff-487e-82da-d0a43e52605c-S0 deactivated
../../src/tests/api_tests.cpp:1509: Failure
Actual function call count doesn't match EXPECT_CALL(*executor, acknowledged(_, _))...
         Expected: to be called once
           Actual: never called - unsatisfied and active
../../src/tests/api_tests.cpp:1505: Failure
Actual function call count doesn't match EXPECT_CALL(*executor, launch(_, _))...
         Expected: to be called once
           Actual: never called - unsatisfied and active
../../src/tests/api_tests.cpp:1503: Failure
Actual function call count doesn't match EXPECT_CALL(*executor, subscribed(_, _))...
         Expected: to be called once
           Actual: never called - unsatisfied and active
../../src/tests/api_tests.cpp:1496: Failure
Actual function call count doesn't match EXPECT_CALL(*scheduler, update(_, _))...
         Expected: to be called twice
           Actual: never called - unsatisfied and active
I0708 11:43:03.572598 1927435008 master.cpp:1218] Master terminating
I0708 11:43:03.572844 352055296 hierarchical.cpp:510] Removed agent e055d60c-05ff-487e-82da-d0a43e52605c-S0
[  FAILED  ] ContentType/MasterAPITest.Subscribe/0, where GetParam() = application/x-protobuf (15105 ms)
{noformat}

On CentOS 7
{noformat}
[ RUN      ] ContentType/MasterAPITest.Subscribe/0
I0708 15:42:16.042171 29138 cluster.cpp:155] Creating default 'local' authorizer
I0708 15:42:16.154358 29138 leveldb.cpp:174] Opened db in 111.818825ms
I0708 15:42:16.197175 29138 leveldb.cpp:181] Compacted db in 42.714984ms
I0708 15:42:16.197293 29138 leveldb.cpp:196] Created db iterator in 32582ns
I0708 15:42:16.197324 29138 leveldb.cpp:202] Seeked to beginning of db in 4050ns
I0708 15:42:16.197343 29138 leveldb.cpp:271] Iterated through 0 keys in the db in 538ns
I0708 15:42:16.197417 29138 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0708 15:42:16.198655 29157 recover.cpp:451] Starting replica recovery
I0708 15:42:16.199364 29161 recover.cpp:477] Replica is in EMPTY status
I0708 15:42:16.200865 29161 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (16431)@172.17.0.3:34502
I0708 15:42:16.201282 29158 recover.cpp:197] Received a recover response from a replica in EMPTY status
I0708 15:42:16.203222 29160 recover.cpp:568] Updating replica status to STARTING
I0708 15:42:16.204633 29158 master.cpp:382] Master 2aea5b7f-ec9f-4fda-8f34-877d8adf064f (0382d073a49a) started on 172.17.0.3:34502
I0708 15:42:16.204675 29158 master.cpp:384] Flags at startup: --acls="""" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate_agents=""true"" --authenticate_frameworks=""true"" --authenticate_http=""true"" --authenticate_http_frameworks=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/Lu916I/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_framework_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-1.1.0/_inst/share/mesos/webui"" --work_dir=""/tmp/Lu916I/master"" --zk_session_timeout=""10secs""
I0708 15:42:16.205265 29158 master.cpp:434] Master only allowing authenticated frameworks to register
I0708 15:42:16.205283 29158 master.cpp:448] Master only allowing authenticated agents to register
I0708 15:42:16.205294 29158 master.cpp:461] Master only allowing authenticated HTTP frameworks to register
I0708 15:42:16.205307 29158 credentials.hpp:37] Loading credentials for authentication from '/tmp/Lu916I/credentials'
I0708 15:42:16.205705 29158 master.cpp:506] Using default 'crammd5' authenticator
I0708 15:42:16.205940 29158 master.cpp:578] Using default 'basic' HTTP authenticator
I0708 15:42:16.206192 29158 master.cpp:658] Using default 'basic' HTTP framework authenticator
I0708 15:42:16.206374 29158 master.cpp:705] Authorization enabled
I0708 15:42:16.206866 29172 hierarchical.cpp:151] Initialized hierarchical allocator process
I0708 15:42:16.207018 29172 whitelist_watcher.cpp:77] No whitelist given
I0708 15:42:16.210026 29165 master.cpp:1973] The newly elected leader is master@172.17.0.3:34502 with id 2aea5b7f-ec9f-4fda-8f34-877d8adf064f
I0708 15:42:16.210187 29165 master.cpp:1986] Elected as the leading master!
I0708 15:42:16.210330 29165 master.cpp:1673] Recovering from registrar
I0708 15:42:16.210577 29171 registrar.cpp:332] Recovering registrar
I0708 15:42:16.239378 29160 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 35.540287ms
I0708 15:42:16.239485 29160 replica.cpp:320] Persisted replica status to STARTING
I0708 15:42:16.239938 29161 recover.cpp:477] Replica ",3.0,0,0.5,0.5701357466063348,0.0,0.0,0.0,0.0,0.0,0.6842105263157894,0.5642857142857143,0.038461538461538464,0.07272727272727272,0.07272727272727272,0.0
Bug,ExamplesTest.DynamicReservationFramework is flaky,"Showed up on ASF CI:_x000D_
https://builds.apache.org/job/Mesos/BUILDTOOL=autotools,COMPILER=clang,CONFIGURATION=--verbose%20--enable-libevent%20--enable-ssl,ENVIRONMENT=GLOG_v=1%20MESOS_VERBOSE=1,OS=ubuntu%3A14.04,label_exp=(docker%7C%7CHadoop)&&(!ubuntu-us1)&&(!ubuntu-6)/2466/changes_x000D_
_x000D_
{code}_x000D_
[ RUN      ] ExamplesTest.DynamicReservationFramework_x000D_
Using temporary directory '/tmp/ExamplesTest_DynamicReservationFramework_xp2TU9'_x000D_
/mesos/mesos-1.0.0/src/tests/dynamic_reservation_framework_test.sh: line 19: /mesos/mesos-1.0.0/_build/src/colors.sh: No such file or directory_x000D_
/mesos/mesos-1.0.0/src/tests/dynamic_reservation_framework_test.sh: line 20: /mesos/mesos-1.0.0/_build/src/atexit.sh: No such file or directory_x000D_
WARNING: Logging before InitGoogleLogging() is written to STDERR_x000D_
I0707 19:30:31.102650 29946 resources.cpp:572] Parsing resources as JSON failed: cpus:1;mem:128_x000D_
Trying semicolon-delimited string format instead_x000D_
I0707 19:30:31.125845 29946 process.cpp:1066] libprocess is initialized on 172.17.0.7:37568 with 16 worker threads_x000D_
I0707 19:30:31.125954 29946 logging.cpp:199] Logging to STDERR_x000D_
I0707 19:30:31.237936 29946 leveldb.cpp:174] Opened db in 101.67046ms_x000D_
I0707 19:30:31.272083 29946 leveldb.cpp:181] Compacted db in 34.088797ms_x000D_
I0707 19:30:31.272655 29946 leveldb.cpp:196] Created db iterator in 104307ns_x000D_
I0707 19:30:31.272855 29946 leveldb.cpp:202] Seeked to beginning of db in 20581ns_x000D_
I0707 19:30:31.273027 29946 leveldb.cpp:271] Iterated through 0 keys in the db in 13839ns_x000D_
I0707 19:30:31.273460 29946 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned_x000D_
I0707 19:30:31.277535 29979 recover.cpp:451] Starting replica recovery_x000D_
I0707 19:30:31.279044 29979 recover.cpp:477] Replica is in EMPTY status_x000D_
I0707 19:30:31.285576 29984 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (3)@172.17.0.7:37568_x000D_
I0707 19:30:31.290812 29983 recover.cpp:197] Received a recover response from a replica in EMPTY status_x000D_
I0707 19:30:31.300268 29972 recover.cpp:568] Updating replica status to STARTING_x000D_
I0707 19:30:31.307143 29946 local.cpp:255] Creating default 'local' authorizer_x000D_
I0707 19:30:31.324632 29972 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 23.394808ms_x000D_
I0707 19:30:31.325036 29972 replica.cpp:320] Persisted replica status to STARTING_x000D_
I0707 19:30:31.325812 29972 recover.cpp:477] Replica is in STARTING status_x000D_
I0707 19:30:31.328284 29972 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (5)@172.17.0.7:37568_x000D_
I0707 19:30:31.328945 29972 recover.cpp:197] Received a recover response from a replica in STARTING status_x000D_
I0707 19:30:31.329859 29972 recover.cpp:568] Updating replica status to VOTING_x000D_
I0707 19:30:31.335539 29974 master.cpp:382] Master 443ee691-d272-454c-90fe-959c95948252 (89b080073abb) started on 172.17.0.7:37568_x000D_
I0707 19:30:31.335839 29974 master.cpp:384] Flags at startup: --acls=""permissive: true_x000D_
register_frameworks {_x000D_
  principals {_x000D_
    type: ANY_x000D_
  }_x000D_
  roles {_x000D_
    type: SOME_x000D_
    values: ""test""_x000D_
  }_x000D_
}_x000D_
"" --agent_ping_timeout=""15secs"" --agent_reregister_timeout=""10mins"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate_agents=""false"" --authenticate_frameworks=""false"" --authenticate_http=""false"" --authenticate_http_frameworks=""false"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/ExamplesTest_DynamicReservationFramework_xp2TU9/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_agent_ping_timeouts=""5"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --quiet=""false"" --recovery_agent_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""20secs"" --registry_strict=""false"" --root_submissions=""true"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-1.0.0/src/webui"" --work_dir=""/tmp/mesos-zPIQS8"" --zk_session_timeout=""10secs""_x000D_
I0707 19:30:31.337158 29974 master.cpp:436] Master allowing unauthenticated frameworks to register_x000D_
I0707 19:30:31.337323 29974 master.cpp:450] Master allowing unauthenticated agents to register_x000D_
I0707 19:30:31.337527 29974 master.cpp:464] Master allowing HTTP frameworks to register without authentication_x000D_
I0707 19:30:31.337689 29974 credentials.hpp:37] Loading credentials for authentication from '/tmp/ExamplesTest_DynamicReservationFramework_xp2TU9/credentials'_x000D_
W0707 19:30:31.337962 29974 credentials.hpp:52] Permissions on credentials file '/tmp/ExamplesTest_DynamicReservationFramework_xp2TU9/credentials' are too open. It is recommended that your credentials file is NOT accessible by others._x000D_
I0707 19:30:31.338336 29974 master.cpp:506] Using default 'crammd5' authenticator_x000D_
I0707 19:30:31.338723 29974 authenticator.cpp:519] Initializing server SASL_x000D_
I0707 19:30:31.340744 29974 auxprop.cpp:73] Initialized in-memory auxiliary property plugin_x000D_
I0707 19:30:31.341084 29974 master.cpp:705] Authorization enabled_x000D_
I0707 19:30:31.342696 29971 hierarchical.cpp:151] Initialized hierarchical allocator process_x000D_
I0707 19:30:31.342895 29977 whitelist_watcher.cpp:77] No whitelist given_x000D_
I0707 19:30:31.358129 29972 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 27.780299ms_x000D_
I0707 19:30:31.358496 29972 replica.cpp:320] Persisted replica status to VOTING_x000D_
I0707 19:30:31.358949 29972 recover.cpp:582] Successfully joined the Paxos group_x000D_
I0707 19:30:31.359601 29972 recover.cpp:466] Recover process terminated_x000D_
I0707 19:30:31.365345 29946 containerizer.cpp:196] Using isolation: filesystem/posix,posix/cpu,posix/mem,network/cni_x000D_
W0707 19:30:31.368975 29946 backend.cpp:75] Failed to create 'aufs' backend: AufsBackend requires root privileges, but is running as user mesos_x000D_
W0707 19:30:31.369699 29946 backend.cpp:75] Failed to create 'bind' backend: BindBackend requires root privileges_x000D_
I0707 19:30:31.393633 29977 slave.cpp:205] Agent started on 1)@172.17.0.7:37568_x000D_
I0707 19:30:31.394129 29977 slave.cpp:206] Flags at startup: --acls=""permissive: true_x000D_
register_frameworks {_x000D_
  principals {_x000D_
    type: ANY_x000D_
  }_x000D_
  roles {_x000D_
    type: SOME_x000D_
    values: ""test""_x000D_
  }_x000D_
}_x000D_
"" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""false"" --authenticatee=""crammd5"" --authentication_backoff_factor=""1secs"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/mesos/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_command_executor=""false"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""filesystem/posix,posix/cpu,posix/mem"" --launcher=""posix"" --launcher_dir=""/mesos/mesos-1.0.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""1secs"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/mesos-zPIQS8/0""_x000D_
I0707 19:30:31.395762 29977 resources.cpp:572] Parsing resources as JSON failed: _x000D_
Trying semicolon-delimited string format instead_x000D_
I0707 19:30:31.396198 29977 resources.cpp:572] Parsing resources as JSON failed: _x000D_
Trying semicolon-delimited string format instead_x000D_
I0707 19:30:31.397099 29977 slave.cpp:594] Agent resources: cpus(*):16; mem(*):47270; disk(*):3.70122e+06; ports(*):[31000-32000]_x000D_
I0707 19:30:31.397364 29977 slave.cpp:602] Agent attributes: [  ]_x000D_
I0707 19:30:31.397557 29977 slave.cpp:607] Agent hostname: 89b080073abb_x000D_
I0707 19:30:31.403342 29981 state.cpp:57] Recovering state from '/tmp/mesos-zPIQS8/0/meta'_x000D_
I0707 19:30:31.411643 29973 status_update_manager.cpp:200] Recovering status update manager_x000D_
I0707 19:30:31.412467 29983 containerizer.cpp:522] Recovering containerizer_x000D_
I0707 19:30:31.417868 29975 provisioner.cpp:253] Provisioner recovery complete_x000D_
I0707 19:30:31.419260 29977 slave.cpp:4856] Finished recovery_x000D_
I0707 19:30:31.420929 29977 slave.cpp:5028] Querying resource estimator for oversubscribable resources_x000D_
I0707 19:30:31.422238 29970 status_update_manager.cpp:174] Pausing sending status updates_x000D_
I0707 19:30:31.422533 29977 slave.cpp:969] New master detected at master@172.17.0.7:37568_x000D_
I0707 19:30:31.422721 29977 slave.cpp:990] No credentials provided. Attempting to register without authentication_x000D_
I0707 19:30:31.422902 29977 slave.cpp:1001] Detecting new master_x000D_
I0707 19:30:31.423362 29977 slave.cpp:5042] Received oversubscribable resources  from the resource estimator_x000D_
I0707 19:30:31.429898 29974 master.cpp:1973] The newly elected leader is master@172.17.0.7:37568 with id 443ee691-d272-454c-90fe-959c95948252_x000D_
I0707 19:30:31.429949 29974 master.cpp:1986] Elected as the leading master!_x000D_
I0707 19:30:31.429968 29974 master.cpp:1673] Recovering from registrar_x000D_
I0707 19:30:31.431020 29976 registrar.cpp:332] Recovering registrar_x000D_
I0707 19:30:31.433168 29971 log.cpp:553] Attempting to start the writer_x000D_
I0707 19:30:31.439359 29982 replica.cpp:493] Replica received implicit promise request from (21)@172.17.0.7:37568 with proposal 1_x000D_
I0707 19:30:31.441862 29946 containerizer.cpp:196] Using isolation: filesystem/posix,posix/cpu,posix/mem,network/cni_x000D_
W0707 19:30:31.443104 29946 backend.cpp:75] Failed to create 'aufs' backend: AufsBackend requires root privileges, but is running as user mesos_x000D_
W0707 19:30:31.443366 29946 backend.cpp:75] Failed to create 'bind' backend: BindBackend requires root privileges_x000D_
I0707 19:30:31.457201 29975 slave.cpp:205] Agent started on 2)@172.17.0.7:37568_x000D_
I0707 19:30:31.457254 29975 slave.cpp:206] Flags at startup: --acls=""permissive: true_x000D_
register_frameworks {_x000D_
  principals {_x000D_
    type: ANY_x000D_
  }_x000D_
  roles {_x000D_
    type: SOME_x000D_
    values: ""test""_x000D_
  }_x000D_
}_x000D_
"" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""false"" --authenticatee=""crammd5"" --authentication_backoff_factor=""1secs"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/mesos/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_command_executor=""false"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""filesystem/posix,posix/cpu,posix/mem"" --launcher=""posix"" --launcher_dir=""/mesos/mesos-1.0.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""1secs"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/mesos-zPIQS8/1""_x000D_
I0707 19:30:31.458678 29982 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 19.283309ms_x000D_
I0707 19:30:31.458717 29982 replica.cpp:342] Persisted promised to 1_x000D_
I0707 19:30:31.461284 29969 coordinator.cpp:238] Coordinator attempting to fill missing positions_x000D_
I0707 19:30:31.461690 29975 resources.cpp:572] Parsing resources as JSON failed: _x000D_
Trying semicolon-delimited string format instead_x000D_
I0707 19:30:31.461866 29975 resources.cpp:572] Parsing resources as JSON failed: _x000D_
Trying semicolon-delimited string format instead_x000D_
I0707 19:30:31.462319 29975 slave.cpp:594] Agent resources: cpus(*):16; mem(*):47270; disk(*):3.70122e+06; ports(*):[31000-32000]_x000D_
I0707 19:30:31.462396 29975 slave.cpp:602] Agent attributes: [  ]_x000D_
I0707 19:30:31.464599 29975 slave.cpp:607] Agent hostname: 89b080073abb_x000D_
I0707 19:30:31.466464 29978 replica.cpp:388] Replica received explicit promise request from (33)@172.17.0.7:37568 for position 0 with proposal 2_x000D_
I0707 19:30:31.468361 29975 state.cpp:57] Recovering state from '/tmp/mesos-zPIQS8/1/meta'_x000D_
I0707 19:30:31.468951 29975 status_update_manager.cpp:200] Recovering status update manager_x000D_
I0707 19:30:31.469187 29975 containerizer.cpp:522] Recovering containerizer_x000D_
I0707 19:30:31.472386 29969 provisioner.cpp:253] Provisioner recovery complete_x000D_
I0707 19:30:31.473125 29969 slave.cpp:4856] Finished recovery_x000D_
I0707 19:30:31.473996 29969 slave.cpp:5028] Querying resource estimator for oversubscribable resources_x000D_
I0707 19:30:31.474643 29982 slave.cpp:969] New master detected at master@172.17.0.7:37568_x000D_
I0707 19:30:31.474673 29982 slave.cpp:990] No credentials provided. Attempting to register without authentication_x000D_
I0707 19:30:31.474726 29982 slave.cpp:1001] Detecting new master_x000D_
I0707 19:30:31.474833 29982 status_update_manager.cpp:174] Pausing sending status updates_x000D_
I0707 19:30:31.475157 29969 slave.cpp:5042] Received oversubscribable resources  from the resource estimator_x000D_
I0707 19:30:31.479303 29946 containerizer.cpp:196] Using isolation: filesystem/posix,posix/cpu,posix/mem,network/cni_x000D_
W0707 19:30:31.484933 29946 backend.cpp:75] Failed to create 'aufs' backend: AufsBackend requires root privileges, but is running as user mesos_x000D_
W0707 19:30:31.485230 29946 backend.cpp:75] Failed to create 'bind' backend: BindBackend requires root privileges_x000D_
I0707 19:30:31.492482 29978 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 25.968225ms_x000D_
I0707 19:30:31.492543 29978 replica.cpp:712] Persisted action at 0_x000D_
I0707 19:30:31.495333 29972 replica.cpp:537] Replica received write request for position 0 from (46)@172.17.0.7:37568_x000D_
I0707 19:30:31.495918 29972 leveldb.cpp:436] Reading position from leveldb took 553942ns_x000D_
I0707 19:30:31.505445 29973 slave.cpp:205] Agent started on 3)@172.17.0.7:37568_x000D_
I0707 19:30:31.505492 29973 slave.cpp:206] Flags at startup: --acls=""permissive: true_x000D_
register_frameworks {_x000D_
  principals {_x000D_
    type: ANY_x000D_
  }_x000D_
  roles {_x000D_
    type: SOME_x000D_
    values: ""test""_x000D_
  }_x000D_
}_x000D_
"" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""false"" --authenticatee=""crammd5"" --authentication_backoff_factor=""1secs"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/mesos/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_command_executor=""false"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""filesystem/posix,posix/cpu,posix/mem"" --launcher=""posix"" --launcher_dir=""/mesos/mesos-1.0.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""1secs"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/mesos-zPIQS8/2""_x000D_
I0707 19:30:31.506813 29973 resources.cpp:572] Parsing resources as JSON failed: _x000D_
Trying semicolon-delimited string format instead_x000D_
I0707 19:30:31.506990 29973 resources.cpp:572] Parsing resources as JSON failed: _x000D_
Trying semicolon-delimited string format instead_x000D_
I0707 19:30:31.507602 29973 slave.cpp:594] Agent resources: cpus(*):16; mem(*):47270; disk(*):3.70122e+06; ports(*):[31000-32000]_x000D_
I0707 19:30:31.507680 29973 slave.cpp:602] Agent attributes: [  ]_x000D_
I0707 19:30:31.507695 29973 slave.cpp:607] Agent hostname: 89b080073abb_x000D_
I0707 19:30:31.510499 29973 state.cpp:57] Recovering state from '/tmp/mesos-zPIQS8/2/meta'_x000D_
I0707 19:30:31.511034 29973 status_update_manager.cpp:200] Recovering status update manager_x000D_
I0707 19:30:31.511270 29973 containerizer.cpp:522] Recovering containerizer_x000D_
I0707 19:30:31.514657 29984 provisioner.cpp:253] Provisioner recovery complete_x000D_
I0707 19:30:31.515745 29970 slave.cpp:4856] Finished recovery_x000D_
I0707 19:30:31.516332 29970 slave.cpp:5028] Querying resource estimator for oversubscribable resources_x000D_
I0707 19:30:31.517103 29970 slave.cpp:969] New master detected at master@172.17.0.7:37568_x000D_
I0707 19:30:31.517134 29970 slave.cpp:990] No credentials provided. Attempting to register without authentication_x000D_
I0707 19:30:31.517190 29970 slave.cpp:1001] Detecting new master_x000D_
I0707 19:30:31.517294 29970 slave.cpp:5042] Received oversubscribable resources  from the resource estimator_x000D_
I0707 19:30:31.517375 29970 status_update_manager.cpp:174] Pausing sending status updates_x000D_
I0707 19:30:31.519979 29946 sched.cpp:226] Version: 1.0.0_x000D_
I0707 19:30:31.521474 29980 sched.cpp:330] New master detected at master@172.17.0.7:37568_x000D_
I0707 19:30:31.521586 29980 sched.cpp:341] No credentials provided. Attempting to register without authentication_x000D_
I0707 19:30:31.521613 29980 sched.cpp:820] Sending SUBSCRIBE call to master@172.17.0.7:37568_x000D_
I0707 19:30:31.521769 29980 sched.cpp:853] Will retry registration in 898.210224ms if necessary_x000D_
I0707 19:30:31.521977 29980 master.cpp:1500] Dropping 'mesos.scheduler.Call' message since not recovered yet_x000D_
I0707 19:30:31.522469 29972 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 26.469135ms_x000D_
I0707 19:30:31.522514 29972 replica.cpp:712] Persisted action at 0_x000D_
I0707 19:30:31.523948 29980 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0_x000D_
I0707 19:30:31.538797 29972 slave.cpp:1529] Will retry registration in 1.972934225secs if necessary_x000D_
I0707 19:30:31.538925 29972 master.cpp:1500] Dropping 'mesos.internal.RegisterSlaveMessage' message since not recovered yet_x000D_
I0707 19:30:31.555934 29980 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 31.978704ms_x000D_
I0707 19:30:31.556016 29980 replica.cpp:712] Persisted action at 0_x000D_
I0707 19:30:31.556066 29980 replica.cpp:697] Replica learned NOP action at position 0_x000D_
I0707 19:30:31.557960 29980 log.cpp:569] Writer started with ending position 0_x000D_
I0707 19:30:31.561957 29976 leveldb.cpp:436] Reading position from leveldb took 90775ns_x000D_
I0707 19:30:31.566825 29979 slave.cpp:1529] Will retry registration in 382.223275ms if necessary_x000D_
I0707 19:30:31.566967 29979 master.cpp:1500] Dropping 'mesos.internal.RegisterSlaveMessage' message since not recovered yet_x000D_
I0707 19:30:31.582073 29981 registrar.cpp:365] Successfully fetched the registry (0B) in 150.98496ms_x000D_
I0707 19:30:31.582437 29981 registrar.cpp:464] Applied 1 operations in 94170ns; attempting to update the 'registry'_x000D_
I0707 19:30:31.587924 29975 log.cpp:577] Attempting to append 168 bytes to the log_x000D_
I0707 19:30:31.588234 29975 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1_x000D_
I0707 19:30:31.589561 29978 replica.cpp:537] Replica received write request for position 1 from (51)@172.17.0.7:37568_x000D_
I0707 19:30:31.621119 29978 leveldb.cpp:341] Persisting action (187 bytes) to leveldb took 31.540172ms_x000D_
I0707 19:30:31.621209 29978 replica.cpp:712] Persisted action at 1_x000D_
I0707 19:30:31.623564 29978 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0_x000D_
I0707 19:30:31.656234 29978 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 32.657222ms_x000D_
I0707 19:30:31.656424 29978 replica.cpp:712] Persisted action at 1_x000D_
I0707 19:30:31.656786 29978 replica.cpp:697] Replica learned APPEND action at position 1_x000D_
I0707 19:30:31.660815 29978 registrar.cpp:509] Successfully updated the 'registry' in 78.219008ms_x000D_
I0707 19:30:31.661057 29978 registrar.cpp:395] Successfully recovered registrar_x000D_
I0707 19:30:31.661593 29978 log.cpp:596] Attempting to truncate the log to 1_x000D_
I0707 19:30:31.662271 29978 master.cpp:1781] Recovered 0 agents from the Registry (129B) ; allowing 10mins for agents to re-register_x000D_
I0707 19:30:31.662566 29978 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2_x000D_
I0707 19:30:31.663004 29978 hierarchical.cpp:178] Skipping recovery of hierarchical allocator: nothing to recover_x000D_
I0707 19:30:31.664005 29975 replica.cpp:537] Replica received write request for position 2 from (52)@172.17.0.7:37568_x000D_
I0707 19:30:31.696493 29975 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 32.24974ms_x000D_
I0707 19:30:31.696583 29975 replica.cpp:712] Persisted action at 2_x000D_
I0707 19:30:31.698271 29984 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0_x000D_
I0707 19:30:31.731513 29984 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 32.894448ms_x000D_
I0707 19:30:31.731775 29984 leveldb.cpp:399] Deleting ~1 keys from leveldb took 95908ns_x000D_
I0707 19:30:31.732022 29984 replica.cpp:712] Persisted action at 2_x000D_
I0707 19:30:31.732120 29984 replica.cpp:697] Replica learned TRUNCATE action at position 2_x000D_
I0707 19:30:31.950920 29984 slave.cpp:1529] Will retry registration in 3.638047644secs if necessary_x000D_
I0707 19:30:31.951601 29983 master.cpp:4676] Registering agent at slave(3)@172.17.0.7:37568 (89b080073abb) with id 443ee691-d272-454c-90fe-959c95948252-S0_x000D_
I0707 19:30:31.953089 29974 registrar.cpp:464] Applied 1 operations in 182983ns; attempting to update the 'registry'_x000D_
I0707 19:30:31.957223 29983 log.cpp:577] Attempting to append 337 bytes to the log_x000D_
I0707 19:30:31.957545 29983 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3_x000D_
I0707 19:30:31.958920 29983 replica.cpp:537] Replica received write request for position 3 from (53)@172.17.0.7:37568_x000D_
I0707 19:30:31.989977 29983 leveldb.cpp:341] Persisting action (356 bytes) to leveldb took 30.902846ms_x000D_
I0707 19:30:31.990154 29983 replica.cpp:712] Persisted action at 3_x000D_
I0707 19:30:31.991781 29974 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0_x000D_
I0707 19:30:32.024132 29974 leveldb.cpp:341] Persisting action (358 bytes) to leveldb took 32.308737ms_x000D_
I0707 19:30:32.024305 29974 replica.cpp:712] Persisted action at 3_x000D_
I0707 19:30:32.024449 29974 replica.cpp:697] Replica learned APPEND action at position 3_x000D_
I0707 19:30:32.027683 29975 registrar.cpp:509] Successfully updated the 'registry' in 74.444032ms_x000D_
I0707 19:30:32.029734 29974 log.cpp:596] Attempting to truncate the log to 3_x000D_
I0707 19:30:32.030093 29974 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4_x000D_
I0707 19:30:32.030804 29974 slave.cpp:3760] Received ping from slave-observer(1)@172.17.0.7:37568_x000D_
I0707 19:30:32.031373 29974 slave.cpp:1169] Registered with master master@172.17.0.7:37568; given agent ID 443ee691-d272-454c-90fe-959c95948252-S0_x000D_
I0707 19:30:32.031460 29974 fetcher.cpp:86] Clearing fetcher cache_x000D_
I0707 19:30:32.032008 29974 slave.cpp:1192] Checkpointing SlaveInfo to '/tmp/mesos-zPIQS8/2/meta/slaves/443ee691-d272-454c-90fe-959c95948252-S0/slave.info'_x000D_
I0707 19:30:32.031088 29975 master.cpp:4745] Registered agent 443ee691-d272-454c-90fe-959c95948252-S0 at slave(3)@172.17.0.7:37568 (89b080073abb) with cpus(*):16; mem(*):47270; disk(*):3.70122e+06; ports(*):[31000-32000]_x000D_
I0707 19:30:32.033082 29975 hierarchical.cpp:478] Added agent 443ee691-d272-454c-90fe-959c95948252-S0 (89b080073abb) with cpus(*):16; mem(*):47270; disk(*):3.70122e+06; ports(*):[31000-32000] (allocated: )_x000D_
I0707 19:30:32.033608 29975 hierarchical.cpp:1537] No allocations performed_x000D_
I0707 19:30:32.033747 29975 hierarchical.cpp:1195] Performed allocation for agent 443ee691-d272-454c-90fe-959c95948252-S0 in 584676ns_x000D_
I0707 19:30:32.034116 29975 status_update_manager.cpp:181] Resuming sending status updates_x000D_
I0707 19:30:32.034010 29974 slave.cpp:1229] Forwarding total oversubscribed resources _x000D_
I0707 19:30:32.034950 29974 master.cpp:5128] Received update of agent 443ee691-d272-454c-90fe-959c95948252-S0 at slave(3)@172.17.0.7:37568 (89b080073abb) with total oversubscribed resources _x000D_
I0707 19:30:32.035320 29975 replica.cpp:537] Replica received write request for position 4 from (54)@172.17.0.7:37568_x000D_
I0707 19:30:32.036041 29971 hierarchical.cpp:542] Agent 443ee691-d272-454c-90fe-959c95948252-S0 (89b080073abb) updated with oversubscribed resources  (total: cpus(*):16; mem(*):47270; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: )_x000D_
I0707 19:30:32.036212 29971 hierarchical.cpp:1537] No allocations performed_x000D_
I0707 19:30:32.036327 29971 hierarchical.cpp:1195] Performed allocation for agent 443ee691-d272-454c-90fe-959c95948252-S0 in 212809ns_x000D_
I0707 19:30:32.196679 29976 master.cpp:4676] Registering agent at slave(2)@172.17.0.7:37568 (89b080073abb) with id 443ee691-d272-454c-90fe-959c95948252-S1_x000D_
I0707 19:30:32.196384 29979 slave.cpp:1529] Will retry registration in 1.893622708secs if necessary_x000D_
I0707 19:30:32.197633 29976 registrar.cpp:464] Applied 1 operations in 273890ns; attempting to update the 'registry'_x000D_
I0707 19:30:32.343791 29979 hierarchical.cpp:1537] No allocations performed_x000D_
I0707 19:30:32.344105 29979 hierarchical.cpp:1172] Performed allocation for 1 agents in 555357ns_x000D_
I0707 19:30:32.373800 29975 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 338.056804ms_x000D_
I0707 19:30:32.373987 29975 replica.cpp:712] Persisted action at 4_x000D_
I0707 19:30:32.387712 29973 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0_x000D_
I0707 19:30:32.420934 29981 sched.cpp:820] Sending SUBSCRIBE call to master@172.17.0.7:37568_x000D_
I0707 19:30:32.421331 29981 sched.cpp:853] Will retry registration in 2.058099434secs if necessary_x000D_
I0707 19:30:32.421792 29981 master.cpp:2550] Received SUBSCRIBE call for framework 'Dynamic Reservation Framework (C++)' at scheduler-a956abb7-0f5d-46e3-a670-a3f684eccbb5@172.17.0.7:37568_x000D_
I0707 19:30:32.421934 29981 master.cpp:2012] Authorizing framework principal 'test' to receive offers for role 'test'_x000D_
I0707 19:30:32.423535 29981 master.cpp:2626] Subscribing framework Dynamic Reservation Framework (C++) with checkpointing disabled and capabilities [  ]_x000D_
I0707 19:30:32.425323 29976 hierarchical.cpp:271] Added framework 443ee691-d272-454c-90fe-959c95948252-0000_x000D_
I0707 19:30:32.426686 29973 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 38.63187ms_x000D_
I0707 19:30:32.426865 29973 leveldb.cpp:399] Deleting ~2 keys from leveldb took 95262ns_x000D_
I0707 19:30:32.426981 29973 replica.cpp:712] Persisted action at 4_x000D_
I0707 19:30:32.427096 29973 replica.cpp:697] Replica learned TRUNCATE action at position 4_x000D_
I0707 19:30:32.428614 29973 log.cpp:577] Attempting to append 503 bytes to the log_x000D_
I0707 19:30:32.426307 29981 sched.cpp:743] Framework registered with 443ee691-d272-454c-90fe-959c95948252-0000_x000D_
I0707 19:30:32.428905 29981 dynamic_reservation_framework.cpp:73] Registered!_x000D_
I0707 19:30:32.429059 29981 sched.cpp:757] Scheduler::registered took 167468ns_x000D_
I0707 19:30:32.429239 29981 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 5_x000D_
I0707 19:30:32.431745 29976 hierarchical.cpp:1632] No inverse offers to send out!_x000D_
I0707 19:30:32.432610 29984 master.cpp:5835] Sending 1 offers to framework 443ee691-d272-454c-90fe-959c95948252-0000 (Dynamic Reservation Framework (C++)) at scheduler-a956abb7-0f5d-46e3-a670-a3f684eccbb5@172.17.0.7:37568_x000D_
I0707 19:30:32.433627 29984 dynamic_reservation_framework.cpp:84] Received offer 443ee691-d272-454c-90fe-959c95948252-O0 with cpus(*):16; mem(*):47270; disk(*):3.70122e+06; ports(*):[31000-32000]_x000D_
I0707 19:30:32.434248 29984 sched.cpp:917] Scheduler::resourceOffers took 642030ns_x000D_
I0707 19:30:32.436048 29984 master.cpp:3468] Processing ACCEPT call for offers: [ 443ee691-d272-454c-90fe-959c95948252-O0 ] on agent 443ee691-d272-454c-90fe-959c95948252-S0 at slave(3)@172.17.0.7:37568 (89b080073abb) for framework 443ee691-d272-454c-90fe-959c95948252-0000 (Dynamic Reservation Framework (C++)) at scheduler-a956abb7-0f5d-46e3-a670-a3f684eccbb5@172.17.0.7:37568_x000D_
I0707 19:30:32.436368 29984 master.cpp:3144] Authorizing principal 'test' to reserve resources 'cpus(test, test):1; mem(test, test):128'_x000D_
I0707 19:30:32.438547 29976 hierarchical.cpp:1172] Performed allocation for 1 agents in 12.203221ms_x000D_
I0707 19:30:32.432860 29981 replica.cpp:537] Replica received write request for position 5 from (55)@172.17.0.7:37568_x000D_
I0707 19:30:32.439970 29984 master.cpp:3695] Applying RESERVE operation for resources cpus(test, test):1; mem(test, test):128 from framework 443ee691-d272-454c-90fe-959c95948252-0000 (Dynamic Reservation Framework (C++)) at scheduler-a956abb7-0f5d-46e3-a670-a3f684eccbb5@172.17.0.7:37568 to agent 443ee691-d272-454c-90fe-959c95948252-S0 at slave(3)@172.17.0.7:37568 (89b080073abb)_x000D_
I0707 19:30:32.440765 29984 master.cpp:7098] Sending checkpointed resources cpus(test, test):1; mem(test, test):128 to agent 443ee691-d272-454c-90fe-959c95948252-S0 at slave(3)@172.17.0.7:37568 (89b080073abb)_x000D_
I0707 19:30:32.444211 29976 hierarchical.cpp:683] Updated allocation of framework 443ee691-d272-454c-90fe-959c95948252-0000 on agent 443ee691-d272-454c-90fe-959c95948252-S0 from cpus(*):16; mem(*):47270; disk(*):3.70122e+06; ports(*):[31000-32000] to cpus(*):15; mem(*):47142; disk(*):3.70122e+06; ports(*):[31000-32000]; cpus(test, test):1; mem(test, test):128_x000D_
I0707 19:30:32.444527 29984 slave.cpp:2600] Updated checkpointed resources from  to cpus(test, test):1; mem(test, test):128_x000D_
I0707 19:30:32.445664 29976 hierarchical.cpp:924] Recovered cpus(*):15; mem(*):47142; disk(*):3.70122e+06; ports(*):[31000-32000]; cpus(test, test):1; mem(test, test):128 (total: cpus(*):15; mem(*):47142; disk(*):3.70122e+06; ports(*):[31000-32000]; cpus(test, test):1; mem(test, test):128, allocated: ) on agent 443ee691-d272-454c-90fe-959c95948252-S0 from framework 443ee691-d272-454c-90fe-959c95948252-0000_x000D_
I0707 19:30:32.467499 29981 leveldb.cpp:341] Persisting action (522 bytes) to leveldb took 28.613107ms_x000D_
I0707 19:30:32.467705 29981 replica.cpp:712] Persisted action at 5_x000D_
I0707 19:30:32.483840 29971 replica.cpp:691] Replica received learned notice for position 5 from @0.0.0.0:0_x000D_
I0707 19:30:32.511849 29971 leveldb.cpp:341] Persisting action (524 bytes) to leveldb took 27.859875ms_x000D_
I0707 19:30:32.512235 29971 replica.cpp:712] Persisted action at 5_x000D_
I0707 19:30:32.512511 29971 replica.cpp:697] Replica learned APPEND action at position 5_x000D_
I0707 19:30:32.516393 29971 registrar.cpp:509] Successfully upd",3.0,0,0.5,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.3846153846153846,0.4181818181818182,0.4121212121212121,0.0
Bug,SlaveAuthorizerTest/0.ViewFlags is flaky.,"{noformat}
[15:24:47] :	 [Step 10/10] [ RUN      ] SlaveAuthorizerTest/0.ViewFlags
[15:24:47]W:	 [Step 10/10] I0707 15:24:47.025609 25322 containerizer.cpp:196] Using isolation: posix/cpu,posix/mem,filesystem/posix,network/cni
[15:24:47]W:	 [Step 10/10] I0707 15:24:47.030421 25322 linux_launcher.cpp:101] Using /sys/fs/cgroup/freezer as the freezer hierarchy for the Linux launcher
[15:24:47]W:	 [Step 10/10] I0707 15:24:47.032060 25339 slave.cpp:205] Agent started on 335)@172.30.2.7:43076
[15:24:47]W:	 [Step 10/10] I0707 15:24:47.032078 25339 slave.cpp:206] Flags at startup: --acls="""" --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""true"" --authenticatee=""crammd5"" --authentication_backoff_factor=""1secs"" --authorizer=""local"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/mnt/teamcity/temp/buildTmp/SlaveAuthorizerTest_0_ViewFlags_OsJb5C/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --docker_volume_checkpoint_dir=""/var/run/mesos/isolators/docker/volume"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/mnt/teamcity/temp/buildTmp/SlaveAuthorizerTest_0_ViewFlags_OsJb5C/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""true"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_command_executor=""false"" --http_credentials=""/mnt/teamcity/temp/buildTmp/SlaveAuthorizerTest_0_ViewFlags_OsJb5C/http_credentials"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mnt/teamcity/work/4240ba9ddd0997c3/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/mnt/teamcity/temp/buildTmp/SlaveAuthorizerTest_0_ViewFlags_OsJb5C"" --xfs_project_range=""[5000-10000]""
[15:24:47]W:	 [Step 10/10] I0707 15:24:47.032306 25339 credentials.hpp:86] Loading credential for authentication from '/mnt/teamcity/temp/buildTmp/SlaveAuthorizerTest_0_ViewFlags_OsJb5C/credential'
[15:24:47]W:	 [Step 10/10] I0707 15:24:47.032424 25339 slave.cpp:343] Agent using credential for: test-principal
[15:24:47]W:	 [Step 10/10] I0707 15:24:47.032441 25339 credentials.hpp:37] Loading credentials for authentication from '/mnt/teamcity/temp/buildTmp/SlaveAuthorizerTest_0_ViewFlags_OsJb5C/http_credentials'
[15:24:47]W:	 [Step 10/10] I0707 15:24:47.032528 25339 slave.cpp:395] Using default 'basic' HTTP authenticator
[15:24:47]W:	 [Step 10/10] I0707 15:24:47.032754 25339 resources.cpp:572] Parsing resources as JSON failed: cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]
[15:24:47]W:	 [Step 10/10] Trying semicolon-delimited string format instead
[15:24:47]W:	 [Step 10/10] I0707 15:24:47.032838 25339 resources.cpp:572] Parsing resources as JSON failed: cpus:2;gpus:0;mem:1024;disk:1024;ports:[31000-32000]
[15:24:47]W:	 [Step 10/10] Trying semicolon-delimited string format instead
[15:24:47]W:	 [Step 10/10] I0707 15:24:47.032968 25339 slave.cpp:594] Agent resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
[15:24:47]W:	 [Step 10/10] I0707 15:24:47.032994 25339 slave.cpp:602] Agent attributes: [  ]
[15:24:47]W:	 [Step 10/10] I0707 15:24:47.032999 25339 slave.cpp:607] Agent hostname: ip-172-30-2-7.ec2.internal.mesosphere.io
[15:24:47]W:	 [Step 10/10] I0707 15:24:47.033291 25339 process.cpp:3322] Handling HTTP event for process 'slave(335)' with path: '/slave(335)/flags'
[15:24:47]W:	 [Step 10/10] I0707 15:24:47.033329 25343 state.cpp:57] Recovering state from '/mnt/teamcity/temp/buildTmp/SlaveAuthorizerTest_0_ViewFlags_OsJb5C/meta'
[15:24:47]W:	 [Step 10/10] I0707 15:24:47.033576 25342 status_update_manager.cpp:200] Recovering status update manager
[15:24:47] :	 [Step 10/10] ../../src/tests/slave_authorization_tests.cpp:316: Failure
[15:24:47]W:	 [Step 10/10] I0707 15:24:47.033604 25340 http.cpp:269] HTTP GET for /slave(335)/flags from 172.30.2.7:33866
[15:24:47] :	 [Step 10/10] Value of: (response).get().status
[15:24:47] :	 [Step 10/10]   Actual: ""503 Service Unavailable""
[15:24:47]W:	 [Step 10/10] I0707 15:24:47.033687 25340 containerizer.cpp:522] Recovering containerizer
[15:24:47] :	 [Step 10/10] Expected: OK().status
[15:24:47] :	 [Step 10/10] Which is: ""200 OK""
[15:24:47]W:	 [Step 10/10] I0707 15:24:47.034953 25340 process.cpp:3322] Handling HTTP event for process 'slave(335)' with path: '/slave(335)/state'
[15:24:47] :	 [Step 10/10] Agent has not finished recovery
[15:24:47] :	 [Step 10/10] ../../src/tests/slave_authorization_tests.cpp:320: Failure
[15:24:47]W:	 [Step 10/10] I0707 15:24:47.035152 25343 http.cpp:269] HTTP GET for /slave(335)/state from 172.30.2.7:33868
[15:24:47] :	 [Step 10/10] parse: syntax error at line 1 near: Agent has not finished recovery
[15:24:47]W:	 [Step 10/10] I0707 15:24:47.035768 25341 slave.cpp:841] Agent terminating
[15:24:47]W:	 [Step 10/10] I0707 15:24:47.036150 25337 provisioner.cpp:253] Provisioner recovery complete
[15:24:47] :	 [Step 10/10] [  FAILED  ] SlaveAuthorizerTest/0.ViewFlags, where TypeParam = mesos::internal::LocalAuthorizer (14 ms)
{noformat}",2.0,0,0.5,0.004524886877828055,0.0,0.0,0.0,0.0,0.2,0.12280701754385964,0.24285714285714285,0.7948717948717948,1.0,1.0,0.0
Improvement,Add ability to set framework capabilities in 'mesos-execute',"For now, we want to add this so that we can run {{mesos-execute}} against agents that offer GPU resources. In the future, as we add more framework capabilities, this functionality will become more generally useful.",2.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Improvement,Add get_abi_version() to ELF abstraction in stout,This function allows us to inspect the {{.note.ABI-tag}} section of an ELF binary to determine the ABI version of the executable / library.  This is needed for checking soe of the logic in building up an NvidiaVolume for injection into a container. ,2.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Improvement,Reimplement the stout ELF abstraction in terms of ELFIO,"With the introduction of the new bundled ELFIO library, we need to reimplement our stout ELF abstraction in terms of it.
As part of this, we need to update the tests that use it (i.e. ldcache_test.cpp)",2.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Improvement,Add ELFIO as bundled Dependency to Mesos,"ELFIO is a header-only replacement for parsing ELF binaries. Previously we were using libelf, which introduced both a new build-time dependency as well as a runtime dependence even though we only really needed this library when operating on machines that have GPUs.

By using this header-only library and bundling it with Mesos, we can remove this external dependence altogether.",2.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Bug,Missing License Information for Bundled NVML headers,See Summary,1.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Improvement,Add 'systemGetDriverVersion' to NVML abstraction.,This command returns a string representing the version of the underlying Nvidia drivers installed on a host. It will be used by the upcoming {{NvidiaVolume}} component.,2.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Bug,ProcessRemoteLinkTest.RemoteUseStaleLink and RemoteStaleLinkRelink are flaky,"{{ProcessRemoteLinkTest.RemoteUseStaleLink}} and {{ProcessRemoteLinkTest.RemoteStaleLinkRelink}} are failing occasionally with the error:
{code}
[ RUN      ] ProcessRemoteLinkTest.RemoteStaleLinkRelink
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0630 07:42:34.661110 18888 process.cpp:1066] libprocess is initialized on 172.17.0.2:56294 with 16 worker threads
E0630 07:42:34.666393 18765 process.cpp:2104] Failed to shutdown socket with fd 7: Transport endpoint is not connected
/mesos/3rdparty/libprocess/src/tests/process_tests.cpp:1059: Failure
Value of: exitedPid.isPending()
  Actual: false
Expected: true
[  FAILED  ] ProcessRemoteLinkTest.RemoteStaleLinkRelink (56 ms)
{code}

There appears to be a race between establishing a socket connection and the test calling {{::shutdown}} on the socket.  Under some circumstances, the {{::shutdown}} may actually result in failing the future in {{SocketManager::link_connect}} error and thereby trigger {{SocketManager::close}}.",1.0,1.0.0,0.0,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.9174311926605504
Bug,NVML headers are not installed as part of 3rdparty install with --enable-install-module-dependencies,"Review: https://reviews.apache.org/r/49480/
",2.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Bug,Potential segfault in `link` and `send` when linking to a remote process,"There is a race in the SocketManager, between a remote {{link}} and disconnection of the underlying socket.

We potentially segfault here: https://github.com/apache/mesos/blob/215e79f571a989e998488077d713c28c7528926e/3rdparty/libprocess/src/process.cpp#L1512

{{\*socket}} dereferences the shared pointer underpinning the {{Socket*}} object.  However, the code above this line actually has ownership of the pointer:
https://github.com/apache/mesos/blob/215e79f571a989e998488077d713c28c7528926e/3rdparty/libprocess/src/process.cpp#L1494-L1499

If the socket dies during the link, the {{ignore_recv_data}} may delete the Socket underneath {{link}}:
https://github.com/apache/mesos/blob/215e79f571a989e998488077d713c28c7528926e/3rdparty/libprocess/src/process.cpp#L1399-L1411

----
The same race exists for {{send}}.

This race was discovered while running a new test in repetition:
https://reviews.apache.org/r/49175/

On OSX, I hit the race consistently every 500-800 repetitions:
{code}
3rdparty/libprocess/libprocess-tests --gtest_filter=""ProcessRemoteLinkTest.RemoteLink""  --gtest_break_on_failure --gtest_repeat=1000
{code}",2.0,"0.22.0,0.23.0,0.24.0,0.25.0,0.26.0,0.27.0,0.28.0",0.0,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.2293577981651376
Bug,"When start an agent with `--resources`, the GPU resource can be fractional","So far, the GPU resource is not fractional, only integer values are allowed. But when starting agents with {{\-\-resources='gpu:1.2'}}, it can also work without any warning or error. And in the webui the GPU resource is `1.2`.",1.0,1.0.0,0.5,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.02564102564102564,0.006060606060606061,0.006060606060606061,0.9174311926605504
Improvement,Consider adding `relink` functionality to libprocess,"Currently we don't have the {{relink}} functionality in libprocess.  i.e. A way to create a new persistent connection between actors, even if a connection already exists. 

This can benefit us in a couple of ways:
- The application may have more information on the state of a connection than libprocess does, as libprocess only checks if the connection is alive or not.  For example, a linkee may accept a connection, then fork, pass the connection to a child, and subsequently exit.  As the connection is still active, libprocess may not detect the exit.
- Sometimes, the {{ExitedEvent}} might be delayed or might be dropped due to the remote instance being unavailable (e.g., partition, network intermediaries not sending RST's etc). 
",3.0,0,0.0,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.0
Task,Benchmark the v1 Operator API,"Just like what we did with the v1 framework API, we need to benchmark the performance of v1 operator API.

As part of this benchmarking, we should evaluate whether evolving un-versioned protos to versioned protos in some of the API handlers (e.g., getFrameworks) is expensive.",3.0,0,0.5,0.0,0.6666666666666666,1.0,0.5714285714285714,0.0,0.0,0.5438596491228069,0.5357142857142857,0.8846153846153846,0.9878787878787878,0.9878787878787878,0.0
Bug,SSL-enabled libprocess will leak incoming links to forks,"Encountered two different buggy behaviors that can be tracked down to the same underlying problem.

Repro #1 (non-crashy):
(1) Start a master.  Doesn't matter if SSL is enabled or not.
(2) Start an agent, with SSL enabled.  Downgrade support has the same problem.  The master/agent {{link}} to one another.
(3) Run a sleep task.  Keep this alive.  If you inspect FDs at this point, you'll notice the task has inherited the {{link}} FD (master -> agent).
(4) Restart the agent.  Due to (3), the master's {{link}} stays open.
(5) Check master's logs for the agent's re-registration message.
(6) Check the agent's logs for re-registration.  The message will not appear.  The master is actually using the old {{link}} which is not connected to the agent.

----

Repro #2 (crashy):
(1) Start a master.  Doesn't matter if SSL is enabled or not.
(2) Start an agent, with SSL enabled.  Downgrade support has the same problem.
(3) Run ~100 sleep task one after the other, keep them all alive.  Each task links back to the agent.  Due to an FD leak, each task will inherit the incoming links from all other actors...
(4) At some point, the agent will run out of FDs and kernel panic.

----

It appears that the SSL socket {{accept}} call is missing {{os::nonblock}} and {{os::cloexec}} calls:
https://github.com/apache/mesos/blob/4b91d936f50885b6a66277e26ea3c32fe942cf1a/3rdparty/libprocess/src/libevent_ssl_socket.cpp#L794-L806

For reference, here's {{poll}} socket's {{accept}}:
https://github.com/apache/mesos/blob/4b91d936f50885b6a66277e26ea3c32fe942cf1a/3rdparty/libprocess/src/poll_socket.cpp#L53-L75
",2.0,"0.24.0,0.25.0,0.26.0,0.27.0,0.28.0",0.0,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.2385321100917431
Bug,Can't autodiscovery GPU resources without '--enable-nvidia-gpu-support' and '--nvidia_gpu_devices' flags,"Prerequisite: In MESOS\-5257  ""By default, with no '\-\-nvidia_gpu_devices' flag or `gpus` resources flag, the new auto-discovery will simply enumerate all of the GPUs on the system"" and in MESOS\-5630 ""removes this flag(\-\-enable-nvidia-gpu-support) and enables this support for all builds on Linux.""

So, I '../configure' without any flag, and start agent without '\-\-resources' or '\-\-nvidia_gpu_devices' ,  but can not discovery GPU resources, and I also start agent with '\-\-resources' and '\-\-nvidia_gpu_devices' , it also does not work.

I'm sure the NVIDIA GPUs on my machines are OK, because with '\-\-enable-nvidia-gpu-support' when './configure' and with '\-\-resources', '\-\-nvidia_gpu_devices' when starting agents it works well.",2.0,0,0.5,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.02564102564102564,0.006060606060606061,0.006060606060606061,0.0
Task,Document exactly what is handled by GET_ENDPOINTS_WITH_PATH acl,"Users may expect that the GET_ENDPOINT_WITH_PATH acl can be used with any Mesos endpoint, but that is not (yet) the case. We should clearly document the list of applicable endpoints, in authorization.md and probably even upgrades.md.",1.0,0,0.5,0.004524886877828055,0.0,0.0,0.0,0.0,0.2,0.12280701754385964,0.24285714285714285,0.05128205128205128,0.1515151515151515,0.15757575757575756,0.0
Task,Update AUTHORIZATION strings in endpoint help,"The endpoint help macros support AUTHENTICATION and AUTHORIZATION sections. We added AUTHORIZATION help for some of the newer endpoints, but not the previously authenticated endpoints.

Authorization endpoints needing help string updates:
Master::Http::CREATE_VOLUMES_HELP
Master::Http::DESTROY_VOLUMES_HELP
Master::Http::RESERVE_HELP
Master::Http::STATE_HELP
Master::Http::STATESUMMARY_HELP
Master::Http::TEARDOWN_HELP
Master::Http::TASKS_HELP
Master::Http::UNRESERVE_HELP
Slave::Http::STATE_HELP",2.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.12280701754385964,0.32857142857142857,0.05128205128205128,0.1515151515151515,0.15757575757575756,0.0
Task,Authorization for /roles,"The /roles endpoint exposes the list of all roles and their weights, as well as the list of all frameworkIds registered with each role. This is a superset of the information exposed on GET /weights, which we already protect. We should protect the data in /roles the same way.
- Should we reuse VIEW_FRAMEWORK with role (from /state)?
- Should we add a new VIEW_ROLE and adapt GET_WEIGHTS to use it?",3.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.12280701754385964,0.32857142857142857,0.05128205128205128,0.1515151515151515,0.15757575757575756,0.0
Task,Add authz to /files/debug,"The /files/debug endpoint exposes the attached master/agent log paths and every attached sandbox path, which includes the frameworkId and executorId. Even if sandboxes are protected, we still don't want to expose this information to unauthorized users.",3.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02857142857142857,0.05128205128205128,0.1515151515151515,0.15757575757575756,0.0
Task,LocalAuthorizer should error if passed a GET_ENDPOINT ACL with an unhandled path,"Since GET_ENDPOINT_WITH_PATH doesn't (yet) work with any arbitrary path, we should
a) validate --acls and error if GET_ENDPOINT_WITH_PATH has a path object that doesn't match an endpoint that uses this authz strategy.
b) document exactly which endpoints support GET_ENDPOINT_WITH_PATH",3.0,0,0.5,0.004524886877828055,0.0,0.0,0.0,0.0,0.2,0.12280701754385964,0.24285714285714285,0.05128205128205128,0.1515151515151515,0.15757575757575756,0.0
Task,GET_ENDPOINT_WITH_PATH authz doesn't make sense for /flags,"The master or agent flags are exposed in /state as well as /flags, so any user who wants to disable/control access to the flags likely intends to control access to flags no matter what endpoint exposes them. As such, /flags is a poor candidate for GET_ENDPOINT_WITH_PATH authz, since we care more about protecting the flag data than the specific endpoint path.
We should remove the GET_ENDPOINT authz from master and agent /flags until we can come up with a better solution, perhaps a first-class VIEW_FLAGS acl.",2.0,0,0.5,0.004524886877828055,0.0,0.0,0.0,0.0,0.2,0.12280701754385964,0.24285714285714285,0.05128205128205128,0.1515151515151515,0.15757575757575756,0.0
Task,Fine-grained authorization on /frameworks,"Even if ACLs were defined for the actions VIEW_FRAMEWORKS,
VIEW_EXECUTORS and VIEW_TASKS, the data these actions were
supposed to protect, could still leaked through the master's
/frameworks endpoint, since it didn't enable any authorization
mechanism.",3.0,0,0.5,0.004524886877828055,0.0,0.0,0.0,0.0,0.2,0.12280701754385964,0.24285714285714285,0.05128205128205128,0.1515151515151515,0.15757575757575756,0.0
Task,Create new documentation for Mesos networking.,"With introduction of CNI and dockers support docker user-defined networks, there are quite a few options within Mesos for IP-per-container solutions for container networking. 

We therefore need to re-write networking documentation for Mesos highlighting all the networking support that Mesos provides for orchestrating containers on IP networks.",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2807017543859649,0.39285714285714285,0.1282051282051282,0.3090909090909091,0.3090909090909091,0.0
Bug,The /files/download endpoint's authorization can be compromised,"If a forward slash is appended to the path of a file a user wishes to download via {{/files/download}}, the authorization logic for that path will be bypassed and the file will be downloaded regardless of permissions. This is because we store the authorization callbacks for these paths in a map which is keyed by the path name, so a request to {{/master/log/}} fails to find the callback which is installed for {{/master/log}}. When the master fails to find the callback, it assumes authorization is not required for that path and authorizes the action.

Consider the following excerpt:
{code}
gmann@gmac:~/src/mesos/build⚡  http GET http://127.0.0.1:5050/files/download\?path\=/master/log -a foo:bar
HTTP/1.1 403 Forbidden
Content-Length: 0
Date: Wed, 22 Jun 2016 21:28:53 GMT

gmann@gmac:~/src/mesos/build⚡  http GET http://127.0.0.1:5050/files/download\?path\=/master/log/ -a foo:bar
HTTP/1.1 200 OK
Content-Disposition: attachment; filename=mesos-master.gmac.gmann.log.INFO.20160622-142843.65615
Content-Length: 14432
Content-Type: application/octet-stream
Date: Wed, 22 Jun 2016 21:28:56 GMT

Log file created at: 2016/06/22 14:28:43
Running on machine: gmac
Log line format: [IWEF]mmdd hh:mm:ss.uuuuuu threadid file:line] msg
I0622 14:28:43.476925 2080764672 logging.cpp:194] INFO level logging started!
I0622 14:28:43.477522 2080764672 main.cpp:367] Using 'HierarchicalDRF' allocator
I0622 14:28:43.480650 2080764672 leveldb.cpp:174] Opened db in 2961us
I0622 14:28:43.481046 2080764672 leveldb.cpp:181] Compacted db in 372us
I0622 14:28:43.481078 2080764672 leveldb.cpp:196] Created db iterator in 13us
I0622 14:28:43.481096 2080764672 leveldb.cpp:202] Seeked to beginning of db in 9us
I0622 14:28:43.481111 2080764672 leveldb.cpp:271] Iterated through 0 keys in the db in 8us
I0622 14:28:43.481165 2080764672 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0622 14:28:43.481967 219914240 recover.cpp:451] Starting replica recovery
I0622 14:28:43.482193 219914240 recover.cpp:477] Replica is in EMPTY status
I0622 14:28:43.482589 2080764672 main.cpp:488] Creating default 'local' authorizer
I0622 14:28:43.482719 2080764672 main.cpp:545] Starting Mesos master
I0622 14:28:43.483085 218841088 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (4)@127.0.0.1:5050
I0622 14:28:43.487284 218304512 recover.cpp:197] Received a recover response from a replica in EMPTY status
I0622 14:28:43.487694 219914240 recover.cpp:568] Updating replica status to STARTING
{code}

We could consider disallowing paths which end in trailing slashes.",2.0,0,0.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Bug,Master captures `this` when creating authorization callback,"When exposing its log file, the master currently installs an authorization callback for the log file which captures the master's {{this}} pointer. Such captures have previously caused bugs (MESOS-5629), and this one should be fixed as well. The callback should be dispatched to the master process, and it should be dispatched via the {{self()}} PID.",1.0,0,0.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Improvement,Add support for master capabilities,"Right now, frameworks can advertise their capabilities to the master via the {{FrameworkInfo}} they use for registration/re-registration. This allows masters to provide backward compatibility for old frameworks that don't support new functionality.

To allow new frameworks to support backward compatibility with old masters, the inverse concept would be useful: masters would tell frameworks which capabilities are supported by the master, which the frameworks could then use to decide whether to use features only supported by more recent versions of the master.

For now, frameworks can workaround this by looking at the master's version number, but that seems a bit fragile and hacky.",3.0,0,0.5,0.0,0.3333333333333333,0.5,0.14285714285714285,0.0,0.2,0.5087719298245614,0.4357142857142857,0.48717948717948717,0.4303030303030303,0.4303030303030303,0.0
Bug,Invalid resources sent to '/reserve' are silently dropped,"If an invalid resource is passed to the master's {{/reserve}} endpoint, it will be silently dropped and not cause an error. This can lead, for example, to a {{/reserve}} request containing a single invalid resource receiving a 200 OK response, despite the fact that no resources were reserved as a result of the request.

This is due to the fact that the {{+=}} operator for {{Resources}} silently drops invalid resources, and this operator is used when parsing the resources in the HTTP request. This could be addressed by validating the resource objects one at a time as they are parsed.",1.0,"0.28.2,1.0.0",0.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.5872477064220183
Task,Remove hard dependence on libelf for Linux,"    We recently added a hard dependency for `libelf` on Linux. This was in
    preparation for some upcoming Nvidia GPU support for injecting volumes
    into containers. Since this dependence is not actually necessary for
    the upcoming release, we should remove it for now, and rethink the
    best way to add it back in later (possibly as a runtime dependence
    instead of a linktime one).",1.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Bug,ContainerizerTest.ROOT_CGROUPS_BalloonFramework fails because executor environment isn't inherited,A recent change forbits the executor to inherit environment variables from the agent's environment. As a regression this break {{ContainerizerTest.ROOT_CGROUPS_BalloonFramework}}.,2.0,0,0.5,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.28205128205128205,0.296969696969697,0.296969696969697,0.0
Task,Build an example framework to consume GPUs,This framework should show how to build a GPU capable framework that can accept offers with GPUs and launch tasks that use them.,3.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Task,Add Framework Capability for GPU_RESOURCES,"Due to the scarce resource problem described in MESOS-5377, we plan to introduce a GPU_RESOURCES Framework capability. This capability will allow the Mesos allocator to make better decisions about which frameworks should receive resources from GPU capable machines.  In essence, the allocator will ONLY allocate resources from GPU capable machines to frameworks that have this capability. This is necessary to prevent non-GPU workloads from filling up the GPU machines and preventing GPU workloads to run.",3.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Improvement,Change build to always enable Nvidia GPU support for Linux,See Summary,2.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Bug,Agent segfaults after request to '/files/browse',"We observed a number of agent segfaults today on an internal testing cluster. Here is a log excerpt:
{code}
Jun 16 17:12:28 ip-10-10-0-87 mesos-slave[24818]: I0616 17:12:28.522925 24830 status_update_manager.cpp:392] Received status update acknowledgement (UUID: e79ab0f4-2fa2-4df2-9b59-89b97a482167) for task datadog-monitor.804b138b-33e5-11e6-ac16-566ccbdde23e of framework 6d4248cd-2832-4152-b5d0-defbf36f6759-0000
Jun 16 17:12:28 ip-10-10-0-87 mesos-slave[24818]: I0616 17:12:28.523006 24830 status_update_manager.cpp:824] Checkpointing ACK for status update TASK_RUNNING (UUID: e79ab0f4-2fa2-4df2-9b59-89b97a482167) for task datadog-monitor.804b138b-33e5-11e6-ac16-566ccbdde23e of framework 6d4248cd-2832-4152-b5d0-defbf36f6759-0000
Jun 16 17:12:29 ip-10-10-0-87 mesos-slave[24818]: I0616 17:12:29.147181 24824 http.cpp:192] HTTP GET for /slave(1)/state from 10.10.0.87:33356
Jun 16 17:12:29 ip-10-10-0-87 mesos-slave[24818]: *** Aborted at 1466097149 (unix time) try ""date -d @1466097149"" if you are using GNU date ***
Jun 16 17:12:29 ip-10-10-0-87 mesos-slave[24818]: PC: @     0x7ff4d68b12a3 (unknown)
Jun 16 17:12:29 ip-10-10-0-87 mesos-slave[24818]: *** SIGSEGV (@0x0) received by PID 24818 (TID 0x7ff4d31ab700) from PID 0; stack trace: ***
Jun 16 17:12:29 ip-10-10-0-87 mesos-slave[24818]: @     0x7ff4d6431100 (unknown)
Jun 16 17:12:29 ip-10-10-0-87 mesos-slave[24818]: @     0x7ff4d68b12a3 (unknown)
Jun 16 17:12:29 ip-10-10-0-87 mesos-slave[24818]: @     0x7ff4d7eced33 process::dispatch<>()
Jun 16 17:12:29 ip-10-10-0-87 mesos-slave[24818]: @     0x7ff4d7e7aad7 _ZNSt17_Function_handlerIFN7process6FutureIbEERK6OptionISsEEZN5mesos8internal5slave9Framework15recoverExecutorERKNSA_5state13ExecutorStateEEUlS6_E_E9_M_invokeERKSt9_Any_dataS6_
Jun 16 17:12:29 ip-10-10-0-87 mesos-slave[24818]: @     0x7ff4d7bd1752 mesos::internal::FilesProcess::authorize()
Jun 16 17:12:29 ip-10-10-0-87 mesos-slave[24818]: @     0x7ff4d7bd1bea mesos::internal::FilesProcess::browse()
Jun 16 17:12:29 ip-10-10-0-87 mesos-slave[24818]: @     0x7ff4d7bd6e43 std::_Function_handler<>::_M_invoke()
Jun 16 17:12:29 ip-10-10-0-87 mesos-slave[24818]: @     0x7ff4d85478cb _ZZZN7process11ProcessBase5visitERKNS_9HttpEventEENKUlRKNS_6FutureI6OptionINS_4http14authentication20AuthenticationResultEEEEE0_clESC_ENKUlRKNS4_IbEEE1_clESG_
Jun 16 17:12:29 ip-10-10-0-87 mesos-slave[24818]: @     0x7ff4d8551341 process::ProcessManager::resume()
Jun 16 17:12:29 ip-10-10-0-87 mesos-slave[24818]: @     0x7ff4d8551647 _ZNSt6thread5_ImplISt12_Bind_simpleIFZN7process14ProcessManager12init_threadsEvEUt_vEEE6_M_runEv
Jun 16 17:12:29 ip-10-10-0-87 mesos-slave[24818]: @     0x7ff4d6909220 (unknown)
Jun 16 17:12:29 ip-10-10-0-87 mesos-slave[24818]: @     0x7ff4d6429dc5 start_thread
Jun 16 17:12:29 ip-10-10-0-87 mesos-slave[24818]: @     0x7ff4d615728d __clone
Jun 16 17:12:29 ip-10-10-0-87 systemd[1]: dcos-mesos-slave.service: main process exited, code=killed, status=11/SEGV
Jun 16 17:12:29 ip-10-10-0-87 systemd[1]: Unit dcos-mesos-slave.service entered failed state.
Jun 16 17:12:29 ip-10-10-0-87 systemd[1]: dcos-mesos-slave.service failed.
Jun 16 17:12:34 ip-10-10-0-87 systemd[1]: dcos-mesos-slave.service holdoff time over, scheduling restart.
{code}

In every case, the stack trace indicates one of the {{/files/*}} endpoints; I observed this a number of times coming from {{browse()}}, and twice from {{read()}}.

The agent was built from the 1.0.0-rc1 branch, with two cherry-picks applied: [this|https://reviews.apache.org/r/48563/] and [this|https://reviews.apache.org/r/48566/], which were done to repair a different [segfault issue|https://issues.apache.org/jira/browse/MESOS-5587] on the master and agent.

Thanks go to [~bmahler] for digging into this a bit and discovering a possible cause [here|https://github.com/apache/mesos/blob/master/src/slave/slave.cpp#L5737-L5745], where use of {{defer()}} may be necessary to keep execution in the correct context.",3.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.12280701754385964,0.32857142857142857,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Improvement,Pass NetworkInfo to CNI Plugins,"Mesos has adopted the Container Network Interface as a simple means of networking Mesos tasks launched by the Unified Containerizer. The CNI specification covers a minimum feature set, granting the flexibility to add customized networking functionality in the form of agreements made between the orchestrator and CNI plugin.

This proposal is to pass NetworkInfo.Labels to the CNI plugin by injecting it into the CNI network configuration json during plugin invocation.

Design Doc on this change: https://docs.google.com/document/d/1rxruCCcJqpppsQxQrzTbHFVnnW6CgQ2oTieYAmwL284/edit?usp=sharing

reviewboard: https://reviews.apache.org/r/48527/",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.017543859649122806,0.014285714285714285,0.01282051282051282,0.01212121212121212,0.01212121212121212,0.0
Documentation,Improve authorization documentation when setting permissive flag.,"A common problem for a users starting to use acls is that once they set `permisse = false` and not add acls allowing common operations (e.g., register_framework) their Mesos cluster don't
behave as expected. ",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12280701754385964,0.32857142857142857,0.11538461538461538,0.23636363636363636,0.23636363636363636,0.0
Improvement,Create a `cgroups/devices` isolator.,"Currently, all the logic for the `cgroups/devices` isolator is bundled into the Nvidia GPU Isolator. We should abstract it out into it's own component and remove the redundant logic from the Nvidia GPU Isolator. Assuming the guaranteed ordering between isolators from MESOS-5581, we can be sure that the dependency order between the `cgroups/devices` and `gpu/nvidia` isolators is met.",2.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Improvement,Guarantee ordering between Isolators,"Some isolators depend on other isolators. However, we currently do not have a generic method of expressing these dependencies. We special case the `filesystem/*` isolators to make sure that dependencies on them are satisfied, but no other dependencies can be expressed.
    
Instead, we should use a vector to represent the pairing of isolator name to isolator creator function. This way, the relative dependencies between each isolator will be implicit in the ordering of the vector. Currently, a hashmap is used to hold this pairing, but this is inadequate because hashmaps are inherently unordered. The new implementation using a vector will ensure everything is processed in the order it is listed.",3.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Bug,Modules using replicated log state API require zookeeper headers,The state API uses zookeeper client headers and hence the bundled zookeeper headers need to be installed during Mesos installation. ,1.0,1.0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2807017543859649,0.39285714285714285,0.1282051282051282,0.3090909090909091,0.3090909090909091,0.9174311926605504
Improvement,Rearrange Nvidia GPU files to cleanup semantics for header inclusion.,"Currently, components outside of `src/slave/containerizers/mesos/isolators/gpu` have to protect their #includes for certain Nvidia header files with the ENABLE_NVIDIA_GPU_SUPPORT flag. Other headers strictly *could not* be wrapped in this flag.
    
We need to clean up this header madness, by creating a common ""nvidia.hpp"" header that takes care of all the dependencies. All componenents outside of `src/slave/containerizers/mesos/isolators/gpu`
should only need to #include this one header instead of managing everything themselves.",1.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Improvement,Add class to share Nvidia-specific components between containerizers,"Once we have an `NvidiaGPUAllocator` component, we need some way to share it across multiple containerizers.  Moreover, we anticipate needing other Nvidia components to share across multiple containerizers as well (e.g. an `NvidiaVolumeManager` component). As such, we should add a wrapper class around these components to make it easily passable to each containerizer without having to continually add a bunch of parameters to the Containerizer interface.",2.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Bug,"Need to remove references to ""messages/messages.hpp"" from `State` API","In order to expose the `State` API for using replicated log in Mesos modules it is necessary that the `State` API does not reference headers that are not exposed as part of the Mesos installation. 

Currently include/mesos/state/protobuf.hpp references src/messages/messages.hpp making the `State` API unusable in a module. 

We need to move the protobuf `serialize`/`deserialize` functions out of messages.hpp and move them to `stout/protobuf.hpp`. This will help us remove references to messages.hpp from the `State` API.",2.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2807017543859649,0.39285714285714285,0.1282051282051282,0.3090909090909091,0.3090909090909091,0.0
Improvement,Update `Containerizer::resources()` to use the `NvidiaGpuAllocator`,"With the introduction of the shared `NvidiaGpuAllocator` component, `Containerizer::resources()` should be updated to use it.",2.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Bug,"Fix method of populating device entries for `/dev/nvidia-uvm`, etc.","Currently, the major/minor numbers of `/dev/nvidiactl` and `/dev/nvidia-uvm` are hard-coded. This causes problems for `/dev/nvidia-uvm` because its major number is part of the ""Experimental"" device range on Linux.

Because this range is experimental, there is no guarantee which device
number will be assigned to it on a given machine.  We should use `os:stat::rdev()` to extract the major/minor numbers programatically.",2.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Improvement,Always provide access to NVIDIA control devices within containers (if GPU isolation is enabled).,"Currently, access to `/dev/nvidiactl` and `/dev/nvidia-uvm` is only granted to / revoked from a container as GPUs are added and removed from them. On some level, this makes sense because most jobs don't need access to these devices unless they are also using a GPU. However, there are cases when access to these files is appropriate, even when not making use of a GPU. Running `nvidia-smi` to control the global state of the underlying nvidia driver, for example.
    
We should add `/dev/nvidiactl` and `/dev/nvidia-uvm` to the default whitelist of devices to include in every container when the `gpu/nvidia` isolator is enabled. This will allow a container to run standard nvidia driver tools (such as `nvidia-smi`) without failing with abnormal errors when no GPUs have been granted to it. As such, these tools will now report that no GPUs are installed instead of failing abnormally.",3.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Bug,Change major/minor device types for Nvidia GPUs to `unsigned int`,"Currently, the GPU struct specifies the type of its `major` and `minor` fields as `dev_t`, which is actually a concatenation of both the major and minor device numbers accessible through the `major()` and `minor()` macros. These macros return an `unsigned int` when handed a `dev_t`, so it makes sense for these fields to be of that type instead.",1.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Improvement,Bundle NVML headers for Nvidia GPU support.,"Currently, we rely on a script to install the Nvidia GDK as a build dependence for building Mesos with Nvidia GPU support.

A previous ticket removed the Mesos build dependence on `libnvidia-ml` which comes as part of the GDK. This ticket proposes bundling the NVML headers with Mesos in order to completely remove the build dependence on the GDK.

With this change it will be much simpler to configure and build with Nvidia GPU support.  All that will be required is:
{noformat}
../configure --enable-nvidia-gpu-support
make -j
{noformat}
",1.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Improvement,Move the Nvidia GPU isolator from `cgroups/devices/gpu/nvidia` to `gpu/nvidia`,"Currently, the Nvidia GPU isolator lives in `src/slave/containerizers/mesos/isolators/cgroups/devices/gpu/nvidia`. However, in the future this isolator will do more than simply isolate GPUs using the cgroups devices subsystem (e.g. volume management for injecting machine specific Nvidia libraries into a container). For this reason, we should preemptively move this isolator up to `src/slave/containerizers/mesos/isolators/gpu/nvidia`. As part of this, we should update the string we pass to the `--isolator` agent flag to reflect this.",2.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Improvement,Remove Nvidia GPU Isolator's link-time dependence on `libnvidia-ml`,"The current Nvidia GPU isolator has a dependence on `libnvidia-ml`, and as such, pulls a hard dependence on this library into `libmesos`. The consequence of this is that any process that relies on `libmesos` has to have `libnvidia-ml` available as well, even on machines where no GPUs are available.  Since this library is not easily installable through standard package managers, having such a hard dependence can be burdensome.

This ticket proposes to pull in `libnvidia-ml` as a run-time dependence instead of a link-time dependence. As such, only machines that actually have GPUs installed and would like to rely on this library need to have it installed.",2.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Task,Document aufs provisioner backend.,We should update container-image.md with the newly supported backend.,2.0,0,0.5,0.033182503770739065,0.0,0.0,0.0,0.0,0.2,0.017543859649122806,0.06428571428571428,0.7948717948717948,1.0,1.0,0.0
Bug,http v1 SUBSCRIBED scheduler event always has nil http_interval_seconds,"I'm writing a controller in Go to monitor heartbeats. I'd like to use the interval as communicated by the master, which should be specified in the SUBSCRIBED event. But it's not.

{code}
2016/06/03 18:34:04 {Type:SUBSCRIBED Subscribed:&Event_Subscribed{FrameworkID:&mesos.FrameworkID{Value:ffdb6d6e-0167-4fa2-98f9-2c3f8157fc25-0004,},HeartbeatIntervalSeconds:nil,} Offers:nil Rescind:nil Update:nil Message:nil Failure:nil Error:nil}
{code}

{code}
$ dpkg -l |grep -e mesos
ii  mesos                               0.28.0-2.0.16.ubuntu1404         amd64        Cluster resource manager with efficient resource isolation
{code}

I *am* seeing HEARTBEAT events. Just not seeing the interval specified in the SUBSCRIBED event.",1.0,0,0.5,0.5701357466063348,0.0,0.0,0.0,0.0,0.0,0.6842105263157894,0.5642857142857143,0.08974358974358974,0.04242424242424243,0.04242424242424243,0.0
Improvement,Maven build is too verbose for batch builds,"During a non-interactive (without terminal) Mesos build, maven generates several thousands of log lines when downloading artifacts. This often makes several web-based log viewers unresponsive.

Further, these several thousand line long progress indicator logs don't provide any meaningful information either. From a user's point of view, just knowing that the artifact download succeeded/failed is often enough.

We should be using '--batch-mode' flag to disable these additionals log lines.",1.0,0,0.0,0.3453996983408748,0.0,0.0,0.0,0.0,0.2,0.38596491228070173,0.35714285714285715,0.28205128205128205,0.29090909090909095,0.28484848484848485,0.0
Bug,Re-enable style-check for stout.,"After the 3rdparty reorg, the mesos-style checker stopped checking stout.",1.0,0,0.0,0.3453996983408748,0.0,0.0,0.0,0.0,0.2,0.38596491228070173,0.35714285714285715,0.28205128205128205,0.29090909090909095,0.28484848484848485,0.0
Improvement,Master anonymous modules should initialized before any other components.,"Anonymous modules on the Master are by design supposed to be independent of any Mesos components. However, there might be a dependency in the reverse direction. For e.g., Anonymous modules might want to influence the behavior of Mesos components (say by generating configuration, that might be consumed later by the components). 

The Anonymous modules on the Master therefore need to be initialized before other Mesos components. ",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2807017543859649,0.39285714285714285,0.1282051282051282,0.3090909090909091,0.3090909090909091,0.0
Bug,CNI should not store subnet of address in NetworkInfo,"When the CNI isolator executes the CNI plugin, that CNI plugin will return an IP Address and Subnet (192.168.0.1/32). Mesos should strip the subnet before storing the address in the Task.NetworkInfo.IPAddress.

Reason being - most current mesos components are not expecting a subnet in the Task's NetworkInfo.IPAddress, and instead expect just the IP address. This can cause errors in those components, such as Mesos-DNS failing to return a NetworkInfo address (and instead defaulting to the next configured IPSource), and Marathon generating invalid links to tasks (as it includes /32 in the link)",2.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.017543859649122806,0.014285714285714285,0.01282051282051282,0.01212121212121212,0.01212121212121212,0.0
Improvement,Agent modules should be initialized before all components except firewall.,"On Mesos Agents Anonymous modules should not have any dependencies, by design, on any other Mesos components. This implies that Anonymous modules should be initialized before all other Mesos components other than `Firewall`. The dependency on `Firewall` is primarily to enforce any policies to secure endpoints that might be owned by the Anonymous module.",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2807017543859649,0.39285714285714285,0.1282051282051282,0.3090909090909091,0.3090909090909091,0.0
Bug,Make the SASL dependency optional.,"Right now there is a hard dependency on SASL, which probably won't work well on Windows (at least) in the near future for our use cases.

In the future, it would be nice to have a pluggable authentication layer.",2.0,0,0.0,0.44494720965309204,0.0,0.0,0.0,0.0,0.0,0.14035087719298245,0.15,0.14102564102564102,0.1515151515151515,0.16969696969696968,0.0
Bug,Allow libprocess/stout to build without first doing `make` in 3rdparty.,"After the 3rdparty reorg, libprocess/stout are enable to build their dependencies and so one has to do `make` in 3rdpart/ before building libprocess/stout.",2.0,0,0.0,0.3453996983408748,0.0,0.0,0.0,0.0,0.2,0.38596491228070173,0.35714285714285715,0.28205128205128205,0.29090909090909095,0.28484848484848485,0.0
Bug,AppC  appc_simple_discovery_uri_prefix is lost in configuration.md,AppC  appc_simple_discovery_uri_prefix is lost in configuration.md,1.0,1.0.0,0.0,0.3137254901960784,0.0,0.0,0.0,0.0,0.4,0.19298245614035087,0.14285714285714285,0.10256410256410256,0.06666666666666667,0.06666666666666667,0.9174311926605504
Improvement,Add default implementations to all Isolator virtual functions,"Currently, all of the virtual functions in `mesos::slave::Isolator` are pure virtual (expect status()). For many isolators, however, it doesn't make sense to implement all of these virtual functions. Each isolator has to provide its own default implementation of these functions even if they aren't really relying on them. This adds unnecessary extra code to many isolators that don't need them.

Moreover, the `MesosIsolatorProcess` has the same problem for each of its virtual functions.

We should provide defaults for these instead of making each and every isolator implement even in cases when it doesn't make sense.",1.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Improvement,Consider using IntervalSet for Port range resource math,"Follow-up JIRA for comments raised in MESOS-3051 (see comments there).

We should consider utilizing [{{IntervalSet}}|https://github.com/apache/mesos/blob/a0b798d2fac39445ce0545cfaf05a682cd393abe/3rdparty/stout/include/stout/interval.hpp] in [Port range resource math|https://github.com/apache/mesos/blob/a0b798d2fac39445ce0545cfaf05a682cd393abe/src/common/values.cpp#L143].",3.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.007142857142857143,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.0
Improvement,Implement os::exists for processes,"os::exists returns true if the process identified by the parameter is still running or was running and we are able to get information about it, such us the exit code. In Windows after obtaining a handle to the process it is possible perform those operations. ",1.0,0,0.0,0.13122171945701358,0.0,0.0,0.0,0.0,0.0,0.0,0.049999999999999996,0.01282051282051282,0.0303030303030303,0.0303030303030303,0.0
Documentation,Document all known client libraries for the Scheduler/Executor API,"Previously during various community syncs, we had decided that we would only be supporting the C++ scheduler/executor library in the Mesos code base going forward. We should however, still document the client libraries available in various languages to drive adoption/have a recommended list for users to look up.

This can be similar to the already existing frameworks doc: http://mesos.apache.org/documentation/latest/frameworks/

Other projects also seem to have been following a similar practice:
https://docs.docker.com/engine/reference/api/remote_api_client_libraries/
https://github.com/kubernetes/kubernetes/blob/master/docs/devel/client-libraries.md",2.0,0,0.0,0.5701357466063348,0.0,0.0,0.0,0.0,0.0,0.6842105263157894,0.5642857142857143,0.3846153846153846,0.4181818181818182,0.4121212121212121,0.0
Bug,Make fields in authorization::Request protobuf optional.,"Currently {{authorization::Request}} protobuf declares {{subject}} and {{object}} as required fields. However, in the codebase we not always set them, which renders the message in the uninitialized state, for example:
 * https://github.com/apache/mesos/blob/0bfd6999ebb55ddd45e2c8566db17ab49bc1ffec/src/common/http.cpp#L603
 * https://github.com/apache/mesos/blob/0bfd6999ebb55ddd45e2c8566db17ab49bc1ffec/src/master/http.cpp#L2057

I believe that the reason why we don't see issues related to this is because we never send authz requests over the wire, i.e., never serialize/deserialize them. However, they are still invalid protobuf messages. Moreover, some external authorizers may serialize these messages.

We can either ensure all required fields are set or make both {{subject}} and {{object}} fields optional. This will also require updating local authorizer, which should properly handle the situation when these fields are absent. We may also want to notify authors of external authorizers to update their code accordingly.

It looks like no deprecation is necessary, mainly because we already—erroneously!—treat these fields as optional.",3.0,0,0.5,0.3740573152337858,0.0,0.0,0.0,0.2,0.4,0.43859649122807015,0.2714285714285714,0.717948717948718,0.8242424242424242,0.7212121212121212,0.0
Improvement,Allow `Task` to be authorized.,"As we need to be able to authorize `Tasks` (e.g., for deciding whether to include them in the /state endpoint when applying authorization based filtering) we need to expose it to the authorizer. Secondly we also need to include some additional information (`user` and `Env variables`) in order to provide the authorizer  with meaning information.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12280701754385964,0.32857142857142857,0.11538461538461538,0.23636363636363636,0.23636363636363636,0.0
Improvement,Rewrite os::read() to be friendlier to reading binary files,"The existing read() implementation is based on calling getline() to
read in chunks of data from a file. This is fine for text-based files,
but is a little strange for binary files.",3.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Bug,v1 Executor Protos not included in maven jar,"According to MESOS-4793 the Executor v1 HTTP API was released in Mesos 0.28.0 however the corresponding protos are not included in the maven jar for version 0.28.0 or 0.28.1.

Script to verify
{code}
wget https://repo.maven.apache.org/maven2/org/apache/mesos/mesos/0.28.1/mesos-0.28.1.jar && unzip -lf mesos-0.28.1.jar | grep ""v1\/executor"" | wc -l
{code}",1.0,"0.28.0,0.28.1",0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.017543859649122806,0.03571428571428571,0.038461538461538464,0.01818181818181818,0.01818181818181818,0.2569266055045872
Bug,Terminating a framework during master failover leads to orphaned tasks,"Repro steps:

1) Setup:
{code}
bin/mesos-master.sh --work_dir=/tmp/master
bin/mesos-slave.sh --work_dir=/tmp/slave --master=localhost:5050
src/mesos-execute --checkpoint --command=""sleep 1000"" --master=localhost:5050 --name=""test""
{code}

2) Kill all three from (1), in the order they were started.

3) Restart the master and agent.  Do not restart the framework.

Result)
* The agent will reconnect to an orphaned task.
* The Web UI will report no memory usage
* {{curl localhost:5050/metrics/snapshot}} will say:  {{""master/mem_used"": 128,}}

Cause) 
When a framework registers with the master, it provides a {{failover_timeout}}, in case the framework disconnects.  If the framework disconnects and does not reconnect within this {{failover_timeout}}, the master will kill all tasks belonging to the framework.

However, the master does not persist this {{failover_timeout}} across master failover.  The master will ""forget"" about a framework if:
1) The master dies before {{failover_timeout}} passes.
2) The framework dies while the master is dead.

When the master comes back up, the agent will re-register.  The agent will report the orphaned task(s).  Because the master failed over, it does not know these tasks are orphans (i.e. it thinks the frameworks might re-register).

Proposed solution)
The master should save the {{FrameworkID}} and {{failover_timeout}} in the registry.  Upon recovery, the master should resume the {{failover_timeout}} timers.",3.0,"0.27.2,0.28.1",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.2524311926605505
Improvement,Add support for Console Ctrl handling in `slave.cpp`,Extract supporting code to handle POSIX signals in a separate header and add support for CTRL handler when running on Windows. ,3.0,0,0.0,0.13122171945701358,0.0,0.0,0.0,0.0,0.0,0.0,0.049999999999999996,0.01282051282051282,0.0303030303030303,0.0303030303030303,0.0
Improvement,Remove `Zookeeper's` NTDDI_VERSION define,"Zookeeper client library defines NTDDI_VERSION to 0x0400 in ""winconfig.h"". While this API level is suficient to compile the client library,  Mesos have to use a newer API set. After this improvement the code will compile with the latest NTDDI_VERSION.   
",2.0,1.0.0,0.0,0.13122171945701358,0.0,0.0,0.0,0.0,0.0,0.0,0.049999999999999996,0.01282051282051282,0.0303030303030303,0.0303030303030303,0.9174311926605504
Improvement,Add random() to os:: namespace ,"The function ""random()"" is not available in Windows. After this improvement the calls to ""os::random()"" will result in calls to ""::random()"" on POSIX and ""::rand()"" on Windows.  ",1.0,0,0.0,0.13122171945701358,0.0,0.0,0.0,0.0,0.0,0.0,0.049999999999999996,0.01282051282051282,0.0303030303030303,0.0303030303030303,0.0
Improvement,Add authentication to example frameworks,Some example frameworks do not have the ability to authenticate with the master. Adding authentication to the example frameworks that don't already have it implemented would allow us to use these frameworks for testing in authenticated/authorized scenarios.,2.0,0,0.5,0.3740573152337858,0.0,0.0,0.0,0.2,0.4,0.43859649122807015,0.2714285714285714,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Bug,The scheduler library should have a delay before initiating a connection with master.,"Currently, the scheduler library {{src/scheduler/scheduler.cpp}} does have an artificially induced delay when trying to initially establish a connection with the master. In the event of a master failover or ZK disconnect, a large number of frameworks can get disconnected and then thereby overwhelm the master with TCP SYN requests. 

On a large cluster with many agents, the master is already overwhelmed with handling connection requests from the agents. This compounds the issue further on the master.",3.0,1.0.0,0.5,0.0,0.3333333333333333,0.5,0.14285714285714285,0.0,0.0,0.017543859649122806,0.007142857142857143,0.3846153846153846,0.4181818181818182,0.4121212121212121,0.9174311926605504
Improvement,Enhance the log message when launching docker containerizer.,Log the launch flag which includes the executor command and other information when launching the docker containerizer.,2.0,0,0.5,0.3137254901960784,0.0,0.0,0.0,0.0,0.4,0.19298245614035087,0.14285714285714285,0.6794871794871795,0.5939393939393939,0.5939393939393939,0.0
Improvement,Enhance the log message when launching mesos containerizer.,"Log the launch flag which includes the executor command, pre-launch commands and other information when launching the mesos containerizer. ",2.0,0,0.5,0.3137254901960784,0.0,0.0,0.0,0.0,0.4,0.19298245614035087,0.14285714285714285,0.6794871794871795,0.5939393939393939,0.5939393939393939,0.0
Improvement,Add `user` to `Task` protobuf message.,"The LocalAuthorizer is supposed to use the OS `user` under which tasks are running for authorization.
As the master keeps track of running and completed processes we need access to this information in Task in order to authorize such tasks.",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12280701754385964,0.32857142857142857,0.11538461538461538,0.23636363636363636,0.23636363636363636,0.0
Improvement,Add Master Flag to enable fine-grained filtering of HTTP endpoints.,"As the fine-grained filtering of endpoints can the rather expensive, we should create a master flag to enable/disable this feature.",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12280701754385964,0.32857142857142857,0.11538461538461538,0.23636363636363636,0.23636363636363636,0.0
Improvement,Add authorization to GET /quota.,"We already authorize which http users can set/remove quota for particular roles, but even knowing of the existence of these roles (let alone their quotas) may be sensitive information. We should add authz around GET operations on /quota.",3.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.4,0.07017543859649122,0.12142857142857143,0.05128205128205128,0.1515151515151515,0.15757575757575756,0.0
Improvement,Add authorization to GET /weights.,"We already authorize which http users can update weights for particular roles, but even knowing of the existence of these roles (let alone their weights) may be sensitive information. We should add authz around GET operations on /weights.

Easy option: GET_ENDPOINT_WITH_PATH /weights
- Pro: No new verb
- Con: All or nothing

Complex option: GET_WEIGHTS_WITH_ROLE
- Pro: Filters contents based on roles the user is authorized to see
- Con: More authorize calls (one per role in each /weights request)",3.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.017543859649122806,0.03571428571428571,0.05128205128205128,0.1515151515151515,0.15757575757575756,0.0
Bug,GET /master/maintenance/schedule/ produces 404.,"Attempts to make a GET request to /master/maintenance/schedule/ result in a 404. However, if I make a GET request to /master/maintenance/schedule (without the trailing /), it works. My current (untested) theory is that this might be related to the fact that there is also a /master/maintenance/schedule/status endpoint (an endpoint built on top of a functioning endpoint), as requests to /help and /help/ (with and without the trailing slash) produce the same functioning result.",3.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.01282051282051282,0.0,0.0,0.0
Bug,SSL related error messages can be misguiding or incomplete,"I was trying to activate SSL within Mesos but had rendered an invalid certificate, it was signed with a mismatching key. Once I started the master, the error message I received was rather confusing to me:
{noformat}
W0503 10:15:58.027343  6696 openssl.cpp:363] Failed SSL connections will be downgraded to a non-SSL socket
Could not load key file
{noformat} 

To me, this error message hinted that the key file was not existing or had rights issues. However, a quick {{strace}} revealed  that the key-file was properly accessed, no sign of a file-not-found or alike.

The problem here is the hardcoded error-message, not taking OpenSSL's human readable error strings into account.

The code that misguided me is located at  https://github.com/apache/mesos/blob/master/3rdparty/libprocess/src/openssl.cpp#L471

We might want to change
{noformat}
  // Set private key.
  if (SSL_CTX_use_PrivateKey_file(
          ctx,
          ssl_flags->key_file.get().c_str(),
          SSL_FILETYPE_PEM) != 1) {
    EXIT(EXIT_FAILURE) << ""Could not load key file"";
  }
{noformat}

Towards something like this
{noformat}
  // Set private key.
  if (SSL_CTX_use_PrivateKey_file(
          ctx,
          ssl_flags->key_file.get().c_str(),
          SSL_FILETYPE_PEM) != 1) {
    EXIT(EXIT_FAILURE) << ""Could not use key file: "" << ERR_error_string(ERR_get_error(), NULL);
  }
{noformat}

To receive a much more helpful message like this
{noformat}
W0503 13:18:12.551364 11572 openssl.cpp:363] Failed SSL connections will be downgraded to a non-SSL socket
Could not use key file: error:0B080074:x509 certificate routines:X509_check_private_key:key values mismatch
{noformat}


A quick scan of the implementation within {{openssl.cpp}} to me suggests that there are more places that we might want to update with more deterministic error messages. ",3.0,1.0.0,0.0,0.3740573152337858,0.0,0.0,0.0,0.2,0.4,0.43859649122807015,0.2714285714285714,0.3717948717948718,0.24848484848484845,0.24848484848484845,0.9174311926605504
Improvement,Authorize the agent's '/containers' endpoint.,"After the agent's {{/containers}} endpoint is authenticated, we should enabled authorization as well.",2.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02857142857142857,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Improvement,Authenticate the agent's '/containers' endpoint.,The {{/containers}} endpoint was recently added to the agent. Authentication should be enabled on this endpoint.,2.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02857142857142857,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Documentation,Failed to set quota and update weight according to document,"{code}
root@mesos002:~/test# curl -d jsonMessageBody -X POST http://192.168.56.12:5050/quota
Failed to parse set quota request JSON 'jsonMessageBody': syntax error at line 1 near: jsonMessageBodyroot@mesos002:~/test# cat jsonMessageBody
{
	""role"": ""role1"",
	""guarantee"": [{
		""name"": ""cpus"",
		""type"": ""SCALAR"",
		""scalar"": {
			""value"": 1
		}
	}, {
		""name"": ""mem"",
		""type"": ""SCALAR"",
		""scalar"": {
			""value"": 128
		}
	}]
}
root@mesos002:~/test# curl -d weight.json -X PUT http://192.168.56.12:5050/weights
Failed to parse update weights request JSON ('weight.json'): syntax error at line 1 near: weight.js
root@mesos002:~/test# cat weight.json
    [
      {
        ""role"": ""role1"",
        ""weight"": 2.0
      },
      {
        ""role"": ""role2"",
        ""weight"": 3.5
      }
    ]
{code}

The right command should be adding {{@}} before the quota json file {{jsonMessageBody}}.",1.0,"0.27.2,0.28.1",0.0,0.3137254901960784,0.0,0.0,0.0,0.0,0.4,0.19298245614035087,0.14285714285714285,0.10256410256410256,0.06666666666666667,0.06666666666666667,0.2524311926605505
Bug,Env `MESOS_SANDBOX` is not set properly for command tasks that changes rootfs.,"This is in the context of Mesos containerizer (a.k.a., unified containerizer).

I did a simple test:
{noformat}
sudo sbin/mesos-master --work_dir=/tmp/mesos/master
sudo GLOG_v=1 sbin/mesos-slave --master=10.0.2.15:5050 --isolation=docker/runtime,filesystem/linux --work_dir=/tmp/mesos/slave/ --image_providers=docker --executor_environment_variables=""{}""
sudo bin/mesos-execute --master=10.0.2.15:5050 --name=test --docker_image=alpine --command=""env"" 

MESOS_EXECUTOR_ID=test
SHLVL=1
MESOS_CHECKPOINT=0
MESOS_EXECUTOR_SHUTDOWN_GRACE_PERIOD=5secs
LIBPROCESS_PORT=0
MESOS_AGENT_ENDPOINT=10.0.2.15:5051
MESOS_SANDBOX=/tmp/mesos/slave/slaves/2d7e44bb-3282-4193-bdc4-eeab9e0943c2-S0/frameworks/1a1cad18-2d87-43dd-97b6-1dbf2d229061-0000/executors/test/runs/bb8dd72c-fb4c-426a-be18-51b0621339f6
MESOS_NATIVE_JAVA_LIBRARY=/home/vagrant/dist/mesos/lib/libmesos-0.29.0.so
MESOS_FRAMEWORK_ID=1a1cad18-2d87-43dd-97b6-1dbf2d229061-0000
MESOS_SLAVE_ID=2d7e44bb-3282-4193-bdc4-eeab9e0943c2-S0
MESOS_NATIVE_LIBRARY=/home/vagrant/dist/mesos/lib/libmesos-0.29.0.so
MESOS_DIRECTORY=/tmp/mesos/slave/slaves/2d7e44bb-3282-4193-bdc4-eeab9e0943c2-S0/frameworks/1a1cad18-2d87-43dd-97b6-1dbf2d229061-0000/executors/test/runs/bb8dd72c-fb4c-426a-be18-51b0621339f6
PWD=/mnt/mesos/sandbox
MESOS_SLAVE_PID=slave(1)@10.0.2.15:5051
{noformat}

`MESOS_SANDBOX` above should be `/mnt/mesos/sandbox`.",2.0,"0.28.0,0.28.1",0.5,0.033182503770739065,0.0,0.0,0.0,0.0,0.2,0.017543859649122806,0.06428571428571428,0.7948717948717948,1.0,1.0,0.2569266055045872
Bug,/metrics/snapshot endpoint help disappeared on agent.,"After https://github.com/apache/mesos/commit/066fc4bd0df6690a5e1a929d3836e307c1e22586
the help for the /metrics/snapshot endpoint on the agent doesn't appear anymore (Master endpoint help is unchanged).",1.0,0,0.5,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.11538461538461538,0.23636363636363636,0.23636363636363636,0.0
Task,"Add authorization to the master's ""/flags"" endpoint.","Coarse HTTP endpoint authorization using the {{GET_ENDPOINT_WITH_PATH}} ACL rule needs to be added to the ""/flags"" endpoint of the master.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.2631578947368421,0.36428571428571427,0.28205128205128205,0.296969696969697,0.296969696969697,0.0
Improvement,Need support for Authorization information via HELP.,We should add information about authentication to the help message and thereby endpoint documentation (similarly as MESOS-4934 has done for authentication).,3.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.12280701754385964,0.32857142857142857,0.28205128205128205,0.296969696969697,0.296969696969697,0.0
Bug,Update mesos-execute to support docker volume isolator.,The mesos-execute needs to be updated to support docker volume isolator.,3.0,0,0.0,0.3137254901960784,0.0,0.0,0.0,0.0,0.4,0.19298245614035087,0.14285714285714285,0.10256410256410256,0.06666666666666667,0.06666666666666667,0.0
Task,Add autodiscovery for GPU resources,"Right now, the only way to enumerate the available GPUs on an agent is to use the `--nvidia_gpu_devices` flag and explicitly list them out.  Instead, we should leverage NVML to autodiscover the GPUs that are available and only use this flag as a way to explicitly list out the GPUs you want to make available in order to restrict access to some of them.",3.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Task,Add support for per-containerizer resource enumeration,"Currently the top level containerizer includes a static function for enumerating the resources available on a given agent. Ideally, this functionality should be the responsibility of individual containerizers (and specifically the responsibility of each isolator used to control access to those resources).

Adding support for this will involve making the `Containerizer::resources()` function virtual instead of static and then implementing it on a per-containerizer basis.  We should consider providing a default to make this easier in cases where there is only really one good way of enumerating a given set of resources.",3.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Bug,Command executor may escalate after the task is reaped.,"In command executor, {{escalated()}} may be scheduled before the task has been killed, i.e. {{reaped()}}, but called after. In this case {{escalated()}} should be a no-op.",1.0,"0.26.1,0.27.2,0.28.1",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.717948717948718,0.8242424242424242,0.7212121212121212,0.24782874617737002
Bug,The windows version of `os::access` has differing behavior than the POSIX version.,"The POSIX version of {{os::access}} looks like this:

{code}
inline Try<bool> access(const std::string& path, int how)
{
  if (::access(path.c_str(), how) < 0) {
    if (errno == EACCES) {
      return false;
    } else {
      return ErrnoError();
    }
  }
  return true;
}
{code}

Compare this to the Windows version of {{os::access}} which looks like this following:

{code}
inline Try<bool> access(const std::string& fileName, int how)
{
  if (::_access(fileName.c_str(), how) != 0) {
    return ErrnoError(""access: Could not access path '"" + fileName + ""'"");
  }

  return true;
}
{code}

As we can see, the case where {{errno}} is set to {{EACCES}} is handled differently between the 2 functions.

We can actually consolidate the 2 functions by simply using the POSIX version. The challenge is that on POSIX, we should use {{::access}} and {{::_access}} on Windows. Note however, that this problem is already solved, as we have an implementation of {{::access}} for Windows in {{3rdparty/libprocess/3rdparty/stout/include/stout/windows.hpp}} which simply defers to {{::_access}}.

Thus, I propose to simply consolidate the 2 implementations.",2.0,0,0.0,0.6726998491704375,0.0,0.0,0.0,0.1,0.6000000000000001,0.2807017543859649,0.29285714285714287,0.21794871794871795,0.26666666666666666,0.26666666666666666,0.0
Task,Add benchmark for writing events on the persistent connection.,It would be good to add a benchmark for testing writing events on the persistent connection for HTTP frameworks wrt driver based frameworks. The benchmark can be as simple as trying to stream generated reconciliation status update events on the persistent connection between the master and the scheduler.,3.0,0,0.0,0.5701357466063348,0.0,0.0,0.0,0.0,0.0,0.6842105263157894,0.5642857142857143,0.3846153846153846,0.4181818181818182,0.4121212121212121,0.0
Documentation,Update the documentation for '/reserve' and '/create-volumes',There are a couple issues related to the {{principal}} field in {{DiskInfo}} and {{ReservationInfo}} (see linked JIRAs) that should be better documented. We need to help users understand the purpose of these fields and how they interact with the principal provided in the HTTP authentication header. See linked tickets for background.,1.0,0.28.1,0.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.25697247706422016
Improvement,Populate FrameworkInfo.principal for authenticated frameworks,"If a framework authenticates and then does not provide a {{principal}} in its {{FrameworkInfo}}, we currently allow this and leave {{FrameworkInfo.principal}} unset. Instead, we should populate {{FrameworkInfo.principal}} for them automatically in that case to ensure that the two principals are equal.",2.0,0.28.1,0.5,0.5701357466063348,0.0,0.0,0.0,0.0,0.0,0.6842105263157894,0.5642857142857143,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.25697247706422016
Improvement,Allow any principal in ReservationInfo when HTTP authentication is off,"Mesos currently provides no way for operators to pass their principal to HTTP endpoints when HTTP authentication is off. Since we enforce that {{ReservationInfo.principal}} be equal to the operator principal in requests to {{/reserve}}, this means that when HTTP authentication is disabled, the {{ReservationInfo.principal}} field cannot be set.

To address this in the short-term, we should allow {{ReservationInfo.principal}} to hold any value when HTTP authentication is disabled.",1.0,0.28.1,0.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.25697247706422016
Bug,The mesos-execute prints confusing message when launching tasks.,"{code}
root@mesos002:~/src/mesos/m2/mesos/build# src/mesos-execute --master=192.168.56.12:5050 --name=test --docker_image=ubuntu:14.04 --command=""ls /root""
I0413 07:28:03.833521  2295 scheduler.cpp:175] Version: 0.29.0
Subscribed with ID '3a1af11e-cf66-4ce2-826d-48b332977999-0001'
Submitted task 'test' to agent '3a1af11e-cf66-4ce2-826d-48b332977999-S0'
Received status update TASK_RUNNING for task 'test'
  source: SOURCE_EXECUTOR
  reason: REASON_COMMAND_EXECUTOR_FAILED <<< 
Received status update TASK_FINISHED for task 'test'
  message: 'Command exited with status 0'
  source: SOURCE_EXECUTOR
  reason: REASON_COMMAND_EXECUTOR_FAILED <<<
root@mesos002:~/src/mesos/m2/mesos/build#
{code}",1.0,0,0.0,0.3137254901960784,0.0,0.0,0.0,0.0,0.4,0.19298245614035087,0.14285714285714285,0.10256410256410256,0.06666666666666667,0.06666666666666667,0.0
Bug,Master should reject calls from the scheduler driver if the scheduler is not connected.,"When a scheduler registers, the master will create a link from master to scheduler.  If this link breaks, the master will consider the scheduler {{inactive}} and mark it as {{disconnected}}.

This causes a couple problems:
1) Master does not send offers to {{inactive}} schedulers.  But these schedulers might consider themselves ""registered"" in a one-way network partition scenario.
2) Any calls from the {{inactive}} scheduler is still accepted, which leaves the scheduler in a starved, but semi-functional state.

See the related issue for more context: MESOS-5180

There should be an additional guard for registered, but {{inactive}} schedulers here:
https://github.com/apache/mesos/blob/94f4f4ebb7d491ec6da1473b619600332981dd8e/src/master/master.cpp#L1977

The HTTP API already does this:
https://github.com/apache/mesos/blob/94f4f4ebb7d491ec6da1473b619600332981dd8e/src/master/http.cpp#L459

Since the scheduler driver cannot return a 403, it may be necessary to return a {{Event::ERROR}} and force the scheduler to abort.",1.0,0.24.0,0.5,0.5701357466063348,0.0,0.0,0.0,0.0,0.0,0.6842105263157894,0.5642857142857143,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.2201834862385321
Improvement,Enhance the error message for Duration flag.,Enhance the error message for  https://github.com/apache/mesos/blob/4dfa91fc21f80204f5125b2e2f35c489f8fb41d8/3rdparty/libprocess/3rdparty/stout/include/stout/duration.hpp#L70 to list all of the supported duration unit.,1.0,0,0.0,0.3137254901960784,0.0,0.0,0.0,0.0,0.4,0.19298245614035087,0.14285714285714285,0.10256410256410256,0.06666666666666667,0.06666666666666667,0.0
Task,Add logic to validate for non-fractional GPU requests in the master,"We should not put this logic directly into the  'Resources::validate()' function.
The primary reason is that the existing 'Resources::validate()' function doesn't consider the semantics of any particular resource when performing its validation (it only makes sure that the fields in the 'Resource' protobuf message are correctly formed). Since a fractional 'gpus' resources is actually well-formed (and only semantically incorrect), we should push this validation logic up into the master.
    
Moreover, the existing logic to construct a 'Resources' object from a 'RepeatedPtrField<Resource>' silently drops any resources that don't pass 'Resources::validate()'. This means that if we were to push the non-fractional 'gpus' validation into 'Resources::validate()', the 'gpus' resources would just be silently dropped rather than causing a TASK_ERROR in the master. This is obviously *not* the desired behaviour.",2.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Improvement,Update the balloon-framework to run on test clusters,"There are a couple of problems with the balloon framework that prevent it from being deployed (easily) on an actual cluster:

* The framework accepts 100% of memory in an offer.  This means the expected behavior (finish or OOM) is dependent on the offer size.
* The framework assumes the {{balloon-executor}} binary is available on each agent.  This is generally only true in the build environment or in single-agent test environments.
* The framework does not specify CPUs with the executor.  This is required by many isolators.
* The executor's {{TASK_FINISHED}} logic path was untested and is flaky.
* The framework has no metrics.
* The framework only launches a single task and then exits.  With this behavior, we can't have useful metrics.
",3.0,0,0.0,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.0
Task,Allow master/agent to take multiple modules manifest files,"When loading multiple modules into master/agent, one has to merge all module metadata (library name, module name, parameters, etc.) into a single json file which is then passed on to the --modules flag. This quickly becomes cumbersome especially if the modules are coming from different vendors/developers.

An alternate would be to allow multiple invocations of --modules flag that can then be passed on to the module manager. That way, each flag corresponds to just one module library and modules from that library.

Another approach is to create a new flag (e.g., --modules-dir) that contains a path to a directory that would contain multiple json files. One can think of it as an analogous to systemd units. The operator that drops a new file into this directory and the file would automatically be picked up by the master/agent module manager. Further, the naming scheme can also be inherited to prefix the filename with an ""NN_"" to signify oad order.",3.0,0,0.0,0.3453996983408748,0.0,0.0,0.0,0.0,0.2,0.38596491228070173,0.35714285714285715,0.28205128205128205,0.29090909090909095,0.28484848484848485,0.0
Task,Expose state/state.hpp to public headers,"We want the Modules to be able to use replicated log along with the APIs to communicate with Zookeeper. This change would require us to expose at least the following headers state/storage.hpp, and any additional files that state.hpp depends on (e.g., zookeeper/authentication.hpp).",3.0,0,0.0,0.3453996983408748,0.0,0.0,0.0,0.0,0.2,0.38596491228070173,0.35714285714285715,0.28205128205128205,0.29090909090909095,0.28484848484848485,0.0
Improvement,Introduce new Authorizer Actions for Authorized based filtering of endpoints.,For authorization based endpoint filtering we need to introduce the authorizer actions outlined via MESOS-4932.,3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12280701754385964,0.32857142857142857,0.11538461538461538,0.23636363636363636,0.23636363636363636,0.0
Improvement,Benchmark overhead of authorization based filtering.,"When adding authorization based filtering as outlined in MESOS-4931 we need to be careful especially for performance critical endpoints such as /state.

We should ensure via a benchmark that performance does not degreade below an acceptable state.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12280701754385964,0.32857142857142857,0.11538461538461538,0.23636363636363636,0.23636363636363636,0.0
Bug,"Commit message hook behaves incorrectly when a message includes a ""*"".","If there is a ""\*"" in a commit message (there often is when we have bulleted lists), due to the current use of {{echo $LINE}}, the {{$LINE}} gets expanded with a ""*"" in it, which becomes a matcher in bash and therefore subsequently gets expanded into the list of files/directories in the current directory.

In order to avoid this mess, we need to wrap such variables in quotes, like so: {{echo ""$LINE""}}.",2.0,0,0.0,0.6726998491704375,0.0,0.0,0.0,0.1,0.6000000000000001,0.2807017543859649,0.29285714285714287,0.21794871794871795,0.26666666666666666,0.26666666666666666,0.0
Task,Make `network/cni` enabled as the default network isolator for `MesosContainerizer`.,"Currently there are no default `network` isolators for `MesosContainerizer`. With the development of the `network/cni` isolator we have an interface to run Mesos on multitude of IP networks. Given that its based on an open standard (the CNI spec) which is gathering a lot of traction from vendors (calico, weave, coreOS) and already works on some default networks (bridge, ipvlan, macvlan) it makes sense to make it as the default network isolator. ",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2807017543859649,0.39285714285714285,0.1282051282051282,0.3090909090909091,0.3090909090909091,0.0
Task,Add test to verify error when requesting fractional GPUs,Fractional GPU requests should immediately cause a TASK_FAILED without ever launching the task.,1.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Task,Update webui for GPU metrics,"After adding the GPU metrics and updating the resources JSON to include GPU information, the webui should be updated accordingly.",1.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Bug,Run mesos builds on PowerPC platform in ASF CI,"This is the last step to declare official support for PowerPC.

This is currently blocked on ASF INFRA adding PowerPC based Jenkins machines to the ASF CI.
",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.8846153846153846,0.9878787878787878,0.9878787878787878,0.0
Bug,MasterAllocatorTest/1.RebalancedForUpdatedWeights is flaky.,"Observed on the ASF CI:

{code}
[ RUN      ] MasterAllocatorTest/1.RebalancedForUpdatedWeights
I0407 22:34:10.330394 29278 cluster.cpp:149] Creating default 'local' authorizer
I0407 22:34:10.466182 29278 leveldb.cpp:174] Opened db in 135.608207ms
I0407 22:34:10.516398 29278 leveldb.cpp:181] Compacted db in 50.159558ms
I0407 22:34:10.516464 29278 leveldb.cpp:196] Created db iterator in 34959ns
I0407 22:34:10.516484 29278 leveldb.cpp:202] Seeked to beginning of db in 10195ns
I0407 22:34:10.516496 29278 leveldb.cpp:271] Iterated through 0 keys in the db in 7324ns
I0407 22:34:10.516547 29278 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0407 22:34:10.517277 29298 recover.cpp:447] Starting replica recovery
I0407 22:34:10.517693 29300 recover.cpp:473] Replica is in EMPTY status
I0407 22:34:10.520251 29310 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (4775)@172.17.0.3:35855
I0407 22:34:10.520611 29311 recover.cpp:193] Received a recover response from a replica in EMPTY status
I0407 22:34:10.521164 29299 recover.cpp:564] Updating replica status to STARTING
I0407 22:34:10.523435 29298 master.cpp:382] Master f59f9057-a5c7-43e1-b129-96862e640a12 (129e11060069) started on 172.17.0.3:35855
I0407 22:34:10.523473 29298 master.cpp:384] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/3rZY8C/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.29.0/_inst/share/mesos/webui"" --work_dir=""/tmp/3rZY8C/master"" --zk_session_timeout=""10secs""
I0407 22:34:10.523885 29298 master.cpp:433] Master only allowing authenticated frameworks to register
I0407 22:34:10.523901 29298 master.cpp:438] Master only allowing authenticated agents to register
I0407 22:34:10.523913 29298 credentials.hpp:37] Loading credentials for authentication from '/tmp/3rZY8C/credentials'
I0407 22:34:10.524298 29298 master.cpp:480] Using default 'crammd5' authenticator
I0407 22:34:10.524441 29298 master.cpp:551] Using default 'basic' HTTP authenticator
I0407 22:34:10.524564 29298 master.cpp:589] Authorization enabled
I0407 22:34:10.525269 29305 hierarchical.cpp:145] Initialized hierarchical allocator process
I0407 22:34:10.525333 29305 whitelist_watcher.cpp:77] No whitelist given
I0407 22:34:10.527331 29298 master.cpp:1832] The newly elected leader is master@172.17.0.3:35855 with id f59f9057-a5c7-43e1-b129-96862e640a12
I0407 22:34:10.527441 29298 master.cpp:1845] Elected as the leading master!
I0407 22:34:10.527545 29298 master.cpp:1532] Recovering from registrar
I0407 22:34:10.527889 29298 registrar.cpp:331] Recovering registrar
I0407 22:34:10.549734 29299 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 28.25177ms
I0407 22:34:10.549782 29299 replica.cpp:320] Persisted replica status to STARTING
I0407 22:34:10.550010 29299 recover.cpp:473] Replica is in STARTING status
I0407 22:34:10.551352 29299 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (4777)@172.17.0.3:35855
I0407 22:34:10.551676 29299 recover.cpp:193] Received a recover response from a replica in STARTING status
I0407 22:34:10.552315 29308 recover.cpp:564] Updating replica status to VOTING
I0407 22:34:10.574865 29308 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 22.413614ms
I0407 22:34:10.574928 29308 replica.cpp:320] Persisted replica status to VOTING
I0407 22:34:10.575103 29308 recover.cpp:578] Successfully joined the Paxos group
I0407 22:34:10.575346 29308 recover.cpp:462] Recover process terminated
I0407 22:34:10.575913 29308 log.cpp:659] Attempting to start the writer
I0407 22:34:10.577512 29308 replica.cpp:493] Replica received implicit promise request from (4778)@172.17.0.3:35855 with proposal 1
I0407 22:34:10.599984 29308 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 22.453613ms
I0407 22:34:10.600026 29308 replica.cpp:342] Persisted promised to 1
I0407 22:34:10.601773 29304 coordinator.cpp:238] Coordinator attempting to fill missing positions
I0407 22:34:10.603757 29307 replica.cpp:388] Replica received explicit promise request from (4779)@172.17.0.3:35855 for position 0 with proposal 2
I0407 22:34:10.634392 29307 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 30.269987ms
I0407 22:34:10.634829 29307 replica.cpp:712] Persisted action at 0
I0407 22:34:10.637017 29297 replica.cpp:537] Replica received write request for position 0 from (4780)@172.17.0.3:35855
I0407 22:34:10.637099 29297 leveldb.cpp:436] Reading position from leveldb took 52948ns
I0407 22:34:10.676170 29297 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 38.917487ms
I0407 22:34:10.676352 29297 replica.cpp:712] Persisted action at 0
I0407 22:34:10.677564 29306 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I0407 22:34:10.717959 29306 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 40.306229ms
I0407 22:34:10.718202 29306 replica.cpp:712] Persisted action at 0
I0407 22:34:10.718399 29306 replica.cpp:697] Replica learned NOP action at position 0
I0407 22:34:10.719883 29306 log.cpp:675] Writer started with ending position 0
I0407 22:34:10.721688 29305 leveldb.cpp:436] Reading position from leveldb took 75934ns
I0407 22:34:10.723640 29306 registrar.cpp:364] Successfully fetched the registry (0B) in 195648us
I0407 22:34:10.723999 29306 registrar.cpp:463] Applied 1 operations in 108099ns; attempting to update the 'registry'
I0407 22:34:10.725077 29311 log.cpp:683] Attempting to append 170 bytes to the log
I0407 22:34:10.725328 29308 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I0407 22:34:10.726552 29299 replica.cpp:537] Replica received write request for position 1 from (4781)@172.17.0.3:35855
I0407 22:34:10.759747 29299 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 33.089719ms
I0407 22:34:10.759976 29299 replica.cpp:712] Persisted action at 1
I0407 22:34:10.761739 29299 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I0407 22:34:10.801522 29299 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 39.694064ms
I0407 22:34:10.801602 29299 replica.cpp:712] Persisted action at 1
I0407 22:34:10.801638 29299 replica.cpp:697] Replica learned APPEND action at position 1
I0407 22:34:10.803371 29311 registrar.cpp:508] Successfully updated the 'registry' in 79.163904ms
I0407 22:34:10.803829 29311 registrar.cpp:394] Successfully recovered registrar
I0407 22:34:10.804585 29311 master.cpp:1640] Recovered 0 agents from the Registry (131B) ; allowing 10mins for agents to re-register
I0407 22:34:10.805269 29308 log.cpp:702] Attempting to truncate the log to 1
I0407 22:34:10.805721 29310 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I0407 22:34:10.805276 29296 hierarchical.cpp:172] Skipping recovery of hierarchical allocator: nothing to recover
I0407 22:34:10.806529 29307 replica.cpp:537] Replica received write request for position 2 from (4782)@172.17.0.3:35855
I0407 22:34:10.843320 29307 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 36.77593ms
I0407 22:34:10.843531 29307 replica.cpp:712] Persisted action at 2
I0407 22:34:10.845369 29311 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I0407 22:34:10.885098 29311 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 39.641102ms
I0407 22:34:10.885401 29311 leveldb.cpp:399] Deleting ~1 keys from leveldb took 88701ns
I0407 22:34:10.885745 29311 replica.cpp:712] Persisted action at 2
I0407 22:34:10.885862 29311 replica.cpp:697] Replica learned TRUNCATE action at position 2
I0407 22:34:10.900660 29278 containerizer.cpp:155] Using isolation: posix/cpu,posix/mem,filesystem/posix
W0407 22:34:10.901793 29278 backend.cpp:66] Failed to create 'bind' backend: BindBackend requires root privileges
I0407 22:34:10.905488 29302 slave.cpp:201] Agent started on 111)@172.17.0.3:35855
I0407 22:34:10.905553 29302 slave.cpp:202] Flags at startup: --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""true"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_9aCAYa/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_9aCAYa/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_credentials=""/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_9aCAYa/http_credentials"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.29.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:4096;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_9aCAYa""
I0407 22:34:10.906365 29302 credentials.hpp:86] Loading credential for authentication from '/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_9aCAYa/credential'
I0407 22:34:10.906787 29302 slave.cpp:339] Agent using credential for: test-principal
I0407 22:34:10.907202 29302 credentials.hpp:37] Loading credentials for authentication from '/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_9aCAYa/http_credentials'
I0407 22:34:10.907713 29302 slave.cpp:391] Using default 'basic' HTTP authenticator
I0407 22:34:10.908499 29302 resources.cpp:572] Parsing resources as JSON failed: cpus:2;mem:1024;disk:4096;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0407 22:34:10.910189 29302 slave.cpp:590] Agent resources: cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000]
I0407 22:34:10.910362 29302 slave.cpp:598] Agent attributes: [  ]
I0407 22:34:10.910465 29302 slave.cpp:603] Agent hostname: 129e11060069
I0407 22:34:10.913280 29303 state.cpp:57] Recovering state from '/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_9aCAYa/meta'
I0407 22:34:10.914621 29303 status_update_manager.cpp:200] Recovering status update manager
I0407 22:34:10.915226 29303 containerizer.cpp:416] Recovering containerizer
I0407 22:34:10.917246 29301 provisioner.cpp:245] Provisioner recovery complete
I0407 22:34:10.917733 29301 slave.cpp:4784] Finished recovery
I0407 22:34:10.918226 29301 slave.cpp:4956] Querying resource estimator for oversubscribable resources
I0407 22:34:10.918529 29301 slave.cpp:4970] Received oversubscribable resources  from the resource estimator
I0407 22:34:10.918908 29304 slave.cpp:939] New master detected at master@172.17.0.3:35855
I0407 22:34:10.918988 29304 slave.cpp:1002] Authenticating with master master@172.17.0.3:35855
I0407 22:34:10.919098 29301 status_update_manager.cpp:174] Pausing sending status updates
I0407 22:34:10.919309 29304 slave.cpp:1007] Using default CRAM-MD5 authenticatee
I0407 22:34:10.919535 29304 slave.cpp:975] Detecting new master
I0407 22:34:10.919747 29308 authenticatee.cpp:121] Creating new client SASL connection
I0407 22:34:10.920413 29308 master.cpp:5695] Authenticating slave(111)@172.17.0.3:35855
I0407 22:34:10.920650 29308 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(278)@172.17.0.3:35855
I0407 22:34:10.921020 29308 authenticator.cpp:98] Creating new server SASL connection
I0407 22:34:10.921308 29308 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0407 22:34:10.921424 29308 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0407 22:34:10.921596 29308 authenticator.cpp:203] Received SASL authentication start
I0407 22:34:10.921752 29308 authenticator.cpp:325] Authentication requires more steps
I0407 22:34:10.921957 29307 authenticatee.cpp:258] Received SASL authentication step
I0407 22:34:10.922178 29308 authenticator.cpp:231] Received SASL authentication step
I0407 22:34:10.922214 29308 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '129e11060069' server FQDN: '129e11060069' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0407 22:34:10.922229 29308 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0407 22:34:10.922281 29308 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0407 22:34:10.922309 29308 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '129e11060069' server FQDN: '129e11060069' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0407 22:34:10.922322 29308 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0407 22:34:10.922332 29308 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0407 22:34:10.922353 29308 authenticator.cpp:317] Authentication success
I0407 22:34:10.922436 29307 authenticatee.cpp:298] Authentication success
I0407 22:34:10.922587 29308 master.cpp:5725] Successfully authenticated principal 'test-principal' at slave(111)@172.17.0.3:35855
I0407 22:34:10.922668 29299 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(278)@172.17.0.3:35855
I0407 22:34:10.923256 29307 slave.cpp:1072] Successfully authenticated with master master@172.17.0.3:35855
I0407 22:34:10.923429 29307 slave.cpp:1468] Will retry registration in 3.220345ms if necessary
I0407 22:34:10.923707 29302 master.cpp:4406] Registering agent at slave(111)@172.17.0.3:35855 (129e11060069) with id f59f9057-a5c7-43e1-b129-96862e640a12-S0
I0407 22:34:10.924239 29309 registrar.cpp:463] Applied 1 operations in 105794ns; attempting to update the 'registry'
I0407 22:34:10.925787 29309 log.cpp:683] Attempting to append 339 bytes to the log
I0407 22:34:10.926028 29309 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I0407 22:34:10.927139 29309 replica.cpp:537] Replica received write request for position 3 from (4797)@172.17.0.3:35855
I0407 22:34:10.929083 29305 slave.cpp:1468] Will retry registration in 39.293556ms if necessary
I0407 22:34:10.929363 29305 master.cpp:4394] Ignoring register agent message from slave(111)@172.17.0.3:35855 (129e11060069) as admission is already in progress
I0407 22:34:10.968843 29309 leveldb.cpp:341] Persisting action (358 bytes) to leveldb took 41.68025ms
I0407 22:34:10.969005 29309 replica.cpp:712] Persisted action at 3
I0407 22:34:10.969741 29309 slave.cpp:1468] Will retry registration in 54.852242ms if necessary
I0407 22:34:10.970118 29309 master.cpp:4394] Ignoring register agent message from slave(111)@172.17.0.3:35855 (129e11060069) as admission is already in progress
I0407 22:34:10.970852 29306 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I0407 22:34:11.010634 29306 leveldb.cpp:341] Persisting action (360 bytes) to leveldb took 39.680272ms
I0407 22:34:11.010840 29306 replica.cpp:712] Persisted action at 3
I0407 22:34:11.011014 29306 replica.cpp:697] Replica learned APPEND action at position 3
I0407 22:34:11.014020 29306 registrar.cpp:508] Successfully updated the 'registry' in 89.684224ms
I0407 22:34:11.014181 29296 log.cpp:702] Attempting to truncate the log to 3
I0407 22:34:11.014606 29296 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I0407 22:34:11.015836 29298 replica.cpp:537] Replica received write request for position 4 from (4798)@172.17.0.3:35855
I0407 22:34:11.016973 29296 master.cpp:4474] Registered agent f59f9057-a5c7-43e1-b129-96862e640a12-S0 at slave(111)@172.17.0.3:35855 (129e11060069) with cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000]
I0407 22:34:11.017518 29304 hierarchical.cpp:476] Added agent f59f9057-a5c7-43e1-b129-96862e640a12-S0 (129e11060069) with cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000] (allocated: )
I0407 22:34:11.017763 29311 slave.cpp:1116] Registered with master master@172.17.0.3:35855; given agent ID f59f9057-a5c7-43e1-b129-96862e640a12-S0
I0407 22:34:11.018362 29311 fetcher.cpp:81] Clearing fetcher cache
I0407 22:34:11.018870 29311 slave.cpp:1139] Checkpointing SlaveInfo to '/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_9aCAYa/meta/slaves/f59f9057-a5c7-43e1-b129-96862e640a12-S0/slave.info'
I0407 22:34:11.018890 29307 status_update_manager.cpp:181] Resuming sending status updates
I0407 22:34:11.019182 29304 hierarchical.cpp:1491] No resources available to allocate!
I0407 22:34:11.019304 29304 hierarchical.cpp:1165] Performed allocation for agent f59f9057-a5c7-43e1-b129-96862e640a12-S0 in 1.077349ms
I0407 22:34:11.019493 29311 slave.cpp:1176] Forwarding total oversubscribed resources 
I0407 22:34:11.019726 29311 slave.cpp:3675] Received ping from slave-observer(112)@172.17.0.3:35855
I0407 22:34:11.019878 29299 master.cpp:4818] Received update of agent f59f9057-a5c7-43e1-b129-96862e640a12-S0 at slave(111)@172.17.0.3:35855 (129e11060069) with total oversubscribed resources 
I0407 22:34:11.020845 29305 hierarchical.cpp:534] Agent f59f9057-a5c7-43e1-b129-96862e640a12-S0 (129e11060069) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000], allocated: )
I0407 22:34:11.021005 29305 hierarchical.cpp:1491] No resources available to allocate!
I0407 22:34:11.021065 29305 hierarchical.cpp:1165] Performed allocation for agent f59f9057-a5c7-43e1-b129-96862e640a12-S0 in 173907ns
I0407 22:34:11.022289 29278 containerizer.cpp:155] Using isolation: posix/cpu,posix/mem,filesystem/posix
W0407 22:34:11.023422 29278 backend.cpp:66] Failed to create 'bind' backend: BindBackend requires root privileges
I0407 22:34:11.026309 29309 slave.cpp:201] Agent started on 112)@172.17.0.3:35855
I0407 22:34:11.026410 29309 slave.cpp:202] Flags at startup: --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""true"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_h8KW9O/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_h8KW9O/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_credentials=""/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_h8KW9O/http_credentials"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.29.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:4096;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_h8KW9O""
I0407 22:34:11.027070 29309 credentials.hpp:86] Loading credential for authentication from '/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_h8KW9O/credential'
I0407 22:34:11.027308 29309 slave.cpp:339] Agent using credential for: test-principal
I0407 22:34:11.027354 29309 credentials.hpp:37] Loading credentials for authentication from '/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_h8KW9O/http_credentials'
I0407 22:34:11.027698 29309 slave.cpp:391] Using default 'basic' HTTP authenticator
I0407 22:34:11.028147 29309 resources.cpp:572] Parsing resources as JSON failed: cpus:2;mem:1024;disk:4096;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0407 22:34:11.028854 29309 slave.cpp:590] Agent resources: cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000]
I0407 22:34:11.028998 29309 slave.cpp:598] Agent attributes: [  ]
I0407 22:34:11.029064 29309 slave.cpp:603] Agent hostname: 129e11060069
I0407 22:34:11.031188 29309 state.cpp:57] Recovering state from '/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_h8KW9O/meta'
I0407 22:34:11.031844 29300 status_update_manager.cpp:200] Recovering status update manager
I0407 22:34:11.032091 29300 containerizer.cpp:416] Recovering containerizer
I0407 22:34:11.033805 29300 provisioner.cpp:245] Provisioner recovery complete
I0407 22:34:11.034364 29300 slave.cpp:4784] Finished recovery
I0407 22:34:11.061807 29300 slave.cpp:4956] Querying resource estimator for oversubscribable resources
I0407 22:34:11.062371 29300 slave.cpp:939] New master detected at master@172.17.0.3:35855
I0407 22:34:11.062450 29300 slave.cpp:1002] Authenticating with master master@172.17.0.3:35855
I0407 22:34:11.062469 29300 slave.cpp:1007] Using default CRAM-MD5 authenticatee
I0407 22:34:11.062630 29300 slave.cpp:975] Detecting new master
I0407 22:34:11.062737 29300 slave.cpp:4970] Received oversubscribable resources  from the resource estimator
I0407 22:34:11.062820 29300 status_update_manager.cpp:174] Pausing sending status updates
I0407 22:34:11.062952 29300 authenticatee.cpp:121] Creating new client SASL connection
I0407 22:34:11.063413 29300 master.cpp:5695] Authenticating slave(112)@172.17.0.3:35855
I0407 22:34:11.063591 29300 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(279)@172.17.0.3:35855
I0407 22:34:11.063907 29300 authenticator.cpp:98] Creating new server SASL connection
I0407 22:34:11.064159 29300 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0407 22:34:11.064201 29300 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0407 22:34:11.064296 29300 authenticator.cpp:203] Received SASL authentication start
I0407 22:34:11.064363 29300 authenticator.cpp:325] Authentication requires more steps
I0407 22:34:11.064443 29300 authenticatee.cpp:258] Received SASL authentication step
I0407 22:34:11.064537 29300 authenticator.cpp:231] Received SASL authentication step
I0407 22:34:11.064569 29300 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '129e11060069' server FQDN: '129e11060069' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0407 22:34:11.064584 29300 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0407 22:34:11.064640 29300 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0407 22:34:11.064668 29300 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '129e11060069' server FQDN: '129e11060069' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0407 22:34:11.064680 29300 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0407 22:34:11.064689 29300 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0407 22:34:11.064708 29300 authenticator.cpp:317] Authentication success
I0407 22:34:11.064856 29300 authenticatee.cpp:298] Authentication success
I0407 22:34:11.064941 29300 master.cpp:5725] Successfully authenticated principal 'test-principal' at slave(112)@172.17.0.3:35855
I0407 22:34:11.065019 29300 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(279)@172.17.0.3:35855
I0407 22:34:11.065431 29305 slave.cpp:1072] Successfully authenticated with master master@172.17.0.3:35855
I0407 22:34:11.065580 29305 slave.cpp:1468] Will retry registration in 14.268351ms if necessary
I0407 22:34:11.065948 29305 master.cpp:4406] Registering agent at slave(112)@172.17.0.3:35855 (129e11060069) with id f59f9057-a5c7-43e1-b129-96862e640a12-S1
I0407 22:34:11.066653 29296 registrar.cpp:463] Applied 1 operations in 190813ns; attempting to update the 'registry'
I0407 22:34:11.075197 29298 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 59.338116ms
I0407 22:34:11.075359 29298 replica.cpp:712] Persisted action at 4
I0407 22:34:11.076177 29301 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I0407 22:34:11.080481 29309 slave.cpp:1468] Will retry registration in 23.018984ms if necessary
I0407 22:34:11.080770 29309 master.cpp:4394] Ignoring register agent message from slave(112)@172.17.0.3:35855 (129e11060069) as admission is already in progress
I0407 22:34:11.100519 29301 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 24.288152ms
I0407 22:34:11.100792 29301 leveldb.cpp:399] Deleting ~2 keys from leveldb took 98264ns
I0407 22:34:11.100883 29301 replica.cpp:712] Persisted action at 4
I0407 22:34:11.101002 29301 replica.cpp:697] Replica learned TRUNCATE action at position 4
I0407 22:34:11.102180 29309 log.cpp:683] Attempting to append 505 bytes to the log
I0407 22:34:11.102334 29301 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 5
I0407 22:34:11.103551 29309 replica.cpp:537] Replica received write request for position 5 from (4813)@172.17.0.3:35855
I0407 22:34:11.105705 29305 slave.cpp:1468] Will retry registration in 49.972787ms if necessary
I0407 22:34:11.106020 29305 master.cpp:4394] Ignoring register agent message from slave(112)@172.17.0.3:35855 (129e11060069) as admission is already in progress
I0407 22:34:11.126212 29309 leveldb.cpp:341] Persisting action (524 bytes) to leveldb took 22.638848ms
I0407 22:34:11.126296 29309 replica.cpp:712] Persisted action at 5
I0407 22:34:11.127374 29305 replica.cpp:691] Replica received learned notice for position 5 from @0.0.0.0:0
I0407 22:34:11.150754 29305 leveldb.cpp:341] Persisting action (526 bytes) to leveldb took 23.376079ms
I0407 22:34:11.150952 29305 replica.cpp:712] Persisted action at 5
I0407 22:34:11.150992 29305 replica.cpp:697] Replica learned APPEND action at position 5
I0407 22:34:11.154031 29305 registrar.cpp:508] Successfully updated the 'registry' in 87.26784ms
I0407 22:34:11.154491 29305 log.cpp:702] Attempting to truncate the log to 5
I0407 22:34:11.154824 29305 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 6
I0407 22:34:11.155413 29308 slave.cpp:3675] Received ping from slave-observer(113)@172.17.0.3:35855
I0407 22:34:11.155467 29303 master.cpp:4474] Registered agent f59f9057-a5c7-43e1-b129-96862e640a12-S1 at slave(112)@172.17.0.3:35855 (129e11060069) with cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000]
I0407 22:34:11.155580 29308 slave.cpp:1116] Registered with master master@172.17.0.3:35855; given agent ID f59f9057-a5c7-43e1-b129-96862e640a12-S1
I0407 22:34:11.155606 29308 fetcher.cpp:81] Clearing fetcher cache
I0407 22:34:11.155856 29304 status_update_manager.cpp:181] Resuming sending status updates
I0407 22:34:11.156281 29308 slave.cpp:1139] Checkpointing SlaveInfo to '/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_h8KW9O/meta/slaves/f59f9057-a5c7-43e1-b129-96862e640a12-S1/slave.info'
I0407 22:34:11.156661 29304 replica.cpp:537] Replica received write request for position 6 from (4814)@172.17.0.3:35855
I0407 22:34:11.156949 29305 hierarchical.cpp:476] Added agent f59f9057-a5c7-43e1-b129-96862e640a12-S1 (129e11060069) with cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000] (allocated: )
I0407 22:34:11.157217 29305 hierarchical.cpp:1491] No resources available to allocate!
I0407 22:34:11.157346 29305 hierarchical.cpp:1165] Performed allocation for agent f59f9057-a5c7-43e1-b129-96862e640a12-S1 in 304432ns
I0407 22:34:11.157224 29308 slave.cpp:1176] Forwarding total oversubscribed resources 
I0407 22:34:11.157788 29303 master.cpp:4818] Received update of agent f59f9057-a5c7-43e1-b129-96862e640a12-S1 at slave(112)@172.17.0.3:35855 (129e11060069) with total oversubscribed resources 
I0407 22:34:11.158424 29303 hierarchical.cpp:534] Agent f59f9057-a5c7-43e1-b129-96862e640a12-S1 (129e11060069) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):4096; ports(*):[31000-32000], allocated: )
I0407 22:34:11.158633 29303 hierarchical.cpp:1491] No resources available to allocate!
I0407 22:34:11.158699 29303 hierarchical.cpp:1165] Performed allocation for agent f59f9057-a5c7-43e1-b129-96862e640a12-S1 in 178482ns
I0407 22:34:11.162139 29278 containerizer.cpp:155] Using isolation: posix/cpu,posix/mem,filesystem/posix
W0407 22:34:11.192978 29278 backend.cpp:66] Failed to create 'bind' backend: BindBackend requires root privileges
I0407 22:34:11.197527 29307 slave.cpp:201] Agent started on 113)@172.17.0.3:35855
I0407 22:34:11.197581 29307 slave.cpp:202] Flags at startup: --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""true"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_EG5sru/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_EG5sru/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_credentials=""/tmp/MasterAllocatorTest_1_RebalancedForUpdatedWeights_EG5sru/http_credentials"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.29.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=",1.0,0.28.0,0.5,0.0015082956259426848,0.3333333333333333,0.5,0.14285714285714285,0.2,0.2,0.03508771929824561,0.05714285714285714,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.25688073394495414
Task,Cleanup memory leaks in libprocess finalize(),"libprocess's {{finalize}} function currently leaks memory for a few different reasons. Cleaning up the {{SocketManager}} will be somewhat involved (MESOS-3910), but the remaining memory leaks should be fairly easy to address.",2.0,0,0.0,1.0,0.0,0.0,0.0,0.0,0.8,0.5438596491228069,0.39999999999999997,0.48717948717948717,0.4303030303030303,0.4303030303030303,0.0
Bug,Add agent flags for HTTP authorization.,"Flags should be added to the agent to:
1. Enable authorization ({{--authorizers}})
2. Provide ACLs ({{--acls}})",2.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.2631578947368421,0.36428571428571427,0.28205128205128205,0.296969696969697,0.296969696969697,0.0
Bug,Fix Nvidia GPU test build for namespace change of MasterDetector,An update to master the day after all of the Nvidia GPU stuff landed has a build error in the Nvidia GPU tests. The namespace that MasterDetector lives in has changed and the test needs to be updated to pull in the class from the proper namespace now.,1.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Task,Remove 'dashboard.js' from the webui.,This file is no longer in use anywhere.,1.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Task,Update the default JSON representation of a Resource to include GPUs,"The default JSON representation of a Resource currently lists a value of ""0"" if no value is set on a first class SCALAR resource (i.e. cpus, mem, disk).  We should add GPUs in here as well.  ",1.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Task,Update existing documentation to Include references to GPUs as a first class resource.,"Specifically, the documentation in the following files should be udated:

{noformat}
docs/attributes-resources.md
docs/monitoring.md
{noformat}",1.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Bug,Commit message hook lints the diff in verbose mode.,"In verbose mode (i.e., {{git commit --verbose}}), the commit message includes the diff of the commit at the bottom, delimited by the following lines:

{code}
# ------------------------ >8 ------------------------
# Do not touch the line above.
# Everything below will be removed.
{code}

We should {{break}} once we encounter such a line.",2.0,0,0.0,0.6726998491704375,0.0,0.0,0.0,0.1,0.6000000000000001,0.2807017543859649,0.29285714285714287,0.21794871794871795,0.26666666666666666,0.26666666666666666,0.0
Task,Enable `newtork/cni` isolator in `MesosContainerizer` as the default `network` isolator.,"Currently there are no default `network` isolators for `MesosContainerizer`. With the development of the `network/cni` isolator we have an interface to run Mesos on multitude of IP networks. Given that its based on an open standard (the CNI spec) which is gathering a lot of traction from vendors (calico, weave, coreOS) and already works on some default networks (bridge, ipvlan, macvlan) it makes sense to make it as the default network isolator.",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2807017543859649,0.39285714285714285,0.1282051282051282,0.3090909090909091,0.3090909090909091,0.0
Bug,PersistentVolumeTest.AccessPersistentVolume is flaky,"Observed on ASF CI:

{code}
[ RUN      ] DiskResource/PersistentVolumeTest.AccessPersistentVolume/0
I0405 17:29:19.134435 31837 cluster.cpp:139] Creating default 'local' authorizer
I0405 17:29:19.251143 31837 leveldb.cpp:174] Opened db in 116.386403ms
I0405 17:29:19.310050 31837 leveldb.cpp:181] Compacted db in 58.80688ms
I0405 17:29:19.310180 31837 leveldb.cpp:196] Created db iterator in 37145ns
I0405 17:29:19.310199 31837 leveldb.cpp:202] Seeked to beginning of db in 4212ns
I0405 17:29:19.310210 31837 leveldb.cpp:271] Iterated through 0 keys in the db in 410ns
I0405 17:29:19.310279 31837 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0405 17:29:19.311069 31861 recover.cpp:447] Starting replica recovery
I0405 17:29:19.311362 31861 recover.cpp:473] Replica is in EMPTY status
I0405 17:29:19.312641 31861 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (14359)@172.17.0.4:43972
I0405 17:29:19.313045 31860 recover.cpp:193] Received a recover response from a replica in EMPTY status
I0405 17:29:19.313608 31860 recover.cpp:564] Updating replica status to STARTING
I0405 17:29:19.316416 31867 master.cpp:376] Master 9565ff6f-f1b6-4259-8430-690e635c391f (4090d10eba90) started on 172.17.0.4:43972
I0405 17:29:19.316470 31867 master.cpp:378] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/0A9ELu/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.29.0/_inst/share/mesos/webui"" --work_dir=""/tmp/0A9ELu/master"" --zk_session_timeout=""10secs""
I0405 17:29:19.316938 31867 master.cpp:427] Master only allowing authenticated frameworks to register
I0405 17:29:19.316951 31867 master.cpp:432] Master only allowing authenticated agents to register
I0405 17:29:19.316961 31867 credentials.hpp:37] Loading credentials for authentication from '/tmp/0A9ELu/credentials'
I0405 17:29:19.317402 31867 master.cpp:474] Using default 'crammd5' authenticator
I0405 17:29:19.317643 31867 master.cpp:545] Using default 'basic' HTTP authenticator
I0405 17:29:19.317854 31867 master.cpp:583] Authorization enabled
I0405 17:29:19.318081 31864 whitelist_watcher.cpp:77] No whitelist given
I0405 17:29:19.318079 31861 hierarchical.cpp:144] Initialized hierarchical allocator process
I0405 17:29:19.320838 31864 master.cpp:1826] The newly elected leader is master@172.17.0.4:43972 with id 9565ff6f-f1b6-4259-8430-690e635c391f
I0405 17:29:19.320888 31864 master.cpp:1839] Elected as the leading master!
I0405 17:29:19.320909 31864 master.cpp:1526] Recovering from registrar
I0405 17:29:19.321218 31871 registrar.cpp:331] Recovering registrar
I0405 17:29:19.347045 31860 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 33.164133ms
I0405 17:29:19.347126 31860 replica.cpp:320] Persisted replica status to STARTING
I0405 17:29:19.347611 31869 recover.cpp:473] Replica is in STARTING status
I0405 17:29:19.349215 31871 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (14361)@172.17.0.4:43972
I0405 17:29:19.349653 31870 recover.cpp:193] Received a recover response from a replica in STARTING status
I0405 17:29:19.350236 31866 recover.cpp:564] Updating replica status to VOTING
I0405 17:29:19.388882 31864 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 38.38299ms
I0405 17:29:19.388993 31864 replica.cpp:320] Persisted replica status to VOTING
I0405 17:29:19.389369 31856 recover.cpp:578] Successfully joined the Paxos group
I0405 17:29:19.389735 31856 recover.cpp:462] Recover process terminated
I0405 17:29:19.390476 31868 log.cpp:659] Attempting to start the writer
I0405 17:29:19.392125 31862 replica.cpp:493] Replica received implicit promise request from (14362)@172.17.0.4:43972 with proposal 1
I0405 17:29:19.430706 31862 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 38.505062ms
I0405 17:29:19.430816 31862 replica.cpp:342] Persisted promised to 1
I0405 17:29:19.431918 31856 coordinator.cpp:238] Coordinator attempting to fill missing positions
I0405 17:29:19.433725 31861 replica.cpp:388] Replica received explicit promise request from (14363)@172.17.0.4:43972 for position 0 with proposal 2
I0405 17:29:19.472491 31861 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 38.659492ms
I0405 17:29:19.472595 31861 replica.cpp:712] Persisted action at 0
I0405 17:29:19.474556 31864 replica.cpp:537] Replica received write request for position 0 from (14364)@172.17.0.4:43972
I0405 17:29:19.474652 31864 leveldb.cpp:436] Reading position from leveldb took 49423ns
I0405 17:29:19.528175 31864 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 53.443616ms
I0405 17:29:19.528300 31864 replica.cpp:712] Persisted action at 0
I0405 17:29:19.529389 31865 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I0405 17:29:19.571137 31865 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 41.676495ms
I0405 17:29:19.571254 31865 replica.cpp:712] Persisted action at 0
I0405 17:29:19.571302 31865 replica.cpp:697] Replica learned NOP action at position 0
I0405 17:29:19.572322 31856 log.cpp:675] Writer started with ending position 0
I0405 17:29:19.574060 31861 leveldb.cpp:436] Reading position from leveldb took 83200ns
I0405 17:29:19.575417 31864 registrar.cpp:364] Successfully fetched the registry (0B) in 0ns
I0405 17:29:19.575565 31864 registrar.cpp:463] Applied 1 operations in 46419ns; attempting to update the 'registry'
I0405 17:29:19.576517 31857 log.cpp:683] Attempting to append 170 bytes to the log
I0405 17:29:19.576849 31857 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I0405 17:29:19.578390 31857 replica.cpp:537] Replica received write request for position 1 from (14365)@172.17.0.4:43972
I0405 17:29:19.780277 31857 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 201.808617ms
I0405 17:29:19.780366 31857 replica.cpp:712] Persisted action at 1
I0405 17:29:19.782024 31857 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I0405 17:29:19.823770 31857 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 41.667662ms
I0405 17:29:19.823851 31857 replica.cpp:712] Persisted action at 1
I0405 17:29:19.823889 31857 replica.cpp:697] Replica learned APPEND action at position 1
I0405 17:29:19.825701 31867 registrar.cpp:508] Successfully updated the 'registry' in 0ns
I0405 17:29:19.825929 31867 registrar.cpp:394] Successfully recovered registrar
I0405 17:29:19.826015 31857 log.cpp:702] Attempting to truncate the log to 1
I0405 17:29:19.826262 31867 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I0405 17:29:19.827647 31867 replica.cpp:537] Replica received write request for position 2 from (14366)@172.17.0.4:43972
I0405 17:29:19.828018 31857 master.cpp:1634] Recovered 0 agents from the Registry (131B) ; allowing 10mins for agents to re-register
I0405 17:29:19.828065 31861 hierarchical.cpp:171] Skipping recovery of hierarchical allocator: nothing to recover
I0405 17:29:19.865555 31867 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 37.822178ms
I0405 17:29:19.865661 31867 replica.cpp:712] Persisted action at 2
I0405 17:29:19.866921 31867 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I0405 17:29:19.907341 31867 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 40.356649ms
I0405 17:29:19.907531 31867 leveldb.cpp:399] Deleting ~1 keys from leveldb took 91109ns
I0405 17:29:19.907560 31867 replica.cpp:712] Persisted action at 2
I0405 17:29:19.907599 31867 replica.cpp:697] Replica learned TRUNCATE action at position 2
I0405 17:29:19.923305 31837 resources.cpp:572] Parsing resources as JSON failed: cpus:2;mem:2048
Trying semicolon-delimited string format instead
I0405 17:29:19.926491 31837 containerizer.cpp:155] Using isolation: posix/cpu,posix/mem,filesystem/posix
W0405 17:29:19.927836 31837 backend.cpp:66] Failed to create 'bind' backend: BindBackend requires root privileges
I0405 17:29:19.932029 31862 slave.cpp:200] Agent started on 441)@172.17.0.4:43972
I0405 17:29:19.932086 31862 slave.cpp:201] Flags at startup: --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticate_http=""true"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --http_credentials=""/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/http_credentials"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.29.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""[{""name"":""cpus"",""role"":""*"",""scalar"":{""value"":2.0},""type"":""SCALAR""},{""name"":""mem"",""role"":""*"",""scalar"":{""value"":2048.0},""type"":""SCALAR""},{""name"":""disk"",""role"":""role1"",""scalar"":{""value"":4096.0},""type"":""SCALAR""}]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC""
I0405 17:29:19.932665 31862 credentials.hpp:86] Loading credential for authentication from '/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/credential'
I0405 17:29:19.932934 31862 slave.cpp:338] Agent using credential for: test-principal
I0405 17:29:19.932968 31862 credentials.hpp:37] Loading credentials for authentication from '/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/http_credentials'
I0405 17:29:19.933284 31862 slave.cpp:390] Using default 'basic' HTTP authenticator
I0405 17:29:19.934916 31837 sched.cpp:222] Version: 0.29.0
I0405 17:29:19.935566 31862 slave.cpp:589] Agent resources: cpus(*):2; mem(*):2048; disk(role1):4096; ports(*):[31000-32000]
I0405 17:29:19.935664 31862 slave.cpp:597] Agent attributes: [  ]
I0405 17:29:19.935679 31862 slave.cpp:602] Agent hostname: 4090d10eba90
I0405 17:29:19.938390 31864 state.cpp:57] Recovering state from '/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/meta'
I0405 17:29:19.940608 31869 sched.cpp:326] New master detected at master@172.17.0.4:43972
I0405 17:29:19.940749 31869 sched.cpp:382] Authenticating with master master@172.17.0.4:43972
I0405 17:29:19.940773 31869 sched.cpp:389] Using default CRAM-MD5 authenticatee
I0405 17:29:19.942371 31869 authenticatee.cpp:121] Creating new client SASL connection
I0405 17:29:19.942873 31859 master.cpp:5679] Authenticating scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972
I0405 17:29:19.943156 31859 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(896)@172.17.0.4:43972
I0405 17:29:19.943507 31863 authenticator.cpp:98] Creating new server SASL connection
I0405 17:29:19.943740 31859 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0405 17:29:19.943783 31859 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0405 17:29:19.943892 31859 authenticator.cpp:203] Received SASL authentication start
I0405 17:29:19.943977 31859 authenticator.cpp:325] Authentication requires more steps
I0405 17:29:19.944066 31859 authenticatee.cpp:258] Received SASL authentication step
I0405 17:29:19.944164 31859 authenticator.cpp:231] Received SASL authentication step
I0405 17:29:19.944193 31859 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '4090d10eba90' server FQDN: '4090d10eba90' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0405 17:29:19.944206 31859 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0405 17:29:19.944268 31859 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0405 17:29:19.944300 31859 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '4090d10eba90' server FQDN: '4090d10eba90' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0405 17:29:19.944313 31859 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0405 17:29:19.944321 31859 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0405 17:29:19.944339 31859 authenticator.cpp:317] Authentication success
I0405 17:29:19.944541 31859 authenticatee.cpp:298] Authentication success
I0405 17:29:19.944655 31859 master.cpp:5709] Successfully authenticated principal 'test-principal' at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972
I0405 17:29:19.944737 31859 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(896)@172.17.0.4:43972
I0405 17:29:19.945111 31859 sched.cpp:472] Successfully authenticated with master master@172.17.0.4:43972
I0405 17:29:19.945132 31859 sched.cpp:777] Sending SUBSCRIBE call to master@172.17.0.4:43972
I0405 17:29:19.945591 31859 sched.cpp:810] Will retry registration in 372.80738ms if necessary
I0405 17:29:19.945744 31865 master.cpp:2346] Received SUBSCRIBE call for framework 'default' at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972
I0405 17:29:19.945838 31865 master.cpp:1865] Authorizing framework principal 'test-principal' to receive offers for role 'role1'
I0405 17:29:19.946194 31865 master.cpp:2417] Subscribing framework default with checkpointing disabled and capabilities [  ]
I0405 17:29:19.946866 31866 hierarchical.cpp:266] Added framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:19.946974 31866 hierarchical.cpp:1490] No resources available to allocate!
I0405 17:29:19.947010 31866 hierarchical.cpp:1585] No inverse offers to send out!
I0405 17:29:19.947054 31865 sched.cpp:704] Framework registered with 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:19.947074 31866 hierarchical.cpp:1141] Performed allocation for 0 agents in 178242ns
I0405 17:29:19.947124 31865 sched.cpp:718] Scheduler::registered took 38907ns
I0405 17:29:19.948712 31866 status_update_manager.cpp:200] Recovering status update manager
I0405 17:29:19.948901 31866 containerizer.cpp:416] Recovering containerizer
I0405 17:29:19.951021 31866 provisioner.cpp:245] Provisioner recovery complete
I0405 17:29:19.951802 31866 slave.cpp:4773] Finished recovery
I0405 17:29:19.952518 31866 slave.cpp:4945] Querying resource estimator for oversubscribable resources
I0405 17:29:19.953248 31866 slave.cpp:928] New master detected at master@172.17.0.4:43972
I0405 17:29:19.953305 31865 status_update_manager.cpp:174] Pausing sending status updates
I0405 17:29:19.953626 31866 slave.cpp:991] Authenticating with master master@172.17.0.4:43972
I0405 17:29:19.953716 31866 slave.cpp:996] Using default CRAM-MD5 authenticatee
I0405 17:29:19.954074 31866 slave.cpp:964] Detecting new master
I0405 17:29:19.954167 31861 authenticatee.cpp:121] Creating new client SASL connection
I0405 17:29:19.954372 31866 slave.cpp:4959] Received oversubscribable resources  from the resource estimator
I0405 17:29:19.954756 31866 master.cpp:5679] Authenticating slave(441)@172.17.0.4:43972
I0405 17:29:19.954944 31861 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(897)@172.17.0.4:43972
I0405 17:29:19.955368 31863 authenticator.cpp:98] Creating new server SASL connection
I0405 17:29:19.955687 31861 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0405 17:29:19.955801 31861 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0405 17:29:19.956075 31861 authenticator.cpp:203] Received SASL authentication start
I0405 17:29:19.956279 31861 authenticator.cpp:325] Authentication requires more steps
I0405 17:29:19.956455 31861 authenticatee.cpp:258] Received SASL authentication step
I0405 17:29:19.956676 31861 authenticator.cpp:231] Received SASL authentication step
I0405 17:29:19.956815 31861 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '4090d10eba90' server FQDN: '4090d10eba90' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0405 17:29:19.956907 31861 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0405 17:29:19.957044 31861 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0405 17:29:19.957166 31861 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '4090d10eba90' server FQDN: '4090d10eba90' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0405 17:29:19.957264 31861 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0405 17:29:19.957353 31861 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0405 17:29:19.957449 31861 authenticator.cpp:317] Authentication success
I0405 17:29:19.957664 31857 authenticatee.cpp:298] Authentication success
I0405 17:29:19.957813 31857 master.cpp:5709] Successfully authenticated principal 'test-principal' at slave(441)@172.17.0.4:43972
I0405 17:29:19.958008 31861 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(897)@172.17.0.4:43972
I0405 17:29:19.958732 31857 slave.cpp:1061] Successfully authenticated with master master@172.17.0.4:43972
I0405 17:29:19.958930 31857 slave.cpp:1457] Will retry registration in 18.568334ms if necessary
I0405 17:29:19.959262 31857 master.cpp:4390] Registering agent at slave(441)@172.17.0.4:43972 (4090d10eba90) with id 9565ff6f-f1b6-4259-8430-690e635c391f-S0
I0405 17:29:19.959934 31857 registrar.cpp:463] Applied 1 operations in 99197ns; attempting to update the 'registry'
I0405 17:29:19.961587 31857 log.cpp:683] Attempting to append 343 bytes to the log
I0405 17:29:19.961879 31857 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I0405 17:29:19.963135 31857 replica.cpp:537] Replica received write request for position 3 from (14381)@172.17.0.4:43972
I0405 17:29:19.999408 31857 leveldb.cpp:341] Persisting action (362 bytes) to leveldb took 36.200109ms
I0405 17:29:19.999512 31857 replica.cpp:712] Persisted action at 3
I0405 17:29:20.001049 31869 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I0405 17:29:20.038849 31869 leveldb.cpp:341] Persisting action (364 bytes) to leveldb took 37.709507ms
I0405 17:29:20.038930 31869 replica.cpp:712] Persisted action at 3
I0405 17:29:20.038965 31869 replica.cpp:697] Replica learned APPEND action at position 3
I0405 17:29:20.041484 31869 registrar.cpp:508] Successfully updated the 'registry' in 0ns
I0405 17:29:20.041785 31869 log.cpp:702] Attempting to truncate the log to 3
I0405 17:29:20.042364 31859 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I0405 17:29:20.043767 31859 replica.cpp:537] Replica received write request for position 4 from (14382)@172.17.0.4:43972
I0405 17:29:20.044585 31869 master.cpp:4458] Registered agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 at slave(441)@172.17.0.4:43972 (4090d10eba90) with cpus(*):2; mem(*):2048; disk(role1):4096; ports(*):[31000-32000]
I0405 17:29:20.044910 31864 slave.cpp:1105] Registered with master master@172.17.0.4:43972; given agent ID 9565ff6f-f1b6-4259-8430-690e635c391f-S0
I0405 17:29:20.045075 31864 fetcher.cpp:81] Clearing fetcher cache
I0405 17:29:20.045140 31870 hierarchical.cpp:476] Added agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 (4090d10eba90) with cpus(*):2; mem(*):2048; disk(role1):4096; ports(*):[31000-32000] (allocated: )
I0405 17:29:20.045581 31864 slave.cpp:1128] Checkpointing SlaveInfo to '/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/meta/slaves/9565ff6f-f1b6-4259-8430-690e635c391f-S0/slave.info'
I0405 17:29:20.045974 31864 slave.cpp:1165] Forwarding total oversubscribed resources 
I0405 17:29:20.046077 31864 slave.cpp:3664] Received ping from slave-observer(399)@172.17.0.4:43972
I0405 17:29:20.046193 31864 status_update_manager.cpp:181] Resuming sending status updates
I0405 17:29:20.046289 31870 hierarchical.cpp:1585] No inverse offers to send out!
I0405 17:29:20.046370 31870 hierarchical.cpp:1164] Performed allocation for agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 in 1.153377ms
I0405 17:29:20.046499 31864 master.cpp:4802] Received update of agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 at slave(441)@172.17.0.4:43972 (4090d10eba90) with total oversubscribed resources 
I0405 17:29:20.047142 31868 hierarchical.cpp:534] Agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 (4090d10eba90) updated with oversubscribed resources  (total: cpus(*):2; mem(*):2048; disk(role1):4096; ports(*):[31000-32000], allocated: disk(role1):4096; cpus(*):2; mem(*):2048; ports(*):[31000-32000])
I0405 17:29:20.047960 31868 hierarchical.cpp:1490] No resources available to allocate!
I0405 17:29:20.048009 31868 hierarchical.cpp:1585] No inverse offers to send out!
I0405 17:29:20.048065 31868 hierarchical.cpp:1164] Performed allocation for agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 in 866803ns
I0405 17:29:20.048591 31864 master.cpp:5508] Sending 1 offers to framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 (default) at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972
I0405 17:29:20.049188 31860 sched.cpp:874] Scheduler::resourceOffers took 114867ns
I0405 17:29:20.080921 31859 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 37.025538ms
I0405 17:29:20.081001 31859 replica.cpp:712] Persisted action at 4
I0405 17:29:20.082425 31859 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I0405 17:29:20.106056 31859 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 23.583037ms
I0405 17:29:20.106205 31859 leveldb.cpp:399] Deleting ~2 keys from leveldb took 76995ns
I0405 17:29:20.106240 31859 replica.cpp:712] Persisted action at 4
I0405 17:29:20.106278 31859 replica.cpp:697] Replica learned TRUNCATE action at position 4
I0405 17:29:20.119488 31837 resources.cpp:572] Parsing resources as JSON failed: cpus:1;mem:128
Trying semicolon-delimited string format instead
I0405 17:29:20.121356 31859 master.cpp:3288] Processing ACCEPT call for offers: [ 9565ff6f-f1b6-4259-8430-690e635c391f-O0 ] on agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 at slave(441)@172.17.0.4:43972 (4090d10eba90) for framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 (default) at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972
I0405 17:29:20.121485 31859 master.cpp:3046] Authorizing principal 'test-principal' to create volumes
I0405 17:29:20.121692 31859 master.cpp:2891] Authorizing framework principal 'test-principal' to launch task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 as user 'mesos'
I0405 17:29:20.123877 31871 master.cpp:3617] Applying CREATE operation for volumes disk(role1)[id1:path1]:2048 from framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 (default) at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972 to agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 at slave(441)@172.17.0.4:43972 (4090d10eba90)
I0405 17:29:20.125424 31871 master.cpp:6747] Sending checkpointed resources disk(role1)[id1:path1]:2048 to agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 at slave(441)@172.17.0.4:43972 (4090d10eba90)
I0405 17:29:20.126397 31856 hierarchical.cpp:656] Updated allocation of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 on agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 from disk(role1):4096; cpus(*):2; mem(*):2048; ports(*):[31000-32000] to disk(role1):2048; cpus(*):2; mem(*):2048; ports(*):[31000-32000]; disk(role1)[id1:path1]:2048
I0405 17:29:20.126667 31871 master.hpp:177] Adding task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 with resources cpus(*):1; mem(*):128; disk(role1)[id1:path1]:2048 on agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 (4090d10eba90)
I0405 17:29:20.126875 31871 master.cpp:3773] Launching task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 (default) at scheduler-bdf68f7f-d938-47ed-a132-bb3f218628bf@172.17.0.4:43972 with resources cpus(*):1; mem(*):128; disk(role1)[id1:path1]:2048 on agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 at slave(441)@172.17.0.4:43972 (4090d10eba90)
I0405 17:29:20.127390 31856 slave.cpp:2523] Updated checkpointed resources from  to disk(role1)[id1:path1]:2048
I0405 17:29:20.127615 31856 slave.cpp:1497] Got assigned task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 for framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.127876 31856 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I0405 17:29:20.127841 31871 hierarchical.cpp:893] Recovered disk(role1):2048; cpus(*):1; mem(*):1920; ports(*):[31000-32000] (total: cpus(*):2; mem(*):2048; disk(role1):2048; ports(*):[31000-32000]; disk(role1)[id1:path1]:2048, allocated: disk(role1)[id1:path1]:2048; cpus(*):1; mem(*):128) on agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 from framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.127913 31871 hierarchical.cpp:930] Framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 filtered agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0 for 5secs
I0405 17:29:20.128667 31856 slave.cpp:1616] Launching task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 for framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.128937 31856 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I0405 17:29:20.129776 31856 paths.cpp:528] Trying to chown '/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/slaves/9565ff6f-f1b6-4259-8430-690e635c391f-S0/frameworks/9565ff6f-f1b6-4259-8430-690e635c391f-0000/executors/91050005-0b1d-4a37-9ea1-f8ae1ff3b542/runs/bc8b48e5-dd32-4283-a1a6-e1988c82ae09' to user 'mesos'
I0405 17:29:20.145324 31856 slave.cpp:5575] Launching executor 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/slaves/9565ff6f-f1b6-4259-8430-690e635c391f-S0/frameworks/9565ff6f-f1b6-4259-8430-690e635c391f-0000/executors/91050005-0b1d-4a37-9ea1-f8ae1ff3b542/runs/bc8b48e5-dd32-4283-a1a6-e1988c82ae09'
I0405 17:29:20.146057 31858 containerizer.cpp:675] Starting container 'bc8b48e5-dd32-4283-a1a6-e1988c82ae09' for executor '91050005-0b1d-4a37-9ea1-f8ae1ff3b542' of framework '9565ff6f-f1b6-4259-8430-690e635c391f-0000'
I0405 17:29:20.146078 31856 slave.cpp:1834] Queuing task '91050005-0b1d-4a37-9ea1-f8ae1ff3b542' for executor '91050005-0b1d-4a37-9ea1-f8ae1ff3b542' of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.146203 31856 slave.cpp:881] Successfully attached file '/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/slaves/9565ff6f-f1b6-4259-8430-690e635c391f-S0/frameworks/9565ff6f-f1b6-4259-8430-690e635c391f-0000/executors/91050005-0b1d-4a37-9ea1-f8ae1ff3b542/runs/bc8b48e5-dd32-4283-a1a6-e1988c82ae09'
I0405 17:29:20.147619 31859 posix.cpp:206] Changing the ownership of the persistent volume at '/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/volumes/roles/role1/id1' with uid 1000 and gid 1000
I0405 17:29:20.162421 31859 posix.cpp:250] Adding symlink from '/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/volumes/roles/role1/id1' to '/tmp/DiskResource_PersistentVolumeTest_AccessPersistentVolume_0_fJS7AC/slaves/9565ff6f-f1b6-4259-8430-690e635c391f-S0/frameworks/9565ff6f-f1b6-4259-8430-690e635c391f-0000/executors/91050005-0b1d-4a37-9ea1-f8ae1ff3b542/runs/bc8b48e5-dd32-4283-a1a6-e1988c82ae09/path1' for persistent volume disk(role1)[id1:path1]:2048 of container bc8b48e5-dd32-4283-a1a6-e1988c82ae09
I0405 17:29:20.172133 31861 launcher.cpp:123] Forked child with pid '7927' for container 'bc8b48e5-dd32-4283-a1a6-e1988c82ae09'
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0405 17:29:20.376197  7941 process.cpp:986] libprocess is initialized on 172.17.0.4:50952 with 16 worker threads
I0405 17:29:20.378132  7941 logging.cpp:195] Logging to STDERR
I0405 17:29:20.380861  7941 exec.cpp:150] Version: 0.29.0
I0405 17:29:20.396257  7966 exec.cpp:200] Executor started at: executor(1)@172.17.0.4:50952 with pid 7941
I0405 17:29:20.399426 31860 slave.cpp:2825] Got registration for executor '91050005-0b1d-4a37-9ea1-f8ae1ff3b542' of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 from executor(1)@172.17.0.4:50952
I0405 17:29:20.402995  7966 exec.cpp:225] Executor registered on agent 9565ff6f-f1b6-4259-8430-690e635c391f-S0
I0405 17:29:20.403014 31860 slave.cpp:1999] Sending queued task '91050005-0b1d-4a37-9ea1-f8ae1ff3b542' to executor '91050005-0b1d-4a37-9ea1-f8ae1ff3b542' of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 at executor(1)@172.17.0.4:50952
I0405 17:29:20.405624  7966 exec.cpp:237] Executor::registered took 393272ns
I0405 17:29:20.406108  7966 exec.cpp:312] Executor asked to run task '91050005-0b1d-4a37-9ea1-f8ae1ff3b542'
Registered executor on 4090d10eba90
I0405 17:29:20.406708  7966 exec.cpp:321] Executor::launchTask took 568039ns
Starting task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542
Forked command at 7972
sh -c 'echo abc > path1/file'
I0405 17:29:20.411375  7966 exec.cpp:535] Executor sending status update TASK_RUNNING (UUID: cf4f8fe9-44f2-43ce-8868-b3a09b7298cf) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.413156 31857 slave.cpp:3184] Handling status update TASK_RUNNING (UUID: cf4f8fe9-44f2-43ce-8868-b3a09b7298cf) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 from executor(1)@172.17.0.4:50952
I0405 17:29:20.415714 31857 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: cf4f8fe9-44f2-43ce-8868-b3a09b7298cf) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.415788 31857 status_update_manager.cpp:497] Creating StatusUpdate stream for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000
I0405 17:29:20.416345 31857 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: cf4f8fe9-44f2-43ce-8868-b3a09b7298cf) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 to the agent
I0405 17:29:20.416720 31870 slave.cpp:3582] Forwarding the update TASK_RUNNING (UUID: cf4f8fe9-44f2-43ce-8868-b3a09b7298cf) for task 91050005-0b1d-4a37-9ea1-f8ae1ff3b542 of framework 9565ff6f-f1b6-4259-8430-690e635c391f-0000 to master@172.17.0.4:4397",3.0,0.28.0,0.5,1.0,0.0,0.0,0.0,0.0,0.8,0.5438596491228069,0.39999999999999997,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.25688073394495414
Bug,Reset `LIBPROCESS_IP` in `network\cni` isolator.,"Currently the `LIBPROCESS_IP` environment variable was being set to
    the Agent IP if the environment variable has not be defined by the
    `Framework`. For containers having their own IP address (as with
    containers on CNI networks) this becomes a problem since the command
    executor tries to bind to the `LIBPROCESS_IP` that does not exist in
    its network namespace, and fails. Thus, for containers launched on CNI
    networks the `LIBPROCESS_IP` should not be set, or rather is set to
    ""0.0.0.0"", allowing the container to bind to the IP address provided
    by the CNI network.",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2807017543859649,0.39285714285714285,0.1282051282051282,0.3090909090909091,0.3090909090909091,0.0
Bug,Commit message hook iterates over the commented lines.,"Currently, the commit message hook iterates over the commented lines.
For example, if there is a modified file for which its path is longer than 72 characters, the commit hook errors out. We should skip over the commented lines.",2.0,0,0.0,0.6726998491704375,0.0,0.0,0.0,0.1,0.6000000000000001,0.2807017543859649,0.29285714285714287,0.21794871794871795,0.26666666666666666,0.26666666666666666,0.0
Bug,"Commit message hook iterates over words, rather than lines.","{{for LINE in $COMMIT_MESSAGE}} iterates over one word at a time, rather than one line at a time. We should use the following pattern instead:
{code}
while read LINE;
do
  ...
done <<< ""$COMMIT_MESSAGE""
{code}",2.0,0,0.0,0.6726998491704375,0.0,0.0,0.0,0.1,0.6000000000000001,0.2807017543859649,0.29285714285714287,0.21794871794871795,0.26666666666666666,0.26666666666666666,0.0
Improvement,TASK_KILLING is not supported by mesos-execute.,Recently {{TASK_KILLING}} state (MESOS-4547) have been introduced to Mesos. We should add support for this feature to {{mesos-execute}}.,3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.717948717948718,0.8242424242424242,0.7212121212121212,0.0
Bug,Grant access to /dev/nvidiactl and /dev/nvidia-uvm in the Nvidia GPU isolator.," Calls to 'nvidia-smi'  fail inside a container even if access to a GPU has been granted. Moreover, access to /dev/nvidiactl is actually required for a container to do anything useful with a GPU even if it has access to it.
    
We should grant/revoke access to /dev/nvidiactl and /dev/nvidia-uvm as GPUs are added and removed from a container in the Nvidia GPU isolator.",2.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Bug,Flags::parse does not handle empty string correctly.,"A missing default for quorum size has generated the following master config 
{code}
MESOS_WORK_DIR=""/var/lib/mesos/master""
MESOS_ZK=""zk://zk1:2181,zk2:2181,zk3:2181/mesos""
MESOS_QUORUM=

MESOS_PORT=5050
MESOS_CLUSTER=""mesos""
MESOS_LOG_DIR=""/var/log/mesos""
MESOS_LOGBUFSECS=1
MESOS_LOGGING_LEVEL=""INFO""
{code}

This was causing each elected leader to attempt replica recovery.

E.g. {{group.cpp:700] Trying to get '/mesos/log_replicas/0000000012' in ZooKeeper}}

And eventually:
{{master.cpp:1458] Recovery failed: Failed to recover registrar: Failed to perform fetch within 1mins}}

Full log on one of the masters https://gist.github.com/clehene/09a9ddfe49b92a5deb4c1b421f63479e

All masters and zk nodes were reachable over the network. 
Also once the quorum was configured the master recovery protocol finished gracefully. 
",2.0,"0.23.1,0.24.1,0.25.0,0.26.0,0.27.2,0.28.0",0.5,0.6726998491704375,0.0,0.0,0.0,0.1,0.6000000000000001,0.2807017543859649,0.29285714285714287,0.01282051282051282,0.0,0.0,0.23400611620795106
Bug,`network/cni` isolator crashes when launched without the --network_cni_plugins_dir flag,"If we start the agent with the --isolation='network/cni' but do not specify the --network_cni_plugins_dir flag, the agent crashes with the following stack dump:
0x00007ffff2324cc9 in __GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56
56      ../nptl/sysdeps/unix/sysv/linux/raise.c: No such file or directory.
(gdb) bt
#0  0x00007ffff2324cc9 in __GI_raise (sig=sig@entry=6) at ../nptl/sysdeps/unix/sysv/linux/raise.c:56
#1  0x00007ffff23280d8 in __GI_abort () at abort.c:89
#2  0x00007ffff231db86 in __assert_fail_base (fmt=0x7ffff246e830 ""%s%s%s:%u: %s%sAssertion `%s' failed.\n%n"", assertion=assertion@entry=0x451f5c ""isSome()"",
    file=file@entry=0x451f65 ""../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp"", line=line@entry=111,
    function=function@entry=0x45294a ""const T &Option<std::basic_string<char> >::get() const & [T = std::basic_string<char>]"") at assert.c:92
#3  0x00007ffff231dc32 in __GI___assert_fail (assertion=0x451f5c ""isSome()"", file=0x451f65 ""../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp"", line=111,
    function=0x45294a ""const T &Option<std::basic_string<char> >::get() const & [T = std::basic_string<char>]"") at assert.c:101
#4  0x0000000000432c0d in Option<std::string>::get() const & (this=0x6c1ea8) at ../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp:111
Python Exception <class 'IndexError'> list index out of range:
#5  0x00007ffff63ef7cc in mesos::internal::slave::NetworkCniIsolatorProcess::recover (this=0x6c1e70, states=empty std::list, orphans=...) at ../../src/slave/containerizer/mesos/isolators/network/cni/cni.cpp:331
#6  0x00007ffff60cddd8 in operator() (this=0x7fffc0001e00, process=0x6c1ef8) at ../../3rdparty/libprocess/include/process/dispatch.hpp:239
#7  0x00007ffff60cd972 in std::_Function_handler<void (process::ProcessBase*), process::Future<Nothing> process::dispatch<Nothing, mesos::internal::slave::MesosIsolatorProcess, std::list<mesos::slave::ContainerState, std::allocator<mesos::slave::ContainerState> > const&, hashset<mesos::ContainerID, std::hash<mesos::ContainerID>, std::equal_to<mesos::ContainerID> > const&, std::list<mesos::slave::ContainerState, std::allocator<mesos::slave::ContainerState> >, hashset<mesos::ContainerID, std::hash<mesos::ContainerID>, std::equal_to<mesos::ContainerID> > >(process::PID<mesos::internal::slave::MesosIsolatorProcess> const&, process::Future<Nothing> (mesos::internal::slave::MesosIsolatorProcess::*)(std::list<mesos::slave::ContainerState, std::allocator<mesos::slave::ContainerState> > const&, hashset<mesos::ContainerID, std::hash<mesos::ContainerID>, std::equal_to<mesos::ContainerID> > const&), std::list<mesos::slave::ContainerState, std::allocator<mesos::slave::ContainerState> >, hashset<mesos::ContainerID, std::hash<mesos::ContainerID>, std::equal_to<mesos::ContainerID> >)::{lambda(process::ProcessBase*)#1}>::_M_invoke(std::_Any_data const&, process::ProcessBase*) (__functor=..., __args=0x6c1ef8) at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/functional:2071
#8  0x00007ffff6a6bf38 in std::function<void (process::ProcessBase*)>::operator()(process::ProcessBase*) const (this=0x7fffc0001d70, __args=0x6c1ef8)
    at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/functional:2471
#9  0x00007ffff6a561b4 in process::ProcessBase::visit (this=0x6c1ef8, event=...) at ../../../3rdparty/libprocess/src/process.cpp:3130
#10 0x00007ffff6aac5fe in process::DispatchEvent::visit (this=0x7fffc0001570, visitor=0x6c1ef8) at ../../../3rdparty/libprocess/include/process/event.hpp:161
#11 0x00007ffff55e9c91 in process::ProcessBase::serve (this=0x6c1ef8, event=...) at ../../3rdparty/libprocess/include/process/process.hpp:82
#12 0x00007ffff6a53ed4 in process::ProcessManager::resume (this=0x67cca0, process=0x6c1ef8) at ../../../3rdparty/libprocess/src/process.cpp:2570
#13 0x00007ffff6a5bff5 in operator() (this=0x697d70, joining=...) at ../../../3rdparty/libprocess/src/process.cpp:2218
#14 0x00007ffff6a5bf33 in std::_Bind<process::ProcessManager::init_threads()::$_1 (std::reference_wrapper<std::atomic_bool const>)>::__call<void, , 0ul>(std::tuple<>&&, std::_Index_tuple<0ul>) (this=0x697d70,
    __args=<unknown type in /home/vagrant/mesosphere/mesos/build/src/.libs/libmesos-0.29.0.so, CU 0x45bb552, DIE 0x469efe5>) at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/functional:1295
#15 0x00007ffff6a5bee6 in std::_Bind<process::ProcessManager::init_threads()::$_1 (std::reference_wrapper<std::atomic_bool const>)>::operator()<, void>() (this=0x697d70)
    at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/functional:1353
#16 0x00007ffff6a5be95 in std::_Bind_simple<std::_Bind<process::ProcessManager::init_threads()::$_1 (std::reference_wrapper<std::atomic_bool const>)> ()>::_M_invoke<>(std::_Index_tuple<>) (this=0x697d70)
    at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/functional:1731
#17 0x00007ffff6a5be65 in std::_Bind_simple<std::_Bind<process::ProcessManager::init_threads()::$_1 (std::reference_wrapper<std::atomic_bool const>)> ()>::operator()() (this=0x697d70)
    at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/functional:1720
#18 0x00007ffff6a5be3c in std::thread::_Impl<std::_Bind_simple<std::_Bind<process::ProcessManager::init_threads()::$_1 (std::reference_wrapper<std::atomic_bool const>)> ()> >::_M_run() (this=0x697d58)
    at /usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/thread:115
#19 0x00007ffff2b98a60 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#20 0x00007ffff26bb182 in start_thread (arg=0x7fffeb92d700) at pthread_create.c:312
#21 0x00007ffff23e847d in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:111
(gdb) frame 4
#4  0x0000000000432c0d in Option<std::string>::get() const & (this=0x6c1ea8) at ../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp:111",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2807017543859649,0.39285714285714285,0.1282051282051282,0.3090909090909091,0.3090909090909091,0.0
Task,Introduce `WindowsSocketError`.,"{{WindowsError}} invokes {{::GetLastError}} to retrieve the error code. Windows has a {{::WSAGetLastError}} function which at the interface level, is intended for failed socket operations. We should introduce a {{WindowsSocketError}} which invokes {{::WSAGetLastError}} and use them accordingly.",2.0,0,0.0,0.6726998491704375,0.0,0.0,0.0,0.1,0.6000000000000001,0.2807017543859649,0.29285714285714287,0.21794871794871795,0.26666666666666666,0.26666666666666666,0.0
Task,Update `network::connect` to use the typed error state of `Try`.,"{{network::connect}} function returns a {{Try<int>}} currently and the caller is required to inspect the state of {{errno}} out-of-band. {{network::connect}} should really return something like a {{Try<int, ErrnoError>}}.",2.0,0,0.0,0.6726998491704375,0.0,0.0,0.0,0.1,0.6000000000000001,0.2807017543859649,0.29285714285714287,0.21794871794871795,0.26666666666666666,0.26666666666666666,0.0
Task,Introduce an additional template parameter to `Try` for typed error.,"Add an additional template parameter {{E}} to the {{Try}} class template.

{code}
template <typename T, typename E = Error>
class Try {
  /* ... */
};
{code}",3.0,0,0.0,0.6726998491704375,0.0,0.0,0.0,0.1,0.6000000000000001,0.2807017543859649,0.29285714285714287,0.21794871794871795,0.26666666666666666,0.26666666666666666,0.0
Task,Capture the error code in `ErrnoError` and `WindowsError`.,"The {{ErrnoError}} and {{WindowsError}} classes simply construct the error string via a mechanism such as {{strerror}}. They should also capture the error code, as it is an essential piece of information for such an error type.",2.0,0,0.0,0.6726998491704375,0.0,0.0,0.0,0.1,0.6000000000000001,0.2807017543859649,0.29285714285714287,0.21794871794871795,0.26666666666666666,0.26666666666666666,0.0
Improvement,Add CMake build to docker_build.sh,Add the CMake build system to docker_build.sh to automatically test the build on Jenkins alongside gcc and clang.,2.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.007142857142857143,0.0,0.0,0.0,0.0
Bug,Fix a bug in the Nvidia GPU device isolator that exposes a discrepancy between clang and gcc in 'using' declarations,"There appears to be a discrepancy between clang and gcc, which allows
clang to accept `using` declarations of the form `using ns_name::name;`
that contain nested classes, structs, and enums after the `name` field
in the declaration (e.g. `using ns_name::name::enum;`).

The language for describing this functionality is ambiguous in the
C++11 specification as referenced here:
http://en.cppreference.com/w/cpp/language/namespace#Using-declarations",1.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Improvement,Introduce more flexible subprocess interface for child options.,"We introduced a number of parameters to the subprocess interface with MESOS-5049.
Adding all options explicitly to the subprocess interface makes it inflexible. 
We should investigate a flexible options, which still prevents arbitrary code to be executed.",2.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12280701754385964,0.32857142857142857,0.11538461538461538,0.23636363636363636,0.23636363636363636,0.0
Bug,Remove default value for the agent `work_dir`,"Following a crash report from the user we need to be more explicit about the dangers of using {{/tmp}} as agent {{work_dir}}. In addition, we can remove the default value for the {{\-\-work_dir}} flag, forcing users to explicitly set the work directory for the agent.",2.0,0,0.5,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.11538461538461538,0.11515151515151516,0.11515151515151516,0.0
Improvement,Update the long-lived-framework example to run on test clusters,"There are a couple of problems with the long-lived framework that prevent it from being deployed (easily) on an actual cluster:

* The framework will greedily accept all offers; it runs one executor per agent in the cluster.
* The framework assumes the {{long-lived-executor}} binary is available on each agent.  This is generally only true in the build environment or in single-agent test environments.
* The framework does not specify an resources with the executor.  This is required by many isolators.
* The framework has no metrics.",3.0,0,0.0,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.0
Task,Slave/Agent Rename Phase I - Update strings in error messages and other strings,"This is a sub ticket of MESOS-3780. In this ticket, we will update all the slave to agent in the error messages and other strings in the code",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.017543859649122806,0.03571428571428571,0.0,0.01212121212121212,0.01212121212121212,0.0
Task,Replace Master/Slave Terminology Phase I - Update strings in the shell scripts outputs,"This is a sub ticket of MESOS-3780. In this ticket, we will rename slave to agent in the shell script outputs",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.017543859649122806,0.03571428571428571,0.0,0.01212121212121212,0.01212121212121212,0.0
Task,Slave/Agent Rename Phase I - Update strings in the log message and standard output,"This is a sub ticket of MESOS-3780. In this ticket, we will rename all the slave to agent in the log messages and standard output.",2.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.017543859649122806,0.03571428571428571,0.0,0.01212121212121212,0.01212121212121212,0.0
Improvement,Refactore subproces setup functions.,"Executing arbitrary setup functions while creating new processes is
dangerous as all functions called have to be async safe. As setup
functions are used for only very few purposes (setsid, chdir, monitoring
and killing a process (see upcoming review) it makes sense to support
them safely via parameters to subprocess. 
Another common use of child setup are is to block the child while doing some work in the parent. This pattern can be more cleanly expressed with parentHooks. ",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12280701754385964,0.32857142857142857,0.11538461538461538,0.23636363636363636,0.23636363636363636,0.0
Improvement,Temporary directories created by environment->mkdtemp cleanup can be problematic.,"Currently in mesos test, we have the temporary directories created by `environment->mkdtemp()` cleaned up until the end of the test suite, which can be problematic. For instance, if we have many tests in a test suite, each of those tests is performing large size disk read/write in its temp dir, which may lead to out of disk issue on some resource limited machines. 

We should have these temp dir created by `environment->mkdtemp` cleaned up during each test teardown. Currently we only clean up the sandbox for each test.",1.0,0,0.5,0.42533936651583715,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3142857142857143,0.6794871794871795,0.5939393939393939,0.5939393939393939,0.0
Bug,Authorization Action enum does not support upgrades.,"We need to make the Action enum optional in authorization::Request, and add an `UNKNOWN = 0;` enum value. See MESOS-4997 for details.",2.0,1.0.0,0.5,0.03167420814479638,0.3333333333333333,0.5,0.14285714285714285,0.0,0.0,0.05263157894736842,0.049999999999999996,0.05128205128205128,0.1515151515151515,0.15757575757575756,0.9174311926605504
Improvement,Add a reconnect() method to the C++ scheduler library,A reconnect() method on the library would allow the scheduler to force a reconnection (disconnect and reconnect) by the library. This might be used by the scheduler to react to lack of HEARTBEATs.,3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.8846153846153846,0.9878787878787878,0.9878787878787878,0.0
Improvement,Call and Event Type enums in executor.proto should be optional,Having a 'required' Type enum has backwards compatibility issues when adding new enum types. See MESOS-4997 for details.,2.0,0,0.5,0.03167420814479638,0.3333333333333333,0.5,0.14285714285714285,0.0,0.0,0.05263157894736842,0.049999999999999996,0.8846153846153846,0.9878787878787878,0.9878787878787878,0.0
Improvement,Call and Event Type enums in scheduler.proto should be optional,Having a 'required' Type enum has backwards compatibility issues when adding new enum types. See MESOS-4997 for details.,2.0,0,0.5,0.03167420814479638,0.3333333333333333,0.5,0.14285714285714285,0.0,0.0,0.05263157894736842,0.049999999999999996,0.8846153846153846,0.9878787878787878,0.9878787878787878,0.0
Bug,Enforce that DiskInfo principal is equal to framework/operator principal,"Currently, we require that {{ReservationInfo.principal}} be equal to the principal provided for authentication, which means that when HTTP authentication is disabled this field cannot be set. Based on comments in 'mesos.proto', the original intention was to enforce this same constraint for {{Persistence.principal}}, but it seems that we don't enforce it. This should be changed to make the two fields equivalent, with one exception: when the framework/operator principal is {{None}}, we should allow the principal in {{DiskInfo}} to take any value, along the same lines as MESOS-5212.",3.0,0,0.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Documentation,Clarify docs on '/reserve' and '/create-volumes' without authentication,"For both reservations and persistent volume creation, the behavior of the HTTP endpoints differs slightly from that of the framework operations. Due to the implementation of HTTP authentication, it is not possible for a framework/operator to provide a principal when HTTP authentication is disabled. This means that when HTTP authentication is disabled, the endpoint handlers will _always_ receive {{None()}} as the principal associated with the request, and thus if authorization is enabled, the request will only succeed if the NONE principal is authorized to do stuff.

The docs should be updated to explain this behavior explicitly.",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Bug,MasterTest.SlavesEndpointTwoSlaves is flaky,"Observed on Arch Linux with GCC 6, running in a virtualbox VM:

[ RUN      ] MasterTest.SlavesEndpointTwoSlaves
/mesos-2/src/tests/master_tests.cpp:1710: Failure
Value of: array.get().values.size()
  Actual: 1
Expected: 2u
Which is: 2
[  FAILED  ] MasterTest.SlavesEndpointTwoSlaves (86 ms)

Seems to fail non-deterministically, perhaps more often when there is concurrent CPU load on the machine.",2.0,0,0.5,0.5701357466063348,0.0,0.0,0.0,0.0,0.0,0.6842105263157894,0.5642857142857143,0.48717948717948717,0.4303030303030303,0.4303030303030303,0.0
Bug,Update mesos-execute with Appc changes.,mesos-execute cli application currently does not have support for Appc images. Adding support would make integration tests easier.,3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3357142857142857,0.14102564102564102,0.2424242424242424,0.2424242424242424,0.0
Bug,ContainerLoggerTest.LOGROTATE_RotateInSandbox is flaky,"The logger subprocesses may exit before we reach the {{waitpid}} in the test.  If this happens, {{waitpid}} will return a {{-1}} as the process no longer exists.

Verbose logs:
{code}
[ RUN      ] ContainerLoggerTest.LOGROTATE_RotateInSandbox
I0316 14:28:51.329337  1242 cluster.cpp:139] Creating default 'local' authorizer
I0316 14:28:51.332823  1242 leveldb.cpp:174] Opened db in 3.079559ms
I0316 14:28:51.333916  1242 leveldb.cpp:181] Compacted db in 1.054247ms
I0316 14:28:51.333979  1242 leveldb.cpp:196] Created db iterator in 21450ns
I0316 14:28:51.334005  1242 leveldb.cpp:202] Seeked to beginning of db in 2205ns
I0316 14:28:51.334025  1242 leveldb.cpp:271] Iterated through 0 keys in the db in 410ns
I0316 14:28:51.334089  1242 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0316 14:28:51.334661  1275 recover.cpp:447] Starting replica recovery
I0316 14:28:51.335044  1275 recover.cpp:473] Replica is in EMPTY status
I0316 14:28:51.336207  1262 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (484)@172.17.0.3:45919
I0316 14:28:51.336730  1270 recover.cpp:193] Received a recover response from a replica in EMPTY status
I0316 14:28:51.337257  1275 recover.cpp:564] Updating replica status to STARTING
I0316 14:28:51.338001  1267 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 537200ns
I0316 14:28:51.338032  1267 replica.cpp:320] Persisted replica status to STARTING
I0316 14:28:51.338183  1261 master.cpp:376] Master c7653f60-33e9-4406-9f62-dc74c906bf83 (2cbb23302fe5) started on 172.17.0.3:45919
I0316 14:28:51.338295  1263 recover.cpp:473] Replica is in STARTING status
I0316 14:28:51.338213  1261 master.cpp:378] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/XtqwkS/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.29.0/_inst/share/mesos/webui"" --work_dir=""/tmp/XtqwkS/master"" --zk_session_timeout=""10secs""
I0316 14:28:51.338562  1261 master.cpp:423] Master only allowing authenticated frameworks to register
I0316 14:28:51.338572  1261 master.cpp:428] Master only allowing authenticated slaves to register
I0316 14:28:51.338580  1261 credentials.hpp:35] Loading credentials for authentication from '/tmp/XtqwkS/credentials'
I0316 14:28:51.338877  1261 master.cpp:468] Using default 'crammd5' authenticator
I0316 14:28:51.339030  1262 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (485)@172.17.0.3:45919
I0316 14:28:51.339246  1261 master.cpp:537] Using default 'basic' HTTP authenticator
I0316 14:28:51.339393  1261 master.cpp:571] Authorization enabled
I0316 14:28:51.339390  1266 recover.cpp:193] Received a recover response from a replica in STARTING status
I0316 14:28:51.339606  1271 whitelist_watcher.cpp:77] No whitelist given
I0316 14:28:51.339607  1275 hierarchical.cpp:144] Initialized hierarchical allocator process
I0316 14:28:51.340077  1268 recover.cpp:564] Updating replica status to VOTING
I0316 14:28:51.340533  1270 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 331558ns
I0316 14:28:51.340558  1270 replica.cpp:320] Persisted replica status to VOTING
I0316 14:28:51.340672  1270 recover.cpp:578] Successfully joined the Paxos group
I0316 14:28:51.340827  1270 recover.cpp:462] Recover process terminated
I0316 14:28:51.341684  1270 master.cpp:1806] The newly elected leader is master@172.17.0.3:45919 with id c7653f60-33e9-4406-9f62-dc74c906bf83
I0316 14:28:51.341717  1270 master.cpp:1819] Elected as the leading master!
I0316 14:28:51.341740  1270 master.cpp:1508] Recovering from registrar
I0316 14:28:51.341954  1263 registrar.cpp:307] Recovering registrar
I0316 14:28:51.342499  1273 log.cpp:659] Attempting to start the writer
I0316 14:28:51.343616  1266 replica.cpp:493] Replica received implicit promise request from (487)@172.17.0.3:45919 with proposal 1
I0316 14:28:51.344183  1266 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 536941ns
I0316 14:28:51.344208  1266 replica.cpp:342] Persisted promised to 1
I0316 14:28:51.344825  1267 coordinator.cpp:238] Coordinator attempting to fill missing positions
I0316 14:28:51.346009  1276 replica.cpp:388] Replica received explicit promise request from (488)@172.17.0.3:45919 for position 0 with proposal 2
I0316 14:28:51.346371  1276 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 327890ns
I0316 14:28:51.346393  1276 replica.cpp:712] Persisted action at 0
I0316 14:28:51.347363  1267 replica.cpp:537] Replica received write request for position 0 from (489)@172.17.0.3:45919
I0316 14:28:51.347414  1267 leveldb.cpp:436] Reading position from leveldb took 24861ns
I0316 14:28:51.347774  1267 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 323654ns
I0316 14:28:51.347796  1267 replica.cpp:712] Persisted action at 0
I0316 14:28:51.348323  1276 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I0316 14:28:51.348714  1276 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 361981ns
I0316 14:28:51.348738  1276 replica.cpp:712] Persisted action at 0
I0316 14:28:51.348760  1276 replica.cpp:697] Replica learned NOP action at position 0
I0316 14:28:51.349318  1274 log.cpp:675] Writer started with ending position 0
I0316 14:28:51.350275  1267 leveldb.cpp:436] Reading position from leveldb took 23849ns
I0316 14:28:51.351171  1271 registrar.cpp:340] Successfully fetched the registry (0B) in 9.173248ms
I0316 14:28:51.351300  1271 registrar.cpp:439] Applied 1 operations in 32119ns; attempting to update the 'registry'
I0316 14:28:51.351989  1272 log.cpp:683] Attempting to append 170 bytes to the log
I0316 14:28:51.352108  1266 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I0316 14:28:51.352802  1263 replica.cpp:537] Replica received write request for position 1 from (490)@172.17.0.3:45919
I0316 14:28:51.353313  1263 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 474854ns
I0316 14:28:51.353338  1263 replica.cpp:712] Persisted action at 1
I0316 14:28:51.354101  1273 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I0316 14:28:51.354483  1273 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 338210ns
I0316 14:28:51.354507  1273 replica.cpp:712] Persisted action at 1
I0316 14:28:51.354529  1273 replica.cpp:697] Replica learned APPEND action at position 1
I0316 14:28:51.355444  1275 registrar.cpp:484] Successfully updated the 'registry' in 4.084224ms
I0316 14:28:51.355569  1275 registrar.cpp:370] Successfully recovered registrar
I0316 14:28:51.355697  1268 log.cpp:702] Attempting to truncate the log to 1
I0316 14:28:51.355870  1269 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I0316 14:28:51.356016  1274 master.cpp:1616] Recovered 0 slaves from the Registry (131B) ; allowing 10mins for slaves to re-register
I0316 14:28:51.356032  1272 hierarchical.cpp:171] Skipping recovery of hierarchical allocator: nothing to recover
I0316 14:28:51.356761  1273 replica.cpp:537] Replica received write request for position 2 from (491)@172.17.0.3:45919
I0316 14:28:51.357203  1273 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 406053ns
I0316 14:28:51.357226  1273 replica.cpp:712] Persisted action at 2
I0316 14:28:51.357718  1270 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I0316 14:28:51.358093  1270 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 345370ns
I0316 14:28:51.358175  1270 leveldb.cpp:399] Deleting ~1 keys from leveldb took 57us
I0316 14:28:51.358201  1270 replica.cpp:712] Persisted action at 2
I0316 14:28:51.358220  1270 replica.cpp:697] Replica learned TRUNCATE action at position 2
I0316 14:28:51.368399  1242 containerizer.cpp:149] Using isolation: posix/cpu,posix/mem,filesystem/posix
W0316 14:28:51.406371  1242 backend.cpp:66] Failed to create 'bind' backend: BindBackend requires root privileges
I0316 14:28:51.410480  1266 slave.cpp:193] Slave started on 12)@172.17.0.3:45919
I0316 14:28:51.410518  1266 slave.cpp:194] Flags at startup: --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --container_logger=""org_apache_mesos_LogrotateContainerLogger"" --containerizers=""mesos"" --credential=""/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.29.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy""
I0316 14:28:51.411118  1266 credentials.hpp:83] Loading credential for authentication from '/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy/credential'
I0316 14:28:51.411381  1266 slave.cpp:324] Slave using credential for: test-principal
I0316 14:28:51.411696  1266 resources.cpp:572] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0316 14:28:51.412075  1266 slave.cpp:464] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0316 14:28:51.412148  1266 slave.cpp:472] Slave attributes: [  ]
I0316 14:28:51.412160  1266 slave.cpp:477] Slave hostname: 2cbb23302fe5
I0316 14:28:51.413516  1263 state.cpp:58] Recovering state from '/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy/meta'
I0316 14:28:51.413774  1266 status_update_manager.cpp:200] Recovering status update manager
I0316 14:28:51.414029  1276 containerizer.cpp:407] Recovering containerizer
I0316 14:28:51.415222  1269 provisioner.cpp:245] Provisioner recovery complete
I0316 14:28:51.415650  1268 slave.cpp:4565] Finished recovery
I0316 14:28:51.416115  1268 slave.cpp:4737] Querying resource estimator for oversubscribable resources
I0316 14:28:51.416365  1268 slave.cpp:796] New master detected at master@172.17.0.3:45919
I0316 14:28:51.416448  1276 status_update_manager.cpp:174] Pausing sending status updates
I0316 14:28:51.416445  1268 slave.cpp:859] Authenticating with master master@172.17.0.3:45919
I0316 14:28:51.416522  1268 slave.cpp:864] Using default CRAM-MD5 authenticatee
I0316 14:28:51.416671  1268 slave.cpp:832] Detecting new master
I0316 14:28:51.416731  1275 authenticatee.cpp:121] Creating new client SASL connection
I0316 14:28:51.416807  1268 slave.cpp:4751] Received oversubscribable resources  from the resource estimator
I0316 14:28:51.417006  1263 master.cpp:5659] Authenticating slave(12)@172.17.0.3:45919
I0316 14:28:51.417103  1262 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(38)@172.17.0.3:45919
I0316 14:28:51.417348  1273 authenticator.cpp:98] Creating new server SASL connection
I0316 14:28:51.417548  1266 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0316 14:28:51.417582  1266 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0316 14:28:51.417696  1264 authenticator.cpp:203] Received SASL authentication start
I0316 14:28:51.417753  1264 authenticator.cpp:325] Authentication requires more steps
I0316 14:28:51.417948  1265 authenticatee.cpp:258] Received SASL authentication step
I0316 14:28:51.418107  1267 authenticator.cpp:231] Received SASL authentication step
I0316 14:28:51.418159  1267 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '2cbb23302fe5' server FQDN: '2cbb23302fe5' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0316 14:28:51.418180  1267 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0316 14:28:51.418233  1267 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0316 14:28:51.418270  1267 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '2cbb23302fe5' server FQDN: '2cbb23302fe5' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0316 14:28:51.418289  1267 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0316 14:28:51.418300  1267 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0316 14:28:51.418323  1267 authenticator.cpp:317] Authentication success
I0316 14:28:51.418414  1264 authenticatee.cpp:298] Authentication success
I0316 14:28:51.418473  1269 master.cpp:5689] Successfully authenticated principal 'test-principal' at slave(12)@172.17.0.3:45919
I0316 14:28:51.418514  1275 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(38)@172.17.0.3:45919
I0316 14:28:51.418781  1276 slave.cpp:927] Successfully authenticated with master master@172.17.0.3:45919
I0316 14:28:51.418937  1276 slave.cpp:1321] Will retry registration in 1.983001ms if necessary
I0316 14:28:51.419108  1262 master.cpp:4370] Registering slave at slave(12)@172.17.0.3:45919 (2cbb23302fe5) with id c7653f60-33e9-4406-9f62-dc74c906bf83-S0
I0316 14:28:51.419643  1266 registrar.cpp:439] Applied 1 operations in 75642ns; attempting to update the 'registry'
I0316 14:28:51.420670  1272 log.cpp:683] Attempting to append 339 bytes to the log
I0316 14:28:51.420820  1269 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I0316 14:28:51.421495  1270 slave.cpp:1321] Will retry registration in 1.437257ms if necessary
I0316 14:28:51.421716  1275 master.cpp:4358] Ignoring register slave message from slave(12)@172.17.0.3:45919 (2cbb23302fe5) as admission is already in progress
I0316 14:28:51.422107  1267 replica.cpp:537] Replica received write request for position 3 from (505)@172.17.0.3:45919
I0316 14:28:51.423033  1267 leveldb.cpp:341] Persisting action (358 bytes) to leveldb took 762815ns
I0316 14:28:51.423066  1267 replica.cpp:712] Persisted action at 3
I0316 14:28:51.424069  1267 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I0316 14:28:51.424232  1264 slave.cpp:1321] Will retry registration in 66.01292ms if necessary
I0316 14:28:51.424342  1269 master.cpp:4358] Ignoring register slave message from slave(12)@172.17.0.3:45919 (2cbb23302fe5) as admission is already in progress
I0316 14:28:51.424686  1267 leveldb.cpp:341] Persisting action (360 bytes) to leveldb took 574743ns
I0316 14:28:51.424757  1267 replica.cpp:712] Persisted action at 3
I0316 14:28:51.424792  1267 replica.cpp:697] Replica learned APPEND action at position 3
I0316 14:28:51.426441  1272 registrar.cpp:484] Successfully updated the 'registry' in 6.721024ms
I0316 14:28:51.426677  1262 log.cpp:702] Attempting to truncate the log to 3
I0316 14:28:51.426808  1264 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I0316 14:28:51.427584  1261 slave.cpp:3482] Received ping from slave-observer(11)@172.17.0.3:45919
I0316 14:28:51.428213  1262 hierarchical.cpp:473] Added slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 (2cbb23302fe5) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
I0316 14:28:51.427865  1266 master.cpp:4438] Registered slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 at slave(12)@172.17.0.3:45919 (2cbb23302fe5) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0316 14:28:51.428270  1267 slave.cpp:971] Registered with master master@172.17.0.3:45919; given slave ID c7653f60-33e9-4406-9f62-dc74c906bf83-S0
I0316 14:28:51.428412  1265 replica.cpp:537] Replica received write request for position 4 from (506)@172.17.0.3:45919
I0316 14:28:51.428443  1267 fetcher.cpp:81] Clearing fetcher cache
I0316 14:28:51.428503  1262 hierarchical.cpp:1453] No resources available to allocate!
I0316 14:28:51.428535  1262 hierarchical.cpp:1150] Performed allocation for slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 in 205421ns
I0316 14:28:51.428750  1273 status_update_manager.cpp:181] Resuming sending status updates
I0316 14:28:51.429157  1265 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 695258ns
I0316 14:28:51.429225  1267 slave.cpp:994] Checkpointing SlaveInfo to '/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy/meta/slaves/c7653f60-33e9-4406-9f62-dc74c906bf83-S0/slave.info'
I0316 14:28:51.429275  1265 replica.cpp:712] Persisted action at 4
I0316 14:28:51.429759  1267 slave.cpp:1030] Forwarding total oversubscribed resources 
I0316 14:28:51.430055  1265 master.cpp:4782] Received update of slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 at slave(12)@172.17.0.3:45919 (2cbb23302fe5) with total oversubscribed resources 
I0316 14:28:51.430614  1271 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I0316 14:28:51.430891  1242 sched.cpp:222] Version: 0.29.0
I0316 14:28:51.431043  1265 hierarchical.cpp:531] Slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 (2cbb23302fe5) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: )
I0316 14:28:51.431236  1271 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 536892ns
I0316 14:28:51.431267  1265 hierarchical.cpp:1453] No resources available to allocate!
I0316 14:28:51.431584  1271 leveldb.cpp:399] Deleting ~2 keys from leveldb took 66904ns
I0316 14:28:51.431538  1273 sched.cpp:326] New master detected at master@172.17.0.3:45919
I0316 14:28:51.431622  1271 replica.cpp:712] Persisted action at 4
I0316 14:28:51.431623  1265 hierarchical.cpp:1150] Performed allocation for slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 in 518588ns
I0316 14:28:51.431660  1271 replica.cpp:697] Replica learned TRUNCATE action at position 4
I0316 14:28:51.431711  1273 sched.cpp:382] Authenticating with master master@172.17.0.3:45919
I0316 14:28:51.431737  1273 sched.cpp:389] Using default CRAM-MD5 authenticatee
I0316 14:28:51.431982  1266 authenticatee.cpp:121] Creating new client SASL connection
I0316 14:28:51.432369  1261 master.cpp:5659] Authenticating scheduler-96f85a94-b6a8-4363-bc3b-b8a233b90e79@172.17.0.3:45919
I0316 14:28:51.432509  1263 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(39)@172.17.0.3:45919
I0316 14:28:51.432868  1267 authenticator.cpp:98] Creating new server SASL connection
I0316 14:28:51.433135  1276 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0316 14:28:51.433233  1276 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0316 14:28:51.433423  1276 authenticator.cpp:203] Received SASL authentication start
I0316 14:28:51.433502  1276 authenticator.cpp:325] Authentication requires more steps
I0316 14:28:51.433606  1274 authenticatee.cpp:258] Received SASL authentication step
I0316 14:28:51.433744  1273 authenticator.cpp:231] Received SASL authentication step
I0316 14:28:51.433785  1273 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '2cbb23302fe5' server FQDN: '2cbb23302fe5' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0316 14:28:51.433801  1273 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0316 14:28:51.433861  1273 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0316 14:28:51.433897  1273 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '2cbb23302fe5' server FQDN: '2cbb23302fe5' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0316 14:28:51.433912  1273 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0316 14:28:51.433924  1273 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0316 14:28:51.433944  1273 authenticator.cpp:317] Authentication success
I0316 14:28:51.434037  1274 authenticatee.cpp:298] Authentication success
I0316 14:28:51.434108  1268 master.cpp:5689] Successfully authenticated principal 'test-principal' at scheduler-96f85a94-b6a8-4363-bc3b-b8a233b90e79@172.17.0.3:45919
I0316 14:28:51.434211  1272 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(39)@172.17.0.3:45919
I0316 14:28:51.434512  1274 sched.cpp:471] Successfully authenticated with master master@172.17.0.3:45919
I0316 14:28:51.434535  1274 sched.cpp:776] Sending SUBSCRIBE call to master@172.17.0.3:45919
I0316 14:28:51.434648  1274 sched.cpp:809] Will retry registration in 356.547014ms if necessary
I0316 14:28:51.434819  1266 master.cpp:2326] Received SUBSCRIBE call for framework 'default' at scheduler-96f85a94-b6a8-4363-bc3b-b8a233b90e79@172.17.0.3:45919
I0316 14:28:51.434905  1266 master.cpp:1845] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0316 14:28:51.435464  1265 master.cpp:2397] Subscribing framework default with checkpointing disabled and capabilities [  ]
I0316 14:28:51.435979  1269 hierarchical.cpp:265] Added framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:51.436213  1272 sched.cpp:703] Framework registered with c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:51.436316  1272 sched.cpp:717] Scheduler::registered took 73782ns
I0316 14:28:51.436928  1269 hierarchical.cpp:1548] No inverse offers to send out!
I0316 14:28:51.436978  1269 hierarchical.cpp:1130] Performed allocation for 1 slaves in 970638ns
I0316 14:28:51.437278  1272 master.cpp:5488] Sending 1 offers to framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 (default) at scheduler-96f85a94-b6a8-4363-bc3b-b8a233b90e79@172.17.0.3:45919
I0316 14:28:51.437782  1262 sched.cpp:873] Scheduler::resourceOffers took 129952ns
I0316 14:28:51.440006  1274 master.cpp:3268] Processing ACCEPT call for offers: [ c7653f60-33e9-4406-9f62-dc74c906bf83-O0 ] on slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 at slave(12)@172.17.0.3:45919 (2cbb23302fe5) for framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 (default) at scheduler-96f85a94-b6a8-4363-bc3b-b8a233b90e79@172.17.0.3:45919
I0316 14:28:51.440094  1274 master.cpp:2871] Authorizing framework principal 'test-principal' to launch task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 as user 'mesos'
I0316 14:28:51.442152  1274 master.hpp:177] Adding task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 (2cbb23302fe5)
I0316 14:28:51.442348  1274 master.cpp:3753] Launching task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 (default) at scheduler-96f85a94-b6a8-4363-bc3b-b8a233b90e79@172.17.0.3:45919 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 at slave(12)@172.17.0.3:45919 (2cbb23302fe5)
I0316 14:28:51.442749  1265 slave.cpp:1361] Got assigned task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 for framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:51.443006  1265 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I0316 14:28:51.443624  1265 slave.cpp:1480] Launching task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 for framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:51.443730  1265 resources.cpp:572] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I0316 14:28:51.444629  1265 paths.cpp:528] Trying to chown '/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy/slaves/c7653f60-33e9-4406-9f62-dc74c906bf83-S0/frameworks/c7653f60-33e9-4406-9f62-dc74c906bf83-0000/executors/864698ee-117b-4b95-b8d7-4c3ec6e0b917/runs/6e2770ca-32d3-47ad-b4fe-7d9f26489621' to user 'mesos'
I0316 14:28:51.449493  1265 slave.cpp:5367] Launching executor 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy/slaves/c7653f60-33e9-4406-9f62-dc74c906bf83-S0/frameworks/c7653f60-33e9-4406-9f62-dc74c906bf83-0000/executors/864698ee-117b-4b95-b8d7-4c3ec6e0b917/runs/6e2770ca-32d3-47ad-b4fe-7d9f26489621'
I0316 14:28:51.450256  1261 containerizer.cpp:666] Starting container '6e2770ca-32d3-47ad-b4fe-7d9f26489621' for executor '864698ee-117b-4b95-b8d7-4c3ec6e0b917' of framework 'c7653f60-33e9-4406-9f62-dc74c906bf83-0000'
I0316 14:28:51.450299  1265 slave.cpp:1698] Queuing task '864698ee-117b-4b95-b8d7-4c3ec6e0b917' for executor '864698ee-117b-4b95-b8d7-4c3ec6e0b917' of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:51.450428  1265 slave.cpp:749] Successfully attached file '/tmp/ContainerLoggerTest_LOGROTATE_RotateInSandbox_JHP0gy/slaves/c7653f60-33e9-4406-9f62-dc74c906bf83-S0/frameworks/c7653f60-33e9-4406-9f62-dc74c906bf83-0000/executors/864698ee-117b-4b95-b8d7-4c3ec6e0b917/runs/6e2770ca-32d3-47ad-b4fe-7d9f26489621'
I0316 14:28:51.459421  1268 launcher.cpp:147] Forked child with pid '1453' for container '6e2770ca-32d3-47ad-b4fe-7d9f26489621'
I0316 14:28:51.613296  1274 slave.cpp:2643] Got registration for executor '864698ee-117b-4b95-b8d7-4c3ec6e0b917' of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 from executor(1)@172.17.0.3:56062
I0316 14:28:51.615416  1271 slave.cpp:1863] Sending queued task '864698ee-117b-4b95-b8d7-4c3ec6e0b917' to executor '864698ee-117b-4b95-b8d7-4c3ec6e0b917' of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 at executor(1)@172.17.0.3:56062
I0316 14:28:51.622187  1272 slave.cpp:3002] Handling status update TASK_RUNNING (UUID: aee0de1c-8acd-46eb-8723-d26cd203228f) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 from executor(1)@172.17.0.3:56062
I0316 14:28:51.623610  1275 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: aee0de1c-8acd-46eb-8723-d26cd203228f) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:51.623646  1275 status_update_manager.cpp:497] Creating StatusUpdate stream for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:51.624053  1275 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: aee0de1c-8acd-46eb-8723-d26cd203228f) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 to the slave
I0316 14:28:51.624423  1274 slave.cpp:3400] Forwarding the update TASK_RUNNING (UUID: aee0de1c-8acd-46eb-8723-d26cd203228f) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 to master@172.17.0.3:45919
I0316 14:28:51.624621  1274 slave.cpp:3294] Status update manager successfully handled status update TASK_RUNNING (UUID: aee0de1c-8acd-46eb-8723-d26cd203228f) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:51.624677  1274 slave.cpp:3310] Sending acknowledgement for status update TASK_RUNNING (UUID: aee0de1c-8acd-46eb-8723-d26cd203228f) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 to executor(1)@172.17.0.3:56062
I0316 14:28:51.624836  1270 master.cpp:4927] Status update TASK_RUNNING (UUID: aee0de1c-8acd-46eb-8723-d26cd203228f) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 from slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0 at slave(12)@172.17.0.3:45919 (2cbb23302fe5)
I0316 14:28:51.624881  1270 master.cpp:4975] Forwarding status update TASK_RUNNING (UUID: aee0de1c-8acd-46eb-8723-d26cd203228f) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:51.625077  1270 master.cpp:6588] Updating the state of task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I0316 14:28:51.625355  1269 sched.cpp:981] Scheduler::statusUpdate took 141149ns
I0316 14:28:51.625671  1266 master.cpp:4082] Processing ACKNOWLEDGE call aee0de1c-8acd-46eb-8723-d26cd203228f for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 (default) at scheduler-96f85a94-b6a8-4363-bc3b-b8a233b90e79@172.17.0.3:45919 on slave c7653f60-33e9-4406-9f62-dc74c906bf83-S0
I0316 14:28:51.625977  1267 status_update_manager.cpp:392] Received status update acknowledgement (UUID: aee0de1c-8acd-46eb-8723-d26cd203228f) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:51.626369  1265 slave.cpp:2412] Status update manager successfully handled status update acknowledgement (UUID: aee0de1c-8acd-46eb-8723-d26cd203228f) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:52.340801  1266 hierarchical.cpp:1453] No resources available to allocate!
I0316 14:28:52.340884  1266 hierarchical.cpp:1548] No inverse offers to send out!
I0316 14:28:52.340922  1266 hierarchical.cpp:1130] Performed allocation for 1 slaves in 350313ns
I0316 14:28:53.342003  1263 hierarchical.cpp:1453] No resources available to allocate!
I0316 14:28:53.342077  1263 hierarchical.cpp:1548] No inverse offers to send out!
I0316 14:28:53.342110  1263 hierarchical.cpp:1130] Performed allocation for 1 slaves in 332715ns
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0316 14:28:53.619144  1451 process.cpp:986] libprocess is initialized on 172.17.0.3:40885 for 16 cpus
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0316 14:28:53.790701  1452 process.cpp:986] libprocess is initialized on 172.17.0.3:50144 for 16 cpus
I0316 14:28:53.939643  1268 slave.cpp:3002] Handling status update TASK_FINISHED (UUID: a873c6e2-442e-439e-a13f-54bb19df1881) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 from executor(1)@172.17.0.3:56062
I0316 14:28:53.940950  1267 slave.cpp:5677] Terminating task 864698ee-117b-4b95-b8d7-4c3ec6e0b917
I0316 14:28:53.942181  1275 status_update_manager.cpp:320] Received status update TASK_FINISHED (UUID: a873c6e2-442e-439e-a13f-54bb19df1881) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000
I0316 14:28:53.942358  1275 status_update_manager.cpp:374] Forwarding update TASK_FINISHED (UUID: a873c6e2-442e-439e-a13f-54bb19df1881) for task 864698ee-117b-4b95-b8d7-4c3ec6e0b917 of framework c7653f60-33e9-4406-9f62-dc74c906bf83-0000 to the slave
I031",1.0,0,0.0,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.0
Improvement,Enable actors to pass an authentication realm to libprocess,"To prepare for MESOS-4902, the Mesos master and agent need a way to pass the desired authentication realm to libprocess. Since some endpoints (like {{/profiler/*}}) get installed in libprocess, the master/agent should be able to specify during initialization what authentication realm the libprocess-level endpoints will be authenticated under.",2.0,0,0.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Bug,Implement reconnect funtionality in the scheduler library.,"Currently, there is no way for the schedulers to force a reconnection attempt with the master using the scheduler library {{src/scheduler/scheduler.cpp}}. It is specifically useful in scenarios where there is a one way network partition with the master. Due to this, the scheduler has not received any {{HEARTBEAT}} events from the master. In this case, the scheduler might want to force a reconnection attempt with the master instead of relying on the {{disconnected}} callback.",3.0,0,0.0,0.5701357466063348,0.0,0.0,0.0,0.0,0.0,0.6842105263157894,0.5642857142857143,0.3846153846153846,0.4181818181818182,0.4121212121212121,0.0
Improvement,Executor shutdown grace period should be configurable.,"Currently, executor shutdown grace period is specified by an agent flag, which is propagated to executors via the {{MESOS_EXECUTOR_SHUTDOWN_GRACE_PERIOD}} environment variable. There is no way to adjust this timeout for the needs of a particular executor.

To tackle this problem, we propose to introduce an optional {{shutdown_grace_period}} field in {{ExecutorInfo}}.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.717948717948718,0.8242424242424242,0.7212121212121212,0.0
Task,Enable HELP to include authentication status of endpoint.,As we enable authentication for more and more endpoints we should document which endpoints support authentication and which ones don't.,2.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12280701754385964,0.32857142857142857,0.11538461538461538,0.23636363636363636,0.23636363636363636,0.0
Task,Registrar HTTP Authentication.,"Now that the master (and agents in progress) provide http authentication the registrar should do the same. 

See http://mesos.apache.org/documentation/latest/endpoints/registrar/registry/",3.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.2,0.2631578947368421,0.36428571428571427,0.11538461538461538,0.23636363636363636,0.23636363636363636,0.0
Improvement,Remove all '.get().' calls on Option / Try variables in the resources abstraction.,"When possible, {{.get()}} calls should be replaced by {{->}} for {{Option}} / {{Try}} variables.  This ticket only proposes a blanket change for this in the resource abstraction files, not the code base as a whole.  This is in preparation for introducing the new GPU resource.  Without this change, I would need to use the old {{.get()}} calls.  Instead, I propose to fix the old code surrounding it so that consistency has me doing it the right way.  ",1.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Improvement,"The flag parser for `hashmap<string, string>` should live in stout, not mesos.",The title says it all.,1.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Improvement,Add a list parser for comma separated integers in flags.,Some flags require lists of integers to be passed in.  We should have an explicit parser for this instead of relying on ad hoc solutions.,2.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Improvement,"ProcessorManager delegate should be an Option<string>, not just a string.","Currently, the delegate field in the ProcessManager is just a string type. We check for 'existence' of a delegate by comparing (delegate != """"). Using an Option is the preferred method for things like this.",1.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Bug,Executor driver does not respect executor shutdown grace period.,"Executor shutdown grace period, configured on the agent, is
propagated to executors via the `MESOS_EXECUTOR_SHUTDOWN_GRACE_PERIOD`
environment variable. The executor driver must use this timeout to delay
the hard shutdown of the related executor.",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.717948717948718,0.8242424242424242,0.7212121212121212,0.0
Improvement,Deprecate the --docker_stop_timeout agent flag.,"Instead, a combination of {{executor_shutdown_grace_period}}
agent flag and optionally task kill policies should be used.",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.717948717948718,0.8242424242424242,0.7212121212121212,0.0
Bug,Allow multiple loads of module manifests,"The ModuleManager::load() is designed to be called exactly once during a process lifetime. This works well for Master/Agent environments. However, it can fail in Scheduler environments. For example, a single Scheduler binary might implement multiple scheduler drivers causing multiple calls to ModuleManager::load() leading to a failure.",3.0,0,0.0,0.3453996983408748,0.0,0.0,0.0,0.0,0.2,0.38596491228070173,0.35714285714285715,0.28205128205128205,0.29090909090909095,0.28484848484848485,0.0
Improvement,Support mesos containerizer force_pull_image option.,"Currently for unified containerizer, images that are already cached by metadata manager cannot be updated. User has to delete corresponding images in store if an update is need. We should support `force_pull_image` option for unified containerizer, to provide override option if existed.",3.0,0,0.5,0.3137254901960784,0.0,0.0,0.0,0.0,0.4,0.19298245614035087,0.14285714285714285,0.6794871794871795,0.5939393939393939,0.5939393939393939,0.0
Improvement,PersistentVolumeTests do not need to set up ACLs.,"The {{PersistentVolumeTest}} s have a custom helper for setting up ACLs in the {{master::Flags}}:
{code}
ACLs acls;
    hashset<string> roles;

    foreach (const FrameworkInfo& framework, frameworks) {
      mesos::ACL::RegisterFramework* acl = acls.add_register_frameworks();
      acl->mutable_principals()->add_values(framework.principal());
      acl->mutable_roles()->add_values(framework.role());

      roles.insert(framework.role());
    }

    flags.acls = acls;
    flags.roles = strings::join("","", roles);
{code}

This is no longer necessary with implicit roles.",1.0,0,0.5,0.03167420814479638,0.3333333333333333,0.5,0.14285714285714285,0.0,0.0,0.05263157894736842,0.049999999999999996,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.0
Task,Add GPUs as an explicit resource.,"We will add ""gpus"" as an explicitly recognized resource in Mesos, akin to cpus, memory, ports, and disk.  In the containerizer, we will verify that the number of GPU resources passed in via the --resources flag matches the list of GPUs passed in via the --nvidia_gpus flag.  In the future we will add autodiscovery so this matching is unnecessary.  However, we will always have to pass ""gpus"" as a resource to make any GPU available on the system (unlike for cpus and memory, where the default is probed).",3.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Task,Add flag to specify available Nvidia GPUs on an agent's command line.,"In the initial GPU support we will not do auto-discovery of GPUs on an agent.  As such, an operator will need to specify a flag on the command line, listing all of the GPUs available on the system.",3.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Task,Add Nvidia GPU isolator tests.,"We need to be able to run unit tests that verify GPU isolation, as well as run full blown tests that actually exercise the GPUs.

These tests should only build when the proper configure flags are set for enabling nvidia GPU support.",2.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Task,Add configure flags to build with Nvidia GPU support.,"The configure flags can be used to enable Nvidia GPU support, as well as specify the installation directories of the nvml header and library files if not already installed in standard include/library paths on the system.

They will also be used to conditionally build support for Nvidia GPUs into Mesos.",2.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Task,Add a script to install the Nvidia GDK on a host.,"This script can be used to install the Nvidia GDK for Cuda 7.5 on a
mesos development machine. The purpose of the Nvidia GDK is to provide
all the necessary header files (nvml.h) and library files
(libnvidia-ml.so) necessary to build mesos with Nvidia GPU support.

If the machine on which Mesos is being compiled doesn't have any GPUs,
then libnvidia-ml.so consists only of stubs, allowing Mesos to build
and run, but not actually do anything useful under the hood. This
enables us to build a GPU-enabled mesos on a development machine
without GPUs and then deploy it to a production machine with GPUs and
be reasonably sure it will work.",2.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Task,Make changes to executor v1 library around managing connections.,"While implementing pipelining changes for the scheduler library (MESOS-3570), we noticed a couple of small bugs that we would like to fix in the executor library:

- Don't pass {{Connection}} objects to {{defer}} callbacks as they can sometimes lead to deadlocks.
- Minor cleanups around not accepting {{SUBSCRIBE}} call if one is currently in progress.
- Create a random UUID (connectionId) before we initiate a connection to the agent, as in some scenarios, we can accept connection attempts from stale connections.",3.0,0,0.0,0.5701357466063348,0.0,0.0,0.0,0.0,0.0,0.6842105263157894,0.5642857142857143,0.3846153846153846,0.4181818181818182,0.4121212121212121,0.0
Documentation,Update CHANGELOG with net_cls isolator,Need to update the CHANGELOG for 0.28 release.,1.0,0.28.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2807017543859649,0.39285714285714285,0.1282051282051282,0.3090909090909091,0.3090909090909091,0.25688073394495414
Task,Add authentication to agent endpoints /state and /flags,"The {{/state}} and {{/flags}} endpoints are installed in {{src/slave/slave.cpp}}, and thus are straightforward to make authenticated. Other agent endpoints require a bit more consideration, and are tracked in MESOS-4902.

For more information on agent endpoints, see http://mesos.apache.org/documentation/latest/endpoints/
or search for `route(` in the source code:
{code}
$ grep -rn ""route("" src/ |grep -v master |grep -v tests |grep -v json
src/version/version.cpp:75:  route(""/"", VERSION_HELP(), &VersionProcess::version);
src/files/files.cpp:150:  route(""/browse"",
src/files/files.cpp:153:  route(""/read"",
src/files/files.cpp:156:  route(""/download"",
src/files/files.cpp:159:  route(""/debug"",
src/slave/slave.cpp:580:  route(""/api/v1/executor"",
src/slave/slave.cpp:595:  route(""/state"",
src/slave/slave.cpp:601:  route(""/flags"",
src/slave/slave.cpp:607:  route(""/health"",
src/slave/monitor.cpp:100:    route(""/statistics"",
$ grep -rn ""route("" 3rdparty/ |grep -v tests |grep -v README |grep -v examples |grep -v help |grep -v ""process..pp""
3rdparty/libprocess/include/process/profiler.hpp:34:    route(""/start"", START_HELP(), &Profiler::start);
3rdparty/libprocess/include/process/profiler.hpp:35:    route(""/stop"", STOP_HELP(), &Profiler::stop);
3rdparty/libprocess/include/process/system.hpp:70:    route(""/stats.json"", statsHelp(), &System::stats);
3rdparty/libprocess/include/process/logging.hpp:44:    route(""/toggle"", TOGGLE_HELP(), &This::toggle);
{code}",3.0,0,0.5,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.05128205128205128,0.1515151515151515,0.15757575757575756,0.0
Task,Add agent flags for HTTP authentication,"Flags should be added to the agent to:
1. Enable HTTP authentication ({{--authenticate_http}})
2. Specify credentials ({{--http_credentials}})
3. Specify HTTP authenticators ({{--authenticators}})",2.0,0,0.5,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.05128205128205128,0.1515151515151515,0.15757575757575756,0.0
Task,Agent Authn Research Spike,"Research the master authentication flags to see what changes will be necessary for agent http authentication.
Write up a 1-2 page summary/design doc.",2.0,0,0.5,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.05128205128205128,0.1515151515151515,0.15757575757575756,0.0
Task,Add authentication to master endpoints,"Before we can add authorization around operator endpoints, we need to add authentication support, so that unauthenticated requests are denied when --authenticate_http is enabled, and so that the principal is passed into `route()`.",2.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.12280701754385964,0.32857142857142857,0.05128205128205128,0.1515151515151515,0.15757575757575756,0.0
Task,Remove internal usage of deprecated ShutdownFramework ACL,{{ShutdownFramework}} acl was deprecated a couple of versions ago in favor of the {{TeardownFramework}} message. Its deprecation cycle came with 0.27. That means we should remove the message and its references in the code base.,2.0,0.28.0,0.0,0.004524886877828055,0.0,0.0,0.0,0.0,0.2,0.12280701754385964,0.24285714285714285,0.10256410256410256,0.11515151515151516,0.11515151515151516,0.25688073394495414
Improvement,Move placement new processes into the freezer cgroup into a parent hook.,"The Linux Launcher places new processes into the freezer cgroup.
This is currently done by a combination of childSetup function (blocking the new process until parent is done) and the parent (placing child process into the cgroup and then signaling child to continue).
ParentHooks support this behavior (blocking child until some work is done in the parent) in a much cleaner way. 
",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12280701754385964,0.32857142857142857,0.11538461538461538,0.23636363636363636,0.23636363636363636,0.0
Bug,Fix rmdir for windows,This is due to a bug in MESOS-4415 that landed for 0.27.0.,1.0,0,0.5,0.44494720965309204,0.0,0.0,0.0,0.0,0.0,0.14035087719298245,0.15,0.8846153846153846,0.9878787878787878,0.9878787878787878,0.0
Bug,CgroupsAnyHierarchyWithFreezerTest.ROOT_CGROUPS_DestroyTracedProcess is flaky,"Verbose logs: 
{code}
[ RUN      ] CgroupsAnyHierarchyWithFreezerTest.ROOT_CGROUPS_DestroyTracedProcess
I0302 00:43:14.127846 11755 cgroups.cpp:2427] Freezing cgroup /sys/fs/cgroup/freezer/mesos_test
I0302 00:43:14.267411 11758 cgroups.cpp:1409] Successfully froze cgroup /sys/fs/cgroup/freezer/mesos_test after 139.46496ms
I0302 00:43:14.409395 11751 cgroups.cpp:2445] Thawing cgroup /sys/fs/cgroup/freezer/mesos_test
I0302 00:43:14.551304 11751 cgroups.cpp:1438] Successfullly thawed cgroup /sys/fs/cgroup/freezer/mesos_test after 141.811968ms
../../src/tests/containerizer/cgroups_tests.cpp:949: Failure
Value of: ::waitpid(pid, &status, 0)
  Actual: 23809
Expected: -1
../../src/tests/containerizer/cgroups_tests.cpp:950: Failure
Value of: (*__errno_location ())
  Actual: 0
Expected: 10
[  FAILED  ] CgroupsAnyHierarchyWithFreezerTest.ROOT_CGROUPS_DestroyTracedProcess (1055 ms)
{code}",2.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.0
Task,Add 'file' fetcher plugin.,"Add support for ""file"" based URI fetcher. This could be useful for container image provisioning from local file system.",2.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3357142857142857,0.14102564102564102,0.2424242424242424,0.2424242424242424,0.0
Task,Remove `grace_period_seconds` field from Shutdown event v1 protobuf.,"There are two ways in which a shutdown of executor can be triggered:
1. If it receives an explicit `Shutdown` message from the agent.
2. If the recovery timeout period has elapsed, and the executor still hasn’t been able to (re-)connect with the agent.

Currently, the executor library relies on the field `grace_period_seconds` having a default value of 5 seconds to handle the second scenario. https://github.com/apache/mesos/blob/master/src/executor/executor.cpp#L608

The driver used to trigger the grace period via a constant defined in src/slave/constants.cpp. https://github.com/apache/mesos/blob/master/src/exec/exec.cpp#L92

The agent may want to force a shorter shutdown grace period (e.g. oversubscription eviction may have shorter deadline) in the future. For now, we can just read the value via an environment variable.",3.0,0,0.0,0.5701357466063348,0.0,0.0,0.0,0.0,0.0,0.6842105263157894,0.5642857142857143,0.3846153846153846,0.4181818181818182,0.4121212121212121,0.0
Bug,Master's slave reregister logic does not update version field,The master's logic for reregistering a slave does not update the version field if the slave re-registers with a new version.,1.0,0,0.5,0.03469079939668175,0.0,0.0,0.0,0.0,0.0,0.07017543859649122,0.04285714285714286,0.24358974358974358,0.22424242424242424,0.22424242424242424,0.0
Bug,"""filesystem/linux"" isolator does not unmount orphaned persistent volumes","A persistent volume can be orphaned when:
# A framework registers with checkpointing enabled.
# The framework starts a task + a persistent volume.
# The agent exits.  The task continues to run.
# Something wipes the agent's {{meta}} directory.  This removes the checkpointed framework info from the agent.
# The agent comes back and recovers.  The framework for the task is not found, so the task is considered orphaned now.

The agent currently does not unmount the persistent volume, saying (with {{GLOG_v=1}}) 
{code}
I0229 23:55:42.078940  5635 linux.cpp:711] Ignoring cleanup request for unknown container: a35189d3-85d5-4d02-b568-67f675b6dc97
{code}

Test implemented here: https://reviews.apache.org/r/44122/",2.0,"0.24.0,0.25.0,0.26.0,0.27.0",0.0,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.23394495412844035
Task,Implement port forwarding in `network/cni` isolator,"Most docker and appc images wish to expose ports that micro-services are listening on, to the outside world. When containers are running on bridged (or ptp) networking this can be achieved by installing port forwarding rules on the agent (using iptables). This can be done in the `network/cni` isolator. 

The reason we would like this functionality to be implemented in the `network/cni` isolator, and not a CNI plugin, is that the specifications currently do not support specifying port forwarding rules. Further, to install these rules the isolator needs two pieces of information, the exposed ports and the IP address associated with the container. Bother are available to the isolator.",2.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2807017543859649,0.39285714285714285,0.1282051282051282,0.3090909090909091,0.3090909090909091,0.0
Task,Add support for local image fetching in Appc provisioner.,Currently Appc image provisioner supports http(s) fetching. It would be valuable to add support for local file path(URI) based  fetching.,2.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3357142857142857,0.14102564102564102,0.2424242424242424,0.2424242424242424,0.0
Task,Introduce a port field in `ImageManifest` in order to set exposed ports for a container.,Networking isolators such as `network/cni` need to learn about ports that a container wishes to be exposed to the outside world. This can be achieved by adding a field to the `ImageManifest` protobuf and allowing the `ImageProvisioner` to set these fields to inform the isolator of the ports that the container wishes to be exposed. ,1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2807017543859649,0.39285714285714285,0.1282051282051282,0.3090909090909091,0.3090909090909091,0.0
Task,Need to set `EXPOSED` ports from docker images into `ContainerConfig`,"Most docker images have an `EXPOSE` command associated with them. This tells the container run-time the TCP ports that the micro-service ""wishes"" to expose to the outside world. 

With the `Unified containerizer` project since `MesosContainerizer` is going to natively support docker images it is imperative that the Mesos container run time have a mechanism to expose ports listed in a Docker image. The first step to achieve this is to extract this information from the `Docker` image and set in the `ContainerConfig` . The `ContainerConfig` can then be used to pass this information to any isolator (for e.g. `network/cni` isolator) that will install port forwarding rules to expose the desired ports.",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2807017543859649,0.39285714285714285,0.1282051282051282,0.3090909090909091,0.3090909090909091,0.0
Task,Add end to end testing for Appc images.,"Add tests that covers integration test of the Appc provisioner feature with mesos containerizer.
 ",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3357142857142857,0.14102564102564102,0.2424242424242424,0.2424242424242424,0.0
Task,Remove internal usage of deprecated *.json endpoints.,"We still use the deprecated *.json internally (UI, tests, documentation). ",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12280701754385964,0.32857142857142857,0.11538461538461538,0.23636363636363636,0.23636363636363636,0.0
Bug,IOTest.BufferedRead writes to the current directory,"libprocess's {{IOTest.BufferedRead}} writes to the current directory. This is bad for a number of reasons, e.g.,

* should the test fail data might be leaked to random locations,
* the test cannot be executed from a write-only directory, or
* executing the same test in parallel would race on the existence of the created file, and show bogus behavior.

The test should probably be executed from a temporary directory, e.g., via stout's {{TemporaryDirectoryTest}} fixture.",1.0,0,0.5,0.03167420814479638,0.3333333333333333,0.5,0.14285714285714285,0.0,0.0,0.05263157894736842,0.049999999999999996,1.0,0.8121212121212121,0.8121212121212121,0.0
Improvement,Updated `createFrameworkInfo` for hierarchical_allocator_tests.cpp.,The function of {{createFrameworkInfo}} in hierarchical_allocator_tests.cpp should be updated by enabling caller can set a framework capability to create a framework which can use revocable resources.,1.0,0,0.0,0.3137254901960784,0.0,0.0,0.0,0.0,0.4,0.19298245614035087,0.14285714285714285,0.10256410256410256,0.06666666666666667,0.06666666666666667,0.0
Improvement,Revert external linkage of symbols in master/constants.hpp,"src/master/constants.hpp contains:

{code}
// TODO(bmahler): It appears there may be a bug with gcc-4.1.2 in which the
// duration constants were not being initialized when having static linkage.
// This issue did not manifest in newer gcc's. Specifically, 4.2.1 was ok.
// So we've moved these to have external linkage but perhaps in the future
// we can revert this.
{code}

From commit 232a23b2a2e11f4e905b834aa2a11afe5bf6438a. We should investigate whether this is still necessary on supported compilers; it likely is not.",1.0,0,0.0,1.0,0.0,0.0,0.0,0.0,0.8,0.5438596491228069,0.39999999999999997,0.48717948717948717,0.4303030303030303,0.4303030303030303,0.0
Documentation,HTTP endpoint docs should use shorter paths,"My understanding is that the recommended path for the v1 scheduler API is {{/api/v1/scheduler}}, but the HTTP endpoint [docs|http://mesos.apache.org/documentation/latest/endpoints/] for this endpoint list the path as {{/master/api/v1/scheduler}}; the filename of the doc page is also in the {{master}} subdirectory.

Similarly, we document the master state endpoint as {{/master/state}}, whereas the preferred name is now just {{/state}}, and so on for most of the other endpoints. Unlike we the V1 API, we might want to consider backward compatibility and document both forms -- not sure. But certainly it seems like we should encourage people to use the shorter paths, not the longer ones.",2.0,0,0.5,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.48717948717948717,0.4303030303030303,0.4303030303030303,0.0
Improvement,TaskInfo/ExecutorInfo should include fine-grained ownership/namespacing,"We need a way to assign fine-grained ownership to tasks/executors so that multi-user frameworks can tell Mesos to associate the task with a user identity (rather than just the framework principal+role). Then, when an HTTP user requests to view the task's sandbox contents, or kill the task, or list all tasks, the authorizer can determine whether to allow/deny/filter the request based on finer-grained, user-level ownership.
Some systems may want TaskInfo.owner to represent a group rather than an individual user. That's fine as long as the framework sets the field to the group ID in such a way that a group-aware authorizer can interpret it.",2.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05128205128205128,0.1515151515151515,0.15757575757575756,0.0
Task,Document the network/cni isolator.,"We need to document this isolator in mesos-containerizer.md (e.g., how to configure it, what's the pre-requisite, etc.)",3.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.2807017543859649,0.39285714285714285,0.7948717948717948,1.0,1.0,0.0
Bug,MasterMaintenanceTest.InverseOffers is flaky,"[MESOS-4169] significantly sped up this test, but also surfaced some more flakiness.  This can be fixed in the same way as [MESOS-4059].

Verbose logs from ASF Centos7 build:
{code}
[ RUN      ] MasterMaintenanceTest.InverseOffers
I0224 22:35:53.714018  1948 leveldb.cpp:174] Opened db in 2.034387ms
I0224 22:35:53.714663  1948 leveldb.cpp:181] Compacted db in 608839ns
I0224 22:35:53.714709  1948 leveldb.cpp:196] Created db iterator in 19043ns
I0224 22:35:53.714844  1948 leveldb.cpp:202] Seeked to beginning of db in 2330ns
I0224 22:35:53.714956  1948 leveldb.cpp:271] Iterated through 0 keys in the db in 518ns
I0224 22:35:53.715092  1948 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0224 22:35:53.715646  1968 recover.cpp:447] Starting replica recovery
I0224 22:35:53.715915  1981 recover.cpp:473] Replica is in EMPTY status
I0224 22:35:53.717067  1972 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (4533)@172.17.0.1:36678
I0224 22:35:53.717445  1981 recover.cpp:193] Received a recover response from a replica in EMPTY status
I0224 22:35:53.717888  1978 recover.cpp:564] Updating replica status to STARTING
I0224 22:35:53.718585  1979 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 525061ns
I0224 22:35:53.718618  1979 replica.cpp:320] Persisted replica status to STARTING
I0224 22:35:53.718827  1982 recover.cpp:473] Replica is in STARTING status
I0224 22:35:53.719728  1969 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (4534)@172.17.0.1:36678
I0224 22:35:53.719974  1971 recover.cpp:193] Received a recover response from a replica in STARTING status
I0224 22:35:53.720369  1970 recover.cpp:564] Updating replica status to VOTING
I0224 22:35:53.720789  1982 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 322308ns
I0224 22:35:53.720823  1982 replica.cpp:320] Persisted replica status to VOTING
I0224 22:35:53.720968  1982 recover.cpp:578] Successfully joined the Paxos group
I0224 22:35:53.721101  1982 recover.cpp:462] Recover process terminated
I0224 22:35:53.721698  1982 master.cpp:376] Master aab18b61-7811-4c43-a672-d1a63818c880 (4db5fa128d2d) started on 172.17.0.1:36678
I0224 22:35:53.721719  1982 master.cpp:378] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""false"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/MjbcWP/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.28.0/_inst/share/mesos/webui"" --work_dir=""/tmp/MjbcWP/master"" --zk_session_timeout=""10secs""
I0224 22:35:53.722039  1982 master.cpp:425] Master allowing unauthenticated frameworks to register
I0224 22:35:53.722053  1982 master.cpp:428] Master only allowing authenticated slaves to register
I0224 22:35:53.722061  1982 credentials.hpp:35] Loading credentials for authentication from '/tmp/MjbcWP/credentials'
I0224 22:35:53.722394  1982 master.cpp:468] Using default 'crammd5' authenticator
I0224 22:35:53.722525  1982 master.cpp:537] Using default 'basic' HTTP authenticator
I0224 22:35:53.722661  1982 master.cpp:571] Authorization enabled
I0224 22:35:53.722813  1968 hierarchical.cpp:144] Initialized hierarchical allocator process
I0224 22:35:53.722846  1980 whitelist_watcher.cpp:77] No whitelist given
I0224 22:35:53.724957  1977 master.cpp:1712] The newly elected leader is master@172.17.0.1:36678 with id aab18b61-7811-4c43-a672-d1a63818c880
I0224 22:35:53.725000  1977 master.cpp:1725] Elected as the leading master!
I0224 22:35:53.725023  1977 master.cpp:1470] Recovering from registrar
I0224 22:35:53.725306  1967 registrar.cpp:307] Recovering registrar
I0224 22:35:53.725808  1977 log.cpp:659] Attempting to start the writer
I0224 22:35:53.727145  1973 replica.cpp:493] Replica received implicit promise request from (4536)@172.17.0.1:36678 with proposal 1
I0224 22:35:53.727728  1973 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 424560ns
I0224 22:35:53.727828  1973 replica.cpp:342] Persisted promised to 1
I0224 22:35:53.729080  1973 coordinator.cpp:238] Coordinator attempting to fill missing positions
I0224 22:35:53.731009  1979 replica.cpp:388] Replica received explicit promise request from (4537)@172.17.0.1:36678 for position 0 with proposal 2
I0224 22:35:53.731580  1979 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 478479ns
I0224 22:35:53.731613  1979 replica.cpp:712] Persisted action at 0
I0224 22:35:53.734354  1979 replica.cpp:537] Replica received write request for position 0 from (4538)@172.17.0.1:36678
I0224 22:35:53.734485  1979 leveldb.cpp:436] Reading position from leveldb took 60879ns
I0224 22:35:53.735877  1979 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 1.324061ms
I0224 22:35:53.735930  1979 replica.cpp:712] Persisted action at 0
I0224 22:35:53.737061  1970 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I0224 22:35:53.738881  1970 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.772814ms
I0224 22:35:53.738939  1970 replica.cpp:712] Persisted action at 0
I0224 22:35:53.738975  1970 replica.cpp:697] Replica learned NOP action at position 0
I0224 22:35:53.740136  1976 log.cpp:675] Writer started with ending position 0
I0224 22:35:53.741750  1976 leveldb.cpp:436] Reading position from leveldb took 74863ns
I0224 22:35:53.743479  1976 registrar.cpp:340] Successfully fetched the registry (0B) in 18.11968ms
I0224 22:35:53.743755  1976 registrar.cpp:439] Applied 1 operations in 56670ns; attempting to update the 'registry'
I0224 22:35:53.745604  1978 log.cpp:683] Attempting to append 170 bytes to the log
I0224 22:35:53.745905  1977 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I0224 22:35:53.746968  1981 replica.cpp:537] Replica received write request for position 1 from (4539)@172.17.0.1:36678
I0224 22:35:53.747480  1981 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 456947ns
I0224 22:35:53.747609  1981 replica.cpp:712] Persisted action at 1
I0224 22:35:53.750448  1981 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I0224 22:35:53.751158  1981 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 535163ns
I0224 22:35:53.751258  1981 replica.cpp:712] Persisted action at 1
I0224 22:35:53.751389  1981 replica.cpp:697] Replica learned APPEND action at position 1
I0224 22:35:53.753149  1979 registrar.cpp:484] Successfully updated the 'registry' in 9.228032ms
I0224 22:35:53.753324  1979 registrar.cpp:370] Successfully recovered registrar
I0224 22:35:53.753593  1979 log.cpp:702] Attempting to truncate the log to 1
I0224 22:35:53.753805  1979 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I0224 22:35:53.754055  1981 master.cpp:1522] Recovered 0 slaves from the Registry (131B) ; allowing 10mins for slaves to re-register
I0224 22:35:53.754349  1979 hierarchical.cpp:171] Skipping recovery of hierarchical allocator: nothing to recover
I0224 22:35:53.755764  1977 replica.cpp:537] Replica received write request for position 2 from (4540)@172.17.0.1:36678
I0224 22:35:53.756459  1977 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 488559ns
I0224 22:35:53.756561  1977 replica.cpp:712] Persisted action at 2
I0224 22:35:53.757932  1972 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I0224 22:35:53.758400  1972 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 343827ns
I0224 22:35:53.758539  1972 leveldb.cpp:399] Deleting ~1 keys from leveldb took 34231ns
I0224 22:35:53.758658  1972 replica.cpp:712] Persisted action at 2
I0224 22:35:53.758782  1972 replica.cpp:697] Replica learned TRUNCATE action at position 2
I0224 22:35:53.778059  1978 slave.cpp:193] Slave started on 115)@172.17.0.1:36678
I0224 22:35:53.778105  1978 slave.cpp:194] Flags at startup: --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname=""maintenance-host"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.28.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF""
I0224 22:35:53.778609  1978 credentials.hpp:83] Loading credential for authentication from '/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/credential'
I0224 22:35:53.779175  1978 slave.cpp:324] Slave using credential for: test-principal
I0224 22:35:53.779520  1978 resources.cpp:576] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0224 22:35:53.780192  1978 slave.cpp:464] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0224 22:35:53.780362  1978 slave.cpp:472] Slave attributes: [  ]
I0224 22:35:53.780483  1978 slave.cpp:477] Slave hostname: maintenance-host
I0224 22:35:53.782126  1967 state.cpp:58] Recovering state from '/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/meta'
I0224 22:35:53.782892  1969 status_update_manager.cpp:200] Recovering status update manager
I0224 22:35:53.783242  1969 slave.cpp:4565] Finished recovery
I0224 22:35:53.784001  1969 slave.cpp:4737] Querying resource estimator for oversubscribable resources
I0224 22:35:53.784678  1969 slave.cpp:796] New master detected at master@172.17.0.1:36678
I0224 22:35:53.784874  1967 status_update_manager.cpp:174] Pausing sending status updates
I0224 22:35:53.784808  1969 slave.cpp:859] Authenticating with master master@172.17.0.1:36678
I0224 22:35:53.784945  1969 slave.cpp:864] Using default CRAM-MD5 authenticatee
I0224 22:35:53.785181  1969 slave.cpp:832] Detecting new master
I0224 22:35:53.785326  1969 slave.cpp:4751] Received oversubscribable resources  from the resource estimator
I0224 22:35:53.785557  1969 authenticatee.cpp:121] Creating new client SASL connection
I0224 22:35:53.786227  1969 master.cpp:5526] Authenticating slave(115)@172.17.0.1:36678
I0224 22:35:53.786492  1969 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(298)@172.17.0.1:36678
I0224 22:35:53.786962  1969 authenticator.cpp:98] Creating new server SASL connection
I0224 22:35:53.787274  1969 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0224 22:35:53.787308  1969 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0224 22:35:53.787400  1969 authenticator.cpp:203] Received SASL authentication start
I0224 22:35:53.787470  1969 authenticator.cpp:325] Authentication requires more steps
I0224 22:35:53.787884  1972 authenticatee.cpp:258] Received SASL authentication step
I0224 22:35:53.787992  1972 authenticator.cpp:231] Received SASL authentication step
I0224 22:35:53.788027  1972 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '4db5fa128d2d' server FQDN: '4db5fa128d2d' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0224 22:35:53.788040  1972 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0224 22:35:53.788090  1972 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0224 22:35:53.788122  1972 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '4db5fa128d2d' server FQDN: '4db5fa128d2d' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0224 22:35:53.788136  1972 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0224 22:35:53.788146  1972 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0224 22:35:53.788164  1972 authenticator.cpp:317] Authentication success
I0224 22:35:53.788331  1972 authenticatee.cpp:298] Authentication success
I0224 22:35:53.788439  1972 master.cpp:5556] Successfully authenticated principal 'test-principal' at slave(115)@172.17.0.1:36678
I0224 22:35:53.788529  1972 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(298)@172.17.0.1:36678
I0224 22:35:53.788988  1972 slave.cpp:927] Successfully authenticated with master master@172.17.0.1:36678
I0224 22:35:53.789139  1972 slave.cpp:1321] Will retry registration in 1.535786ms if necessary
I0224 22:35:53.789515  1972 master.cpp:4240] Registering slave at slave(115)@172.17.0.1:36678 (maintenance-host) with id aab18b61-7811-4c43-a672-d1a63818c880-S0
I0224 22:35:53.790577  1972 registrar.cpp:439] Applied 1 operations in 78745ns; attempting to update the 'registry'
I0224 22:35:53.791128  1971 process.cpp:3141] Handling HTTP event for process 'master' with path: '/master/maintenance/schedule'
I0224 22:35:53.791877  1971 http.cpp:501] HTTP POST for /master/maintenance/schedule from 172.17.0.1:45095
I0224 22:35:53.793313  1972 log.cpp:683] Attempting to append 343 bytes to the log
I0224 22:35:53.793586  1972 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I0224 22:35:53.794533  1971 replica.cpp:537] Replica received write request for position 3 from (4547)@172.17.0.1:36678
I0224 22:35:53.794862  1971 leveldb.cpp:341] Persisting action (362 bytes) to leveldb took 283614ns
I0224 22:35:53.794893  1971 replica.cpp:712] Persisted action at 3
I0224 22:35:53.796646  1979 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I0224 22:35:53.797102  1972 slave.cpp:1321] Will retry registration in 17.198963ms if necessary
I0224 22:35:53.797186  1979 leveldb.cpp:341] Persisting action (364 bytes) to leveldb took 498502ns
I0224 22:35:53.797230  1979 replica.cpp:712] Persisted action at 3
I0224 22:35:53.797260  1979 replica.cpp:697] Replica learned APPEND action at position 3
I0224 22:35:53.797417  1972 master.cpp:4228] Ignoring register slave message from slave(115)@172.17.0.1:36678 (maintenance-host) as admission is already in progress
I0224 22:35:53.799119  1978 registrar.cpp:484] Successfully updated the 'registry' in 8.45824ms
I0224 22:35:53.799613  1978 registrar.cpp:439] Applied 1 operations in 176193ns; attempting to update the 'registry'
I0224 22:35:53.800472  1972 master.cpp:4308] Registered slave aab18b61-7811-4c43-a672-d1a63818c880-S0 at slave(115)@172.17.0.1:36678 (maintenance-host) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0224 22:35:53.800623  1978 log.cpp:702] Attempting to truncate the log to 3
I0224 22:35:53.801255  1969 hierarchical.cpp:473] Added slave aab18b61-7811-4c43-a672-d1a63818c880-S0 (maintenance-host) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
I0224 22:35:53.801301  1978 slave.cpp:971] Registered with master master@172.17.0.1:36678; given slave ID aab18b61-7811-4c43-a672-d1a63818c880-S0
I0224 22:35:53.801331  1978 fetcher.cpp:81] Clearing fetcher cache
I0224 22:35:53.801431  1969 hierarchical.cpp:1434] No resources available to allocate!
I0224 22:35:53.801466  1969 hierarchical.cpp:1147] Performed allocation for slave aab18b61-7811-4c43-a672-d1a63818c880-S0 in 162751ns
I0224 22:35:53.801532  1969 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I0224 22:35:53.801867  1978 slave.cpp:994] Checkpointing SlaveInfo to '/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/meta/slaves/aab18b61-7811-4c43-a672-d1a63818c880-S0/slave.info'
I0224 22:35:53.801877  1969 status_update_manager.cpp:181] Resuming sending status updates
I0224 22:35:53.802898  1977 replica.cpp:537] Replica received write request for position 4 from (4548)@172.17.0.1:36678
I0224 22:35:53.803252  1978 slave.cpp:1030] Forwarding total oversubscribed resources 
I0224 22:35:53.803640  1970 master.cpp:4649] Received update of slave aab18b61-7811-4c43-a672-d1a63818c880-S0 at slave(115)@172.17.0.1:36678 (maintenance-host) with total oversubscribed resources 
I0224 22:35:53.803858  1977 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 912626ns
I0224 22:35:53.803889  1977 replica.cpp:712] Persisted action at 4
I0224 22:35:53.804144  1978 slave.cpp:3482] Received ping from slave-observer(117)@172.17.0.1:36678
I0224 22:35:53.804535  1971 hierarchical.cpp:531] Slave aab18b61-7811-4c43-a672-d1a63818c880-S0 (maintenance-host) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: )
I0224 22:35:53.804684  1971 hierarchical.cpp:1434] No resources available to allocate!
I0224 22:35:53.804714  1971 hierarchical.cpp:1147] Performed allocation for slave aab18b61-7811-4c43-a672-d1a63818c880-S0 in 131453ns
I0224 22:35:53.805541  1967 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I0224 22:35:53.805941  1967 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 366444ns
I0224 22:35:53.806015  1967 leveldb.cpp:399] Deleting ~2 keys from leveldb took 42808ns
I0224 22:35:53.806041  1967 replica.cpp:712] Persisted action at 4
I0224 22:35:53.806066  1967 replica.cpp:697] Replica learned TRUNCATE action at position 4
I0224 22:35:53.807355  1978 log.cpp:683] Attempting to append 465 bytes to the log
I0224 22:35:53.807551  1978 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 5
I0224 22:35:53.809638  1979 replica.cpp:537] Replica received write request for position 5 from (4549)@172.17.0.1:36678
I0224 22:35:53.810858  1979 leveldb.cpp:341] Persisting action (484 bytes) to leveldb took 1.167663ms
I0224 22:35:53.810904  1979 replica.cpp:712] Persisted action at 5
I0224 22:35:53.811997  1979 replica.cpp:691] Replica received learned notice for position 5 from @0.0.0.0:0
I0224 22:35:53.812348  1979 leveldb.cpp:341] Persisting action (486 bytes) to leveldb took 318928ns
I0224 22:35:53.812376  1979 replica.cpp:712] Persisted action at 5
I0224 22:35:53.812397  1979 replica.cpp:697] Replica learned APPEND action at position 5
I0224 22:35:53.815132  1973 registrar.cpp:484] Successfully updated the 'registry' in 15.437312ms
I0224 22:35:53.815491  1976 log.cpp:702] Attempting to truncate the log to 5
I0224 22:35:53.815610  1973 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 6
I0224 22:35:53.815661  1968 master.cpp:4705] Updating unavailability of slave aab18b61-7811-4c43-a672-d1a63818c880-S0 at slave(115)@172.17.0.1:36678 (maintenance-host), starting at 2410.99235909694weeks
I0224 22:35:53.815845  1968 master.cpp:4705] Updating unavailability of slave aab18b61-7811-4c43-a672-d1a63818c880-S0 at slave(115)@172.17.0.1:36678 (maintenance-host), starting at 2410.99235909694weeks
I0224 22:35:53.816069  1975 hierarchical.cpp:1434] No resources available to allocate!
I0224 22:35:53.816103  1975 hierarchical.cpp:1147] Performed allocation for slave aab18b61-7811-4c43-a672-d1a63818c880-S0 in 175822ns
I0224 22:35:53.816272  1975 hierarchical.cpp:1434] No resources available to allocate!
I0224 22:35:53.816303  1975 hierarchical.cpp:1147] Performed allocation for slave aab18b61-7811-4c43-a672-d1a63818c880-S0 in 110913ns
I0224 22:35:53.817291  1972 replica.cpp:537] Replica received write request for position 6 from (4550)@172.17.0.1:36678
I0224 22:35:53.817908  1972 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 576032ns
I0224 22:35:53.817932  1972 replica.cpp:712] Persisted action at 6
I0224 22:35:53.818686  1980 replica.cpp:691] Replica received learned notice for position 6 from @0.0.0.0:0
I0224 22:35:53.819021  1980 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 305298ns
I0224 22:35:53.819095  1980 leveldb.cpp:399] Deleting ~2 keys from leveldb took 44332ns
I0224 22:35:53.819120  1980 replica.cpp:712] Persisted action at 6
I0224 22:35:53.819162  1980 replica.cpp:697] Replica learned TRUNCATE action at position 6
I0224 22:35:53.820662  1967 process.cpp:3141] Handling HTTP event for process 'master' with path: '/master/maintenance/status'
I0224 22:35:53.821190  1976 http.cpp:501] HTTP GET for /master/maintenance/status from 172.17.0.1:45096
I0224 22:35:53.823709  1948 scheduler.cpp:154] Version: 0.28.0
I0224 22:35:53.824424  1972 scheduler.cpp:236] New master detected at master@172.17.0.1:36678
I0224 22:35:53.825402  1982 scheduler.cpp:298] Sending SUBSCRIBE call to master@172.17.0.1:36678
I0224 22:35:53.827201  1978 process.cpp:3141] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler'
I0224 22:35:53.827636  1978 http.cpp:501] HTTP POST for /master/api/v1/scheduler from 172.17.0.1:45097
I0224 22:35:53.827922  1978 master.cpp:1974] Received subscription request for HTTP framework 'default'
I0224 22:35:53.827991  1978 master.cpp:1751] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0224 22:35:53.828418  1982 master.cpp:2065] Subscribing framework 'default' with checkpointing disabled and capabilities [  ]
I0224 22:35:53.828943  1968 hierarchical.cpp:265] Added framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.829124  1982 master.hpp:1657] Sending heartbeat to aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.829987  1968 hierarchical.cpp:1127] Performed allocation for 1 slaves in 1.011356ms
I0224 22:35:53.830204  1982 master.cpp:5355] Sending 1 offers to framework aab18b61-7811-4c43-a672-d1a63818c880-0000 (default)
I0224 22:35:53.830801  1982 master.cpp:5445] Sending 1 inverse offers to framework aab18b61-7811-4c43-a672-d1a63818c880-0000 (default)
I0224 22:35:53.831132  1969 scheduler.cpp:457] Enqueuing event SUBSCRIBED received from master@172.17.0.1:36678
I0224 22:35:53.832396  1968 scheduler.cpp:457] Enqueuing event HEARTBEAT received from master@172.17.0.1:36678
I0224 22:35:53.833050  1976 master_maintenance_tests.cpp:177] Ignoring HEARTBEAT event
I0224 22:35:53.833256  1979 scheduler.cpp:457] Enqueuing event OFFERS received from master@172.17.0.1:36678
I0224 22:35:53.833775  1979 scheduler.cpp:457] Enqueuing event OFFERS received from master@172.17.0.1:36678
I0224 22:35:53.835662  1980 scheduler.cpp:298] Sending ACCEPT call to master@172.17.0.1:36678
I0224 22:35:53.837591  1967 process.cpp:3141] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler'
I0224 22:35:53.838021  1967 http.cpp:501] HTTP POST for /master/api/v1/scheduler from 172.17.0.1:45098
I0224 22:35:53.838851  1967 master.cpp:3138] Processing ACCEPT call for offers: [ aab18b61-7811-4c43-a672-d1a63818c880-O0 ] on slave aab18b61-7811-4c43-a672-d1a63818c880-S0 at slave(115)@172.17.0.1:36678 (maintenance-host) for framework aab18b61-7811-4c43-a672-d1a63818c880-0000 (default)
I0224 22:35:53.838946  1967 master.cpp:2825] Authorizing framework principal 'test-principal' to launch task 90bcae0c-9d40-40b7-9537-dae7e83479f6 as user 'mesos'
W0224 22:35:53.841048  1967 validation.cpp:404] Executor default for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases.
W0224 22:35:53.841101  1967 validation.cpp:416] Executor default for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases.
I0224 22:35:53.841624  1967 master.hpp:176] Adding task 90bcae0c-9d40-40b7-9537-dae7e83479f6 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave aab18b61-7811-4c43-a672-d1a63818c880-S0 (maintenance-host)
I0224 22:35:53.842157  1967 master.cpp:3623] Launching task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 (default) with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave aab18b61-7811-4c43-a672-d1a63818c880-S0 at slave(115)@172.17.0.1:36678 (maintenance-host)
I0224 22:35:53.842571  1980 slave.cpp:1361] Got assigned task 90bcae0c-9d40-40b7-9537-dae7e83479f6 for framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.843122  1980 slave.cpp:1480] Launching task 90bcae0c-9d40-40b7-9537-dae7e83479f6 for framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.843718  1980 paths.cpp:474] Trying to chown '/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/slaves/aab18b61-7811-4c43-a672-d1a63818c880-S0/frameworks/aab18b61-7811-4c43-a672-d1a63818c880-0000/executors/default/runs/a5a1e49d-20a8-4796-8ec0-5a1595e76159' to user 'mesos'
I0224 22:35:53.852052  1980 slave.cpp:5367] Launching executor default of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 with resources  in work directory '/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/slaves/aab18b61-7811-4c43-a672-d1a63818c880-S0/frameworks/aab18b61-7811-4c43-a672-d1a63818c880-0000/executors/default/runs/a5a1e49d-20a8-4796-8ec0-5a1595e76159'
I0224 22:35:53.854452  1980 exec.cpp:143] Version: 0.28.0
I0224 22:35:53.854812  1967 exec.cpp:193] Executor started at: executor(47)@172.17.0.1:36678 with pid 1948
I0224 22:35:53.855108  1980 slave.cpp:1698] Queuing task '90bcae0c-9d40-40b7-9537-dae7e83479f6' for executor 'default' of framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.855264  1980 slave.cpp:749] Successfully attached file '/tmp/MasterMaintenanceTest_InverseOffers_ywqvFF/slaves/aab18b61-7811-4c43-a672-d1a63818c880-S0/frameworks/aab18b61-7811-4c43-a672-d1a63818c880-0000/executors/default/runs/a5a1e49d-20a8-4796-8ec0-5a1595e76159'
I0224 22:35:53.855362  1980 slave.cpp:2643] Got registration for executor 'default' of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 from executor(47)@172.17.0.1:36678
I0224 22:35:53.855785  1974 exec.cpp:217] Executor registered on slave aab18b61-7811-4c43-a672-d1a63818c880-S0
I0224 22:35:53.855857  1974 exec.cpp:229] Executor::registered took 42512ns
I0224 22:35:53.856391  1980 slave.cpp:1863] Sending queued task '90bcae0c-9d40-40b7-9537-dae7e83479f6' to executor 'default' of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 at executor(47)@172.17.0.1:36678
I0224 22:35:53.856720  1974 exec.cpp:304] Executor asked to run task '90bcae0c-9d40-40b7-9537-dae7e83479f6'
I0224 22:35:53.856812  1974 exec.cpp:313] Executor::launchTask took 65703ns
I0224 22:35:53.856922  1974 exec.cpp:526] Executor sending status update TASK_RUNNING (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.857378  1980 slave.cpp:3002] Handling status update TASK_RUNNING (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 from executor(47)@172.17.0.1:36678
I0224 22:35:53.858175  1980 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.858222  1980 status_update_manager.cpp:497] Creating StatusUpdate stream for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.858687  1980 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 to the slave
I0224 22:35:53.859210  1980 slave.cpp:3400] Forwarding the update TASK_RUNNING (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 to master@172.17.0.1:36678
I0224 22:35:53.859390  1980 slave.cpp:3294] Status update manager successfully handled status update TASK_RUNNING (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.859436  1980 slave.cpp:3310] Sending acknowledgement for status update TASK_RUNNING (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 to executor(47)@172.17.0.1:36678
I0224 22:35:53.859663  1980 exec.cpp:350] Executor received status update acknowledgement 249b169a-6b5f-4776-95c8-c897ba6b3f0b for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.859657  1967 master.cpp:4794] Status update TASK_RUNNING (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 from slave aab18b61-7811-4c43-a672-d1a63818c880-S0 at slave(115)@172.17.0.1:36678 (maintenance-host)
I0224 22:35:53.859851  1967 master.cpp:4842] Forwarding status update TASK_RUNNING (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.860587  1967 master.cpp:6450] Updating the state of task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I0224 22:35:53.862711  1967 scheduler.cpp:457] Enqueuing event UPDATE received from master@172.17.0.1:36678
I0224 22:35:53.866711  1976 scheduler.cpp:298] Sending ACKNOWLEDGE call to master@172.17.0.1:36678
I0224 22:35:53.870667  1972 process.cpp:3141] Handling HTTP event for process 'master' with path: '/master/api/v1/scheduler'
I0224 22:35:53.871269  1972 http.cpp:501] HTTP POST for /master/api/v1/scheduler from 172.17.0.1:45099
I0224 22:35:53.871459  1972 master.cpp:3952] Processing ACKNOWLEDGE call 249b169a-6b5f-4776-95c8-c897ba6b3f0b for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000 (default) on slave aab18b61-7811-4c43-a672-d1a63818c880-S0
I0224 22:35:53.872184  1972 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.872537  1972 slave.cpp:2412] Status update manager successfully handled status update acknowledgement (UUID: 249b169a-6b5f-4776-95c8-c897ba6b3f0b) for task 90bcae0c-9d40-40b7-9537-dae7e83479f6 of framework aab18b61-7811-4c43-a672-d1a63818c880-0000
I0224 22:35:53.874407  1975 scheduler.cpp:298] Sending DECLINE call to master@172.17.0.1:36678
I0224 22:35:53.877537  1979 hierarchical.cpp:1434] No resources available to allocate!
I0224 22:35:53.877795  1979 hierarchical.cpp:1127] Performed allocation for 1 slaves in 482441ns
I0224 22:35:53.878082 ",1.0,0.28.0,0.0,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.25688073394495414
Bug,"The ""executors"" field is exposed under a backwards incompatible schema.","In 0.26.0, the master's {{/state}} endpoint generated the following:

{code}
{
  /* ... */
  ""frameworks"": [
    {
      /* ... */
      ""executors"": [
        {
          ""command"": {
            ""argv"": [],
            ""uris"": [],
            ""value"": ""/Users/mpark/Projects/mesos/build/opt/src/long-lived-executor""
          },
          ""executor_id"": ""default"",
          ""framework_id"": ""0ea528a9-64ba-417f-98ea-9c4b8d418db6-0000"",
          ""name"": ""Long Lived Executor (C++)"",
          ""resources"": {
            ""cpus"": 0,
            ""disk"": 0,
            ""mem"": 0
          },
          ""slave_id"": ""8a513678-03a1-4cb5-9279-c3c0c591f1d8-S0""
        }
      ],
      /* ... */
    }
  ]
  /* ... */
}
{code}

In 0.27.1, the {{ExecutorInfo}} is mistakenly exposed in the raw protobuf schema:

{code}
{
  /* ... */
  ""frameworks"": [
    {
      /* ... */
      ""executors"": [
        {
          ""command"": {
            ""shell"": true,
            ""value"": ""/Users/mpark/Projects/mesos/build/opt/src/long-lived-executor""
          },
          ""executor_id"": {
            ""value"": ""default""
          },
          ""framework_id"": {
            ""value"": ""368a5a49-480b-41f6-a13b-24a69c92a72e-0000""
          },
          ""name"": ""Long Lived Executor (C++)"",
          ""slave_id"": ""8a513678-03a1-4cb5-9279-c3c0c591f1d8-S0"",
          ""source"": ""cpp_long_lived_framework""
        }
      ],
      /* ... */
    }
  ]
  /* ... */
}
{code}

This is a backwards incompatible API change.",2.0,0,0.0,0.6726998491704375,0.0,0.0,0.0,0.1,0.6000000000000001,0.2807017543859649,0.29285714285714287,0.21794871794871795,0.26666666666666666,0.26666666666666666,0.0
Documentation,Document: Mesos Executor expects all SSL_* environment variables to be set,"I was trying to run Docker containers in a fully SSL-ized Mesos cluster but ran into problems because the executor was failing with a ""Failed to shutdown socket with fd 10: Transport endpoint is not connected"".

My understanding of why this is happening is because the executor was trying to report its status to Mesos slave over HTTPS, but doesnt have the appropriate certs/env setup inside the executor.

(Thanks to mslackbot/joseph for helping me figure this out on #mesos)

It turns out, the executor expects all SSL_* variables to be set inside `CommandInfo.environment` which gets picked up by the executor to successfully reports its status to the slave.

This part of __executor needing all the SSL_* variables to be set in its environment__ is missing in the Mesos SSL transitioning guide. I request you to please add this vital information to the doc.",2.0,0.26.0,0.5,0.0,0.0,0.0,0.0,0.0,0.2,0.2631578947368421,0.36428571428571427,0.0,0.0,0.0,0.2385321100917431
Task,Add Appc image fetcher tests.,Mesos now has support for fetching Appc images. Add tests that verifies the new component.,3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3357142857142857,0.14102564102564102,0.2424242424242424,0.2424242424242424,0.0
Bug,DockerContainerizerTest.ROOT_DOCKER_LaunchWithPersistentVolumes fails on CentOS 6,"This test passes consistently on other OS's, but fails consistently on CentOS 6.

Verbose logs from test failure:
{code}
[ RUN      ] DockerContainerizerTest.ROOT_DOCKER_LaunchWithPersistentVolumes
I0222 18:16:12.327957 26681 leveldb.cpp:174] Opened db in 7.466102ms
I0222 18:16:12.330528 26681 leveldb.cpp:181] Compacted db in 2.540139ms
I0222 18:16:12.330580 26681 leveldb.cpp:196] Created db iterator in 16908ns
I0222 18:16:12.330592 26681 leveldb.cpp:202] Seeked to beginning of db in 1403ns
I0222 18:16:12.330600 26681 leveldb.cpp:271] Iterated through 0 keys in the db in 315ns
I0222 18:16:12.330634 26681 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0222 18:16:12.331082 26698 recover.cpp:447] Starting replica recovery
I0222 18:16:12.331289 26698 recover.cpp:473] Replica is in EMPTY status
I0222 18:16:12.332162 26703 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (13761)@172.30.2.148:35274
I0222 18:16:12.332701 26701 recover.cpp:193] Received a recover response from a replica in EMPTY status
I0222 18:16:12.333230 26699 recover.cpp:564] Updating replica status to STARTING
I0222 18:16:12.334102 26698 master.cpp:376] Master 652149b4-3932-4d8b-ba6f-8c9d9045be70 (ip-172-30-2-148.mesosphere.io) started on 172.30.2.148:35274
I0222 18:16:12.334116 26698 master.cpp:378] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/QEhLBS/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/QEhLBS/master"" --zk_session_timeout=""10secs""
I0222 18:16:12.334354 26698 master.cpp:423] Master only allowing authenticated frameworks to register
I0222 18:16:12.334363 26698 master.cpp:428] Master only allowing authenticated slaves to register
I0222 18:16:12.334369 26698 credentials.hpp:35] Loading credentials for authentication from '/tmp/QEhLBS/credentials'
I0222 18:16:12.335366 26698 master.cpp:468] Using default 'crammd5' authenticator
I0222 18:16:12.335492 26698 master.cpp:537] Using default 'basic' HTTP authenticator
I0222 18:16:12.335623 26698 master.cpp:571] Authorization enabled
I0222 18:16:12.335752 26703 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 2.314693ms
I0222 18:16:12.335769 26700 whitelist_watcher.cpp:77] No whitelist given
I0222 18:16:12.335778 26703 replica.cpp:320] Persisted replica status to STARTING
I0222 18:16:12.335821 26697 hierarchical.cpp:144] Initialized hierarchical allocator process
I0222 18:16:12.335965 26701 recover.cpp:473] Replica is in STARTING status
I0222 18:16:12.336771 26703 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (13763)@172.30.2.148:35274
I0222 18:16:12.337191 26696 recover.cpp:193] Received a recover response from a replica in STARTING status
I0222 18:16:12.337635 26700 recover.cpp:564] Updating replica status to VOTING
I0222 18:16:12.337671 26703 master.cpp:1712] The newly elected leader is master@172.30.2.148:35274 with id 652149b4-3932-4d8b-ba6f-8c9d9045be70
I0222 18:16:12.337698 26703 master.cpp:1725] Elected as the leading master!
I0222 18:16:12.337713 26703 master.cpp:1470] Recovering from registrar
I0222 18:16:12.337828 26696 registrar.cpp:307] Recovering registrar
I0222 18:16:12.339972 26702 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 2.06039ms
I0222 18:16:12.339994 26702 replica.cpp:320] Persisted replica status to VOTING
I0222 18:16:12.340082 26700 recover.cpp:578] Successfully joined the Paxos group
I0222 18:16:12.340267 26700 recover.cpp:462] Recover process terminated
I0222 18:16:12.340591 26699 log.cpp:659] Attempting to start the writer
I0222 18:16:12.341594 26698 replica.cpp:493] Replica received implicit promise request from (13764)@172.30.2.148:35274 with proposal 1
I0222 18:16:12.343598 26698 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.97941ms
I0222 18:16:12.343619 26698 replica.cpp:342] Persisted promised to 1
I0222 18:16:12.344182 26698 coordinator.cpp:238] Coordinator attempting to fill missing positions
I0222 18:16:12.345285 26702 replica.cpp:388] Replica received explicit promise request from (13765)@172.30.2.148:35274 for position 0 with proposal 2
I0222 18:16:12.347275 26702 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 1.960198ms
I0222 18:16:12.347296 26702 replica.cpp:712] Persisted action at 0
I0222 18:16:12.348201 26703 replica.cpp:537] Replica received write request for position 0 from (13766)@172.30.2.148:35274
I0222 18:16:12.348247 26703 leveldb.cpp:436] Reading position from leveldb took 21399ns
I0222 18:16:12.350667 26703 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 2.39166ms
I0222 18:16:12.350690 26703 replica.cpp:712] Persisted action at 0
I0222 18:16:12.351191 26696 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I0222 18:16:12.353152 26696 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.935798ms
I0222 18:16:12.353173 26696 replica.cpp:712] Persisted action at 0
I0222 18:16:12.353188 26696 replica.cpp:697] Replica learned NOP action at position 0
I0222 18:16:12.353639 26696 log.cpp:675] Writer started with ending position 0
I0222 18:16:12.354508 26697 leveldb.cpp:436] Reading position from leveldb took 25625ns
I0222 18:16:12.355274 26696 registrar.cpp:340] Successfully fetched the registry (0B) in 17.406976ms
I0222 18:16:12.355357 26696 registrar.cpp:439] Applied 1 operations in 20977ns; attempting to update the 'registry'
I0222 18:16:12.355929 26697 log.cpp:683] Attempting to append 210 bytes to the log
I0222 18:16:12.356032 26703 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I0222 18:16:12.356657 26698 replica.cpp:537] Replica received write request for position 1 from (13767)@172.30.2.148:35274
I0222 18:16:12.358566 26698 leveldb.cpp:341] Persisting action (229 bytes) to leveldb took 1.881945ms
I0222 18:16:12.358588 26698 replica.cpp:712] Persisted action at 1
I0222 18:16:12.359081 26697 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I0222 18:16:12.361002 26697 leveldb.cpp:341] Persisting action (231 bytes) to leveldb took 1.894331ms
I0222 18:16:12.361023 26697 replica.cpp:712] Persisted action at 1
I0222 18:16:12.361038 26697 replica.cpp:697] Replica learned APPEND action at position 1
I0222 18:16:12.361883 26697 registrar.cpp:484] Successfully updated the 'registry' in 6.482944ms
I0222 18:16:12.361981 26697 registrar.cpp:370] Successfully recovered registrar
I0222 18:16:12.362052 26701 log.cpp:702] Attempting to truncate the log to 1
I0222 18:16:12.362167 26703 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I0222 18:16:12.362421 26696 master.cpp:1522] Recovered 0 slaves from the Registry (171B) ; allowing 10mins for slaves to re-register
I0222 18:16:12.362447 26698 hierarchical.cpp:171] Skipping recovery of hierarchical allocator: nothing to recover
I0222 18:16:12.362911 26701 replica.cpp:537] Replica received write request for position 2 from (13768)@172.30.2.148:35274
I0222 18:16:12.364760 26701 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.819954ms
I0222 18:16:12.364783 26701 replica.cpp:712] Persisted action at 2
I0222 18:16:12.365384 26697 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I0222 18:16:12.367961 26697 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 2.55143ms
I0222 18:16:12.368015 26697 leveldb.cpp:399] Deleting ~1 keys from leveldb took 28196ns
I0222 18:16:12.368028 26697 replica.cpp:712] Persisted action at 2
I0222 18:16:12.368044 26697 replica.cpp:697] Replica learned TRUNCATE action at position 2
I0222 18:16:12.376824 26703 slave.cpp:193] Slave started on 396)@172.30.2.148:35274
I0222 18:16:12.376838 26703 slave.cpp:194] Flags at startup: --appc_simple_discovery_uri_prefix=""http://"" --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""https://auth.docker.io"" --docker_kill_orphans=""true"" --docker_puller_timeout=""60"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mnt/teamcity/work/4240ba9ddd0997c3/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpu:2;mem:2048;disk(role1):2048"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_enable_support=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1""
I0222 18:16:12.377109 26703 credentials.hpp:83] Loading credential for authentication from '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/credential'
I0222 18:16:12.377300 26703 slave.cpp:324] Slave using credential for: test-principal
I0222 18:16:12.377439 26703 resources.cpp:576] Parsing resources as JSON failed: cpu:2;mem:2048;disk(role1):2048
Trying semicolon-delimited string format instead
I0222 18:16:12.377804 26703 slave.cpp:464] Slave resources: cpu(*):2; mem(*):2048; disk(role1):2048; cpus(*):8; ports(*):[31000-32000]
I0222 18:16:12.377881 26703 slave.cpp:472] Slave attributes: [  ]
I0222 18:16:12.377889 26703 slave.cpp:477] Slave hostname: ip-172-30-2-148.mesosphere.io
I0222 18:16:12.378779 26701 state.cpp:58] Recovering state from '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/meta'
I0222 18:16:12.379092 26697 status_update_manager.cpp:200] Recovering status update manager
I0222 18:16:12.379156 26681 sched.cpp:222] Version: 0.28.0
I0222 18:16:12.379250 26697 docker.cpp:722] Recovering Docker containers
I0222 18:16:12.379421 26703 slave.cpp:4565] Finished recovery
I0222 18:16:12.379627 26700 sched.cpp:326] New master detected at master@172.30.2.148:35274
I0222 18:16:12.379735 26703 slave.cpp:4737] Querying resource estimator for oversubscribable resources
I0222 18:16:12.379765 26700 sched.cpp:382] Authenticating with master master@172.30.2.148:35274
I0222 18:16:12.379781 26700 sched.cpp:389] Using default CRAM-MD5 authenticatee
I0222 18:16:12.379964 26696 status_update_manager.cpp:174] Pausing sending status updates
I0222 18:16:12.379992 26702 authenticatee.cpp:121] Creating new client SASL connection
I0222 18:16:12.380030 26697 slave.cpp:796] New master detected at master@172.30.2.148:35274
I0222 18:16:12.380106 26697 slave.cpp:859] Authenticating with master master@172.30.2.148:35274
I0222 18:16:12.380127 26697 slave.cpp:864] Using default CRAM-MD5 authenticatee
I0222 18:16:12.380188 26699 master.cpp:5526] Authenticating scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270@172.30.2.148:35274
I0222 18:16:12.380269 26700 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(832)@172.30.2.148:35274
I0222 18:16:12.380280 26698 authenticatee.cpp:121] Creating new client SASL connection
I0222 18:16:12.380307 26697 slave.cpp:832] Detecting new master
I0222 18:16:12.380450 26697 slave.cpp:4751] Received oversubscribable resources  from the resource estimator
I0222 18:16:12.380452 26699 master.cpp:5526] Authenticating slave(396)@172.30.2.148:35274
I0222 18:16:12.380506 26698 authenticator.cpp:98] Creating new server SASL connection
I0222 18:16:12.380540 26697 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(833)@172.30.2.148:35274
I0222 18:16:12.380635 26700 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0222 18:16:12.380659 26700 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0222 18:16:12.380762 26700 authenticator.cpp:203] Received SASL authentication start
I0222 18:16:12.380765 26701 authenticator.cpp:98] Creating new server SASL connection
I0222 18:16:12.380843 26700 authenticator.cpp:325] Authentication requires more steps
I0222 18:16:12.380911 26698 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0222 18:16:12.380931 26702 authenticatee.cpp:258] Received SASL authentication step
I0222 18:16:12.380936 26698 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0222 18:16:12.381036 26702 authenticator.cpp:231] Received SASL authentication step
I0222 18:16:12.381052 26698 authenticator.cpp:203] Received SASL authentication start
I0222 18:16:12.381062 26702 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-148' server FQDN: 'ip-172-30-2-148' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0222 18:16:12.381072 26702 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0222 18:16:12.381104 26702 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0222 18:16:12.381104 26698 authenticator.cpp:325] Authentication requires more steps
I0222 18:16:12.381134 26702 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-148' server FQDN: 'ip-172-30-2-148' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0222 18:16:12.381142 26702 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0222 18:16:12.381147 26702 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0222 18:16:12.381162 26702 authenticator.cpp:317] Authentication success
I0222 18:16:12.381184 26698 authenticatee.cpp:258] Received SASL authentication step
I0222 18:16:12.381247 26699 authenticatee.cpp:298] Authentication success
I0222 18:16:12.381283 26696 authenticator.cpp:231] Received SASL authentication step
I0222 18:16:12.381311 26696 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-148' server FQDN: 'ip-172-30-2-148' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0222 18:16:12.381325 26696 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0222 18:16:12.381319 26701 master.cpp:5556] Successfully authenticated principal 'test-principal' at scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270@172.30.2.148:35274
I0222 18:16:12.381345 26700 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(832)@172.30.2.148:35274
I0222 18:16:12.381361 26696 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0222 18:16:12.381397 26696 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-148' server FQDN: 'ip-172-30-2-148' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0222 18:16:12.381413 26696 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0222 18:16:12.381422 26696 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0222 18:16:12.381441 26696 authenticator.cpp:317] Authentication success
I0222 18:16:12.381548 26698 sched.cpp:471] Successfully authenticated with master master@172.30.2.148:35274
I0222 18:16:12.381563 26698 sched.cpp:776] Sending SUBSCRIBE call to master@172.30.2.148:35274
I0222 18:16:12.381634 26700 authenticatee.cpp:298] Authentication success
I0222 18:16:12.381660 26698 sched.cpp:809] Will retry registration in 770.60771ms if necessary
I0222 18:16:12.381675 26697 master.cpp:5556] Successfully authenticated principal 'test-principal' at slave(396)@172.30.2.148:35274
I0222 18:16:12.381734 26702 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(833)@172.30.2.148:35274
I0222 18:16:12.381811 26697 master.cpp:2280] Received SUBSCRIBE call for framework 'default' at scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270@172.30.2.148:35274
I0222 18:16:12.381882 26697 master.cpp:1751] Authorizing framework principal 'test-principal' to receive offers for role 'role1'
I0222 18:16:12.382004 26698 slave.cpp:927] Successfully authenticated with master master@172.30.2.148:35274
I0222 18:16:12.382123 26698 slave.cpp:1321] Will retry registration in 8.1941ms if necessary
I0222 18:16:12.382282 26701 master.cpp:4240] Registering slave at slave(396)@172.30.2.148:35274 (ip-172-30-2-148.mesosphere.io) with id 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0
I0222 18:16:12.382482 26701 master.cpp:2351] Subscribing framework default with checkpointing disabled and capabilities [  ]
I0222 18:16:12.382612 26703 registrar.cpp:439] Applied 1 operations in 46327ns; attempting to update the 'registry'
I0222 18:16:12.382829 26699 hierarchical.cpp:265] Added framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:12.382910 26699 hierarchical.cpp:1434] No resources available to allocate!
I0222 18:16:12.382915 26701 sched.cpp:703] Framework registered with 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:12.382936 26699 hierarchical.cpp:1529] No inverse offers to send out!
I0222 18:16:12.382953 26699 hierarchical.cpp:1127] Performed allocation for 0 slaves in 89949ns
I0222 18:16:12.382982 26701 sched.cpp:717] Scheduler::registered took 46498ns
I0222 18:16:12.383536 26698 log.cpp:683] Attempting to append 423 bytes to the log
I0222 18:16:12.383628 26699 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I0222 18:16:12.384196 26700 replica.cpp:537] Replica received write request for position 3 from (13775)@172.30.2.148:35274
I0222 18:16:12.386602 26700 leveldb.cpp:341] Persisting action (442 bytes) to leveldb took 2.377119ms
I0222 18:16:12.386625 26700 replica.cpp:712] Persisted action at 3
I0222 18:16:12.387104 26698 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I0222 18:16:12.389159 26698 leveldb.cpp:341] Persisting action (444 bytes) to leveldb took 2.032301ms
I0222 18:16:12.389181 26698 replica.cpp:712] Persisted action at 3
I0222 18:16:12.389196 26698 replica.cpp:697] Replica learned APPEND action at position 3
I0222 18:16:12.390281 26698 registrar.cpp:484] Successfully updated the 'registry' in 7.619072ms
I0222 18:16:12.390444 26702 log.cpp:702] Attempting to truncate the log to 3
I0222 18:16:12.390569 26701 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I0222 18:16:12.390904 26701 slave.cpp:3482] Received ping from slave-observer(364)@172.30.2.148:35274
I0222 18:16:12.391054 26700 master.cpp:4308] Registered slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 at slave(396)@172.30.2.148:35274 (ip-172-30-2-148.mesosphere.io) with cpu(*):2; mem(*):2048; disk(role1):2048; cpus(*):8; ports(*):[31000-32000]
I0222 18:16:12.391144 26703 slave.cpp:971] Registered with master master@172.30.2.148:35274; given slave ID 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0
I0222 18:16:12.391168 26703 fetcher.cpp:81] Clearing fetcher cache
I0222 18:16:12.391238 26700 replica.cpp:537] Replica received write request for position 4 from (13776)@172.30.2.148:35274
I0222 18:16:12.391263 26701 status_update_manager.cpp:181] Resuming sending status updates
I0222 18:16:12.391304 26697 hierarchical.cpp:473] Added slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 (ip-172-30-2-148.mesosphere.io) with cpu(*):2; mem(*):2048; disk(role1):2048; cpus(*):8; ports(*):[31000-32000] (allocated: )
I0222 18:16:12.391388 26703 slave.cpp:994] Checkpointing SlaveInfo to '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/meta/slaves/652149b4-3932-4d8b-ba6f-8c9d9045be70-S0/slave.info'
I0222 18:16:12.391636 26703 slave.cpp:1030] Forwarding total oversubscribed resources 
I0222 18:16:12.391772 26699 master.cpp:4649] Received update of slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 at slave(396)@172.30.2.148:35274 (ip-172-30-2-148.mesosphere.io) with total oversubscribed resources 
I0222 18:16:12.392011 26697 hierarchical.cpp:1529] No inverse offers to send out!
I0222 18:16:12.392053 26697 hierarchical.cpp:1147] Performed allocation for slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 in 708377ns
I0222 18:16:12.392307 26703 master.cpp:5355] Sending 1 offers to framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 (default) at scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270@172.30.2.148:35274
I0222 18:16:12.392374 26697 hierarchical.cpp:531] Slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 (ip-172-30-2-148.mesosphere.io) updated with oversubscribed resources  (total: cpu(*):2; mem(*):2048; disk(role1):2048; cpus(*):8; ports(*):[31000-32000], allocated: disk(role1):2048; cpu(*):2; mem(*):2048; cpus(*):8; ports(*):[31000-32000])
I0222 18:16:12.392500 26697 hierarchical.cpp:1434] No resources available to allocate!
I0222 18:16:12.392531 26697 hierarchical.cpp:1529] No inverse offers to send out!
I0222 18:16:12.392556 26697 hierarchical.cpp:1147] Performed allocation for slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 in 136779ns
I0222 18:16:12.392704 26701 sched.cpp:873] Scheduler::resourceOffers took 94330ns
I0222 18:16:12.393086 26681 resources.cpp:576] Parsing resources as JSON failed: cpus:1;mem:64;
Trying semicolon-delimited string format instead
I0222 18:16:12.393600 26700 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 2.326382ms
I0222 18:16:12.393625 26700 replica.cpp:712] Persisted action at 4
I0222 18:16:12.394162 26696 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I0222 18:16:12.394533 26701 master.cpp:3138] Processing ACCEPT call for offers: [ 652149b4-3932-4d8b-ba6f-8c9d9045be70-O0 ] on slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 at slave(396)@172.30.2.148:35274 (ip-172-30-2-148.mesosphere.io) for framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 (default) at scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270@172.30.2.148:35274
I0222 18:16:12.394567 26701 master.cpp:2926] Authorizing principal 'test-principal' to create volumes
I0222 18:16:12.394628 26701 master.cpp:2825] Authorizing framework principal 'test-principal' to launch task 1 as user 'root'
I0222 18:16:12.395519 26701 master.cpp:3467] Applying CREATE operation for volumes disk(role1)[id1:path1]:64 from framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 (default) at scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270@172.30.2.148:35274 to slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 at slave(396)@172.30.2.148:35274 (ip-172-30-2-148.mesosphere.io)
I0222 18:16:12.395808 26701 master.cpp:6589] Sending checkpointed resources disk(role1)[id1:path1]:64 to slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 at slave(396)@172.30.2.148:35274 (ip-172-30-2-148.mesosphere.io)
I0222 18:16:12.396316 26696 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 2.130659ms
I0222 18:16:12.396317 26703 slave.cpp:2341] Updated checkpointed resources from  to disk(role1)[id1:path1]:64
I0222 18:16:12.396368 26696 leveldb.cpp:399] Deleting ~2 keys from leveldb took 30004ns
I0222 18:16:12.396381 26696 replica.cpp:712] Persisted action at 4
I0222 18:16:12.396397 26696 replica.cpp:697] Replica learned TRUNCATE action at position 4
I0222 18:16:12.396533 26701 master.hpp:176] Adding task 1 with resources cpus(*):1; mem(*):64; disk(role1)[id1:path1]:64 on slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 (ip-172-30-2-148.mesosphere.io)
I0222 18:16:12.396680 26701 master.cpp:3623] Launching task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 (default) at scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270@172.30.2.148:35274 with resources cpus(*):1; mem(*):64; disk(role1)[id1:path1]:64 on slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 at slave(396)@172.30.2.148:35274 (ip-172-30-2-148.mesosphere.io)
I0222 18:16:12.397009 26696 slave.cpp:1361] Got assigned task 1 for framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:12.397143 26696 resources.cpp:576] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I0222 18:16:12.397306 26699 hierarchical.cpp:653] Updated allocation of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 on slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 from disk(role1):2048; cpu(*):2; mem(*):2048; cpus(*):8; ports(*):[31000-32000] to disk(role1):1984; cpu(*):2; mem(*):2048; cpus(*):8; ports(*):[31000-32000]; disk(role1)[id1:path1]:64
I0222 18:16:12.397625 26699 hierarchical.cpp:892] Recovered disk(role1):1984; cpu(*):2; mem(*):1984; cpus(*):7; ports(*):[31000-32000] (total: cpu(*):2; mem(*):2048; disk(role1):1984; cpus(*):8; ports(*):[31000-32000]; disk(role1)[id1:path1]:64, allocated: disk(role1)[id1:path1]:64; cpus(*):1; mem(*):64) on slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 from framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:12.397857 26696 slave.cpp:1480] Launching task 1 for framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:12.397943 26696 resources.cpp:576] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I0222 18:16:12.398560 26696 paths.cpp:474] Trying to chown '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/slaves/652149b4-3932-4d8b-ba6f-8c9d9045be70-S0/frameworks/652149b4-3932-4d8b-ba6f-8c9d9045be70-0000/executors/1/runs/207172a3-0ebd-4faa-946b-75a829fc75fc' to user 'root'
I0222 18:16:12.403491 26696 slave.cpp:5367] Launching executor 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/slaves/652149b4-3932-4d8b-ba6f-8c9d9045be70-S0/frameworks/652149b4-3932-4d8b-ba6f-8c9d9045be70-0000/executors/1/runs/207172a3-0ebd-4faa-946b-75a829fc75fc'
I0222 18:16:12.404115 26696 slave.cpp:1698] Queuing task '1' for executor '1' of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:12.405709 26696 slave.cpp:749] Successfully attached file '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/slaves/652149b4-3932-4d8b-ba6f-8c9d9045be70-S0/frameworks/652149b4-3932-4d8b-ba6f-8c9d9045be70-0000/executors/1/runs/207172a3-0ebd-4faa-946b-75a829fc75fc'
I0222 18:16:12.408308 26697 docker.cpp:1019] Starting container '207172a3-0ebd-4faa-946b-75a829fc75fc' for task '1' (and executor '1') of framework '652149b4-3932-4d8b-ba6f-8c9d9045be70-0000'
I0222 18:16:12.408592 26697 docker.cpp:1053] Running docker -H unix:///var/run/docker.sock inspect alpine:latest
I0222 18:16:12.520663 26702 docker.cpp:390] Docker pull alpine completed
I0222 18:16:12.520853 26702 docker.cpp:479] Changing the ownership of the persistent volume at '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/volumes/roles/role1/id1' with uid 0 and gid 0
I0222 18:16:12.524782 26702 docker.cpp:500] Mounting '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/volumes/roles/role1/id1' to '/tmp/DockerContainerizerTest_ROOT_DOCKER_LaunchWithPersistentVolumes_U5vZX1/slaves/652149b4-3932-4d8b-ba6f-8c9d9045be70-S0/frameworks/652149b4-3932-4d8b-ba6f-8c9d9045be70-0000/executors/1/runs/207172a3-0ebd-4faa-946b-75a829fc75fc/path1' for persistent volume disk(role1)[id1:path1]:64 of container 207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:12.580834 26700 slave.cpp:2643] Got registration for executor '1' of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 from executor(1)@172.30.2.148:56026
I0222 18:16:12.581961 26699 docker.cpp:1299] Ignoring updating container '207172a3-0ebd-4faa-946b-75a829fc75fc' with resources passed to update is identical to existing resources
I0222 18:16:12.582307 26698 slave.cpp:1863] Sending queued task '1' to executor '1' of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 at executor(1)@172.30.2.148:56026
I0222 18:16:13.295573 26703 slave.cpp:3002] Handling status update TASK_RUNNING (UUID: 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 from executor(1)@172.30.2.148:56026
I0222 18:16:13.295940 26703 slave.cpp:3002] Handling status update TASK_FINISHED (UUID: ed5a5eb7-65cc-42fa-bb85-3aaf65d86e6b) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 from executor(1)@172.30.2.148:56026
I0222 18:16:13.296381 26701 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.296422 26701 status_update_manager.cpp:497] Creating StatusUpdate stream for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.296422 26703 slave.cpp:5677] Terminating task 1
I0222 18:16:13.296839 26701 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 to the slave
I0222 18:16:13.296902 26702 docker.cpp:766] Running docker -H unix:///var/run/docker.sock inspect mesos-652149b4-3932-4d8b-ba6f-8c9d9045be70-S0.207172a3-0ebd-4faa-946b-75a829fc75fc
I0222 18:16:13.299427 26699 slave.cpp:3400] Forwarding the update TASK_RUNNING (UUID: 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 to master@172.30.2.148:35274
I0222 18:16:13.299921 26699 slave.cpp:3294] Status update manager successfully handled status update TASK_RUNNING (UUID: 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.299969 26699 slave.cpp:3310] Sending acknowledgement for status update TASK_RUNNING (UUID: 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 to executor(1)@172.30.2.148:56026
I0222 18:16:13.300130 26696 master.cpp:4794] Status update TASK_RUNNING (UUID: 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 from slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 at slave(396)@172.30.2.148:35274 (ip-172-30-2-148.mesosphere.io)
I0222 18:16:13.300176 26696 master.cpp:4842] Forwarding status update TASK_RUNNING (UUID: 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0) for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.300375 26696 master.cpp:6450] Updating the state of task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 (latest state: TASK_FINISHED, status update state: TASK_RUNNING)
I0222 18:16:13.300765 26703 sched.cpp:981] Scheduler::statusUpdate took 164263ns
I0222 18:16:13.300962 26700 hierarchical.cpp:892] Recovered cpus(*):1; mem(*):64; disk(role1)[id1:path1]:64 (total: cpu(*):2; mem(*):2048; disk(role1):1984; cpus(*):8; ports(*):[31000-32000]; disk(role1)[id1:path1]:64, allocated: ) on slave 652149b4-3932-4d8b-ba6f-8c9d9045be70-S0 from framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000
I0222 18:16:13.301178 26699 master.cpp:3952] Processing ACKNOWLEDGE call 0f6cc8cc-cd72-4bda-ba53-2e573ea1e0a0 for task 1 of framework 652149b4-3932-4d8b-ba6f-8c9d9045be70-0000 (default) at scheduler-1850b1cd-3396-4479-b2f3-47ee6c3fa270@172.30.2.148:35274 on slave 652149b4-3932-4d8b-ba6f-8c9d9045be",2.0,0.28.0,0.0,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.25688073394495414
Improvement,Update /frameworks to use jsonify,This should let us remove the duplicated code in {{http.cpp}} between {{model(Framework)}} and {{json(Full<Framework>)}}.,3.0,0,0.0,1.0,0.0,0.0,0.0,0.0,0.8,0.5438596491228069,0.39999999999999997,0.48717948717948717,0.4303030303030303,0.4303030303030303,0.0
Documentation,Document scheduler driver calls in framework development guide.,"The interface examples are slightly out of sync with scheduler.hpp, most notably missing the new acceptOffers call.",2.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12280701754385964,0.32857142857142857,0.11538461538461538,0.23636363636363636,0.23636363636363636,0.0
Bug,"""make DESTDIR=<path> install"" broken",There is a missing '$(DESTDIR)' prefix in the install-data-hook that causes DESTDIR builds to be broken.,2.0,0,0.0,0.3453996983408748,0.0,0.0,0.0,0.0,0.2,0.38596491228070173,0.35714285714285715,0.28205128205128205,0.29090909090909095,0.28484848484848485,0.0
Bug,"Make Stout configuration modular and consumable by downstream (e.g., libprocess and agent)","Stout configuration is replicated in at least 3 configuration files -- stout itself, libprocess, and agent. More will follow in the future.

We should make a StoutConfigure.cmake that can be included by any package downstream.",1.0,0,0.0,0.44494720965309204,0.0,0.0,0.0,0.0,0.0,0.14035087719298245,0.15,0.14102564102564102,0.1515151515151515,0.16969696969696968,0.0
Documentation,"Document default value of ""offer_timeout""","There isn't a default value (i.e., offers do not timeout by default), but we should clarify this in {{flags.cpp}} and {{configuration.md}}.",1.0,0,0.0,1.0,0.0,0.0,0.0,0.0,0.8,0.5438596491228069,0.39999999999999997,0.48717948717948717,0.4303030303030303,0.4303030303030303,0.0
Improvement,Allow Reserve operations by a principal without `ReservationInfo.principal`,"Currently, we require a framework or operator to specify `ReservationInfo.principal` when they reserve resources. This isn't necessary, however; we already know the principal and can fill in the field if it isn't set already.",2.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Task,Add a HierarchicalAllocator benchmark with reservation labels.,"With {{Labels}} being part of the {{ReservationInfo}}, we should ensure that we don't observe a significant performance degradation in the allocator.",3.0,0,0.5,1.0,0.0,0.0,0.0,0.0,0.8,0.5438596491228069,0.39999999999999997,0.21794871794871795,0.26666666666666666,0.26666666666666666,0.0
Task,Implement master failover tests for the scheduler library.,"Currently, the scheduler library creates its own {{MasterDetector}} object internally. We would need to create a standalone detector and create new tests for testing that callbacks are invoked correctly in the event of a master failover.",3.0,0,0.0,0.5701357466063348,0.0,0.0,0.0,0.0,0.0,0.6842105263157894,0.5642857142857143,0.3846153846153846,0.4181818181818182,0.4121212121212121,0.0
Bug,ROOT_DOCKER_Logs is flaky.,"{noformat}
[18:06:25][Step 8/8] [ RUN      ] DockerContainerizerTest.ROOT_DOCKER_Logs
[18:06:25][Step 8/8] I0215 17:06:25.256103  1740 leveldb.cpp:174] Opened db in 6.548327ms
[18:06:25][Step 8/8] I0215 17:06:25.258002  1740 leveldb.cpp:181] Compacted db in 1.837816ms
[18:06:25][Step 8/8] I0215 17:06:25.258059  1740 leveldb.cpp:196] Created db iterator in 22044ns
[18:06:25][Step 8/8] I0215 17:06:25.258076  1740 leveldb.cpp:202] Seeked to beginning of db in 2347ns
[18:06:25][Step 8/8] I0215 17:06:25.258091  1740 leveldb.cpp:271] Iterated through 0 keys in the db in 571ns
[18:06:25][Step 8/8] I0215 17:06:25.258152  1740 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
[18:06:25][Step 8/8] I0215 17:06:25.258936  1758 recover.cpp:447] Starting replica recovery
[18:06:25][Step 8/8] I0215 17:06:25.259177  1758 recover.cpp:473] Replica is in EMPTY status
[18:06:25][Step 8/8] I0215 17:06:25.260327  1757 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (13608)@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.260545  1758 recover.cpp:193] Received a recover response from a replica in EMPTY status
[18:06:25][Step 8/8] I0215 17:06:25.261065  1757 master.cpp:376] Master 112363e2-c680-4946-8fee-d0626ed8b21e (ip-172-30-2-239.mesosphere.io) started on 172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.261209  1761 recover.cpp:564] Updating replica status to STARTING
[18:06:25][Step 8/8] I0215 17:06:25.261086  1757 master.cpp:378] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/HncLLj/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/HncLLj/master"" --zk_session_timeout=""10secs""
[18:06:25][Step 8/8] I0215 17:06:25.261446  1757 master.cpp:423] Master only allowing authenticated frameworks to register
[18:06:25][Step 8/8] I0215 17:06:25.261456  1757 master.cpp:428] Master only allowing authenticated slaves to register
[18:06:25][Step 8/8] I0215 17:06:25.261462  1757 credentials.hpp:35] Loading credentials for authentication from '/tmp/HncLLj/credentials'
[18:06:25][Step 8/8] I0215 17:06:25.261723  1757 master.cpp:468] Using default 'crammd5' authenticator
[18:06:25][Step 8/8] I0215 17:06:25.261855  1757 master.cpp:537] Using default 'basic' HTTP authenticator
[18:06:25][Step 8/8] I0215 17:06:25.262022  1757 master.cpp:571] Authorization enabled
[18:06:25][Step 8/8] I0215 17:06:25.262177  1755 hierarchical.cpp:144] Initialized hierarchical allocator process
[18:06:25][Step 8/8] I0215 17:06:25.262177  1758 whitelist_watcher.cpp:77] No whitelist given
[18:06:25][Step 8/8] I0215 17:06:25.262899  1760 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.517992ms
[18:06:25][Step 8/8] I0215 17:06:25.262924  1760 replica.cpp:320] Persisted replica status to STARTING
[18:06:25][Step 8/8] I0215 17:06:25.263144  1754 recover.cpp:473] Replica is in STARTING status
[18:06:25][Step 8/8] I0215 17:06:25.264010  1757 master.cpp:1712] The newly elected leader is master@172.30.2.239:39785 with id 112363e2-c680-4946-8fee-d0626ed8b21e
[18:06:25][Step 8/8] I0215 17:06:25.264044  1757 master.cpp:1725] Elected as the leading master!
[18:06:25][Step 8/8] I0215 17:06:25.264061  1757 master.cpp:1470] Recovering from registrar
[18:06:25][Step 8/8] I0215 17:06:25.264117  1760 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (13610)@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.264197  1758 registrar.cpp:307] Recovering registrar
[18:06:25][Step 8/8] I0215 17:06:25.264827  1756 recover.cpp:193] Received a recover response from a replica in STARTING status
[18:06:25][Step 8/8] I0215 17:06:25.265219  1757 recover.cpp:564] Updating replica status to VOTING
[18:06:25][Step 8/8] I0215 17:06:25.267302  1754 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.887739ms
[18:06:25][Step 8/8] I0215 17:06:25.267326  1754 replica.cpp:320] Persisted replica status to VOTING
[18:06:25][Step 8/8] I0215 17:06:25.267453  1759 recover.cpp:578] Successfully joined the Paxos group
[18:06:25][Step 8/8] I0215 17:06:25.267632  1759 recover.cpp:462] Recover process terminated
[18:06:25][Step 8/8] I0215 17:06:25.268007  1757 log.cpp:659] Attempting to start the writer
[18:06:25][Step 8/8] I0215 17:06:25.269055  1759 replica.cpp:493] Replica received implicit promise request from (13611)@172.30.2.239:39785 with proposal 1
[18:06:25][Step 8/8] I0215 17:06:25.270488  1759 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 1.406068ms
[18:06:25][Step 8/8] I0215 17:06:25.270511  1759 replica.cpp:342] Persisted promised to 1
[18:06:25][Step 8/8] I0215 17:06:25.271078  1761 coordinator.cpp:238] Coordinator attempting to fill missing positions
[18:06:25][Step 8/8] I0215 17:06:25.272146  1756 replica.cpp:388] Replica received explicit promise request from (13612)@172.30.2.239:39785 for position 0 with proposal 2
[18:06:25][Step 8/8] I0215 17:06:25.273478  1756 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 1.297217ms
[18:06:25][Step 8/8] I0215 17:06:25.273500  1756 replica.cpp:712] Persisted action at 0
[18:06:25][Step 8/8] I0215 17:06:25.274355  1757 replica.cpp:537] Replica received write request for position 0 from (13613)@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.274405  1757 leveldb.cpp:436] Reading position from leveldb took 25294ns
[18:06:25][Step 8/8] I0215 17:06:25.275800  1757 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 1.362978ms
[18:06:25][Step 8/8] I0215 17:06:25.275823  1757 replica.cpp:712] Persisted action at 0
[18:06:25][Step 8/8] I0215 17:06:25.276348  1755 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
[18:06:25][Step 8/8] I0215 17:06:25.277765  1755 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.391531ms
[18:06:25][Step 8/8] I0215 17:06:25.277788  1755 replica.cpp:712] Persisted action at 0
[18:06:25][Step 8/8] I0215 17:06:25.277802  1755 replica.cpp:697] Replica learned NOP action at position 0
[18:06:25][Step 8/8] I0215 17:06:25.278336  1754 log.cpp:675] Writer started with ending position 0
[18:06:25][Step 8/8] I0215 17:06:25.279371  1755 leveldb.cpp:436] Reading position from leveldb took 29214ns
[18:06:25][Step 8/8] I0215 17:06:25.280272  1758 registrar.cpp:340] Successfully fetched the registry (0B) in 16.02688ms
[18:06:25][Step 8/8] I0215 17:06:25.280385  1758 registrar.cpp:439] Applied 1 operations in 31040ns; attempting to update the 'registry'
[18:06:25][Step 8/8] I0215 17:06:25.281054  1755 log.cpp:683] Attempting to append 210 bytes to the log
[18:06:25][Step 8/8] I0215 17:06:25.281165  1757 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
[18:06:25][Step 8/8] I0215 17:06:25.281780  1757 replica.cpp:537] Replica received write request for position 1 from (13614)@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.283159  1757 leveldb.cpp:341] Persisting action (229 bytes) to leveldb took 1.348041ms
[18:06:25][Step 8/8] I0215 17:06:25.283184  1757 replica.cpp:712] Persisted action at 1
[18:06:25][Step 8/8] I0215 17:06:25.283695  1759 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
[18:06:25][Step 8/8] I0215 17:06:25.285059  1759 leveldb.cpp:341] Persisting action (231 bytes) to leveldb took 1.334577ms
[18:06:25][Step 8/8] I0215 17:06:25.285084  1759 replica.cpp:712] Persisted action at 1
[18:06:25][Step 8/8] I0215 17:06:25.285099  1759 replica.cpp:697] Replica learned APPEND action at position 1
[18:06:25][Step 8/8] I0215 17:06:25.285910  1758 registrar.cpp:484] Successfully updated the 'registry' in 5.46816ms
[18:06:25][Step 8/8] I0215 17:06:25.286043  1758 registrar.cpp:370] Successfully recovered registrar
[18:06:25][Step 8/8] I0215 17:06:25.286121  1755 log.cpp:702] Attempting to truncate the log to 1
[18:06:25][Step 8/8] I0215 17:06:25.286301  1756 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
[18:06:25][Step 8/8] I0215 17:06:25.286478  1759 hierarchical.cpp:171] Skipping recovery of hierarchical allocator: nothing to recover
[18:06:25][Step 8/8] I0215 17:06:25.286476  1754 master.cpp:1522] Recovered 0 slaves from the Registry (171B) ; allowing 10mins for slaves to re-register
[18:06:25][Step 8/8] I0215 17:06:25.287137  1755 replica.cpp:537] Replica received write request for position 2 from (13615)@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.289104  1755 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.938609ms
[18:06:25][Step 8/8] I0215 17:06:25.289127  1755 replica.cpp:712] Persisted action at 2
[18:06:25][Step 8/8] I0215 17:06:25.289667  1759 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
[18:06:25][Step 8/8] I0215 17:06:25.290956  1759 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 1.256421ms
[18:06:25][Step 8/8] I0215 17:06:25.291007  1759 leveldb.cpp:399] Deleting ~1 keys from leveldb took 28064ns
[18:06:25][Step 8/8] I0215 17:06:25.291021  1759 replica.cpp:712] Persisted action at 2
[18:06:25][Step 8/8] I0215 17:06:25.291038  1759 replica.cpp:697] Replica learned TRUNCATE action at position 2
[18:06:25][Step 8/8] I0215 17:06:25.300550  1760 slave.cpp:193] Slave started on 393)@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.300573  1760 slave.cpp:194] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/DockerContainerizerTest_ROOT_DOCKER_Logs_a4NS2N/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""https://auth.docker.io"" --docker_kill_orphans=""true"" --docker_puller_timeout=""60"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/DockerContainerizerTest_ROOT_DOCKER_Logs_a4NS2N/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mnt/teamcity/work/4240ba9ddd0997c3/build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/DockerContainerizerTest_ROOT_DOCKER_Logs_a4NS2N""
[18:06:25][Step 8/8] I0215 17:06:25.300868  1760 credentials.hpp:83] Loading credential for authentication from '/tmp/DockerContainerizerTest_ROOT_DOCKER_Logs_a4NS2N/credential'
[18:06:25][Step 8/8] I0215 17:06:25.301030  1760 slave.cpp:324] Slave using credential for: test-principal
[18:06:25][Step 8/8] I0215 17:06:25.301180  1760 resources.cpp:576] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ports:[31000-32000]
[18:06:25][Step 8/8] Trying semicolon-delimited string format instead
[18:06:25][Step 8/8] I0215 17:06:25.301553  1760 slave.cpp:464] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
[18:06:25][Step 8/8] I0215 17:06:25.301609  1760 slave.cpp:472] Slave attributes: [  ]
[18:06:25][Step 8/8] I0215 17:06:25.301620  1760 slave.cpp:477] Slave hostname: ip-172-30-2-239.mesosphere.io
[18:06:25][Step 8/8] I0215 17:06:25.302417  1757 state.cpp:58] Recovering state from '/tmp/DockerContainerizerTest_ROOT_DOCKER_Logs_a4NS2N/meta'
[18:06:25][Step 8/8] I0215 17:06:25.302515  1740 sched.cpp:222] Version: 0.28.0
[18:06:25][Step 8/8] I0215 17:06:25.302772  1755 status_update_manager.cpp:200] Recovering status update manager
[18:06:25][Step 8/8] I0215 17:06:25.302956  1758 docker.cpp:559] Recovering Docker containers
[18:06:25][Step 8/8] I0215 17:06:25.303050  1761 sched.cpp:326] New master detected at master@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.303133  1754 slave.cpp:4565] Finished recovery
[18:06:25][Step 8/8] I0215 17:06:25.303154  1761 sched.cpp:382] Authenticating with master master@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.303169  1761 sched.cpp:389] Using default CRAM-MD5 authenticatee
[18:06:25][Step 8/8] I0215 17:06:25.303364  1759 authenticatee.cpp:121] Creating new client SASL connection
[18:06:25][Step 8/8] I0215 17:06:25.303467  1754 slave.cpp:4737] Querying resource estimator for oversubscribable resources
[18:06:25][Step 8/8] I0215 17:06:25.303668  1756 master.cpp:5523] Authenticating scheduler-806c70e3-1cf6-418f-aa30-6bb26db42d18@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.303707  1760 status_update_manager.cpp:174] Pausing sending status updates
[18:06:25][Step 8/8] I0215 17:06:25.303707  1754 slave.cpp:796] New master detected at master@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.303767  1755 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(829)@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.303791  1754 slave.cpp:859] Authenticating with master master@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.303805  1754 slave.cpp:864] Using default CRAM-MD5 authenticatee
[18:06:25][Step 8/8] I0215 17:06:25.303956  1754 slave.cpp:832] Detecting new master
[18:06:25][Step 8/8] I0215 17:06:25.303971  1761 authenticatee.cpp:121] Creating new client SASL connection
[18:06:25][Step 8/8] I0215 17:06:25.303984  1760 authenticator.cpp:98] Creating new server SASL connection
[18:06:25][Step 8/8] I0215 17:06:25.304131  1754 slave.cpp:4751] Received oversubscribable resources  from the resource estimator
[18:06:25][Step 8/8] I0215 17:06:25.304275  1757 master.cpp:5523] Authenticating slave(393)@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.304344  1754 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
[18:06:25][Step 8/8] I0215 17:06:25.304369  1754 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
[18:06:25][Step 8/8] I0215 17:06:25.304373  1761 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(830)@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.304440  1757 authenticator.cpp:203] Received SASL authentication start
[18:06:25][Step 8/8] I0215 17:06:25.304491  1757 authenticator.cpp:325] Authentication requires more steps
[18:06:25][Step 8/8] I0215 17:06:25.304548  1754 authenticator.cpp:98] Creating new server SASL connection
[18:06:25][Step 8/8] I0215 17:06:25.304582  1761 authenticatee.cpp:258] Received SASL authentication step
[18:06:25][Step 8/8] I0215 17:06:25.304688  1761 authenticator.cpp:231] Received SASL authentication step
[18:06:25][Step 8/8] I0215 17:06:25.304714  1761 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-239.mesosphere.io' server FQDN: 'ip-172-30-2-239.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
[18:06:25][Step 8/8] I0215 17:06:25.304723  1761 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
[18:06:25][Step 8/8] I0215 17:06:25.304767  1761 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
[18:06:25][Step 8/8] I0215 17:06:25.304805  1761 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-239.mesosphere.io' server FQDN: 'ip-172-30-2-239.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
[18:06:25][Step 8/8] I0215 17:06:25.304817  1761 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
[18:06:25][Step 8/8] I0215 17:06:25.304824  1761 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
[18:06:25][Step 8/8] I0215 17:06:25.304836  1761 authenticator.cpp:317] Authentication success
[18:06:25][Step 8/8] I0215 17:06:25.304841  1758 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
[18:06:25][Step 8/8] I0215 17:06:25.304870  1758 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
[18:06:25][Step 8/8] I0215 17:06:25.304909  1757 authenticatee.cpp:298] Authentication success
[18:06:25][Step 8/8] I0215 17:06:25.304983  1756 authenticator.cpp:203] Received SASL authentication start
[18:06:25][Step 8/8] I0215 17:06:25.305033  1756 authenticator.cpp:325] Authentication requires more steps
[18:06:25][Step 8/8] I0215 17:06:25.305042  1759 master.cpp:5553] Successfully authenticated principal 'test-principal' at scheduler-806c70e3-1cf6-418f-aa30-6bb26db42d18@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.305071  1755 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(829)@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.305124  1756 authenticatee.cpp:258] Received SASL authentication step
[18:06:25][Step 8/8] I0215 17:06:25.305222  1758 sched.cpp:471] Successfully authenticated with master master@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.305246  1758 sched.cpp:776] Sending SUBSCRIBE call to master@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.305286  1760 authenticator.cpp:231] Received SASL authentication step
[18:06:25][Step 8/8] I0215 17:06:25.305310  1760 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-239.mesosphere.io' server FQDN: 'ip-172-30-2-239.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
[18:06:25][Step 8/8] I0215 17:06:25.305318  1760 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
[18:06:25][Step 8/8] I0215 17:06:25.305344  1760 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
[18:06:25][Step 8/8] I0215 17:06:25.305363  1758 sched.cpp:809] Will retry registration in 1.888777185secs if necessary
[18:06:25][Step 8/8] I0215 17:06:25.305379  1760 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'ip-172-30-2-239.mesosphere.io' server FQDN: 'ip-172-30-2-239.mesosphere.io' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
[18:06:25][Step 8/8] I0215 17:06:25.305397  1760 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
[18:06:25][Step 8/8] I0215 17:06:25.305408  1760 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
[18:06:25][Step 8/8] I0215 17:06:25.305426  1760 authenticator.cpp:317] Authentication success
[18:06:25][Step 8/8] I0215 17:06:25.305466  1761 master.cpp:2280] Received SUBSCRIBE call for framework 'default' at scheduler-806c70e3-1cf6-418f-aa30-6bb26db42d18@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.305506  1756 authenticatee.cpp:298] Authentication success
[18:06:25][Step 8/8] I0215 17:06:25.305534  1761 master.cpp:1751] Authorizing framework principal 'test-principal' to receive offers for role '*'
[18:06:25][Step 8/8] I0215 17:06:25.305625  1755 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(830)@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.305701  1761 master.cpp:5553] Successfully authenticated principal 'test-principal' at slave(393)@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.305831  1758 slave.cpp:927] Successfully authenticated with master master@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.305902  1757 master.cpp:2351] Subscribing framework default with checkpointing disabled and capabilities [  ]
[18:06:25][Step 8/8] I0215 17:06:25.305953  1758 slave.cpp:1321] Will retry registration in 1.941456ms if necessary
[18:06:25][Step 8/8] I0215 17:06:25.306280  1761 hierarchical.cpp:265] Added framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:25][Step 8/8] I0215 17:06:25.306352  1759 sched.cpp:703] Framework registered with 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:25][Step 8/8] I0215 17:06:25.306363  1761 hierarchical.cpp:1403] No resources available to allocate!
[18:06:25][Step 8/8] I0215 17:06:25.306401  1761 hierarchical.cpp:1498] No inverse offers to send out!
[18:06:25][Step 8/8] I0215 17:06:25.306432  1761 hierarchical.cpp:1096] Performed allocation for 0 slaves in 126082ns
[18:06:25][Step 8/8] I0215 17:06:25.306447  1759 sched.cpp:717] Scheduler::registered took 67960ns
[18:06:25][Step 8/8] I0215 17:06:25.306437  1757 master.cpp:4237] Registering slave at slave(393)@172.30.2.239:39785 (ip-172-30-2-239.mesosphere.io) with id 112363e2-c680-4946-8fee-d0626ed8b21e-S0
[18:06:25][Step 8/8] I0215 17:06:25.306884  1759 registrar.cpp:439] Applied 1 operations in 63175ns; attempting to update the 'registry'
[18:06:25][Step 8/8] I0215 17:06:25.307592  1756 log.cpp:683] Attempting to append 396 bytes to the log
[18:06:25][Step 8/8] I0215 17:06:25.307724  1760 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
[18:06:25][Step 8/8] I0215 17:06:25.308398  1760 replica.cpp:537] Replica received write request for position 3 from (13622)@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.308473  1755 slave.cpp:1321] Will retry registration in 37.671741ms if necessary
[18:06:25][Step 8/8] I0215 17:06:25.308627  1758 master.cpp:4225] Ignoring register slave message from slave(393)@172.30.2.239:39785 (ip-172-30-2-239.mesosphere.io) as admission is already in progress
[18:06:25][Step 8/8] I0215 17:06:25.310000  1760 leveldb.cpp:341] Persisting action (415 bytes) to leveldb took 1.556814ms
[18:06:25][Step 8/8] I0215 17:06:25.310025  1760 replica.cpp:712] Persisted action at 3
[18:06:25][Step 8/8] I0215 17:06:25.310541  1755 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
[18:06:25][Step 8/8] I0215 17:06:25.311928  1755 leveldb.cpp:341] Persisting action (417 bytes) to leveldb took 1.357404ms
[18:06:25][Step 8/8] I0215 17:06:25.311950  1755 replica.cpp:712] Persisted action at 3
[18:06:25][Step 8/8] I0215 17:06:25.311966  1755 replica.cpp:697] Replica learned APPEND action at position 3
[18:06:25][Step 8/8] I0215 17:06:25.313117  1755 registrar.cpp:484] Successfully updated the 'registry' in 6.16704ms
[18:06:25][Step 8/8] I0215 17:06:25.313297  1758 log.cpp:702] Attempting to truncate the log to 3
[18:06:25][Step 8/8] I0215 17:06:25.313391  1755 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
[18:06:25][Step 8/8] I0215 17:06:25.313807  1761 slave.cpp:3482] Received ping from slave-observer(360)@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.313946  1754 master.cpp:4305] Registered slave 112363e2-c680-4946-8fee-d0626ed8b21e-S0 at slave(393)@172.30.2.239:39785 (ip-172-30-2-239.mesosphere.io) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
[18:06:25][Step 8/8] I0215 17:06:25.314067  1756 slave.cpp:971] Registered with master master@172.30.2.239:39785; given slave ID 112363e2-c680-4946-8fee-d0626ed8b21e-S0
[18:06:25][Step 8/8] I0215 17:06:25.314095  1756 fetcher.cpp:81] Clearing fetcher cache
[18:06:25][Step 8/8] I0215 17:06:25.314102  1760 replica.cpp:537] Replica received write request for position 4 from (13623)@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.314164  1758 hierarchical.cpp:473] Added slave 112363e2-c680-4946-8fee-d0626ed8b21e-S0 (ip-172-30-2-239.mesosphere.io) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
[18:06:25][Step 8/8] I0215 17:06:25.314219  1761 status_update_manager.cpp:181] Resuming sending status updates
[18:06:25][Step 8/8] I0215 17:06:25.314370  1756 slave.cpp:994] Checkpointing SlaveInfo to '/tmp/DockerContainerizerTest_ROOT_DOCKER_Logs_a4NS2N/meta/slaves/112363e2-c680-4946-8fee-d0626ed8b21e-S0/slave.info'
[18:06:25][Step 8/8] I0215 17:06:25.314579  1756 slave.cpp:1030] Forwarding total oversubscribed resources 
[18:06:25][Step 8/8] I0215 17:06:25.314707  1756 master.cpp:4646] Received update of slave 112363e2-c680-4946-8fee-d0626ed8b21e-S0 at slave(393)@172.30.2.239:39785 (ip-172-30-2-239.mesosphere.io) with total oversubscribed resources 
[18:06:25][Step 8/8] I0215 17:06:25.314818  1758 hierarchical.cpp:1498] No inverse offers to send out!
[18:06:25][Step 8/8] I0215 17:06:25.314848  1758 hierarchical.cpp:1116] Performed allocation for slave 112363e2-c680-4946-8fee-d0626ed8b21e-S0 in 654176ns
[18:06:25][Step 8/8] I0215 17:06:25.315137  1758 hierarchical.cpp:531] Slave 112363e2-c680-4946-8fee-d0626ed8b21e-S0 (ip-172-30-2-239.mesosphere.io) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000])
[18:06:25][Step 8/8] I0215 17:06:25.315217  1756 master.cpp:5352] Sending 1 offers to framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 (default) at scheduler-806c70e3-1cf6-418f-aa30-6bb26db42d18@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.315238  1758 hierarchical.cpp:1403] No resources available to allocate!
[18:06:25][Step 8/8] I0215 17:06:25.315268  1758 hierarchical.cpp:1498] No inverse offers to send out!
[18:06:25][Step 8/8] I0215 17:06:25.315285  1758 hierarchical.cpp:1116] Performed allocation for slave 112363e2-c680-4946-8fee-d0626ed8b21e-S0 in 118646ns
[18:06:25][Step 8/8] I0215 17:06:25.315635  1755 sched.cpp:873] Scheduler::resourceOffers took 99802ns
[18:06:25][Step 8/8] I0215 17:06:25.317126  1755 master.cpp:3138] Processing ACCEPT call for offers: [ 112363e2-c680-4946-8fee-d0626ed8b21e-O0 ] on slave 112363e2-c680-4946-8fee-d0626ed8b21e-S0 at slave(393)@172.30.2.239:39785 (ip-172-30-2-239.mesosphere.io) for framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 (default) at scheduler-806c70e3-1cf6-418f-aa30-6bb26db42d18@172.30.2.239:39785
[18:06:25][Step 8/8] I0215 17:06:25.317163  1755 master.cpp:2825] Authorizing framework principal 'test-principal' to launch task 1 as user 'root'
[18:06:25][Step 8/8] I0215 17:06:25.317229  1760 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 3.089068ms
[18:06:25][Step 8/8] I0215 17:06:25.317261  1760 replica.cpp:712] Persisted action at 4
[18:06:25][Step 8/8] I0215 17:06:25.317845  1759 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
[18:06:25][Step 8/8] I0215 17:06:25.318722  1755 master.hpp:176] Adding task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 112363e2-c680-4946-8fee-d0626ed8b21e-S0 (ip-172-30-2-239.mesosphere.io)
[18:06:25][Step 8/8] I0215 17:06:25.318886  1755 master.cpp:3623] Launching task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 (default) at scheduler-806c70e3-1cf6-418f-aa30-6bb26db42d18@172.30.2.239:39785 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 112363e2-c680-4946-8fee-d0626ed8b21e-S0 at slave(393)@172.30.2.239:39785 (ip-172-30-2-239.mesosphere.io)
[18:06:25][Step 8/8] I0215 17:06:25.319195  1757 slave.cpp:1361] Got assigned task 1 for framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:25][Step 8/8] I0215 17:06:25.319305  1759 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 1.430044ms
[18:06:25][Step 8/8] I0215 17:06:25.319349  1757 resources.cpp:576] Parsing resources as JSON failed: cpus:0.1;mem:32
[18:06:25][Step 8/8] Trying semicolon-delimited string format instead
[18:06:25][Step 8/8] I0215 17:06:25.319363  1759 leveldb.cpp:399] Deleting ~2 keys from leveldb took 34738ns
[18:06:25][Step 8/8] I0215 17:06:25.319380  1759 replica.cpp:712] Persisted action at 4
[18:06:25][Step 8/8] I0215 17:06:25.319396  1759 replica.cpp:697] Replica learned TRUNCATE action at position 4
[18:06:25][Step 8/8] I0215 17:06:25.320034  1757 slave.cpp:1480] Launching task 1 for framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:25][Step 8/8] I0215 17:06:25.320127  1757 resources.cpp:576] Parsing resources as JSON failed: cpus:0.1;mem:32
[18:06:25][Step 8/8] Trying semicolon-delimited string format instead
[18:06:25][Step 8/8] I0215 17:06:25.320725  1757 paths.cpp:474] Trying to chown '/tmp/DockerContainerizerTest_ROOT_DOCKER_Logs_a4NS2N/slaves/112363e2-c680-4946-8fee-d0626ed8b21e-S0/frameworks/112363e2-c680-4946-8fee-d0626ed8b21e-0000/executors/1/runs/4f58cd28-7ac7-4960-b3e3-8d28918e6d08' to user 'root'
[18:06:25][Step 8/8] I0215 17:06:25.325739  1757 slave.cpp:5351] Launching executor 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/tmp/DockerContainerizerTest_ROOT_DOCKER_Logs_a4NS2N/slaves/112363e2-c680-4946-8fee-d0626ed8b21e-S0/frameworks/112363e2-c680-4946-8fee-d0626ed8b21e-0000/executors/1/runs/4f58cd28-7ac7-4960-b3e3-8d28918e6d08'
[18:06:25][Step 8/8] I0215 17:06:25.326493  1757 slave.cpp:1698] Queuing task '1' for executor '1' of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:25][Step 8/8] I0215 17:06:25.326633  1757 slave.cpp:749] Successfully attached file '/tmp/DockerContainerizerTest_ROOT_DOCKER_Logs_a4NS2N/slaves/112363e2-c680-4946-8fee-d0626ed8b21e-S0/frameworks/112363e2-c680-4946-8fee-d0626ed8b21e-0000/executors/1/runs/4f58cd28-7ac7-4960-b3e3-8d28918e6d08'
[18:06:25][Step 8/8] I0215 17:06:25.331328  1761 docker.cpp:803] Starting container '4f58cd28-7ac7-4960-b3e3-8d28918e6d08' for task '1' (and executor '1') of framework '112363e2-c680-4946-8fee-d0626ed8b21e-0000'
[18:06:25][Step 8/8] I0215 17:06:25.331699  1761 docker.cpp:1053] Running docker -H unix:///var/run/docker.sock inspect alpine:latest
[18:06:25][Step 8/8] I0215 17:06:25.449668  1758 docker.cpp:384] Docker pull alpine completed
[18:06:25][Step 8/8] I0215 17:06:25.511905  1760 slave.cpp:2643] Got registration for executor '1' of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 from executor(1)@172.30.2.239:38602
[18:06:25][Step 8/8] I0215 17:06:25.513098  1759 docker.cpp:1077] Ignoring updating container '4f58cd28-7ac7-4960-b3e3-8d28918e6d08' with resources passed to update is identical to existing resources
[18:06:25][Step 8/8] I0215 17:06:25.513494  1756 slave.cpp:1863] Sending queued task '1' to executor '1' of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 at executor(1)@172.30.2.239:38602
[18:06:26][Step 8/8] 2016-02-15 17:06:25,981:1740(0x7f870b7fe700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:36716] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
[18:06:26][Step 8/8] I0215 17:06:26.227973  1757 slave.cpp:3002] Handling status update TASK_RUNNING (UUID: b0f4bfb3-0f3c-4b03-843d-b5af82a902a3) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 from executor(1)@172.30.2.239:38602
[18:06:26][Step 8/8] I0215 17:06:26.228302  1757 slave.cpp:3002] Handling status update TASK_FAILED (UUID: 05810f46-10e7-4d50-a83d-d05bf79dd8e2) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000 from executor(1)@172.30.2.239:38602
[18:06:26][Step 8/8] I0215 17:06:26.228734  1754 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: b0f4bfb3-0f3c-4b03-843d-b5af82a902a3) for task 1 of framework 112363e2-c680-4946-8fee-d0626ed8b21e-0000
[18:06:26][Step 8/8] I0215 17:06:26.228790  1754 status_update_manager.cpp:497] Creating StatusUpdate stream for task 1 of framework 112363e2-c680-4",2.0,0.27.0,0.5,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.14102564102564102,0.16969696969696968,0.17575757575757575,0.24770642201834864
Bug,Cannot disable systemd support,"On certain platforms the systemd init system is available, but not used.
Not being able to disable the mesos systemd integration on these platforms makes it hard to operate using a different init / monit system.",1.0,"0.25.0,0.26.0,0.27.0",0.0,0.42533936651583715,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3142857142857143,0.24358974358974358,0.22424242424242424,0.22424242424242424,0.2385321100917431
Bug,Linux filesystem isolator tests are flaky.,"LinuxFilesystemIsolatorTest.ROOT_ImageInVolumeWithRootFilesystem sometimes fails on CentOS 7 with this kind of output:
{noformat}
../../src/tests/containerizer/filesystem_isolator_tests.cpp:1054: Failure
Failed to wait 2mins for launch
{noformat}

LinuxFilesystemIsolatorTest.ROOT_MultipleContainers often has this output:
{noformat}
../../src/tests/containerizer/filesystem_isolator_tests.cpp:1138: Failure
Failed to wait 1mins for launch1
{noformat}

Whether SSL is configured makes no difference.

This test may also fail on other platforms, but more rarely.

",3.0,0.27.0,0.5,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.14102564102564102,0.16969696969696968,0.17575757575757575,0.24770642201834864
Bug,Status updates from executor can be forwarded out of order by the Agent.,"Previously, all status update messages from the executor were forwarded by the agent to the master in the order that they had been received. 

However, that seems to be no longer valid due to a recently introduced change in the agent:

{code}
// Before sending update, we need to retrieve the container status.
  containerizer->status(executor->containerId)
    .onAny(defer(self(),
                 &Slave::_statusUpdate,
                 update,
                 pid,
                 executor->id,
                 lambda::_1));
{code}

This can sometimes lead to status updates being sent out of order depending on the order the {{Future}} is fulfilled from the call to {{status(...)}}.",1.0,0.28.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.2807017543859649,0.39285714285714285,0.3846153846153846,0.4181818181818182,0.4121212121212121,0.25688073394495414
Bug,`cgroup_info` not being exposed in state.json when ComposingContainerizer is used.,"The ComposingContainerizer currently does not have a `status` method. This results in no `ContainerStatus` being updated in the agent, when uses `ComposingContainerizer` to launch containers. This would specifically happen when the agent is launched with `--containerizer=docker,mesos`",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2807017543859649,0.39285714285714285,0.1282051282051282,0.3090909090909091,0.3090909090909091,0.0
Bug,Add common compression utility,We need GZIP uncompress utility for Appc image fetching functionality. The images are tar + gzip'ed and they needs to be first uncompressed so that we can compute sha 512 checksum on it.,2.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3357142857142857,0.14102564102564102,0.2424242424242424,0.2424242424242424,0.0
Improvement,Expose persistent volume information in HTTP endpoints,The per-slave {{reserved_resources}} information returned by {{/state}} does not seem to include information about persistent volumes. This makes it hard for operators to use the {{/destroy-volumes}} endpoint.,3.0,0,0.0,1.0,0.0,0.0,0.0,0.0,0.8,0.5438596491228069,0.39999999999999997,0.48717948717948717,0.4303030303030303,0.4303030303030303,0.0
Task,Document net_cls isolator in docs/mesos-containerizer.md.,We need to add a section in the doc to describe how to use cgroups/net_cls isolator.,1.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.2807017543859649,0.39285714285714285,0.7948717948717948,1.0,1.0,0.0
Improvement,Add LOG(INFO) in `cgroups/net_cls` for debugging allocation of net_cls handles.,We need to add LOG(INFO) during the prepare phase of `cgroups/net_cls` for debugging management of `net_cls` handles within the isolator. ,1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2807017543859649,0.39285714285714285,0.1282051282051282,0.3090909090909091,0.3090909090909091,0.0
Task,"Add allocation metrics for ""gpus"" resources.","Allocation metrics are currently hard-coded to include only {{\[""cpus"", ""mem"", ""disk""\]}} resources. We'll need to add ""gpus"" to the list to start, possibly following up on the TODO to remove the hard-coding.

See:
https://github.com/apache/mesos/blob/0.27.0/src/master/metrics.cpp#L266-L269
https://github.com/apache/mesos/blob/0.27.0/src/slave/metrics.cpp#L123-L126
",1.0,0,0.5,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.0
Task,Add a stub Nvidia GPU isolator.,"We'll first wire up a skeleton Nvidia GPU isolator, which needs to be guarded by a configure flag due to the dependency on NVML.",3.0,0,0.5,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.0
Documentation,Update configuration.md with `--cgroups_net_cls_primary_handle` agent flag.,"As part of the net_cls epic, we introduce an agent flag called `--cgroup_net_cls_primary_handle` . We need to update configuration.md with the corresponding help string. ",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2807017543859649,0.39285714285714285,0.1282051282051282,0.3090909090909091,0.3090909090909091,0.0
Bug,Remove markdown files from doxygen pages,The doxygen html pages corresponding to doc/* markdown files are redundant and have broken links. They don't serve any reasonable purpose in doxygen site.,1.0,0,0.0,0.3453996983408748,0.0,0.0,0.0,0.0,0.2,0.38596491228070173,0.35714285714285715,0.28205128205128205,0.29090909090909095,0.28484848484848485,0.0
Bug,ContainerLoggerTest.DefaultToSandbox is flaky,"Just saw this failure on the ASF CI:

{code}
[ RUN      ] ContainerLoggerTest.DefaultToSandbox
I0206 01:25:03.766458  2824 leveldb.cpp:174] Opened db in 72.979786ms
I0206 01:25:03.811712  2824 leveldb.cpp:181] Compacted db in 45.162067ms
I0206 01:25:03.811810  2824 leveldb.cpp:196] Created db iterator in 26090ns
I0206 01:25:03.811828  2824 leveldb.cpp:202] Seeked to beginning of db in 3173ns
I0206 01:25:03.811839  2824 leveldb.cpp:271] Iterated through 0 keys in the db in 497ns
I0206 01:25:03.811900  2824 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0206 01:25:03.812785  2849 recover.cpp:447] Starting replica recovery
I0206 01:25:03.813043  2849 recover.cpp:473] Replica is in EMPTY status
I0206 01:25:03.814668  2854 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (371)@172.17.0.8:37843
I0206 01:25:03.815210  2849 recover.cpp:193] Received a recover response from a replica in EMPTY status
I0206 01:25:03.815732  2854 recover.cpp:564] Updating replica status to STARTING
I0206 01:25:03.819664  2857 master.cpp:376] Master 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de (74ef606c4063) started on 172.17.0.8:37843
I0206 01:25:03.819703  2857 master.cpp:378] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/h5vu5I/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.28.0/_inst/share/mesos/webui"" --work_dir=""/tmp/h5vu5I/master"" --zk_session_timeout=""10secs""
I0206 01:25:03.820241  2857 master.cpp:423] Master only allowing authenticated frameworks to register
I0206 01:25:03.820257  2857 master.cpp:428] Master only allowing authenticated slaves to register
I0206 01:25:03.820269  2857 credentials.hpp:35] Loading credentials for authentication from '/tmp/h5vu5I/credentials'
I0206 01:25:03.821110  2857 master.cpp:468] Using default 'crammd5' authenticator
I0206 01:25:03.821311  2857 master.cpp:537] Using default 'basic' HTTP authenticator
I0206 01:25:03.821636  2857 master.cpp:571] Authorization enabled
I0206 01:25:03.821979  2846 hierarchical.cpp:144] Initialized hierarchical allocator process
I0206 01:25:03.822057  2846 whitelist_watcher.cpp:77] No whitelist given
I0206 01:25:03.825460  2847 master.cpp:1712] The newly elected leader is master@172.17.0.8:37843 with id 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de
I0206 01:25:03.825512  2847 master.cpp:1725] Elected as the leading master!
I0206 01:25:03.825533  2847 master.cpp:1470] Recovering from registrar
I0206 01:25:03.825835  2847 registrar.cpp:307] Recovering registrar
I0206 01:25:03.848212  2854 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 32.226093ms
I0206 01:25:03.848299  2854 replica.cpp:320] Persisted replica status to STARTING
I0206 01:25:03.848702  2854 recover.cpp:473] Replica is in STARTING status
I0206 01:25:03.850728  2858 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (373)@172.17.0.8:37843
I0206 01:25:03.851230  2854 recover.cpp:193] Received a recover response from a replica in STARTING status
I0206 01:25:03.852018  2854 recover.cpp:564] Updating replica status to VOTING
I0206 01:25:03.881681  2854 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 29.184163ms
I0206 01:25:03.881772  2854 replica.cpp:320] Persisted replica status to VOTING
I0206 01:25:03.882058  2854 recover.cpp:578] Successfully joined the Paxos group
I0206 01:25:03.882258  2854 recover.cpp:462] Recover process terminated
I0206 01:25:03.883076  2854 log.cpp:659] Attempting to start the writer
I0206 01:25:03.885040  2854 replica.cpp:493] Replica received implicit promise request from (374)@172.17.0.8:37843 with proposal 1
I0206 01:25:03.915132  2854 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 29.980589ms
I0206 01:25:03.915215  2854 replica.cpp:342] Persisted promised to 1
I0206 01:25:03.916038  2856 coordinator.cpp:238] Coordinator attempting to fill missing positions
I0206 01:25:03.917659  2856 replica.cpp:388] Replica received explicit promise request from (375)@172.17.0.8:37843 for position 0 with proposal 2
I0206 01:25:03.948698  2856 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 30.974607ms
I0206 01:25:03.948786  2856 replica.cpp:712] Persisted action at 0
I0206 01:25:03.950920  2849 replica.cpp:537] Replica received write request for position 0 from (376)@172.17.0.8:37843
I0206 01:25:03.951011  2849 leveldb.cpp:436] Reading position from leveldb took 44263ns
I0206 01:25:03.982026  2849 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 30.947321ms
I0206 01:25:03.982225  2849 replica.cpp:712] Persisted action at 0
I0206 01:25:03.983867  2849 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I0206 01:25:04.015499  2849 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 30.957888ms
I0206 01:25:04.015591  2849 replica.cpp:712] Persisted action at 0
I0206 01:25:04.015682  2849 replica.cpp:697] Replica learned NOP action at position 0
I0206 01:25:04.016666  2849 log.cpp:675] Writer started with ending position 0
I0206 01:25:04.017881  2855 leveldb.cpp:436] Reading position from leveldb took 56779ns
I0206 01:25:04.018934  2852 registrar.cpp:340] Successfully fetched the registry (0B) in 193.048064ms
I0206 01:25:04.019076  2852 registrar.cpp:439] Applied 1 operations in 38180ns; attempting to update the 'registry'
I0206 01:25:04.020100  2844 log.cpp:683] Attempting to append 170 bytes to the log
I0206 01:25:04.020288  2855 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I0206 01:25:04.021323  2844 replica.cpp:537] Replica received write request for position 1 from (377)@172.17.0.8:37843
I0206 01:25:04.054726  2844 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 33.309419ms
I0206 01:25:04.054818  2844 replica.cpp:712] Persisted action at 1
I0206 01:25:04.055933  2844 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I0206 01:25:04.088142  2844 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 32.116643ms
I0206 01:25:04.088230  2844 replica.cpp:712] Persisted action at 1
I0206 01:25:04.088265  2844 replica.cpp:697] Replica learned APPEND action at position 1
I0206 01:25:04.090070  2856 registrar.cpp:484] Successfully updated the 'registry' in 70.90816ms
I0206 01:25:04.090338  2851 log.cpp:702] Attempting to truncate the log to 1
I0206 01:25:04.090358  2856 registrar.cpp:370] Successfully recovered registrar
I0206 01:25:04.090507  2847 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I0206 01:25:04.090867  2858 master.cpp:1522] Recovered 0 slaves from the Registry (131B) ; allowing 10mins for slaves to re-register
I0206 01:25:04.091449  2858 hierarchical.cpp:171] Skipping recovery of hierarchical allocator: nothing to recover
I0206 01:25:04.092280  2857 replica.cpp:537] Replica received write request for position 2 from (378)@172.17.0.8:37843
I0206 01:25:04.125702  2857 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 33.192265ms
I0206 01:25:04.125804  2857 replica.cpp:712] Persisted action at 2
I0206 01:25:04.127400  2857 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I0206 01:25:04.157727  2857 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 30.268594ms
I0206 01:25:04.157905  2857 leveldb.cpp:399] Deleting ~1 keys from leveldb took 88436ns
I0206 01:25:04.157941  2857 replica.cpp:712] Persisted action at 2
I0206 01:25:04.157984  2857 replica.cpp:697] Replica learned TRUNCATE action at position 2
I0206 01:25:04.166174  2824 containerizer.cpp:149] Using isolation: posix/cpu,posix/mem,filesystem/posix
W0206 01:25:04.166954  2824 backend.cpp:48] Failed to create 'bind' backend: BindBackend requires root privileges
I0206 01:25:04.172008  2844 slave.cpp:193] Slave started on 9)@172.17.0.8:37843
I0206 01:25:04.172046  2844 slave.cpp:194] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""https://auth.docker.io"" --docker_kill_orphans=""true"" --docker_puller_timeout=""60"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.28.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw""
I0206 01:25:04.172569  2844 credentials.hpp:83] Loading credential for authentication from '/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/credential'
I0206 01:25:04.172886  2844 slave.cpp:324] Slave using credential for: test-principal
I0206 01:25:04.173141  2844 resources.cpp:564] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0206 01:25:04.173620  2844 slave.cpp:464] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0206 01:25:04.173686  2844 slave.cpp:472] Slave attributes: [  ]
I0206 01:25:04.173702  2844 slave.cpp:477] Slave hostname: 74ef606c4063
I0206 01:25:04.174816  2847 state.cpp:58] Recovering state from '/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/meta'
I0206 01:25:04.175441  2847 status_update_manager.cpp:200] Recovering status update manager
I0206 01:25:04.175678  2858 containerizer.cpp:397] Recovering containerizer
I0206 01:25:04.177573  2858 provisioner.cpp:245] Provisioner recovery complete
I0206 01:25:04.178231  2847 slave.cpp:4496] Finished recovery
I0206 01:25:04.178834  2847 slave.cpp:4668] Querying resource estimator for oversubscribable resources
I0206 01:25:04.179405  2847 slave.cpp:796] New master detected at master@172.17.0.8:37843
I0206 01:25:04.179500  2847 slave.cpp:859] Authenticating with master master@172.17.0.8:37843
I0206 01:25:04.179525  2847 slave.cpp:864] Using default CRAM-MD5 authenticatee
I0206 01:25:04.179656  2858 status_update_manager.cpp:174] Pausing sending status updates
I0206 01:25:04.179798  2847 slave.cpp:832] Detecting new master
I0206 01:25:04.179891  2852 authenticatee.cpp:121] Creating new client SASL connection
I0206 01:25:04.179916  2847 slave.cpp:4682] Received oversubscribable resources  from the resource estimator
I0206 01:25:04.180286  2847 master.cpp:5523] Authenticating slave(9)@172.17.0.8:37843
I0206 01:25:04.180569  2847 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(32)@172.17.0.8:37843
I0206 01:25:04.181000  2847 authenticator.cpp:98] Creating new server SASL connection
I0206 01:25:04.181315  2847 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0206 01:25:04.181387  2847 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0206 01:25:04.181562  2847 authenticator.cpp:203] Received SASL authentication start
I0206 01:25:04.181648  2847 authenticator.cpp:325] Authentication requires more steps
I0206 01:25:04.181843  2847 authenticatee.cpp:258] Received SASL authentication step
I0206 01:25:04.182034  2853 authenticator.cpp:231] Received SASL authentication step
I0206 01:25:04.182071  2853 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '74ef606c4063' server FQDN: '74ef606c4063' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0206 01:25:04.182093  2853 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0206 01:25:04.182145  2853 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0206 01:25:04.182173  2853 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '74ef606c4063' server FQDN: '74ef606c4063' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0206 01:25:04.182185  2853 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0206 01:25:04.182193  2853 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0206 01:25:04.182211  2853 authenticator.cpp:317] Authentication success
I0206 01:25:04.182333  2849 authenticatee.cpp:298] Authentication success
I0206 01:25:04.182422  2853 master.cpp:5553] Successfully authenticated principal 'test-principal' at slave(9)@172.17.0.8:37843
I0206 01:25:04.182510  2853 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(32)@172.17.0.8:37843
I0206 01:25:04.182945  2849 slave.cpp:927] Successfully authenticated with master master@172.17.0.8:37843
I0206 01:25:04.183178  2849 slave.cpp:1321] Will retry registration in 9.87937ms if necessary
I0206 01:25:04.183466  2852 master.cpp:4237] Registering slave at slave(9)@172.17.0.8:37843 (74ef606c4063) with id 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0
I0206 01:25:04.184039  2845 registrar.cpp:439] Applied 1 operations in 89453ns; attempting to update the 'registry'
I0206 01:25:04.185288  2856 log.cpp:683] Attempting to append 339 bytes to the log
I0206 01:25:04.185672  2850 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I0206 01:25:04.186674  2846 replica.cpp:537] Replica received write request for position 3 from (392)@172.17.0.8:37843
I0206 01:25:04.195863  2856 slave.cpp:1321] Will retry registration in 11.038094ms if necessary
I0206 01:25:04.196233  2856 master.cpp:4225] Ignoring register slave message from slave(9)@172.17.0.8:37843 (74ef606c4063) as admission is already in progress
I0206 01:25:04.208094  2856 slave.cpp:1321] Will retry registration in 27.881223ms if necessary
I0206 01:25:04.208472  2856 master.cpp:4225] Ignoring register slave message from slave(9)@172.17.0.8:37843 (74ef606c4063) as admission is already in progress
I0206 01:25:04.216698  2846 leveldb.cpp:341] Persisting action (358 bytes) to leveldb took 29.961291ms
I0206 01:25:04.216789  2846 replica.cpp:712] Persisted action at 3
I0206 01:25:04.218246  2845 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I0206 01:25:04.237861  2846 slave.cpp:1321] Will retry registration in 1.006941ms if necessary
I0206 01:25:04.238221  2846 master.cpp:4225] Ignoring register slave message from slave(9)@172.17.0.8:37843 (74ef606c4063) as admission is already in progress
I0206 01:25:04.239858  2856 slave.cpp:1321] Will retry registration in 167.305686ms if necessary
I0206 01:25:04.240044  2856 master.cpp:4225] Ignoring register slave message from slave(9)@172.17.0.8:37843 (74ef606c4063) as admission is already in progress
I0206 01:25:04.241482  2845 leveldb.cpp:341] Persisting action (360 bytes) to leveldb took 23.193162ms
I0206 01:25:04.241524  2845 replica.cpp:712] Persisted action at 3
I0206 01:25:04.241557  2845 replica.cpp:697] Replica learned APPEND action at position 3
I0206 01:25:04.243746  2844 registrar.cpp:484] Successfully updated the 'registry' in 59.587072ms
I0206 01:25:04.244210  2857 log.cpp:702] Attempting to truncate the log to 3
I0206 01:25:04.244344  2845 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I0206 01:25:04.244597  2856 master.cpp:4305] Registered slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 at slave(9)@172.17.0.8:37843 (74ef606c4063) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0206 01:25:04.244746  2843 slave.cpp:3436] Received ping from slave-observer(8)@172.17.0.8:37843
I0206 01:25:04.244976  2845 hierarchical.cpp:473] Added slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 (74ef606c4063) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
I0206 01:25:04.245072  2843 slave.cpp:971] Registered with master master@172.17.0.8:37843; given slave ID 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0
I0206 01:25:04.245121  2843 fetcher.cpp:81] Clearing fetcher cache
I0206 01:25:04.245146  2845 hierarchical.cpp:1403] No resources available to allocate!
I0206 01:25:04.245178  2845 hierarchical.cpp:1116] Performed allocation for slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 in 159744ns
I0206 01:25:04.245465  2846 status_update_manager.cpp:181] Resuming sending status updates
I0206 01:25:04.245776  2843 slave.cpp:994] Checkpointing SlaveInfo to '/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/meta/slaves/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0/slave.info'
I0206 01:25:04.245745  2846 replica.cpp:537] Replica received write request for position 4 from (393)@172.17.0.8:37843
I0206 01:25:04.246273  2843 slave.cpp:1030] Forwarding total oversubscribed resources 
I0206 01:25:04.246507  2850 master.cpp:4646] Received update of slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 at slave(9)@172.17.0.8:37843 (74ef606c4063) with total oversubscribed resources 
I0206 01:25:04.247180  2824 sched.cpp:222] Version: 0.28.0
I0206 01:25:04.247155  2850 hierarchical.cpp:531] Slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 (74ef606c4063) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: )
I0206 01:25:04.247357  2850 hierarchical.cpp:1403] No resources available to allocate!
I0206 01:25:04.247406  2850 hierarchical.cpp:1116] Performed allocation for slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 in 183250ns
I0206 01:25:04.247938  2854 sched.cpp:326] New master detected at master@172.17.0.8:37843
I0206 01:25:04.248157  2854 sched.cpp:382] Authenticating with master master@172.17.0.8:37843
I0206 01:25:04.248265  2854 sched.cpp:389] Using default CRAM-MD5 authenticatee
I0206 01:25:04.248769  2854 authenticatee.cpp:121] Creating new client SASL connection
I0206 01:25:04.249311  2854 master.cpp:5523] Authenticating scheduler-f50aad75-78d0-4d9f-b1a4-488d5ab932d6@172.17.0.8:37843
I0206 01:25:04.249646  2854 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(33)@172.17.0.8:37843
I0206 01:25:04.250114  2854 authenticator.cpp:98] Creating new server SASL connection
I0206 01:25:04.250453  2854 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0206 01:25:04.250525  2854 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0206 01:25:04.250814  2853 authenticator.cpp:203] Received SASL authentication start
I0206 01:25:04.250881  2853 authenticator.cpp:325] Authentication requires more steps
I0206 01:25:04.250982  2853 authenticatee.cpp:258] Received SASL authentication step
I0206 01:25:04.251092  2853 authenticator.cpp:231] Received SASL authentication step
I0206 01:25:04.251128  2853 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '74ef606c4063' server FQDN: '74ef606c4063' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0206 01:25:04.251144  2853 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0206 01:25:04.251200  2853 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0206 01:25:04.251242  2853 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '74ef606c4063' server FQDN: '74ef606c4063' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0206 01:25:04.251260  2853 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0206 01:25:04.251269  2853 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0206 01:25:04.251288  2853 authenticator.cpp:317] Authentication success
I0206 01:25:04.251471  2853 authenticatee.cpp:298] Authentication success
I0206 01:25:04.251574  2853 master.cpp:5553] Successfully authenticated principal 'test-principal' at scheduler-f50aad75-78d0-4d9f-b1a4-488d5ab932d6@172.17.0.8:37843
I0206 01:25:04.251669  2851 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(33)@172.17.0.8:37843
I0206 01:25:04.252162  2854 sched.cpp:471] Successfully authenticated with master master@172.17.0.8:37843
I0206 01:25:04.252188  2854 sched.cpp:776] Sending SUBSCRIBE call to master@172.17.0.8:37843
I0206 01:25:04.252286  2854 sched.cpp:809] Will retry registration in 1.575999657secs if necessary
I0206 01:25:04.252583  2853 master.cpp:2280] Received SUBSCRIBE call for framework 'default' at scheduler-f50aad75-78d0-4d9f-b1a4-488d5ab932d6@172.17.0.8:37843
I0206 01:25:04.252694  2853 master.cpp:1751] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0206 01:25:04.253110  2853 master.cpp:2351] Subscribing framework default with checkpointing disabled and capabilities [  ]
I0206 01:25:04.253703  2843 hierarchical.cpp:265] Added framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
I0206 01:25:04.255300  2843 hierarchical.cpp:1498] No inverse offers to send out!
I0206 01:25:04.255367  2843 hierarchical.cpp:1096] Performed allocation for 1 slaves in 1.621522ms
I0206 01:25:04.255820  2844 sched.cpp:703] Framework registered with 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
I0206 01:25:04.256006  2844 sched.cpp:717] Scheduler::registered took 105156ns
I0206 01:25:04.256572  2853 master.cpp:5352] Sending 1 offers to framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 (default) at scheduler-f50aad75-78d0-4d9f-b1a4-488d5ab932d6@172.17.0.8:37843
I0206 01:25:04.257524  2853 sched.cpp:873] Scheduler::resourceOffers took 173470ns
I0206 01:25:04.260818  2855 master.cpp:3138] Processing ACCEPT call for offers: [ 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-O0 ] on slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 at slave(9)@172.17.0.8:37843 (74ef606c4063) for framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 (default) at scheduler-f50aad75-78d0-4d9f-b1a4-488d5ab932d6@172.17.0.8:37843
I0206 01:25:04.260968  2855 master.cpp:2825] Authorizing framework principal 'test-principal' to launch task 0e7267ed-c5ed-4914-9042-5970b2aaec1c as user 'mesos'
I0206 01:25:04.264458  2844 master.hpp:176] Adding task 0e7267ed-c5ed-4914-9042-5970b2aaec1c with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 (74ef606c4063)
I0206 01:25:04.264796  2844 master.cpp:3623] Launching task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 (default) at scheduler-f50aad75-78d0-4d9f-b1a4-488d5ab932d6@172.17.0.8:37843 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 at slave(9)@172.17.0.8:37843 (74ef606c4063)
I0206 01:25:04.265341  2855 slave.cpp:1361] Got assigned task 0e7267ed-c5ed-4914-9042-5970b2aaec1c for framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
I0206 01:25:04.265941  2855 resources.cpp:564] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I0206 01:25:04.267323  2855 slave.cpp:1480] Launching task 0e7267ed-c5ed-4914-9042-5970b2aaec1c for framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
I0206 01:25:04.267627  2855 resources.cpp:564] Parsing resources as JSON failed: cpus:0.1;mem:32
Trying semicolon-delimited string format instead
I0206 01:25:04.268705  2855 paths.cpp:474] Trying to chown '/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/slaves/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0/frameworks/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000/executors/0e7267ed-c5ed-4914-9042-5970b2aaec1c/runs/5c952202-44cf-427a-8452-0f501140a4b7' to user 'mesos'
I0206 01:25:04.274116  2855 slave.cpp:5282] Launching executor 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/slaves/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0/frameworks/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000/executors/0e7267ed-c5ed-4914-9042-5970b2aaec1c/runs/5c952202-44cf-427a-8452-0f501140a4b7'
I0206 01:25:04.275185  2844 containerizer.cpp:656] Starting container '5c952202-44cf-427a-8452-0f501140a4b7' for executor '0e7267ed-c5ed-4914-9042-5970b2aaec1c' of framework '914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000'
I0206 01:25:04.275311  2846 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 29.403837ms
I0206 01:25:04.275390  2846 replica.cpp:712] Persisted action at 4
I0206 01:25:04.275511  2855 slave.cpp:1698] Queuing task '0e7267ed-c5ed-4914-9042-5970b2aaec1c' for executor '0e7267ed-c5ed-4914-9042-5970b2aaec1c' of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
I0206 01:25:04.275832  2855 slave.cpp:749] Successfully attached file '/tmp/ContainerLoggerTest_DefaultToSandbox_FMaKSw/slaves/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0/frameworks/914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000/executors/0e7267ed-c5ed-4914-9042-5970b2aaec1c/runs/5c952202-44cf-427a-8452-0f501140a4b7'
I0206 01:25:04.276707  2855 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I0206 01:25:04.284708  2844 launcher.cpp:132] Forked child with pid '2872' for container '5c952202-44cf-427a-8452-0f501140a4b7'
I0206 01:25:04.301365  2855 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 24.497489ms
I0206 01:25:04.301528  2855 leveldb.cpp:399] Deleting ~2 keys from leveldb took 92156ns
I0206 01:25:04.301563  2855 replica.cpp:712] Persisted action at 4
I0206 01:25:04.301640  2855 replica.cpp:697] Replica learned TRUNCATE action at position 4
I0206 01:25:04.823314  2854 hierarchical.cpp:1403] No resources available to allocate!
I0206 01:25:04.823387  2854 hierarchical.cpp:1498] No inverse offers to send out!
I0206 01:25:04.823420  2854 hierarchical.cpp:1096] Performed allocation for 1 slaves in 327509ns
I0206 01:25:05.825943  2850 hierarchical.cpp:1403] No resources available to allocate!
I0206 01:25:05.826027  2850 hierarchical.cpp:1498] No inverse offers to send out!
I0206 01:25:05.826066  2850 hierarchical.cpp:1096] Performed allocation for 1 slaves in 362856ns
I0206 01:25:06.827154  2857 hierarchical.cpp:1403] No resources available to allocate!
I0206 01:25:06.827235  2857 hierarchical.cpp:1498] No inverse offers to send out!
I0206 01:25:06.827275  2857 hierarchical.cpp:1096] Performed allocation for 1 slaves in 328221ns
I0206 01:25:07.828547  2843 hierarchical.cpp:1403] No resources available to allocate!
I0206 01:25:07.828753  2843 hierarchical.cpp:1498] No inverse offers to send out!
I0206 01:25:07.828907  2843 hierarchical.cpp:1096] Performed allocation for 1 slaves in 624979ns
I0206 01:25:08.829737  2855 hierarchical.cpp:1403] No resources available to allocate!
I0206 01:25:08.829918  2855 hierarchical.cpp:1498] No inverse offers to send out!
I0206 01:25:08.830070  2855 hierarchical.cpp:1096] Performed allocation for 1 slaves in 596793ns
I0206 01:25:09.831233  2856 hierarchical.cpp:1403] No resources available to allocate!
I0206 01:25:09.831316  2856 hierarchical.cpp:1498] No inverse offers to send out!
I0206 01:25:09.831352  2856 hierarchical.cpp:1096] Performed allocation for 1 slaves in 353864ns
I0206 01:25:10.832953  2849 hierarchical.cpp:1403] No resources available to allocate!
I0206 01:25:10.833307  2849 hierarchical.cpp:1498] No inverse offers to send out!
I0206 01:25:10.833411  2849 hierarchical.cpp:1096] Performed allocation for 1 slaves in 731864ns
I0206 01:25:11.834967  2847 hierarchical.cpp:1403] No resources available to allocate!
I0206 01:25:11.835149  2847 hierarchical.cpp:1498] No inverse offers to send out!
I0206 01:25:11.835294  2847 hierarchical.cpp:1096] Performed allocation for 1 slaves in 586988ns
I0206 01:25:12.174247  2853 slave.cpp:2643] Got registration for executor '0e7267ed-c5ed-4914-9042-5970b2aaec1c' of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 from executor(1)@172.17.0.8:43659
I0206 01:25:12.179061  2844 slave.cpp:1863] Sending queued task '0e7267ed-c5ed-4914-9042-5970b2aaec1c' to executor '0e7267ed-c5ed-4914-9042-5970b2aaec1c' of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 at executor(1)@172.17.0.8:43659
I0206 01:25:12.194753  2858 slave.cpp:3002] Handling status update TASK_RUNNING (UUID: 9d924a5b-76ab-4886-8091-7af3428ff179) for task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 from executor(1)@172.17.0.8:43659
I0206 01:25:12.195852  2858 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 9d924a5b-76ab-4886-8091-7af3428ff179) for task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
I0206 01:25:12.196094  2858 status_update_manager.cpp:497] Creating StatusUpdate stream for task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
I0206 01:25:12.197000  2858 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: 9d924a5b-76ab-4886-8091-7af3428ff179) for task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 to the slave
I0206 01:25:12.197739  2855 slave.cpp:3354] Forwarding the update TASK_RUNNING (UUID: 9d924a5b-76ab-4886-8091-7af3428ff179) for task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 to master@172.17.0.8:37843
I0206 01:25:12.198442  2855 master.cpp:4791] Status update TASK_RUNNING (UUID: 9d924a5b-76ab-4886-8091-7af3428ff179) for task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 from slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0 at slave(9)@172.17.0.8:37843 (74ef606c4063)
I0206 01:25:12.198673  2855 master.cpp:4839] Forwarding status update TASK_RUNNING (UUID: 9d924a5b-76ab-4886-8091-7af3428ff179) for task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
I0206 01:25:12.199038  2855 master.cpp:6447] Updating the state of task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I0206 01:25:12.199581  2854 sched.cpp:981] Scheduler::statusUpdate took 159022ns
I0206 01:25:12.200568  2854 master.cpp:3949] Processing ACKNOWLEDGE call 9d924a5b-76ab-4886-8091-7af3428ff179 for task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000 (default) at scheduler-f50aad75-78d0-4d9f-b1a4-488d5ab932d6@172.17.0.8:37843 on slave 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-S0
I0206 01:25:12.201513  2858 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 9d924a5b-76ab-4886-8091-7af3428ff179) for task 0e7267ed-c5ed-4914-9042-5970b2aaec1c of framework 914b62f9-95f6-4c57-a7e3-9b06e2c1c8de-0000
../../src/tests/container_logger_tests.cpp:350: Failure
Value of: strings::contains(stdout.get(), ""Hello World!"")
  Actual: false
Expected: true
I0206 01:25:12.201702  2824 sched.cpp:1903] Asked to stop the driver
I",1.0,0.27.0,0.5,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.24770642201834864
Bug,SlaveRecoveryTest/0.CleanupHTTPExecutor is flaky,"Just saw this failure on the ASF CI:

{code}
[ RUN      ] SlaveRecoveryTest/0.CleanupHTTPExecutor
I0206 00:22:44.791671  2824 leveldb.cpp:174] Opened db in 2.539372ms
I0206 00:22:44.792459  2824 leveldb.cpp:181] Compacted db in 740473ns
I0206 00:22:44.792510  2824 leveldb.cpp:196] Created db iterator in 24164ns
I0206 00:22:44.792532  2824 leveldb.cpp:202] Seeked to beginning of db in 1831ns
I0206 00:22:44.792548  2824 leveldb.cpp:271] Iterated through 0 keys in the db in 342ns
I0206 00:22:44.792605  2824 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0206 00:22:44.793256  2847 recover.cpp:447] Starting replica recovery
I0206 00:22:44.793480  2847 recover.cpp:473] Replica is in EMPTY status
I0206 00:22:44.794538  2847 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (9472)@172.17.0.2:43484
I0206 00:22:44.795040  2848 recover.cpp:193] Received a recover response from a replica in EMPTY status
I0206 00:22:44.795644  2848 recover.cpp:564] Updating replica status to STARTING
I0206 00:22:44.796519  2850 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 752810ns
I0206 00:22:44.796545  2850 replica.cpp:320] Persisted replica status to STARTING
I0206 00:22:44.796725  2848 recover.cpp:473] Replica is in STARTING status
I0206 00:22:44.797828  2857 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (9473)@172.17.0.2:43484
I0206 00:22:44.798355  2850 recover.cpp:193] Received a recover response from a replica in STARTING status
I0206 00:22:44.799193  2850 recover.cpp:564] Updating replica status to VOTING
I0206 00:22:44.799583  2855 master.cpp:376] Master 0b206a40-a9c3-4d44-a5bd-8032d60a32ca (6632562f1ade) started on 172.17.0.2:43484
I0206 00:22:44.799609  2855 master.cpp:378] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/n2FxQV/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""100secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.28.0/_inst/share/mesos/webui"" --work_dir=""/tmp/n2FxQV/master"" --zk_session_timeout=""10secs""
I0206 00:22:44.799991  2855 master.cpp:423] Master only allowing authenticated frameworks to register
I0206 00:22:44.800009  2855 master.cpp:428] Master only allowing authenticated slaves to register
I0206 00:22:44.800020  2855 credentials.hpp:35] Loading credentials for authentication from '/tmp/n2FxQV/credentials'
I0206 00:22:44.800245  2850 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 679345ns
I0206 00:22:44.800370  2850 replica.cpp:320] Persisted replica status to VOTING
I0206 00:22:44.800397  2855 master.cpp:468] Using default 'crammd5' authenticator
I0206 00:22:44.800693  2855 master.cpp:537] Using default 'basic' HTTP authenticator
I0206 00:22:44.800815  2855 master.cpp:571] Authorization enabled
I0206 00:22:44.801216  2850 recover.cpp:578] Successfully joined the Paxos group
I0206 00:22:44.801604  2850 recover.cpp:462] Recover process terminated
I0206 00:22:44.801759  2856 whitelist_watcher.cpp:77] No whitelist given
I0206 00:22:44.801725  2847 hierarchical.cpp:144] Initialized hierarchical allocator process
I0206 00:22:44.803982  2855 master.cpp:1712] The newly elected leader is master@172.17.0.2:43484 with id 0b206a40-a9c3-4d44-a5bd-8032d60a32ca
I0206 00:22:44.804026  2855 master.cpp:1725] Elected as the leading master!
I0206 00:22:44.804059  2855 master.cpp:1470] Recovering from registrar
I0206 00:22:44.804424  2855 registrar.cpp:307] Recovering registrar
I0206 00:22:44.805202  2855 log.cpp:659] Attempting to start the writer
I0206 00:22:44.806782  2856 replica.cpp:493] Replica received implicit promise request from (9475)@172.17.0.2:43484 with proposal 1
I0206 00:22:44.807368  2856 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 547939ns
I0206 00:22:44.807395  2856 replica.cpp:342] Persisted promised to 1
I0206 00:22:44.808375  2856 coordinator.cpp:238] Coordinator attempting to fill missing positions
I0206 00:22:44.809460  2848 replica.cpp:388] Replica received explicit promise request from (9476)@172.17.0.2:43484 for position 0 with proposal 2
I0206 00:22:44.809929  2848 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 427561ns
I0206 00:22:44.809967  2848 replica.cpp:712] Persisted action at 0
I0206 00:22:44.811035  2850 replica.cpp:537] Replica received write request for position 0 from (9477)@172.17.0.2:43484
I0206 00:22:44.811149  2850 leveldb.cpp:436] Reading position from leveldb took 36452ns
I0206 00:22:44.811532  2850 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 318924ns
I0206 00:22:44.811615  2850 replica.cpp:712] Persisted action at 0
I0206 00:22:44.812532  2850 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I0206 00:22:44.813117  2850 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 476530ns
I0206 00:22:44.813143  2850 replica.cpp:712] Persisted action at 0
I0206 00:22:44.813166  2850 replica.cpp:697] Replica learned NOP action at position 0
I0206 00:22:44.813984  2848 log.cpp:675] Writer started with ending position 0
I0206 00:22:44.815549  2848 leveldb.cpp:436] Reading position from leveldb took 31800ns
I0206 00:22:44.817061  2848 registrar.cpp:340] Successfully fetched the registry (0B) in 12.591104ms
I0206 00:22:44.817319  2848 registrar.cpp:439] Applied 1 operations in 63480ns; attempting to update the 'registry'
I0206 00:22:44.818780  2845 log.cpp:683] Attempting to append 170 bytes to the log
I0206 00:22:44.818981  2845 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I0206 00:22:44.819941  2845 replica.cpp:537] Replica received write request for position 1 from (9478)@172.17.0.2:43484
I0206 00:22:44.820582  2845 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 600949ns
I0206 00:22:44.820608  2845 replica.cpp:712] Persisted action at 1
I0206 00:22:44.821552  2845 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I0206 00:22:44.821934  2845 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 352813ns
I0206 00:22:44.821960  2845 replica.cpp:712] Persisted action at 1
I0206 00:22:44.821979  2845 replica.cpp:697] Replica learned APPEND action at position 1
I0206 00:22:44.823447  2845 registrar.cpp:484] Successfully updated the 'registry' in 5.987072ms
I0206 00:22:44.823580  2845 registrar.cpp:370] Successfully recovered registrar
I0206 00:22:44.823833  2845 log.cpp:702] Attempting to truncate the log to 1
I0206 00:22:44.824203  2845 master.cpp:1522] Recovered 0 slaves from the Registry (131B) ; allowing 10mins for slaves to re-register
I0206 00:22:44.824291  2845 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I0206 00:22:44.824645  2845 hierarchical.cpp:171] Skipping recovery of hierarchical allocator: nothing to recover
I0206 00:22:44.825222  2850 replica.cpp:537] Replica received write request for position 2 from (9479)@172.17.0.2:43484
I0206 00:22:44.825742  2850 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 481617ns
I0206 00:22:44.825772  2850 replica.cpp:712] Persisted action at 2
I0206 00:22:44.826748  2852 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I0206 00:22:44.827368  2852 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 588591ns
I0206 00:22:44.827432  2852 leveldb.cpp:399] Deleting ~1 keys from leveldb took 33059ns
I0206 00:22:44.827450  2852 replica.cpp:712] Persisted action at 2
I0206 00:22:44.827468  2852 replica.cpp:697] Replica learned TRUNCATE action at position 2
I0206 00:22:44.838011  2824 containerizer.cpp:149] Using isolation: posix/cpu,posix/mem,filesystem/posix
W0206 00:22:44.838873  2824 backend.cpp:48] Failed to create 'bind' backend: BindBackend requires root privileges
I0206 00:22:44.843785  2857 slave.cpp:193] Slave started on 172.17.0.2:43484
I0206 00:22:44.843819  2857 slave.cpp:194] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""https://auth.docker.io"" --docker_kill_orphans=""true"" --docker_puller_timeout=""60"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.28.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw""
I0206 00:22:44.844292  2857 credentials.hpp:83] Loading credential for authentication from '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/credential'
I0206 00:22:44.844518  2857 slave.cpp:324] Slave using credential for: test-principal
I0206 00:22:44.844696  2857 resources.cpp:564] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0206 00:22:44.845243  2857 slave.cpp:464] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0206 00:22:44.845326  2857 slave.cpp:472] Slave attributes: [  ]
I0206 00:22:44.845342  2857 slave.cpp:477] Slave hostname: 6632562f1ade
I0206 00:22:44.845953  2824 sched.cpp:222] Version: 0.28.0
I0206 00:22:44.846853  2848 sched.cpp:326] New master detected at master@172.17.0.2:43484
I0206 00:22:44.846936  2848 sched.cpp:382] Authenticating with master master@172.17.0.2:43484
I0206 00:22:44.846958  2848 sched.cpp:389] Using default CRAM-MD5 authenticatee
I0206 00:22:44.847692  2858 state.cpp:58] Recovering state from '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/meta'
I0206 00:22:44.848108  2850 status_update_manager.cpp:200] Recovering status update manager
I0206 00:22:44.848325  2852 containerizer.cpp:397] Recovering containerizer
I0206 00:22:44.848603  2845 authenticatee.cpp:121] Creating new client SASL connection
I0206 00:22:44.849719  2845 master.cpp:5523] Authenticating scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484
I0206 00:22:44.850052  2852 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(662)@172.17.0.2:43484
I0206 00:22:44.850227  2854 provisioner.cpp:245] Provisioner recovery complete
I0206 00:22:44.850410  2852 authenticator.cpp:98] Creating new server SASL connection
I0206 00:22:44.850692  2852 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0206 00:22:44.850720  2852 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0206 00:22:44.850805  2852 authenticator.cpp:203] Received SASL authentication start
I0206 00:22:44.850862  2852 authenticator.cpp:325] Authentication requires more steps
I0206 00:22:44.850939  2852 authenticatee.cpp:258] Received SASL authentication step
I0206 00:22:44.851027  2852 authenticator.cpp:231] Received SASL authentication step
I0206 00:22:44.851052  2852 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '6632562f1ade' server FQDN: '6632562f1ade' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0206 00:22:44.851063  2852 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0206 00:22:44.851102  2852 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0206 00:22:44.851121  2852 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '6632562f1ade' server FQDN: '6632562f1ade' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0206 00:22:44.851130  2852 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0206 00:22:44.851136  2852 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0206 00:22:44.851150  2852 authenticator.cpp:317] Authentication success
I0206 00:22:44.851219  2850 authenticatee.cpp:298] Authentication success
I0206 00:22:44.851310  2850 master.cpp:5553] Successfully authenticated principal 'test-principal' at scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484
I0206 00:22:44.851485  2849 slave.cpp:4496] Finished recovery
I0206 00:22:44.852154  2843 sched.cpp:471] Successfully authenticated with master master@172.17.0.2:43484
I0206 00:22:44.852175  2843 sched.cpp:776] Sending SUBSCRIBE call to master@172.17.0.2:43484
I0206 00:22:44.852262  2843 sched.cpp:809] Will retry registration in 939.183679ms if necessary
I0206 00:22:44.852375  2844 master.cpp:2280] Received SUBSCRIBE call for framework 'default' at scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484
I0206 00:22:44.852448  2844 master.cpp:1751] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0206 00:22:44.852699  2852 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(662)@172.17.0.2:43484
I0206 00:22:44.852782  2844 master.cpp:2351] Subscribing framework default with checkpointing enabled and capabilities [  ]
I0206 00:22:44.853056  2849 slave.cpp:4668] Querying resource estimator for oversubscribable resources
I0206 00:22:44.853421  2856 hierarchical.cpp:265] Added framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000
I0206 00:22:44.853513  2856 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:44.853582  2844 sched.cpp:703] Framework registered with 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000
I0206 00:22:44.853613  2852 slave.cpp:4682] Received oversubscribable resources  from the resource estimator
I0206 00:22:44.853663  2844 sched.cpp:717] Scheduler::registered took 53762ns
I0206 00:22:44.853899  2843 slave.cpp:796] New master detected at master@172.17.0.2:43484
I0206 00:22:44.853955  2854 status_update_manager.cpp:174] Pausing sending status updates
I0206 00:22:44.853997  2856 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:44.853960  2843 slave.cpp:859] Authenticating with master master@172.17.0.2:43484
I0206 00:22:44.854035  2843 slave.cpp:864] Using default CRAM-MD5 authenticatee
I0206 00:22:44.854030  2856 hierarchical.cpp:1096] Performed allocation for 0 slaves in 581355ns
I0206 00:22:44.854182  2843 slave.cpp:832] Detecting new master
I0206 00:22:44.854277  2854 authenticatee.cpp:121] Creating new client SASL connection
I0206 00:22:44.854517  2843 master.cpp:5523] Authenticating slave@172.17.0.2:43484
I0206 00:22:44.854603  2854 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(663)@172.17.0.2:43484
I0206 00:22:44.854836  2855 authenticator.cpp:98] Creating new server SASL connection
I0206 00:22:44.855013  2852 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0206 00:22:44.855044  2852 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0206 00:22:44.855139  2855 authenticator.cpp:203] Received SASL authentication start
I0206 00:22:44.855186  2855 authenticator.cpp:325] Authentication requires more steps
I0206 00:22:44.855263  2855 authenticatee.cpp:258] Received SASL authentication step
I0206 00:22:44.855352  2855 authenticator.cpp:231] Received SASL authentication step
I0206 00:22:44.855381  2855 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '6632562f1ade' server FQDN: '6632562f1ade' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0206 00:22:44.855389  2855 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0206 00:22:44.855419  2855 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0206 00:22:44.855438  2855 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '6632562f1ade' server FQDN: '6632562f1ade' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0206 00:22:44.855448  2855 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0206 00:22:44.855453  2855 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0206 00:22:44.855464  2855 authenticator.cpp:317] Authentication success
I0206 00:22:44.855540  2851 authenticatee.cpp:298] Authentication success
I0206 00:22:44.855721  2851 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(663)@172.17.0.2:43484
I0206 00:22:44.855832  2852 slave.cpp:927] Successfully authenticated with master master@172.17.0.2:43484
I0206 00:22:44.855615  2855 master.cpp:5553] Successfully authenticated principal 'test-principal' at slave@172.17.0.2:43484
I0206 00:22:44.855973  2852 slave.cpp:1321] Will retry registration in 9.327708ms if necessary
I0206 00:22:44.856145  2854 master.cpp:4237] Registering slave at slave@172.17.0.2:43484 (6632562f1ade) with id 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0
I0206 00:22:44.856598  2851 registrar.cpp:439] Applied 1 operations in 59112ns; attempting to update the 'registry'
I0206 00:22:44.857403  2851 log.cpp:683] Attempting to append 339 bytes to the log
I0206 00:22:44.857525  2855 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I0206 00:22:44.858482  2844 replica.cpp:537] Replica received write request for position 3 from (9493)@172.17.0.2:43484
I0206 00:22:44.858755  2844 leveldb.cpp:341] Persisting action (358 bytes) to leveldb took 228484ns
I0206 00:22:44.858855  2844 replica.cpp:712] Persisted action at 3
I0206 00:22:44.859751  2852 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I0206 00:22:44.860332  2852 leveldb.cpp:341] Persisting action (360 bytes) to leveldb took 549638ns
I0206 00:22:44.860358  2852 replica.cpp:712] Persisted action at 3
I0206 00:22:44.860411  2852 replica.cpp:697] Replica learned APPEND action at position 3
I0206 00:22:44.862709  2856 registrar.cpp:484] Successfully updated the 'registry' in 6.020864ms
I0206 00:22:44.863106  2850 log.cpp:702] Attempting to truncate the log to 3
I0206 00:22:44.863358  2850 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I0206 00:22:44.864321  2850 slave.cpp:3436] Received ping from slave-observer(288)@172.17.0.2:43484
I0206 00:22:44.864706  2849 hierarchical.cpp:473] Added slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 (6632562f1ade) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
I0206 00:22:44.864716  2843 replica.cpp:537] Replica received write request for position 4 from (9494)@172.17.0.2:43484
I0206 00:22:44.865309  2843 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 410199ns
I0206 00:22:44.865337  2843 replica.cpp:712] Persisted action at 4
I0206 00:22:44.866092  2849 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:44.866132  2848 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I0206 00:22:44.866137  2849 hierarchical.cpp:1116] Performed allocation for slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 in 1.30657ms
I0206 00:22:44.866497  2856 master.cpp:4305] Registered slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 at slave@172.17.0.2:43484 (6632562f1ade) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0206 00:22:44.866564  2843 slave.cpp:1321] Will retry registration in 32.803438ms if necessary
I0206 00:22:44.866690  2843 slave.cpp:971] Registered with master master@172.17.0.2:43484; given slave ID 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0
I0206 00:22:44.866716  2843 fetcher.cpp:81] Clearing fetcher cache
I0206 00:22:44.867066  2856 master.cpp:5352] Sending 1 offers to framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 (default) at scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484
I0206 00:22:44.867105  2843 slave.cpp:994] Checkpointing SlaveInfo to '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/meta/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/slave.info'
I0206 00:22:44.867347  2856 master.cpp:4207] Slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 at slave@172.17.0.2:43484 (6632562f1ade) already registered, resending acknowledgement
I0206 00:22:44.867441  2856 status_update_manager.cpp:181] Resuming sending status updates
I0206 00:22:44.867465  2843 slave.cpp:1030] Forwarding total oversubscribed resources 
W0206 00:22:44.867547  2843 slave.cpp:1016] Already registered with master master@172.17.0.2:43484
I0206 00:22:44.867574  2843 slave.cpp:1030] Forwarding total oversubscribed resources 
I0206 00:22:44.867710  2843 master.cpp:4646] Received update of slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 at slave@172.17.0.2:43484 (6632562f1ade) with total oversubscribed resources 
I0206 00:22:44.867951  2856 sched.cpp:873] Scheduler::resourceOffers took 133371ns
I0206 00:22:44.867961  2843 master.cpp:4646] Received update of slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 at slave@172.17.0.2:43484 (6632562f1ade) with total oversubscribed resources 
I0206 00:22:44.868484  2856 hierarchical.cpp:531] Slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 (6632562f1ade) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000])
I0206 00:22:44.868599  2848 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 2.418545ms
I0206 00:22:44.868700  2848 leveldb.cpp:399] Deleting ~2 keys from leveldb took 54053ns
I0206 00:22:44.868751  2848 replica.cpp:712] Persisted action at 4
I0206 00:22:44.868811  2848 replica.cpp:697] Replica learned TRUNCATE action at position 4
I0206 00:22:44.869241  2856 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:44.869287  2856 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:44.869321  2856 hierarchical.cpp:1116] Performed allocation for slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 in 782848ns
I0206 00:22:44.869840  2856 hierarchical.cpp:531] Slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 (6632562f1ade) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000])
I0206 00:22:44.869985  2856 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:44.870028  2856 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:44.870053  2856 hierarchical.cpp:1116] Performed allocation for slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 in 160104ns
I0206 00:22:44.871824  2853 master.cpp:3138] Processing ACCEPT call for offers: [ 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-O0 ] on slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 at slave@172.17.0.2:43484 (6632562f1ade) for framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 (default) at scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484
I0206 00:22:44.871868  2853 master.cpp:2825] Authorizing framework principal 'test-principal' to launch task 1 as user 'mesos'
W0206 00:22:44.873613  2843 validation.cpp:404] Executor http for task 1 uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases.
W0206 00:22:44.873667  2843 validation.cpp:416] Executor http for task 1 uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases.
I0206 00:22:44.874035  2843 master.hpp:176] Adding task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 (6632562f1ade)
I0206 00:22:44.874223  2843 master.cpp:3623] Launching task 1 of framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 (default) at scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0 at slave@172.17.0.2:43484 (6632562f1ade)
I0206 00:22:44.874802  2843 slave.cpp:1361] Got assigned task 1 for framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000
I0206 00:22:44.874966  2843 slave.cpp:5202] Checkpointing FrameworkInfo to '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/meta/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/framework.info'
I0206 00:22:44.875440  2843 slave.cpp:5213] Checkpointing framework pid 'scheduler-63899759-d7fc-42b2-8371-57484f352895@172.17.0.2:43484' to '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/meta/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/framework.pid'
I0206 00:22:44.876106  2843 slave.cpp:1480] Launching task 1 for framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000
I0206 00:22:44.876644  2843 paths.cpp:474] Trying to chown '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/executors/http/runs/fd4649a4-1c82-4eda-b663-b568b6110d17' to user 'mesos'
I0206 00:22:44.884089  2843 slave.cpp:5654] Checkpointing ExecutorInfo to '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/meta/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/executors/http/executor.info'
I0206 00:22:44.900928  2843 slave.cpp:5282] Launching executor http of framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000 with resources  in work directory '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/executors/http/runs/fd4649a4-1c82-4eda-b663-b568b6110d17'
I0206 00:22:44.901449  2853 containerizer.cpp:656] Starting container 'fd4649a4-1c82-4eda-b663-b568b6110d17' for executor 'http' of framework '0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000'
I0206 00:22:44.901561  2843 slave.cpp:5677] Checkpointing TaskInfo to '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/meta/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/executors/http/runs/fd4649a4-1c82-4eda-b663-b568b6110d17/tasks/1/task.info'
I0206 00:22:44.902060  2843 slave.cpp:1698] Queuing task '1' for executor 'http' of framework 0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000
I0206 00:22:44.902207  2843 slave.cpp:749] Successfully attached file '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/executors/http/runs/fd4649a4-1c82-4eda-b663-b568b6110d17'
I0206 00:22:44.907027  2850 launcher.cpp:132] Forked child with pid '8875' for container 'fd4649a4-1c82-4eda-b663-b568b6110d17'
I0206 00:22:44.907229  2850 containerizer.cpp:1094] Checkpointing executor's forked pid 8875 to '/tmp/SlaveRecoveryTest_0_CleanupHTTPExecutor_kAXwvw/meta/slaves/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-S0/frameworks/0b206a40-a9c3-4d44-a5bd-8032d60a32ca-0000/executors/http/runs/fd4649a4-1c82-4eda-b663-b568b6110d17/pids/forked.pid'
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0206 00:22:45.080060  8875 process.cpp:991] libprocess is initialized on 172.17.0.2:49724 for 16 cpus
I0206 00:22:45.082499  8875 logging.cpp:193] Logging to STDERR
I0206 00:22:45.082862  8875 executor.cpp:172] Version: 0.28.0
I0206 00:22:45.087201  8903 executor.cpp:316] Connected with the agent
I0206 00:22:45.802878  2858 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:45.802969  2858 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:45.803014  2858 hierarchical.cpp:1096] Performed allocation for 1 slaves in 424120ns
2016-02-06 00:22:45,982:2824(0x7fd8c5ffb700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:40712] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
W0206 00:22:46.588022  2854 group.cpp:503] Timed out waiting to connect to ZooKeeper. Forcing ZooKeeper session (sessionId=0) expiration
I0206 00:22:46.588969  2854 group.cpp:519] ZooKeeper session expired
2016-02-06 00:22:46,589:2824(0x7fd9fefd1700):ZOO_INFO@zookeeper_close@2522: Freeing zookeeper resources for sessionId=0

2016-02-06 00:22:46,589:2824(0x7fda03fdb700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5
2016-02-06 00:22:46,589:2824(0x7fda03fdb700):ZOO_INFO@log_env@716: Client environment:host.name=6632562f1ade
2016-02-06 00:22:46,589:2824(0x7fda03fdb700):ZOO_INFO@log_env@723: Client environment:os.name=Linux
2016-02-06 00:22:46,589:2824(0x7fda03fdb700):ZOO_INFO@log_env@724: Client environment:os.arch=3.13.0-36-lowlatency
2016-02-06 00:22:46,589:2824(0x7fda03fdb700):ZOO_INFO@log_env@725: Client environment:os.version=#63-Ubuntu SMP PREEMPT Wed Sep 3 21:56:12 UTC 2014
2016-02-06 00:22:46,589:2824(0x7fda03fdb700):ZOO_INFO@log_env@733: Client environment:user.name=(null)
2016-02-06 00:22:46,589:2824(0x7fda03fdb700):ZOO_INFO@log_env@741: Client environment:user.home=/home/mesos
2016-02-06 00:22:46,589:2824(0x7fda03fdb700):ZOO_INFO@log_env@753: Client environment:user.dir=/tmp/n2FxQV
2016-02-06 00:22:46,590:2824(0x7fda03fdb700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=127.0.0.1:40712 sessionTimeout=10000 watcher=0x7fda10e9e520 sessionId=0 sessionPasswd=<null> context=0x7fd9d401bc10 flags=0
2016-02-06 00:22:46,590:2824(0x7fd8c67fc700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:40712] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
I0206 00:22:46.804400  2844 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:46.804481  2844 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:46.804514  2844 hierarchical.cpp:1096] Performed allocation for 1 slaves in 347954ns
I0206 00:22:47.805842  2847 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:47.805934  2847 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:47.805980  2847 hierarchical.cpp:1096] Performed allocation for 1 slaves in 415449ns
I0206 00:22:48.807723  2851 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:48.807814  2851 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:48.807857  2851 hierarchical.cpp:1096] Performed allocation for 1 slaves in 442104ns
I0206 00:22:49.808733  2848 hierarchical.cpp:1403] No resources available to allocate!
I0206 00:22:49.808816  2848 hierarchical.cpp:1498] No inverse offers to send out!
I0206 00:22:49.808856  2848 hierarchical.cpp:1096] Performed allocation for 1 slaves in 384959ns
2016-02-06 00:22:49,926:2824(0x7fd8c67fc700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:40712] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
I0206 00:22:50.810307  2847 hierarchical.cpp:1403] No resources available to allocate!
I020",3.0,0.27.0,0.5,0.5701357466063348,0.0,0.0,0.0,0.0,0.0,0.6842105263157894,0.5642857142857143,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.24770642201834864
Bug,ROOT_DOCKER_DockerHealthyTask is flaky.,"Log from Teamcity that is running {{sudo ./bin/mesos-tests.sh}} on AWS EC2 instances:
{noformat}
[18:27:14][Step 8/8] [----------] 8 tests from HealthCheckTest
[18:27:14][Step 8/8] [ RUN      ] HealthCheckTest.HealthyTask
[18:27:17][Step 8/8] [       OK ] HealthCheckTest.HealthyTask (2222 ms)
[18:27:17][Step 8/8] [ RUN      ] HealthCheckTest.ROOT_DOCKER_DockerHealthyTask
[18:27:36][Step 8/8] ../../src/tests/health_check_tests.cpp:388: Failure
[18:27:36][Step 8/8] Failed to wait 15secs for termination
[18:27:36][Step 8/8] F0204 18:27:35.981302 23085 logging.cpp:64] RAW: Pure virtual method called
[18:27:36][Step 8/8]     @     0x7f7077055e1c  google::LogMessage::Fail()
[18:27:36][Step 8/8]     @     0x7f707705ba6f  google::RawLog__()
[18:27:36][Step 8/8]     @     0x7f70760f76c9  __cxa_pure_virtual
[18:27:36][Step 8/8]     @           0xa9423c  mesos::internal::tests::Cluster::Slaves::shutdown()
[18:27:36][Step 8/8]     @          0x1074e45  mesos::internal::tests::MesosTest::ShutdownSlaves()
[18:27:36][Step 8/8]     @          0x1074de4  mesos::internal::tests::MesosTest::Shutdown()
[18:27:36][Step 8/8]     @          0x1070ec7  mesos::internal::tests::MesosTest::TearDown()
[18:27:36][Step 8/8]     @          0x16eb7b2  testing::internal::HandleSehExceptionsInMethodIfSupported<>()
[18:27:36][Step 8/8]     @          0x16e61a9  testing::internal::HandleExceptionsInMethodIfSupported<>()
[18:27:36][Step 8/8]     @          0x16c56aa  testing::Test::Run()
[18:27:36][Step 8/8]     @          0x16c5e89  testing::TestInfo::Run()
[18:27:36][Step 8/8]     @          0x16c650a  testing::TestCase::Run()
[18:27:36][Step 8/8]     @          0x16cd1f6  testing::internal::UnitTestImpl::RunAllTests()
[18:27:36][Step 8/8]     @          0x16ec513  testing::internal::HandleSehExceptionsInMethodIfSupported<>()
[18:27:36][Step 8/8]     @          0x16e6df1  testing::internal::HandleExceptionsInMethodIfSupported<>()
[18:27:36][Step 8/8]     @          0x16cbe26  testing::UnitTest::Run()
[18:27:36][Step 8/8]     @           0xe54c84  RUN_ALL_TESTS()
[18:27:36][Step 8/8]     @           0xe54867  main
[18:27:36][Step 8/8]     @     0x7f7071560a40  (unknown)
[18:27:36][Step 8/8]     @           0x9b52d9  _start
[18:27:36][Step 8/8] Aborted (core dumped)
[18:27:36][Step 8/8] Process exited with code 134
{noformat}
Happens with Ubuntu 15.04, CentOS 6, CentOS 7 _quite_ often. ",2.0,0,0.5,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.28205128205128205,0.296969696969697,0.296969696969697,0.0
Bug,Logrotate ContainerLogger should not remove IP from environment.,"The {{LogrotateContainerLogger}} starts libprocess-using subprocesses.  Libprocess initialization will attempt to resolve the IP from the hostname.  If a DNS service is not available, this step will fail, which terminates the logger subprocess prematurely.

Since the logger subprocesses live on the agent, they should use the same {{LIBPROCESS_IP}} supplied to the agent.",1.0,0.27.0,0.0,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.24770642201834864
Task,Add common Appc spec utilities.," Add common utility functions such as :
      - validating image information against actual data in the image directory.
      - getting list of dependencies at depth 1 for an image.
      - getting image path simple image discovery.
",2.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3357142857142857,0.14102564102564102,0.2424242424242424,0.2424242424242424,0.0
Bug,`/reserve` and `/create-volumes` endpoints allow operations for any role,"When frameworks reserve resources, the validation of the operation ensures that the {{role}} of the reservation matches the {{role}} of the framework. For the case of the {{/reserve}} operator endpoint, however, the operator has no role to validate, so this check isn't performed.

This means that if an ACL exists which authorizes a framework's principal to reserve resources, that same principal can be used to reserve resources for _any_ role through the operator endpoint.

We should restrict reservations made through the operator endpoint to specified roles. A few possibilities:
* The {{object}} of the {{reserve_resources}} ACL could be changed from {{resources}} to {{roles}}
* A second ACL could be added for authorization of {{reserve}} operations, with an {{object}} of {{role}}
* Our conception of the {{resources}} object in the {{reserve_resources}} ACL could be expanded to include role information, i.e., {{disk(role1);mem(role1)}}",3.0,0.27.0,0.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.24770642201834864
Task,"Add test case for reservations with same role, different principals",We don't have a test case that covers $SUBJECT; we probably should.,2.0,0,0.0,1.0,0.0,0.0,0.0,0.0,0.8,0.5438596491228069,0.39999999999999997,0.48717948717948717,0.4303030303030303,0.4303030303030303,0.0
Bug,Update Rakefile for mesos site generation,The stuff in site/ directory needs some updates to make it easier to generate updates for mesos.apache.org site.,2.0,0,0.0,0.3453996983408748,0.0,0.0,0.0,0.0,0.2,0.38596491228070173,0.35714285714285715,0.28205128205128205,0.29090909090909095,0.28484848484848485,0.0
Bug,Rename `examples/event_call_framework.cpp` to `examples/test_http_framework.cpp`,We already have {{examples/test_framework.cpp}} for testing {{PID}} based frameworks. We would ideally want to rename {{event_call_framework}} to correctly reflect that it's an example for HTTP based framework.,1.0,0,0.5,0.03167420814479638,0.3333333333333333,0.5,0.14285714285714285,0.0,0.0,0.05263157894736842,0.049999999999999996,0.3846153846153846,0.4181818181818182,0.4121212121212121,0.0
Bug,"state.json serving duplicate ""active"" fields","state.json is serving duplicate ""active"" fields in frameworks.  See the framework ""47df96c2-3f85-4bc5-b781-709b2c30c752-0000"" In the attached file",1.0,0.27.0,0.5,0.6726998491704375,0.0,0.0,0.0,0.1,0.6000000000000001,0.2807017543859649,0.29285714285714287,0.02564102564102564,0.006060606060606061,0.006060606060606061,0.24770642201834864
Improvement,"Introduce a stout helper for ""which""","We may want to add a helper to {{stout/os.hpp}} that will natively emulate the functionality of the Linux utility {{which}}.  i.e.
{code}
Option<string> which(const string& command)
{
  Option<string> path = os::getenv(""PATH"");

  // Loop through path and return the first one which os::exists(...).

  return None();
}
{code}

This helper may be useful:
* for test filters in {{src/tests/environment.cpp}}
* a few tests in {{src/tests/containerizer/port_mapping_tests.cpp}}
* the {{sha512}} utility in {{src/common/command_utils.cpp}}
* as runtime checks in the {{LogrotateContainerLogger}}
* etc.",2.0,0,0.5,0.3137254901960784,0.0,0.0,0.0,0.0,0.4,0.19298245614035087,0.14285714285714285,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.0
Improvement,Fix Appc image caching to share with image fetcher,"As Appc image fetcher is being developed, Image cache needs to be shared between store and the image fetcher.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3357142857142857,0.14102564102564102,0.2424242424242424,0.2424242424242424,0.0
Task,Deprecate TASK_STARTING state,"We currently have the following task stages:

* TASK_STAGING -> set by slave
* TASK_STARTING -> set by the executor (?)
* TASK_RUNNING -> set by the executor when the task is running 
* TASK_XXX -> task termination statuses

The confusion here is about TASK_STARTING. This is the state between TASK_STAGING and TASK_RUNNING and is somewhat non-intuitive for the reader. Further, looks like no where in the source code, we are setting the TASK_STARTING state.

Why shouldn't we just deprecate/remove it?",2.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.28205128205128205,0.29090909090909095,0.28484848484848485,0.0
Improvement,Avoid unnecessary temporary `std::string` constructions and copies in `jsonify`.,"A few of the critical code paths in {{jsonify}} involve unnecessary temporary string construction and copies (inherited from the {{JSON::*}}). For example, {{strings::trim}} is used to remove trailing 0s from printing {{double}}. We print {{double}} a lot, and therefore constructing a temporary {{std::string}} on printing of every double is extremely costly. This ticket captures the work involved in avoiding them.",1.0,0,0.0,0.6726998491704375,0.0,0.0,0.0,0.1,0.6000000000000001,0.2807017543859649,0.29285714285714287,0.21794871794871795,0.26666666666666666,0.26666666666666666,0.0
Improvement,Separate Appc protobuf messages to its own file.,It would be cleaner to keep the Appc protobuf messages separate from other mesos messages.,2.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3357142857142857,0.14102564102564102,0.2424242424242424,0.2424242424242424,0.0
Bug,"Mesos UI shows wrong count for ""started"" tasks","The task started field shows the number of tasks in state ""TASKS_STARTING"" as opposed to those in ""TASK_RUNNING"" state.",2.0,0,0.0,0.3453996983408748,0.0,0.0,0.0,0.0,0.2,0.38596491228070173,0.35714285714285715,0.28205128205128205,0.29090909090909095,0.28484848484848485,0.0
Bug,Investigate test suite crashes after ZK socket disconnections.,"Showed up on ASF CI:
https://builds.apache.org/job/Mesos/COMPILER=clang,CONFIGURATION=--verbose%20--enable-libevent%20--enable-ssl,OS=ubuntu:14.04,label_exp=docker%7C%7CHadoop/1579/console

The test crashed with the following logs:
{code}
[ RUN      ] ContentType/ExecutorHttpApiTest.DefaultAccept/1
I0129 02:00:35.137161 31926 leveldb.cpp:174] Opened db in 118.902333ms
I0129 02:00:35.187021 31926 leveldb.cpp:181] Compacted db in 49.836241ms
I0129 02:00:35.187088 31926 leveldb.cpp:196] Created db iterator in 33825ns
I0129 02:00:35.187109 31926 leveldb.cpp:202] Seeked to beginning of db in 7965ns
I0129 02:00:35.187121 31926 leveldb.cpp:271] Iterated through 0 keys in the db in 6350ns
I0129 02:00:35.187165 31926 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0129 02:00:35.188433 31950 recover.cpp:447] Starting replica recovery
I0129 02:00:35.188796 31950 recover.cpp:473] Replica is in EMPTY status
I0129 02:00:35.190021 31949 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (11817)@172.17.0.3:60904
I0129 02:00:35.190569 31958 recover.cpp:193] Received a recover response from a replica in EMPTY status
I0129 02:00:35.190994 31959 recover.cpp:564] Updating replica status to STARTING
I0129 02:00:35.191522 31953 master.cpp:374] Master 823f2212-bf28-4dd6-959d-796029d32afb (90665f991b70) started on 172.17.0.3:60904
I0129 02:00:35.191640 31953 master.cpp:376] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/B9O6zq/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""25secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.28.0/_inst/share/mesos/webui"" --work_dir=""/tmp/B9O6zq/master"" --zk_session_timeout=""10secs""
I0129 02:00:35.191926 31953 master.cpp:421] Master only allowing authenticated frameworks to register
I0129 02:00:35.191936 31953 master.cpp:426] Master only allowing authenticated slaves to register
I0129 02:00:35.191943 31953 credentials.hpp:35] Loading credentials for authentication from '/tmp/B9O6zq/credentials'
I0129 02:00:35.192229 31953 master.cpp:466] Using default 'crammd5' authenticator
I0129 02:00:35.192366 31953 master.cpp:535] Using default 'basic' HTTP authenticator
I0129 02:00:35.192530 31953 master.cpp:569] Authorization enabled
I0129 02:00:35.192719 31950 whitelist_watcher.cpp:77] No whitelist given
I0129 02:00:35.192756 31957 hierarchical.cpp:144] Initialized hierarchical allocator process
I0129 02:00:35.194291 31955 master.cpp:1710] The newly elected leader is master@172.17.0.3:60904 with id 823f2212-bf28-4dd6-959d-796029d32afb
I0129 02:00:35.194335 31955 master.cpp:1723] Elected as the leading master!
I0129 02:00:35.194350 31955 master.cpp:1468] Recovering from registrar
I0129 02:00:35.194545 31958 registrar.cpp:307] Recovering registrar
I0129 02:00:35.220226 31948 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 29.150097ms
I0129 02:00:35.220262 31948 replica.cpp:320] Persisted replica status to STARTING
I0129 02:00:35.220484 31959 recover.cpp:473] Replica is in STARTING status
I0129 02:00:35.221220 31954 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (11819)@172.17.0.3:60904
I0129 02:00:35.221539 31959 recover.cpp:193] Received a recover response from a replica in STARTING status
I0129 02:00:35.221871 31954 recover.cpp:564] Updating replica status to VOTING
I0129 02:00:35.245329 31949 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 23.326002ms
I0129 02:00:35.245367 31949 replica.cpp:320] Persisted replica status to VOTING
I0129 02:00:35.245522 31955 recover.cpp:578] Successfully joined the Paxos group
I0129 02:00:35.245800 31955 recover.cpp:462] Recover process terminated
I0129 02:00:35.246181 31951 log.cpp:659] Attempting to start the writer
I0129 02:00:35.247228 31953 replica.cpp:493] Replica received implicit promise request from (11820)@172.17.0.3:60904 with proposal 1
I0129 02:00:35.270472 31953 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 23.225846ms
I0129 02:00:35.270510 31953 replica.cpp:342] Persisted promised to 1
I0129 02:00:35.271306 31957 coordinator.cpp:238] Coordinator attempting to fill missing positions
I0129 02:00:35.272373 31949 replica.cpp:388] Replica received explicit promise request from (11821)@172.17.0.3:60904 for position 0 with proposal 2
I0129 02:00:35.295600 31949 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 23.181008ms
I0129 02:00:35.295639 31949 replica.cpp:712] Persisted action at 0
I0129 02:00:35.296815 31950 replica.cpp:537] Replica received write request for position 0 from (11822)@172.17.0.3:60904
I0129 02:00:35.296879 31950 leveldb.cpp:436] Reading position from leveldb took 43203ns
I0129 02:00:35.320659 31950 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 23.753935ms
I0129 02:00:35.320699 31950 replica.cpp:712] Persisted action at 0
I0129 02:00:35.321394 31950 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I0129 02:00:35.345837 31950 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 24.358655ms
I0129 02:00:35.345877 31950 replica.cpp:712] Persisted action at 0
I0129 02:00:35.345898 31950 replica.cpp:697] Replica learned NOP action at position 0
I0129 02:00:35.346683 31950 log.cpp:675] Writer started with ending position 0
I0129 02:00:35.347913 31957 leveldb.cpp:436] Reading position from leveldb took 55621ns
I0129 02:00:35.349047 31947 registrar.cpp:340] Successfully fetched the registry (0B) in 154.395904ms
I0129 02:00:35.349185 31947 registrar.cpp:439] Applied 1 operations in 46347ns; attempting to update the 'registry'
I0129 02:00:35.350008 31952 log.cpp:683] Attempting to append 170 bytes to the log
I0129 02:00:35.350132 31957 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I0129 02:00:35.351042 31953 replica.cpp:537] Replica received write request for position 1 from (11823)@172.17.0.3:60904
I0129 02:00:35.370906 31953 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 19.829257ms
I0129 02:00:35.370946 31953 replica.cpp:712] Persisted action at 1
I0129 02:00:35.371840 31952 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I0129 02:00:35.396082 31952 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 24.218894ms
I0129 02:00:35.396122 31952 replica.cpp:712] Persisted action at 1
I0129 02:00:35.396144 31952 replica.cpp:697] Replica learned APPEND action at position 1
I0129 02:00:35.397250 31954 registrar.cpp:484] Successfully updated the 'registry' in 47.99104ms
I0129 02:00:35.397452 31954 registrar.cpp:370] Successfully recovered registrar
I0129 02:00:35.397678 31946 log.cpp:702] Attempting to truncate the log to 1
I0129 02:00:35.397881 31956 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I0129 02:00:35.398066 31951 master.cpp:1520] Recovered 0 slaves from the Registry (131B) ; allowing 10mins for slaves to re-register
I0129 02:00:35.398111 31957 hierarchical.cpp:171] Skipping recovery of hierarchical allocator: nothing to recover
I0129 02:00:35.398982 31955 replica.cpp:537] Replica received write request for position 2 from (11824)@172.17.0.3:60904
I0129 02:00:35.421293 31955 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 22.286476ms
I0129 02:00:35.421339 31955 replica.cpp:712] Persisted action at 2
I0129 02:00:35.422046 31944 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I0129 02:00:35.446316 31944 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 24.246177ms
I0129 02:00:35.446406 31944 leveldb.cpp:399] Deleting ~1 keys from leveldb took 84415ns
I0129 02:00:35.446466 31944 replica.cpp:712] Persisted action at 2
I0129 02:00:35.446491 31944 replica.cpp:697] Replica learned TRUNCATE action at position 2
I0129 02:00:35.452579 31957 slave.cpp:192] Slave started on 372)@172.17.0.3:60904
I0129 02:00:35.452620 31957 slave.cpp:193] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/ContentType_ExecutorHttpApiTest_DefaultAccept_1_r4GUhM/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""https://auth.docker.io"" --docker_kill_orphans=""true"" --docker_puller_timeout=""60"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/ContentType_ExecutorHttpApiTest_DefaultAccept_1_r4GUhM/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.28.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/ContentType_ExecutorHttpApiTest_DefaultAccept_1_r4GUhM""
I0129 02:00:35.453012 31957 credentials.hpp:83] Loading credential for authentication from '/tmp/ContentType_ExecutorHttpApiTest_DefaultAccept_1_r4GUhM/credential'
I0129 02:00:35.453191 31957 slave.cpp:323] Slave using credential for: test-principal
I0129 02:00:35.453368 31957 resources.cpp:564] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0129 02:00:35.453853 31957 slave.cpp:463] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0129 02:00:35.453938 31957 slave.cpp:471] Slave attributes: [  ]
I0129 02:00:35.453953 31957 slave.cpp:476] Slave hostname: 90665f991b70
I0129 02:00:35.454794 31950 state.cpp:58] Recovering state from '/tmp/ContentType_ExecutorHttpApiTest_DefaultAccept_1_r4GUhM/meta'
I0129 02:00:35.455080 31948 status_update_manager.cpp:200] Recovering status update manager
I0129 02:00:35.455225 31926 sched.cpp:222] Version: 0.28.0
I0129 02:00:35.455535 31956 slave.cpp:4495] Finished recovery
I0129 02:00:35.455798 31945 sched.cpp:326] New master detected at master@172.17.0.3:60904
I0129 02:00:35.455879 31945 sched.cpp:382] Authenticating with master master@172.17.0.3:60904
I0129 02:00:35.455904 31945 sched.cpp:389] Using default CRAM-MD5 authenticatee
I0129 02:00:35.455943 31956 slave.cpp:4667] Querying resource estimator for oversubscribable resources
I0129 02:00:35.456167 31950 authenticatee.cpp:121] Creating new client SASL connection
I0129 02:00:35.456218 31953 status_update_manager.cpp:174] Pausing sending status updates
I0129 02:00:35.456219 31956 slave.cpp:795] New master detected at master@172.17.0.3:60904
I0129 02:00:35.456298 31956 slave.cpp:858] Authenticating with master master@172.17.0.3:60904
I0129 02:00:35.456323 31956 slave.cpp:863] Using default CRAM-MD5 authenticatee
I0129 02:00:35.456490 31948 authenticatee.cpp:121] Creating new client SASL connection
I0129 02:00:35.456492 31956 slave.cpp:831] Detecting new master
I0129 02:00:35.456588 31946 master.cpp:5521] Authenticating scheduler-93e745f0-0e48-4a8f-b227-93569976c5e8@172.17.0.3:60904
I0129 02:00:35.456686 31956 slave.cpp:4681] Received oversubscribable resources  from the resource estimator
I0129 02:00:35.456805 31953 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(804)@172.17.0.3:60904
I0129 02:00:35.456878 31946 master.cpp:5521] Authenticating slave(372)@172.17.0.3:60904
I0129 02:00:35.457124 31953 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(805)@172.17.0.3:60904
I0129 02:00:35.457157 31948 authenticator.cpp:98] Creating new server SASL connection
I0129 02:00:35.457373 31946 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0129 02:00:35.457381 31951 authenticator.cpp:98] Creating new server SASL connection
I0129 02:00:35.457491 31946 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0129 02:00:35.457598 31946 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0129 02:00:35.457612 31951 authenticator.cpp:203] Received SASL authentication start
I0129 02:00:35.457635 31946 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0129 02:00:35.457680 31951 authenticator.cpp:325] Authentication requires more steps
I0129 02:00:35.457767 31954 authenticator.cpp:203] Received SASL authentication start
I0129 02:00:35.457768 31948 authenticatee.cpp:258] Received SASL authentication step
I0129 02:00:35.457830 31954 authenticator.cpp:325] Authentication requires more steps
I0129 02:00:35.457885 31948 authenticator.cpp:231] Received SASL authentication step
I0129 02:00:35.457918 31948 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '90665f991b70' server FQDN: '90665f991b70' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0129 02:00:35.457933 31948 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0129 02:00:35.457954 31959 authenticatee.cpp:258] Received SASL authentication step
I0129 02:00:35.457993 31948 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0129 02:00:35.458031 31948 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '90665f991b70' server FQDN: '90665f991b70' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0129 02:00:35.458050 31948 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0129 02:00:35.458065 31948 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0129 02:00:35.458096 31948 authenticator.cpp:317] Authentication success
I0129 02:00:35.458112 31944 authenticator.cpp:231] Received SASL authentication step
I0129 02:00:35.458142 31944 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '90665f991b70' server FQDN: '90665f991b70' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0129 02:00:35.458173 31944 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0129 02:00:35.458206 31954 authenticatee.cpp:298] Authentication success
I0129 02:00:35.458256 31957 master.cpp:5551] Successfully authenticated principal 'test-principal' at scheduler-93e745f0-0e48-4a8f-b227-93569976c5e8@172.17.0.3:60904
I0129 02:00:35.458206 31944 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0129 02:00:35.458360 31944 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '90665f991b70' server FQDN: '90665f991b70' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0129 02:00:35.458382 31944 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0129 02:00:35.458397 31944 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0129 02:00:35.458489 31944 authenticator.cpp:317] Authentication success
I0129 02:00:35.458623 31953 sched.cpp:471] Successfully authenticated with master master@172.17.0.3:60904
I0129 02:00:35.458649 31953 sched.cpp:780] Sending SUBSCRIBE call to master@172.17.0.3:60904
I0129 02:00:35.458653 31956 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(804)@172.17.0.3:60904
I0129 02:00:35.458673 31951 authenticatee.cpp:298] Authentication success
I0129 02:00:35.458709 31952 master.cpp:5551] Successfully authenticated principal 'test-principal' at slave(372)@172.17.0.3:60904
I0129 02:00:35.458906 31955 slave.cpp:926] Successfully authenticated with master master@172.17.0.3:60904
I0129 02:00:35.458983 31956 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(805)@172.17.0.3:60904
I0129 02:00:35.459033 31955 slave.cpp:1320] Will retry registration in 7.075135ms if necessary
I0129 02:00:35.459128 31953 sched.cpp:813] Will retry registration in 86.579738ms if necessary
I0129 02:00:35.459193 31950 master.cpp:4235] Registering slave at slave(372)@172.17.0.3:60904 (90665f991b70) with id 823f2212-bf28-4dd6-959d-796029d32afb-S0
I0129 02:00:35.459489 31950 master.cpp:2278] Received SUBSCRIBE call for framework 'default' at scheduler-93e745f0-0e48-4a8f-b227-93569976c5e8@172.17.0.3:60904
I0129 02:00:35.459513 31950 master.cpp:1749] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0129 02:00:35.459516 31959 registrar.cpp:439] Applied 1 operations in 62499ns; attempting to update the 'registry'
I0129 02:00:35.459766 31956 master.cpp:2349] Subscribing framework default with checkpointing disabled and capabilities [  ]
I0129 02:00:35.460095 31955 log.cpp:683] Attempting to append 339 bytes to the log
I0129 02:00:35.460192 31948 hierarchical.cpp:265] Added framework 823f2212-bf28-4dd6-959d-796029d32afb-0000
I0129 02:00:35.460247 31956 sched.cpp:707] Framework registered with 823f2212-bf28-4dd6-959d-796029d32afb-0000
I0129 02:00:35.460314 31958 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I0129 02:00:35.460388 31948 hierarchical.cpp:1403] No resources available to allocate!
I0129 02:00:35.460449 31948 hierarchical.cpp:1498] No inverse offers to send out!
I0129 02:00:35.460402 31956 sched.cpp:721] Scheduler::registered took 136519ns
I0129 02:00:35.460482 31948 hierarchical.cpp:1096] Performed allocation for 0 slaves in 158218ns
I0129 02:00:35.461187 31944 replica.cpp:537] Replica received write request for position 3 from (11829)@172.17.0.3:60904
I0129 02:00:35.467929 31954 slave.cpp:1320] Will retry registration in 14.701381ms if necessary
I0129 02:00:35.468183 31952 master.cpp:4223] Ignoring register slave message from slave(372)@172.17.0.3:60904 (90665f991b70) as admission is already in progress
I0129 02:00:35.483300 31959 slave.cpp:1320] Will retry registration in 8.003223ms if necessary
I0129 02:00:35.483500 31946 master.cpp:4223] Ignoring register slave message from slave(372)@172.17.0.3:60904 (90665f991b70) as admission is already in progress
I0129 02:00:35.491843 31945 slave.cpp:1320] Will retry registration in 52.952447ms if necessary
I0129 02:00:35.491962 31948 master.cpp:4223] Ignoring register slave message from slave(372)@172.17.0.3:60904 (90665f991b70) as admission is already in progress
I0129 02:00:35.503868 31944 leveldb.cpp:341] Persisting action (358 bytes) to leveldb took 42.66008ms
I0129 02:00:35.503917 31944 replica.cpp:712] Persisted action at 3
I0129 02:00:35.504838 31953 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I0129 02:00:35.545286 31955 slave.cpp:1320] Will retry registration in 298.440134ms if necessary
I0129 02:00:35.545500 31957 master.cpp:4223] Ignoring register slave message from slave(372)@172.17.0.3:60904 (90665f991b70) as admission is already in progress
I0129 02:00:35.545524 31953 leveldb.cpp:341] Persisting action (360 bytes) to leveldb took 40.663886ms
I0129 02:00:35.545560 31953 replica.cpp:712] Persisted action at 3
I0129 02:00:35.545584 31953 replica.cpp:697] Replica learned APPEND action at position 3
I0129 02:00:35.547586 31945 registrar.cpp:484] Successfully updated the 'registry' in 87.995136ms
I0129 02:00:35.547767 31949 log.cpp:702] Attempting to truncate the log to 3
I0129 02:00:35.547906 31954 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I0129 02:00:35.548713 31945 slave.cpp:3435] Received ping from slave-observer(343)@172.17.0.3:60904
I0129 02:00:35.549018 31957 replica.cpp:537] Replica received write request for position 4 from (11830)@172.17.0.3:60904
I0129 02:00:35.549124 31956 slave.cpp:970] Registered with master master@172.17.0.3:60904; given slave ID 823f2212-bf28-4dd6-959d-796029d32afb-S0
I0129 02:00:35.549049 31953 master.cpp:4303] Registered slave 823f2212-bf28-4dd6-959d-796029d32afb-S0 at slave(372)@172.17.0.3:60904 (90665f991b70) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0129 02:00:35.549175 31956 fetcher.cpp:81] Clearing fetcher cache
I0129 02:00:35.549362 31954 status_update_manager.cpp:181] Resuming sending status updates
I0129 02:00:35.549350 31959 hierarchical.cpp:473] Added slave 823f2212-bf28-4dd6-959d-796029d32afb-S0 (90665f991b70) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
I0129 02:00:35.549720 31956 slave.cpp:993] Checkpointing SlaveInfo to '/tmp/ContentType_ExecutorHttpApiTest_DefaultAccept_1_r4GUhM/meta/slaves/823f2212-bf28-4dd6-959d-796029d32afb-S0/slave.info'
I0129 02:00:35.550135 31956 slave.cpp:1029] Forwarding total oversubscribed resources 
I0129 02:00:35.550341 31949 master.cpp:4644] Received update of slave 823f2212-bf28-4dd6-959d-796029d32afb-S0 at slave(372)@172.17.0.3:60904 (90665f991b70) with total oversubscribed resources 
I0129 02:00:35.550400 31959 hierarchical.cpp:1498] No inverse offers to send out!
I0129 02:00:35.550475 31959 hierarchical.cpp:1116] Performed allocation for slave 823f2212-bf28-4dd6-959d-796029d32afb-S0 in 1.046149ms
I0129 02:00:35.550946 31956 hierarchical.cpp:531] Slave 823f2212-bf28-4dd6-959d-796029d32afb-S0 (90665f991b70) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000])
I0129 02:00:35.550976 31949 master.cpp:5350] Sending 1 offers to framework 823f2212-bf28-4dd6-959d-796029d32afb-0000 (default) at scheduler-93e745f0-0e48-4a8f-b227-93569976c5e8@172.17.0.3:60904
I0129 02:00:35.551132 31956 hierarchical.cpp:1403] No resources available to allocate!
I0129 02:00:35.551187 31956 hierarchical.cpp:1498] No inverse offers to send out!
I0129 02:00:35.551225 31956 hierarchical.cpp:1116] Performed allocation for slave 823f2212-bf28-4dd6-959d-796029d32afb-S0 in 229801ns
I0129 02:00:35.551635 31951 sched.cpp:877] Scheduler::resourceOffers took 155532ns
I0129 02:00:35.553310 31944 master.cpp:3136] Processing ACCEPT call for offers: [ 823f2212-bf28-4dd6-959d-796029d32afb-O0 ] on slave 823f2212-bf28-4dd6-959d-796029d32afb-S0 at slave(372)@172.17.0.3:60904 (90665f991b70) for framework 823f2212-bf28-4dd6-959d-796029d32afb-0000 (default) at scheduler-93e745f0-0e48-4a8f-b227-93569976c5e8@172.17.0.3:60904
I0129 02:00:35.553364 31944 master.cpp:2823] Authorizing framework principal 'test-principal' to launch task 558bdc51-38dc-48e3-9b81-ad42b942050c as user 'mesos'
W0129 02:00:35.554951 31944 validation.cpp:404] Executor default for task 558bdc51-38dc-48e3-9b81-ad42b942050c uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases.
W0129 02:00:35.555004 31944 validation.cpp:416] Executor default for task 558bdc51-38dc-48e3-9b81-ad42b942050c uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases.
I0129 02:00:35.555403 31944 master.hpp:176] Adding task 558bdc51-38dc-48e3-9b81-ad42b942050c with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 823f2212-bf28-4dd6-959d-796029d32afb-S0 (90665f991b70)
I0129 02:00:35.555660 31944 master.cpp:3621] Launching task 558bdc51-38dc-48e3-9b81-ad42b942050c of framework 823f2212-bf28-4dd6-959d-796029d32afb-0000 (default) at scheduler-93e745f0-0e48-4a8f-b227-93569976c5e8@172.17.0.3:60904 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 823f2212-bf28-4dd6-959d-796029d32afb-S0 at slave(372)@172.17.0.3:60904 (90665f991b70)
I0129 02:00:35.556067 31948 slave.cpp:1360] Got assigned task 558bdc51-38dc-48e3-9b81-ad42b942050c for framework 823f2212-bf28-4dd6-959d-796029d32afb-0000
I0129 02:00:35.556691 31948 slave.cpp:1479] Launching task 558bdc51-38dc-48e3-9b81-ad42b942050c for framework 823f2212-bf28-4dd6-959d-796029d32afb-0000
I0129 02:00:35.557307 31948 paths.cpp:472] Trying to chown '/tmp/ContentType_ExecutorHttpApiTest_DefaultAccept_1_r4GUhM/slaves/823f2212-bf28-4dd6-959d-796029d32afb-S0/frameworks/823f2212-bf28-4dd6-959d-796029d32afb-0000/executors/default/runs/92af8a30-2bb0-48fc-874a-e854ff82225b' to user 'mesos'
I0129 02:00:35.580426 31948 slave.cpp:5281] Launching executor default of framework 823f2212-bf28-4dd6-959d-796029d32afb-0000 with resources  in work directory '/tmp/ContentType_ExecutorHttpApiTest_DefaultAccept_1_r4GUhM/slaves/823f2212-bf28-4dd6-959d-796029d32afb-S0/frameworks/823f2212-bf28-4dd6-959d-796029d32afb-0000/executors/default/runs/92af8a30-2bb0-48fc-874a-e854ff82225b'
*** Aborted at 1454032835 (unix time) try ""date -d @1454032835"" if you are using GNU date ***
I0129 02:00:35.582674 31948 exec.cpp:134] Version: 0.28.0
PC: @     0x2b342648a8dd (unknown)
I0129 02:00:35.582969 31958 exec.cpp:184] Executor started at: executor(123)@172.17.0.3:60904 with pid 31926
I0129 02:00:35.583271 31948 slave.cpp:1697] Queuing task '558bdc51-38dc-48e3-9b81-ad42b942050c' for executor 'default' of framework 823f2212-bf28-4dd6-959d-796029d32afb-0000
I0129 02:00:35.583444 31948 slave.cpp:748] Successfully attached file '/tmp/ContentType_ExecutorHttpApiTest_DefaultAccept_1_r4GUhM/slaves/823f2212-bf28-4dd6-959d-796029d32afb-S0/frameworks/823f2212-bf28-4dd6-959d-796029d32afb-0000/executors/default/runs/92af8a30-2bb0-48fc-874a-e854ff82225b'
I0129 02:00:35.583636 31948 slave.cpp:2642] Got registration for executor 'default' of framework 823f2212-bf28-4dd6-959d-796029d32afb-0000 from executor(123)@172.17.0.3:60904
I0129 02:00:35.584103 31950 exec.cpp:208] Executor registered on slave 823f2212-bf28-4dd6-959d-796029d32afb-S0
I0129 02:00:35.584170 31950 exec.cpp:220] Executor::registered took 39070ns
I0129 02:00:35.584476 31948 slave.cpp:1862] Sending queued task '558bdc51-38dc-48e3-9b81-ad42b942050c' to executor 'default' of framework 823f2212-bf28-4dd6-959d-796029d32afb-0000 at executor(123)@172.17.0.3:60904
I0129 02:00:35.584918 31944 exec.cpp:295] Executor asked to run task '558bdc51-38dc-48e3-9b81-ad42b942050c'
I0129 02:00:35.585036 31944 exec.cpp:304] Executor::launchTask took 93979ns
I0129 02:00:35.585160 31944 exec.cpp:517] Executor sending status update TASK_RUNNING (UUID: a9be0e7b-c011-4099-aba9-c914c911d7a9) for task 558bdc51-38dc-48e3-9b81-ad42b942050c of framework 823f2212-bf28-4dd6-959d-796029d32afb-0000
I0129 02:00:35.585564 31956 slave.cpp:3001] Handling status update TASK_RUNNING (UUID: a9be0e7b-c011-4099-aba9-c914c911d7a9) for task 558bdc51-38dc-48e3-9b81-ad42b942050c of framework 823f2212-bf28-4dd6-959d-796029d32afb-0000 from executor(123)@172.17.0.3:60904
I0129 02:00:35.585914 31944 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: a9be0e7b-c011-4099-aba9-c914c911d7a9) for task 558bdc51-38dc-48e3-9b81-ad42b942050c of framework 823f2212-bf28-4dd6-959d-796029d32afb-0000
I0129 02:00:35.585973 31944 status_update_manager.cpp:497] Creating StatusUpdate stream for task 558bdc51-38dc-48e3-9b81-ad42b942050c of framework 823f2212-bf28-4dd6-959d-796029d32afb-0000
I0129 02:00:35.586472 31944 status_update_manager.cpp:374] Forwarding update TASK_RUNNING (UUID: a9be0e7b-c011-4099-aba9-c914c911d7a9) for task 558bdc51-38dc-48e3-9b81-ad42b942050c of framework 823f2212-bf28-4dd6-959d-796029d32afb-0000 to the slave
I0129 02:00:35.586774 31953 slave.cpp:3353] Forwarding the update TASK_RUNNING (UUID: a9be0e7b-c011-4099-aba9-c914c911d7a9) for task 558bdc51-38dc-48e3-9b81-ad42b942050c of framework 823f2212-bf28-4dd6-959d-796029d32afb-0000 to master@172.17.0.3:60904
I0129 02:00:35.587050 31953 slave.cpp:3247] Status update manager successfully handled status update TASK_RUNNING (UUID: a9be0e7b-c011-4099-aba9-c914c911d7a9) for task 558bdc51-38dc-48e3-9b81-ad42b942050c of framework 823f2212-bf28-4dd6-959d-796029d32afb-0000
I0129 02:00:35.587118 31953 slave.cpp:3263] Sending acknowledgement for status update TASK_RUNNING (UUID: a9be0e7b-c011-4099-aba9-c914c911d7a9) for task 558bdc51-38dc-48e3-9b81-ad42b942050c of framework 823f2212-bf28-4dd6-959d-796029d32afb-0000 to executor(123)@172.17.0.3:60904
I0129 02:00:35.587172 31948 master.cpp:4789] Status update TASK_RUNNING (UUID: a9be0e7b-c011-4099-aba9-c914c911d7a9) for task 558bdc51-38dc-48e3-9b81-ad42b942050c of framework 823f2212-bf28-4dd6-959d-796029d32afb-0000 from slave 823f2212-bf28-4dd6-959d-796029d32afb-S0 at slave(372)@172.17.0.3:60904 (90665f991b70)
I0129 02:00:35.587226 31948 master.cpp:4837] Forwarding status update TASK_RUNNING (UUID: a9be0e7b-c011-4099-aba9-c914c911d7a9) for task 558bdc51-38dc-48e3-9b81-ad42b942050c of framework 823f2212-bf28-4dd6-959d-796029d32afb-0000
I0129 02:00:35.587316 31953 exec.cpp:341] Executor received status update acknowledgement a9be0e7b-c011-4099-aba9-c914c911d7a9 for task 558bdc51-38dc-48e3-9b81-ad42b942050c of framework 823f2212-bf28-4dd6-959d-796029d32afb-0000
I0129 02:00:35.587404 31948 master.cpp:6445] Updating the state of task 558bdc51-38dc-48e3-9b81-ad42b942050c of framework 823f2212-bf28-4dd6-959d-796029d32afb-0000 (latest state: TASK_RUNNING, status update state: TASK_RUNNING)
I0129 02:00:35.587743 31953 sched.cpp:985] Scheduler::statusUpdate took 84229ns
I0129 02:00:35.588039 31957 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 38.985866ms
I0129 02:00:35.588095 31957 replica.cpp:712] Persisted action at 4
I0129 02:00:35.588568 31948 master.cpp:3947] Processing ACKNOWLEDGE call a9be0e7b-c011-4099-aba9-c914c911d7a9 for task 558bdc51-38dc-48e3-9b81-ad42b942050c of framework 823f2212-bf28-4dd6-959d-796029d32afb-0000 (default) at scheduler-93e745f0-0e48-4a8f-b227-93569976c5e8@172.17.0.3:60904 on slave 823f2212-bf28-4dd6-959d-796029d32afb-S0
I0129 02:00:35.588979 31950 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I0129 02:00:35.589004 31957 status_update_manager.cpp:392] Received status update acknowledgement (UUID: a9be0e7b-c011-4099-aba9-c914c911d7a9) for task 558bdc51-38dc-48e3-9b81-ad42b942050c of framework 823f2212-bf28-4dd6-959d-796029d32afb-0000
I0129 02:00:35.589700 31954 slave.cpp:2411] Status update manager successfully handled status update acknowledgement (UUID: a9be0e7b-c011-4099-aba9-c914c911d7a9) for task 558bdc51-38dc-48e3-9b81-ad42b942050c of framework 823f2212-bf28-4dd6-959d-796029d32afb-0000
I0129 02:00:35.590204 31948 process.cpp:3141] Handling HTTP event for process 'slave(372)' with path: '/slave(372)/api/v1/executor'
I0129 02:00:35.590828 31951 http.cpp:190] HTTP POST for /slave(372)/api/v1/executor from 172.17.0.3:52186
I0129 02:00:35.591156 31951 slave.cpp:2475] Received Subscribe request for HTTP executor 'default' of framework 823f2212-bf28-4dd6-959d-796029d32afb-0000 at executor(123)@172.17.0.3:60904
I0129 02:00:35.593617 31948 master.cpp:1025] Master terminating
W0129 02:00:35.593758 31948 master.cpp:6497] Removing task 558bdc51-38dc-48e3-9b81-ad42b942050c with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] of framework 823f2212-bf28-4dd6-959d-796029d32afb-0000 on slave 823f2212-bf28-4dd6-959d-796029d32afb-S0 at slave(372)@172.17.0.3:60904 (90665f991b70) in non-terminal state TASK_RUNNING
I0129 02:00:35.594292 31958 hierarchical.cpp:505] Removed slave 823f2212-bf28-4dd6-959",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.3846153846153846,0.4181818181818182,0.4121212121212121,0.0
Bug,Mesos Agents needs to re-resolve hosts in zk string on leader change / failure to connect,"Sample Mesos Agent log: https://gist.github.com/brndnmtthws/fb846fa988487250a809

Note, zookeeper has a function to change the list of servers at runtime: https://github.com/apache/zookeeper/blob/735ea78909e67c648a4978c8d31d63964986af73/src/c/src/zookeeper.c#L1207-L1232

This comes up when using an AWS AutoScalingGroup for managing the set of masters. 

The agent when it comes up the first time, resolves the zk:// string. Once all the hosts that were in the original string fail (Each fails, is replaced by a new machine, which has the same DNS name), the agent just keeps spinning in an internal loop, never re-resolving the DNS names.

Two solutions I see are 
1. Update the list of servers / re-resolve
2. Have the agent detect it hasn't connected recently, and kill itself (Which will force a re-resolution when the agent starts back up)",3.0,0,0.5,1.0,0.0,0.0,0.0,0.0,0.8,0.5438596491228069,0.39999999999999997,0.08974358974358974,0.0787878787878788,0.07272727272727272,0.0
Bug,MasterQuotaTest.AvailableResourcesAfterRescinding is flaky.,"Can be reproduced by running {{GLOG_v=1 GTEST_FILTER=""MasterQuotaTest.AvailableResourcesAfterRescinding"" ./bin/mesos-tests.sh --gtest_shuffle --gtest_break_on_failure --gtest_repeat=1000 --verbose}}.

h5. Verbose log from a bad run:
{code}
[ RUN      ] MasterQuotaTest.AvailableResourcesAfterRescinding
I0128 12:20:27.568657 2080858880 resources.cpp:564] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0128 12:20:27.570142 2080858880 resources.cpp:564] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0128 12:20:27.583225 2080858880 leveldb.cpp:174] Opened db in 6241us
I0128 12:20:27.584353 2080858880 leveldb.cpp:181] Compacted db in 1026us
I0128 12:20:27.584429 2080858880 leveldb.cpp:196] Created db iterator in 12us
I0128 12:20:27.584442 2080858880 leveldb.cpp:202] Seeked to beginning of db in 7us
I0128 12:20:27.584453 2080858880 leveldb.cpp:271] Iterated through 0 keys in the db in 6us
I0128 12:20:27.584475 2080858880 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0128 12:20:27.584918 300445696 recover.cpp:447] Starting replica recovery
I0128 12:20:27.585113 300445696 recover.cpp:473] Replica is in EMPTY status
I0128 12:20:27.585916 297226240 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (18274)@192.168.178.24:51278
I0128 12:20:27.586086 297762816 recover.cpp:193] Received a recover response from a replica in EMPTY status
I0128 12:20:27.586449 297226240 recover.cpp:564] Updating replica status to STARTING
I0128 12:20:27.587204 300445696 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 624us
I0128 12:20:27.587242 300445696 replica.cpp:320] Persisted replica status to STARTING
I0128 12:20:27.587376 299372544 recover.cpp:473] Replica is in STARTING status
I0128 12:20:27.588050 300982272 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (18275)@192.168.178.24:51278
I0128 12:20:27.588235 300445696 recover.cpp:193] Received a recover response from a replica in STARTING status
I0128 12:20:27.588572 297762816 recover.cpp:564] Updating replica status to VOTING
I0128 12:20:27.588850 297226240 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 140us
I0128 12:20:27.588879 297226240 replica.cpp:320] Persisted replica status to VOTING
I0128 12:20:27.588975 299909120 recover.cpp:578] Successfully joined the Paxos group
I0128 12:20:27.589154 299909120 recover.cpp:462] Recover process terminated
I0128 12:20:27.599486 298835968 master.cpp:374] Master 531344bd-56f4-4e4f-8f6f-a6a9d36058c7 (alexr.fritz.box) started on 192.168.178.24:51278
I0128 12:20:27.599520 298835968 master.cpp:376] Flags at startup: --acls="""" --allocation_interval=""50ms"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_http=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/private/tmp/NlzPSo/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --http_authenticators=""basic"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_completed_frameworks=""50"" --max_completed_tasks_per_framework=""1000"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""25secs"" --registry_strict=""true"" --roles=""role1,role2"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/private/tmp/NlzPSo/master"" --zk_session_timeout=""10secs""
I0128 12:20:27.599753 298835968 master.cpp:421] Master only allowing authenticated frameworks to register
I0128 12:20:27.599769 298835968 master.cpp:426] Master only allowing authenticated slaves to register
I0128 12:20:27.599781 298835968 credentials.hpp:35] Loading credentials for authentication from '/private/tmp/NlzPSo/credentials'
I0128 12:20:27.600082 298835968 master.cpp:466] Using default 'crammd5' authenticator
I0128 12:20:27.600163 298835968 master.cpp:535] Using default 'basic' HTTP authenticator
I0128 12:20:27.600327 298835968 master.cpp:569] Authorization enabled
W0128 12:20:27.600345 298835968 master.cpp:629] The '--roles' flag is deprecated. This flag will be removed in the future. See the Mesos 0.27 upgrade notes for more information
I0128 12:20:27.600497 297762816 whitelist_watcher.cpp:77] No whitelist given
I0128 12:20:27.600503 297226240 hierarchical.cpp:144] Initialized hierarchical allocator process
I0128 12:20:27.601965 297226240 master.cpp:1710] The newly elected leader is master@192.168.178.24:51278 with id 531344bd-56f4-4e4f-8f6f-a6a9d36058c7
I0128 12:20:27.601995 297226240 master.cpp:1723] Elected as the leading master!
I0128 12:20:27.602007 297226240 master.cpp:1468] Recovering from registrar
I0128 12:20:27.602083 300445696 registrar.cpp:307] Recovering registrar
I0128 12:20:27.602460 297226240 log.cpp:659] Attempting to start the writer
I0128 12:20:27.603514 299909120 replica.cpp:493] Replica received implicit promise request from (18277)@192.168.178.24:51278 with proposal 1
I0128 12:20:27.603734 299909120 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 205us
I0128 12:20:27.603768 299909120 replica.cpp:342] Persisted promised to 1
I0128 12:20:27.604194 299909120 coordinator.cpp:238] Coordinator attempting to fill missing positions
I0128 12:20:27.605311 299372544 replica.cpp:388] Replica received explicit promise request from (18278)@192.168.178.24:51278 for position 0 with proposal 2
I0128 12:20:27.605468 299372544 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 133us
I0128 12:20:27.605494 299372544 replica.cpp:712] Persisted action at 0
I0128 12:20:27.606441 298835968 replica.cpp:537] Replica received write request for position 0 from (18279)@192.168.178.24:51278
I0128 12:20:27.606492 298835968 leveldb.cpp:436] Reading position from leveldb took 29us
I0128 12:20:27.606665 298835968 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 151us
I0128 12:20:27.606688 298835968 replica.cpp:712] Persisted action at 0
I0128 12:20:27.607244 297226240 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I0128 12:20:27.607409 297226240 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 152us
I0128 12:20:27.607441 297226240 replica.cpp:712] Persisted action at 0
I0128 12:20:27.607457 297226240 replica.cpp:697] Replica learned NOP action at position 0
I0128 12:20:27.607853 297226240 log.cpp:675] Writer started with ending position 0
I0128 12:20:27.608649 299372544 leveldb.cpp:436] Reading position from leveldb took 158us
I0128 12:20:27.609539 298835968 registrar.cpp:340] Successfully fetched the registry (0B) in 7.426816ms
I0128 12:20:27.609763 298835968 registrar.cpp:439] Applied 1 operations in 54us; attempting to update the 'registry'
I0128 12:20:27.610216 300982272 log.cpp:683] Attempting to append 186 bytes to the log
I0128 12:20:27.610297 298835968 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I0128 12:20:27.611016 299909120 replica.cpp:537] Replica received write request for position 1 from (18280)@192.168.178.24:51278
I0128 12:20:27.611188 299909120 leveldb.cpp:341] Persisting action (205 bytes) to leveldb took 153us
I0128 12:20:27.611222 299909120 replica.cpp:712] Persisted action at 1
I0128 12:20:27.611843 299909120 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I0128 12:20:27.612004 299909120 leveldb.cpp:341] Persisting action (207 bytes) to leveldb took 147us
I0128 12:20:27.612035 299909120 replica.cpp:712] Persisted action at 1
I0128 12:20:27.612052 299909120 replica.cpp:697] Replica learned APPEND action at position 1
I0128 12:20:27.612742 300982272 registrar.cpp:484] Successfully updated the 'registry' in 2.924032ms
I0128 12:20:27.612846 300982272 registrar.cpp:370] Successfully recovered registrar
I0128 12:20:27.612936 298835968 log.cpp:702] Attempting to truncate the log to 1
I0128 12:20:27.613005 297762816 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I0128 12:20:27.613323 298299392 master.cpp:1520] Recovered 0 slaves from the Registry (147B) ; allowing 10mins for slaves to re-register
I0128 12:20:27.613364 298835968 hierarchical.cpp:171] Skipping recovery of hierarchical allocator: nothing to recover
I0128 12:20:27.613966 300445696 replica.cpp:537] Replica received write request for position 2 from (18281)@192.168.178.24:51278
I0128 12:20:27.614131 300445696 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 151us
I0128 12:20:27.614166 300445696 replica.cpp:712] Persisted action at 2
I0128 12:20:27.614660 299372544 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I0128 12:20:27.614828 299372544 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 158us
I0128 12:20:27.614876 299372544 leveldb.cpp:399] Deleting ~1 keys from leveldb took 28us
I0128 12:20:27.614898 299372544 replica.cpp:712] Persisted action at 2
I0128 12:20:27.614915 299372544 replica.cpp:697] Replica learned TRUNCATE action at position 2
I0128 12:20:27.625591 2080858880 containerizer.cpp:143] Using isolation: posix/cpu,posix/mem,filesystem/posix
I0128 12:20:27.629758 298299392 slave.cpp:192] Slave started on 871)@192.168.178.24:51278
I0128 12:20:27.629791 298299392 slave.cpp:193] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/MasterQuotaTest_AvailableResourcesAfterRescinding_gS9Qcf/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""https://auth.docker.io"" --docker_kill_orphans=""true"" --docker_puller_timeout=""60"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/MasterQuotaTest_AvailableResourcesAfterRescinding_gS9Qcf/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/Users/alex/Projects/mesos/build/default/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --version=""false"" --work_dir=""/tmp/MasterQuotaTest_AvailableResourcesAfterRescinding_gS9Qcf""
I0128 12:20:27.630067 298299392 credentials.hpp:83] Loading credential for authentication from '/tmp/MasterQuotaTest_AvailableResourcesAfterRescinding_gS9Qcf/credential'
I0128 12:20:27.630223 298299392 slave.cpp:323] Slave using credential for: test-principal
I0128 12:20:27.630360 298299392 resources.cpp:564] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0128 12:20:27.630818 298299392 slave.cpp:463] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0128 12:20:27.630869 298299392 slave.cpp:471] Slave attributes: [  ]
I0128 12:20:27.630882 298299392 slave.cpp:476] Slave hostname: alexr.fritz.box
I0128 12:20:27.631352 300982272 state.cpp:58] Recovering state from '/tmp/MasterQuotaTest_AvailableResourcesAfterRescinding_gS9Qcf/meta'
I0128 12:20:27.631515 299909120 status_update_manager.cpp:200] Recovering status update manager
I0128 12:20:27.631702 298835968 containerizer.cpp:390] Recovering containerizer
I0128 12:20:27.632589 297226240 provisioner.cpp:245] Provisioner recovery complete
I0128 12:20:27.632807 298835968 slave.cpp:4495] Finished recovery
I0128 12:20:27.633539 298835968 slave.cpp:4667] Querying resource estimator for oversubscribable resources
I0128 12:20:27.633752 300445696 status_update_manager.cpp:174] Pausing sending status updates
I0128 12:20:27.633754 298835968 slave.cpp:795] New master detected at master@192.168.178.24:51278
I0128 12:20:27.633806 298835968 slave.cpp:858] Authenticating with master master@192.168.178.24:51278
I0128 12:20:27.633824 298835968 slave.cpp:863] Using default CRAM-MD5 authenticatee
I0128 12:20:27.633903 298835968 slave.cpp:831] Detecting new master
I0128 12:20:27.633913 299372544 authenticatee.cpp:121] Creating new client SASL connection
I0128 12:20:27.634016 298835968 slave.cpp:4681] Received oversubscribable resources  from the resource estimator
I0128 12:20:27.634076 297226240 master.cpp:5521] Authenticating slave(871)@192.168.178.24:51278
I0128 12:20:27.634130 299372544 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(1741)@192.168.178.24:51278
I0128 12:20:27.634255 297226240 authenticator.cpp:98] Creating new server SASL connection
I0128 12:20:27.634348 300982272 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0128 12:20:27.634367 300982272 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0128 12:20:27.634454 298835968 authenticator.cpp:203] Received SASL authentication start
I0128 12:20:27.634515 298835968 authenticator.cpp:325] Authentication requires more steps
I0128 12:20:27.634572 298835968 authenticatee.cpp:258] Received SASL authentication step
I0128 12:20:27.634706 297226240 authenticator.cpp:231] Received SASL authentication step
I0128 12:20:27.634757 297226240 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'alexr.fritz.box' server FQDN: 'alexr.fritz.box' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0128 12:20:27.634771 297226240 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0128 12:20:27.634793 297226240 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0128 12:20:27.634809 297226240 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'alexr.fritz.box' server FQDN: 'alexr.fritz.box' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0128 12:20:27.634819 297226240 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0128 12:20:27.634827 297226240 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0128 12:20:27.634893 297226240 authenticator.cpp:317] Authentication success
I0128 12:20:27.634958 298835968 authenticatee.cpp:298] Authentication success
I0128 12:20:27.635030 298299392 master.cpp:5551] Successfully authenticated principal 'test-principal' at slave(871)@192.168.178.24:51278
I0128 12:20:27.635079 300445696 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(1741)@192.168.178.24:51278
I0128 12:20:27.635195 299372544 slave.cpp:926] Successfully authenticated with master master@192.168.178.24:51278
I0128 12:20:27.635273 299372544 slave.cpp:1320] Will retry registration in 5.823453ms if necessary
I0128 12:20:27.635365 299909120 master.cpp:4235] Registering slave at slave(871)@192.168.178.24:51278 (alexr.fritz.box) with id 531344bd-56f4-4e4f-8f6f-a6a9d36058c7-S0
I0128 12:20:27.635542 297762816 registrar.cpp:439] Applied 1 operations in 41us; attempting to update the 'registry'
I0128 12:20:27.635889 299372544 log.cpp:683] Attempting to append 358 bytes to the log
I0128 12:20:27.636011 298299392 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I0128 12:20:27.636693 300982272 replica.cpp:537] Replica received write request for position 3 from (18295)@192.168.178.24:51278
I0128 12:20:27.636860 300982272 leveldb.cpp:341] Persisting action (377 bytes) to leveldb took 139us
I0128 12:20:27.636885 300982272 replica.cpp:712] Persisted action at 3
I0128 12:20:27.637380 299909120 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I0128 12:20:27.637547 299909120 leveldb.cpp:341] Persisting action (379 bytes) to leveldb took 132us
I0128 12:20:27.637573 299909120 replica.cpp:712] Persisted action at 3
I0128 12:20:27.637589 299909120 replica.cpp:697] Replica learned APPEND action at position 3
I0128 12:20:27.638362 298835968 registrar.cpp:484] Successfully updated the 'registry' in 2.77504ms
I0128 12:20:27.638589 300445696 log.cpp:702] Attempting to truncate the log to 3
I0128 12:20:27.638684 298299392 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I0128 12:20:27.638825 300445696 slave.cpp:3435] Received ping from slave-observer(871)@192.168.178.24:51278
I0128 12:20:27.639081 300982272 hierarchical.cpp:473] Added slave 531344bd-56f4-4e4f-8f6f-a6a9d36058c7-S0 (alexr.fritz.box) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
I0128 12:20:27.639117 299909120 master.cpp:4303] Registered slave 531344bd-56f4-4e4f-8f6f-a6a9d36058c7-S0 at slave(871)@192.168.178.24:51278 (alexr.fritz.box) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0128 12:20:27.639165 300982272 hierarchical.cpp:1403] No resources available to allocate!
I0128 12:20:27.639168 297226240 slave.cpp:970] Registered with master master@192.168.178.24:51278; given slave ID 531344bd-56f4-4e4f-8f6f-a6a9d36058c7-S0
I0128 12:20:27.639189 297226240 fetcher.cpp:81] Clearing fetcher cache
I0128 12:20:27.639183 300982272 hierarchical.cpp:1116] Performed allocation for slave 531344bd-56f4-4e4f-8f6f-a6a9d36058c7-S0 in 77us
I0128 12:20:27.639348 297762816 status_update_manager.cpp:181] Resuming sending status updates
I0128 12:20:27.639519 298835968 replica.cpp:537] Replica received write request for position 4 from (18296)@192.168.178.24:51278
I0128 12:20:27.639678 298835968 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 142us
I0128 12:20:27.639708 298835968 replica.cpp:712] Persisted action at 4
I0128 12:20:27.640115 300982272 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I0128 12:20:27.640276 300982272 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 137us
I0128 12:20:27.640312 300982272 leveldb.cpp:399] Deleting ~2 keys from leveldb took 21us
I0128 12:20:27.640326 300982272 replica.cpp:712] Persisted action at 4
I0128 12:20:27.640336 300982272 replica.cpp:697] Replica learned TRUNCATE action at position 4
I0128 12:20:27.642145 297226240 slave.cpp:993] Checkpointing SlaveInfo to '/tmp/MasterQuotaTest_AvailableResourcesAfterRescinding_gS9Qcf/meta/slaves/531344bd-56f4-4e4f-8f6f-a6a9d36058c7-S0/slave.info'
I0128 12:20:27.643354 297226240 slave.cpp:1029] Forwarding total oversubscribed resources 
I0128 12:20:27.643458 300445696 master.cpp:4644] Received update of slave 531344bd-56f4-4e4f-8f6f-a6a9d36058c7-S0 at slave(871)@192.168.178.24:51278 (alexr.fritz.box) with total oversubscribed resources 
I0128 12:20:27.643710 298299392 hierarchical.cpp:531] Slave 531344bd-56f4-4e4f-8f6f-a6a9d36058c7-S0 (alexr.fritz.box) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: )
I0128 12:20:27.643769 298299392 hierarchical.cpp:1403] No resources available to allocate!
I0128 12:20:27.643805 298299392 hierarchical.cpp:1116] Performed allocation for slave 531344bd-56f4-4e4f-8f6f-a6a9d36058c7-S0 in 78us
I0128 12:20:27.644645 2080858880 containerizer.cpp:143] Using isolation: posix/cpu,posix/mem,filesystem/posix
I0128 12:20:27.649093 297226240 slave.cpp:192] Slave started on 872)@192.168.178.24:51278
I0128 12:20:27.649138 297226240 slave.cpp:193] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/MasterQuotaTest_AvailableResourcesAfterRescinding_6ycfWv/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""https://auth.docker.io"" --docker_kill_orphans=""true"" --docker_puller_timeout=""60"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/MasterQuotaTest_AvailableResourcesAfterRescinding_6ycfWv/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/Users/alex/Projects/mesos/build/default/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --version=""false"" --work_dir=""/tmp/MasterQuotaTest_AvailableResourcesAfterRescinding_6ycfWv""
I0128 12:20:27.649353 297226240 credentials.hpp:83] Loading credential for authentication from '/tmp/MasterQuotaTest_AvailableResourcesAfterRescinding_6ycfWv/credential'
I0128 12:20:27.649451 297226240 slave.cpp:323] Slave using credential for: test-principal
I0128 12:20:27.649569 297226240 resources.cpp:564] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ports:[31000-32000]
Trying semicolon-delimited string format instead
I0128 12:20:27.650039 297226240 slave.cpp:463] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0128 12:20:27.650085 297226240 slave.cpp:471] Slave attributes: [  ]
I0128 12:20:27.650096 297226240 slave.cpp:476] Slave hostname: alexr.fritz.box
I0128 12:20:27.650509 299909120 state.cpp:58] Recovering state from '/tmp/MasterQuotaTest_AvailableResourcesAfterRescinding_6ycfWv/meta'
I0128 12:20:27.650699 298299392 status_update_manager.cpp:200] Recovering status update manager
I0128 12:20:27.650701 300445696 hierarchical.cpp:1403] No resources available to allocate!
I0128 12:20:27.650738 300445696 hierarchical.cpp:1096] Performed allocation for 1 slaves in 101us
I0128 12:20:27.650887 297226240 containerizer.cpp:390] Recovering containerizer
I0128 12:20:27.651747 299909120 provisioner.cpp:245] Provisioner recovery complete
I0128 12:20:27.651974 300982272 slave.cpp:4495] Finished recovery
I0128 12:20:27.653733 300982272 slave.cpp:4667] Querying resource estimator for oversubscribable resources
I0128 12:20:27.653928 300982272 slave.cpp:795] New master detected at master@192.168.178.24:51278
I0128 12:20:27.653928 299372544 status_update_manager.cpp:174] Pausing sending status updates
I0128 12:20:27.653975 300982272 slave.cpp:858] Authenticating with master master@192.168.178.24:51278
I0128 12:20:27.653991 300982272 slave.cpp:863] Using default CRAM-MD5 authenticatee
I0128 12:20:27.654091 300982272 slave.cpp:831] Detecting new master
I0128 12:20:27.654098 297226240 authenticatee.cpp:121] Creating new client SASL connection
I0128 12:20:27.654216 300982272 slave.cpp:4681] Received oversubscribable resources  from the resource estimator
I0128 12:20:27.654276 297762816 master.cpp:5521] Authenticating slave(872)@192.168.178.24:51278
I0128 12:20:27.654350 299909120 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(1742)@192.168.178.24:51278
I0128 12:20:27.654498 298299392 authenticator.cpp:98] Creating new server SASL connection
I0128 12:20:27.654602 300982272 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0128 12:20:27.654625 300982272 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0128 12:20:27.654700 299909120 authenticator.cpp:203] Received SASL authentication start
I0128 12:20:27.654752 299909120 authenticator.cpp:325] Authentication requires more steps
I0128 12:20:27.654819 299909120 authenticatee.cpp:258] Received SASL authentication step
I0128 12:20:27.654940 299372544 authenticator.cpp:231] Received SASL authentication step
I0128 12:20:27.654965 299372544 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'alexr.fritz.box' server FQDN: 'alexr.fritz.box' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0128 12:20:27.654978 299372544 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I0128 12:20:27.654997 299372544 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0128 12:20:27.655012 299372544 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: 'alexr.fritz.box' server FQDN: 'alexr.fritz.box' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0128 12:20:27.655024 299372544 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0128 12:20:27.655031 299372544 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0128 12:20:27.655047 299372544 authenticator.cpp:317] Authentication success
I0128 12:20:27.655143 299909120 authenticatee.cpp:298] Authentication success
I0128 12:20:27.655120 297762816 master.cpp:5551] Successfully authenticated principal 'test-principal' at slave(872)@192.168.178.24:51278
I0128 12:20:27.655163 299372544 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(1742)@192.168.178.24:51278
I0128 12:20:27.655326 300445696 slave.cpp:926] Successfully authenticated with master master@192.168.178.24:51278
I0128 12:20:27.655465 300445696 slave.cpp:1320] Will retry registration in 13.985296ms if necessary
I0128 12:20:27.655565 299909120 master.cpp:4235] Registering slave at slave(872)@192.168.178.24:51278 (alexr.fritz.box) with id 531344bd-56f4-4e4f-8f6f-a6a9d36058c7-S1
I0128 12:20:27.655823 300982272 registrar.cpp:439] Applied 1 operations in 64us; attempting to update the 'registry'
I0128 12:20:27.656354 297226240 log.cpp:683] Attempting to append 527 bytes to the log
I0128 12:20:27.656429 300445696 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 5
I0128 12:20:27.657187 300445696 replica.cpp:537] Replica received write request for position 5 from (18310)@192.168.178.24:51278
I0128 12:20:27.657429 300445696 leveldb.cpp:341] Persisting action (546 bytes) to leveldb took 224us
I0128 12:20:27.657464 300445696 replica.cpp:712] Persisted action at 5
I0128 12:20:27.658007 300445696 replica.cpp:691] Replica received learned notice for position 5 from @0.0.0.0:0
I0128 12:20:27.658190 300445696 leveldb.cpp:341] Persisting action (548 bytes) to leveldb took 170us
I0128 12:20:27.658223 300445696 replica.cpp:712] Persisted action at 5
I0128 12:20:27.658239 300445696 replica.cpp:697] Replica learned APPEND action at position 5
I0128 12:20:27.659104 300982272 registrar.cpp:484] Successfully updated the 'registry' in 3.227904ms
I0128 12:20:27.659373 298835968 log.cpp:702] Attempting to truncate the log to 5
I0128 12:20:27.659446 298299392 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 6
I0128 12:20:27.659636 300982272 slave.cpp:3435] Received ping from slave-observer(872)@192.168.178.24:51278
I0128 12:20:27.659855 297226240 hierarchical.cpp:473] Added slave 531344bd-56f4-4e4f-8f6f-a6a9d36058c7-S1 (alexr.fritz.box) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
I0128 12:20:27.659960 297226240 hierarchical.cpp:1403] No resources available to allocate!
I0128 12:20:27.659936 297762816 master.cpp:4303] Registered slave 531344bd-56f4-4e4f-8f6f-a6a9d36058c7-S1 at slave(872)@192.168.178.24:51278 (alexr.fritz.box) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0128 12:20:27.659981 297226240 hierarchical.cpp:1116] Performed allocation for slave 531344bd-56f4-4e4f-8f6f-a6a9d36058c7-S1 in 80us
I0128 12:20:27.659986 299909120 slave.cpp:970] Registered with master master@192.168.178.24:51278; given slave ID 531344bd-56f4-4e4f-8f6f-a6a9d36058c7-S1
I0128 12:20:27.660013 299909120 fetcher.cpp:81] Clearing fetcher cache
I0128 12:20:27.660092 297226240 replica.cpp:537] Replica received write request for position 6 from (18311)@192.168.178.24:51278
I0128 12:20:27.660246 297226240 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 131us
I0128 12:20:27.660270 297226240 replica.cpp:712] Persisted action at 6
I0128 12:20:27.660454 300445696 status_update_manager.cpp:181] Resuming sending status updates
I0128 12:20:27.660742 299372544 replica.cpp:691] Replica received learned notice for position 6 from @0.0.0.0:0
I0128 12:20:27.660924 299372544 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 209us
I0128 12:20:27.661015 299372544 leveldb.cpp:399] Deleting ~2 keys from leveldb took 37us
I0128 12:20:27.661039 299372544 replica.cpp:712] Persisted action at 6
I0128 12:20:27.661061 299372544 replica.cpp:697] Replica learned TRUNCATE action at position 6
I0128 12:20:27.661752 299909120 slave.cpp:993] Checkpointing SlaveInfo to '/tmp/MasterQuotaTest_AvailableResourcesAfterRescinding_6ycfWv/meta/slaves/531344bd-56f4-4e4f-8f6f-a6a9d36058c7-S1/slave.info'
I0128 12:20:27.662113 299909120 slave.cpp:1029] Forwarding total oversubscribed resources 
I0128 12:20:27.662199 297762816 master.cpp:4644] Received update of slave 531344bd-56f4-4e4f-8f6f-a6a9d36058c7-S1 at slave(872)@192.168.178.24:51278 (alexr.fritz.box) with total oversubscribed resources 
I0128 12:20:27.662508 297762816 hierarchical.cpp:531] Slave 531344bd-56f4-4e4f-8f6f-a6a9d36058c7-S1 (alexr.fritz.box) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: )
I0128 12:20:27.662577 297762816 hierarchical.cpp:1403] No resources available to allocate!
I0128 12:20:27.662590 297762816 hierarchical.cpp:1116] Performed allocation for slave 531344bd-56f4-4e4f-8f6f-a6a9d36058c7-S1 in 51us
I0128 12:20:27.663261 2080858880 containerizer.cpp:143] Using isolation: posix/cpu,posix/mem,filesystem/posix
I0128 12:20:27.669075 299372544 slave.cpp:192] Slave started on 873)@192.168.178.24:51278
I0128 12:20:27.669107 299372544 slave.cpp:193] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/MasterQuotaTest_AvailableResourcesAfterRescinding_eAr35P/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""https://auth.docker.io"" --docker_kill_orphans=""true"" --docker_puller_timeout=""60"" --docker_registry=""https://registry-1.docker.io"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/MasterQuotaTest_AvailableResourcesAfterRescinding_eAr35P/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/Users/alex/Projects/mesos/build/default/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --version=""false"" --work_dir=""/tmp/MasterQuotaTest_AvailableResourcesAfterRescinding_eAr35P""
I0128 12:20:27.669395 299372544 credentials.hpp:83] Loading credential for authentication from '/tmp/MasterQuotaTest_AvailableResourcesAfterRescinding_eAr35P/credential'
I0128 12:20:27.669497 299372544 slave.cpp:323] Slave using credential for: test-",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.717948717948718,0.8242424242424242,0.7212121212121212,0.0
Bug,NetClsIsolatorTest.ROOT_CGROUPS_NetClsIsolate fails on CentOS 6,"This test fails in my CentOS 6 VM due to a cgroups issue:

{code}
[ RUN      ] NetClsIsolatorTest.ROOT_CGROUPS_NetClsIsolate
I0127 19:15:06.637328 25347 exec.cpp:134] Version: 0.28.0
I0127 19:15:06.648378 25378 exec.cpp:208] Executor registered on slave 6edafba0-9dbd-4e6e-b10e-c6f935e58d41-S0
Registered executor on localhost
Starting task b745d88e-3fbe-4af9-80b3-e43484e37acf
sh -c 'sleep 1000'
Forked command at 25385
../../src/tests/containerizer/isolator_tests.cpp:926: Failure
pids: Failed to read cgroups control 'cgroup.procs': '/sys/fs/cgroup/net_cls' is not a valid hierarchy
I0127 19:15:06.662083 25376 exec.cpp:381] Executor asked to shutdown
Shutting down
Sending SIGTERM to process tree at pid 25385
[  FAILED  ] NetClsIsolatorTest.ROOT_CGROUPS_NetClsIsolate (335 ms)
{code}",1.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.2807017543859649,0.39285714285714285,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Bug,Logrotate ContainerLogger may not handle FD ownership correctly,"One of the patches for [MESOS-4136] introduced the {{FDType::OWNED}} enum for {{Subprocess::IO::FD}}.

The way the logrotate module uses this is slightly incorrect:
# The module starts a subprocess with an output {{Subprocess::PIPE()}}.
# That pipe's FD is passed into another subprocess via {{Subprocess::IO::FD(pipe, IO::OWNED)}}.
# When the second subprocess starts, the pipe's FD is closed in the parent.
# When the first subprocess terminates, the existing code will try to close the pipe again.  This effectively closes a random FD.",1.0,0,0.0,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.0
Bug,Resources object can be mutated through the public API,"The {{Resources}} object current allows mutation of it's internal state through the public mutable iterator interface.
This can cause issues when the mutation involved stripping certain qualifiers on a {{Resource}}, as they will not be summed together at the end of the mutation (even though they should be).

The {{contains()}} math will not work correctly if two {{addable}} resources are not summed together on the {{lhs}} of the contains check.",3.0,0,0.5,0.6726998491704375,0.0,0.0,0.0,0.1,0.6000000000000001,0.2807017543859649,0.29285714285714287,0.24358974358974358,0.22424242424242424,0.22424242424242424,0.0
Bug,NetClsIsolatorTest.ROOT_CGROUPS_NetClsIsolate is flaky,"While running the command
{noformat}
sudo ./bin/mesos-tests.sh --gtest_filter=""-CgroupsAnyHierarchyWithCpuMemoryTest.ROOT_CGROUPS_Listen:CgroupsAnyHierarchyMemoryPressureTest.ROOT_IncreaseRSS"" --gtest_repeat=10 --gtest_break_on_failure
{noformat}
One eventually gets the following output:
{noformat}
[ RUN      ] NetClsIsolatorTest.ROOT_CGROUPS_NetClsIsolate
../../src/tests/containerizer/isolator_tests.cpp:870: Failure
containerizer: Could not create isolator 'cgroups/net_cls': Unexpected subsystems found attached to the hierarchy /sys/fs/cgroup/net_cls,net_prio
[  FAILED  ] NetClsIsolatorTest.ROOT_CGROUPS_NetClsIsolate (75 ms)
{noformat}",1.0,0.27.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.2807017543859649,0.39285714285714285,0.10256410256410256,0.11515151515151516,0.11515151515151516,0.24770642201834864
Task,Update the allocator to not offer unreserved resources beyond quota.,"Eventually, we will want to offer unreserved resources as revocable beyond the role's quota. Rather than offering non-revocable resources beyond the role's quota's guarantee, in the short term, we choose to not offer resources beyond a role's quota.",2.0,0,0.0,0.6726998491704375,0.0,0.0,0.0,0.1,0.6000000000000001,0.2807017543859649,0.29285714285714287,0.21794871794871795,0.26666666666666666,0.26666666666666666,0.0
Task,Account for reserved resources in the quota guarantee check.,Reserved resources should be accounted for in the quota guarantee check so that frameworks cannot continually reserve resources to pull them out of the quota pool.,2.0,0,0.0,0.6726998491704375,0.0,0.0,0.0,0.1,0.6000000000000001,0.2807017543859649,0.29285714285714287,0.21794871794871795,0.26666666666666666,0.26666666666666666,0.0
Task,Include the allocated portion of reserved resources in the role sorter for DRF.,"Reserved resources should be accounted for fairness calculation whether they are allocated or not, since they model a long or forever running task. That is, the effect of reserving resources is equivalent to launching a task in that the resources that make up the reservation are not available to other roles as non-revocable.

In the short-term, we should at least account for the allocated portion of the reservation.",1.0,0,0.0,0.6726998491704375,0.0,0.0,0.0,0.1,0.6000000000000001,0.2807017543859649,0.29285714285714287,0.21794871794871795,0.26666666666666666,0.26666666666666666,0.0
Improvement,Introduce a status() interface for isolators,"While launching a container mesos isolators end up configuring/modifying various properties of the container. For e.g., cgroup isolators (mem, cpu, net_cls) configure/change the properties associated with their respective subsystems before launching a container. Similary network isolator (net-modules, port mapping) configure the IP address and ports associated with a container. 

Currently, there are not interface in the isolator to extract the run time state of these properties for a given container. Therefore a status() method needs to be implemented in the isolators to allow the containerizer to extract the container status information from the isolator. ",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2807017543859649,0.39285714285714285,0.1282051282051282,0.3090909090909091,0.3090909090909091,0.0
Bug,ContainerLoggerTest.LOGROTATE_RotateInSandbox breaks when running on Centos6.,"{noformat}
[17:24:58][Step 7/7] logrotate: bad argument --version: unknown error
[17:24:58][Step 7/7] F0126 17:24:57.913729  4503 container_logger_tests.cpp:380] CHECK_SOME(containerizer): Failed to create container logger: Failed to create container logger module 'org_apache_mesos_LogrotateContainerLogger': Error creating Module instance for 'org_apache_mesos_LogrotateContainerLogger' 
[17:24:58][Step 7/7] *** Check failure stack trace: ***
[17:24:58][Step 7/7]     @     0x7f11ae0d2d40  google::LogMessage::Fail()
[17:24:58][Step 7/7]     @     0x7f11ae0d2c9c  google::LogMessage::SendToLog()
[17:24:58][Step 7/7]     @     0x7f11ae0d2692  google::LogMessage::Flush()
[17:24:58][Step 7/7]     @     0x7f11ae0d544c  google::LogMessageFatal::~LogMessageFatal()
[17:24:58][Step 7/7]     @           0x983927  _CheckFatal::~_CheckFatal()
[17:24:58][Step 7/7]     @           0xa9a18b  mesos::internal::tests::ContainerLoggerTest_LOGROTATE_RotateInSandbox_Test::TestBody()
[17:24:58][Step 7/7]     @          0x1623a4e  testing::internal::HandleSehExceptionsInMethodIfSupported<>()
[17:24:58][Step 7/7]     @          0x161eab2  testing::internal::HandleExceptionsInMethodIfSupported<>()
[17:24:58][Step 7/7]     @          0x15ffdfd  testing::Test::Run()
[17:24:58][Step 7/7]     @          0x160058b  testing::TestInfo::Run()
[17:24:58][Step 7/7]     @          0x1600bc6  testing::TestCase::Run()
[17:24:58][Step 7/7]     @          0x1607515  testing::internal::UnitTestImpl::RunAllTests()
[17:24:58][Step 7/7]     @          0x16246dd  testing::internal::HandleSehExceptionsInMethodIfSupported<>()
[17:24:58][Step 7/7]     @          0x161f608  testing::internal::HandleExceptionsInMethodIfSupported<>()
[17:24:58][Step 7/7]     @          0x1606245  testing::UnitTest::Run()
[17:24:58][Step 7/7]     @           0xde36b6  RUN_ALL_TESTS()
[17:24:58][Step 7/7]     @           0xde32cc  main
[17:24:58][Step 7/7]     @     0x7f11a8896d5d  __libc_start_main
[17:24:58][Step 7/7]     @           0x981fc9  (unknown)
{noformat}",1.0,0,0.5,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.3717948717948718,0.24848484848484845,0.24848484848484845,0.0
Bug,Build failure when using gcc-4.9 - signed/unsigned mismatch.,"When building the current master, the following happens when using gcc-4.9:

{noformat}
mv -f examples/.deps/persistent_volume_framework-persistent_volume_framework.Tpo examples/.deps/persistent_volume_framework-persistent_volume_framework.Po
g++-4.9 -DPACKAGE_NAME=\""mesos\"" -DPACKAGE_TARNAME=\""mesos\"" -DPACKAGE_VERSION=\""0.27.0\"" -DPACKAGE_STRING=\""mesos\ 0.27.0\"" -DPACKAGE_BUGREPORT=\""\"" -DPACKAGE_URL=\""\"" -DPACKAGE=\""mesos\"" -DVERSION=\""0.27.0\"" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_DLFCN_H=1 -DLT_OBJDIR=\"".libs/\"" -DHAVE_PTHREAD_PRIO_INHERIT=1 -DHAVE_PTHREAD=1 -DHAVE_LIBZ=1 -DHAVE_LIBCURL=1 -DHAVE_APR_POOLS_H=1 -DHAVE_LIBAPR_1=1 -DHAVE_SVN_VERSION_H=1 -DHAVE_LIBSVN_SUBR_1=1 -DHAVE_SVN_DELTA_H=1 -DHAVE_LIBSVN_DELTA_1=1 -DHAVE_LIBSASL2=1 -I. -I../../src   -Wall -Werror -DLIBDIR=\""/usr/local/lib\"" -DPKGLIBEXECDIR=\""/usr/local/libexec/mesos\"" -DPKGDATADIR=\""/usr/local/share/mesos\"" -I../../include -I../../3rdparty/libprocess/include -I../../3rdparty/libprocess/3rdparty/stout/include -I../include -I../include/mesos -isystem ../3rdparty/libprocess/3rdparty/boost-1.53.0 -I../3rdparty/libprocess/3rdparty/picojson-1.3.0 -DPICOJSON_USE_INT64 -D__STDC_FORMAT_MACROS -I../3rdparty/libprocess/3rdparty/protobuf-2.5.0/src -I../3rdparty/libprocess/3rdparty/glog-0.3.3/src -I../3rdparty/libprocess/3rdparty/glog-0.3.3/src -I../3rdparty/leveldb/include -I../3rdparty/zookeeper-3.4.5/src/c/include -I../3rdparty/zookeeper-3.4.5/src/c/generated -I../3rdparty/libprocess/3rdparty/protobuf-2.5.0/src -DSOURCE_DIR=\""/Users/till/Development/mesos-private/build/..\"" -DBUILD_DIR=\""/Users/till/Development/mesos-private/build\"" -I../3rdparty/libprocess/3rdparty/gmock-1.7.0/gtest/include -I../3rdparty/libprocess/3rdparty/gmock-1.7.0/include  -I/usr/local/opt/openssl/include -I/usr/local/opt/libevent/include -I/usr/local/opt/subversion/include/subversion-1 -I/usr/include/apr-1 -I/usr/include/apr-1.0  -D_THREAD_SAFE -pthread -g1 -O0 -Wno-unused-local-typedefs -std=c++11 -DGTEST_USE_OWN_TR1_TUPLE=1 -DGTEST_LANG_CXX11 -MT tests/mesos_tests-container_logger_tests.o -MD -MP -MF tests/.deps/mesos_tests-container_logger_tests.Tpo -c -o tests/mesos_tests-container_logger_tests.o `test -f 'tests/container_logger_tests.cpp' || echo '../../src/'`tests/container_logger_tests.cpp
mv -f slave/qos_controllers/.deps/mesos_tests-load.Tpo slave/qos_controllers/.deps/mesos_tests-load.Po
g++-4.9 -DPACKAGE_NAME=\""mesos\"" -DPACKAGE_TARNAME=\""mesos\"" -DPACKAGE_VERSION=\""0.27.0\"" -DPACKAGE_STRING=\""mesos\ 0.27.0\"" -DPACKAGE_BUGREPORT=\""\"" -DPACKAGE_URL=\""\"" -DPACKAGE=\""mesos\"" -DVERSION=\""0.27.0\"" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_DLFCN_H=1 -DLT_OBJDIR=\"".libs/\"" -DHAVE_PTHREAD_PRIO_INHERIT=1 -DHAVE_PTHREAD=1 -DHAVE_LIBZ=1 -DHAVE_LIBCURL=1 -DHAVE_APR_POOLS_H=1 -DHAVE_LIBAPR_1=1 -DHAVE_SVN_VERSION_H=1 -DHAVE_LIBSVN_SUBR_1=1 -DHAVE_SVN_DELTA_H=1 -DHAVE_LIBSVN_DELTA_1=1 -DHAVE_LIBSASL2=1 -I. -I../../src   -Wall -Werror -DLIBDIR=\""/usr/local/lib\"" -DPKGLIBEXECDIR=\""/usr/local/libexec/mesos\"" -DPKGDATADIR=\""/usr/local/share/mesos\"" -I../../include -I../../3rdparty/libprocess/include -I../../3rdparty/libprocess/3rdparty/stout/include -I../include -I../include/mesos -isystem ../3rdparty/libprocess/3rdparty/boost-1.53.0 -I../3rdparty/libprocess/3rdparty/picojson-1.3.0 -DPICOJSON_USE_INT64 -D__STDC_FORMAT_MACROS -I../3rdparty/libprocess/3rdparty/protobuf-2.5.0/src -I../3rdparty/libprocess/3rdparty/glog-0.3.3/src -I../3rdparty/libprocess/3rdparty/glog-0.3.3/src -I../3rdparty/leveldb/include -I../3rdparty/zookeeper-3.4.5/src/c/include -I../3rdparty/zookeeper-3.4.5/src/c/generated -I../3rdparty/libprocess/3rdparty/protobuf-2.5.0/src -DSOURCE_DIR=\""/Users/till/Development/mesos-private/build/..\"" -DBUILD_DIR=\""/Users/till/Development/mesos-private/build\"" -I../3rdparty/libprocess/3rdparty/gmock-1.7.0/gtest/include -I../3rdparty/libprocess/3rdparty/gmock-1.7.0/include  -I/usr/local/opt/openssl/include -I/usr/local/opt/libevent/include -I/usr/local/opt/subversion/include/subversion-1 -I/usr/include/apr-1 -I/usr/include/apr-1.0  -D_THREAD_SAFE -pthread -g1 -O0 -Wno-unused-local-typedefs -std=c++11 -DGTEST_USE_OWN_TR1_TUPLE=1 -DGTEST_LANG_CXX11 -MT tests/mesos_tests-containerizer.o -MD -MP -MF tests/.deps/mesos_tests-containerizer.Tpo -c -o tests/mesos_tests-containerizer.o `test -f 'tests/containerizer.cpp' || echo '../../src/'`tests/containerizer.cpp
In file included from ../3rdparty/libprocess/3rdparty/gmock-1.7.0/include/gmock/internal/gmock-internal-utils.h:47:0,
                 from ../3rdparty/libprocess/3rdparty/gmock-1.7.0/include/gmock/gmock-actions.h:46,
                 from ../3rdparty/libprocess/3rdparty/gmock-1.7.0/include/gmock/gmock.h:58,
                 from ../../src/tests/container_logger_tests.cpp:21:
../3rdparty/libprocess/3rdparty/gmock-1.7.0/gtest/include/gtest/gtest.h: In instantiation of 'testing::AssertionResult testing::internal::CmpHelperLE(const char*, const char*, const T1&, const T2&) [with T1 = int; T2 = long long unsigned int]':
../../src/tests/container_logger_tests.cpp:467:3:   required from here
../3rdparty/libprocess/3rdparty/gmock-1.7.0/gtest/include/gtest/gtest.h:1579:28: error: comparison between signed and unsigned integer expressions [-Werror=sign-compare]
 GTEST_IMPL_CMP_HELPER_(LE, <=);
                            ^
../3rdparty/libprocess/3rdparty/gmock-1.7.0/gtest/include/gtest/gtest.h:1562:12: note: in definition of macro 'GTEST_IMPL_CMP_HELPER_'
   if (val1 op val2) {\
            ^
../3rdparty/libprocess/3rdparty/gmock-1.7.0/gtest/include/gtest/gtest.h: In instantiation of 'testing::AssertionResult testing::internal::CmpHelperGE(const char*, const char*, const T1&, const T2&) [with T1 = int; T2 = long long unsigned int]':
../../src/tests/container_logger_tests.cpp:468:3:   required from here
../3rdparty/libprocess/3rdparty/gmock-1.7.0/gtest/include/gtest/gtest.h:1583:28: error: comparison between signed and unsigned integer expressions [-Werror=sign-compare]
 GTEST_IMPL_CMP_HELPER_(GE, >=);
                            ^
../3rdparty/libprocess/3rdparty/gmock-1.7.0/gtest/include/gtest/gtest.h:1562:12: note: in definition of macro 'GTEST_IMPL_CMP_HELPER_'
   if (val1 op val2) {\
            ^
mv -f tests/.deps/mesos_tests-anonymous_tests.Tpo tests/.deps/mesos_tests-anonymous_tests.Po
{noformat}",1.0,0,0.0,0.3740573152337858,0.0,0.0,0.0,0.2,0.4,0.43859649122807015,0.2714285714285714,0.3717948717948718,0.24848484848484845,0.24848484848484845,0.0
Bug,Render quota status consistently with other endpoints.,"Currently quota status endpoint returns a collection of {{QuotaInfo}} protos converted to JSON. An example response looks like this:
{code:xml}
{
  ""infos"": [
    {
      ""role"": ""role1"",
      ""guarantee"": [
        {
          ""name"": ""cpus"",
          ""role"": ""*"",
          ""type"": ""SCALAR"",
          ""scalar"": { ""value"": 12 }
        },
        {
          ""name"": ""mem"",
          ""role"": ""*"",
          ""type"": ""SCALAR"",
          ""scalar"": { ""value"": 6144 }
        }
      ]
    }
  ]
}
{code}

Presence of some fields, e.g. ""role"", is misleading. To address this issue and make the output more informative, we should probably introduce a  {{model()}} function for {{QuotaStatus}}.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.717948717948718,0.8242424242424242,0.7212121212121212,0.0
Task,Remove deprecated .json endpoints.,"We deprecated the *.json endpoints with MESOS-2058 and MESOS-2984, we should remove them after the deprecation cycle.",1.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.11538461538461538,0.23636363636363636,0.23636363636363636,0.0
Improvement,Hierarchical allocator performance is slow due to Quota,"Since we do not strip the non-scalar resources during the resource arithmetic for quota, the performance can degrade significantly, as currently resource arithmetic is expensive.

One approach to resolving this is to filter the resources we use to perform this arithmetic to only use scalars. This is valid as quota can currently only be set for scalar resource types.",3.0,0,0.0,0.42533936651583715,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3142857142857143,0.24358974358974358,0.22424242424242424,0.22424242424242424,0.0
Improvement,Get container status information in slave. ,"As part of MESOS-4487 an interface will be introduce into the `Containerizer` to allow agents to retrieve container state information. The agent needs to use this interface to retrieve container state information during status updates from the executor. The container state information can be then use by the agent to expose various isolator specific configuration (for e.g., IP address allocated by network isolators, net_cls handles allocated by `cgroups/net_cls` isolator), that has been applied to the container, in the state.json endpoint.  ",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2807017543859649,0.39285714285714285,0.1282051282051282,0.3090909090909091,0.3090909090909091,0.0
Improvement,The `cgroups/net_cls` isolator needs to expose handles in the ContainerStatus,The `cgroup/net_cls` isolator is responsible for allocating network handles to containers launched within a net_cls cgroup. The `cgroup/net_cls` isolator needs to expose these handles to the containerizer as part of the `ContainerStatus` when the containerizer queries the status() method of the isolator. The information itself will go as part of a `CgroupInfo` protobuf that will be defined as part of MESOS-4488 .  ,1.0,0.28.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2807017543859649,0.39285714285714285,0.1282051282051282,0.3090909090909091,0.3090909090909091,0.25688073394495414
Improvement,Define a CgroupInfo protobuf to expose cgroup isolator configuration.,"Within `MesosContainerizer` we have an isolator associated with each linux cgroup subsystem. The isolators apply subsystem specific configuration on the containers before launching the containers. For e.g cgroup/net_cls isolator applies net_cls handles, cgroup/mem isolator applies memory quotas, cgroups/cpu-share isolator configures cpu shares. 

Currently, there is no message structure defined to capture the configuration information of the container, for each cgroup isolator that has been applied to the container. We therefore need to define a protobuf that can capture the cgroup configuration of each cgroup isolator that has been applied to the container. This protobuf will be filled in by the cgroup isolator and will be stored as part of `ContainerConfig` in the containerizer. ",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2807017543859649,0.39285714285714285,0.1282051282051282,0.3090909090909091,0.3090909090909091,0.0
Improvement,Introduce status() interface in `Containerizer`,"In the Containerizer, during container isolation, the isolators end up modifying the state of the containers. Examples would be IP address allocation to a container by the 'network isolator, or net_cls handle allocation by the cgroup/net_cls isolator. 

Often times the state of the container, needs to be exposed to operators through the state.json end-point. For e.g. operators or frameworks might want to know the IP-address configured on a particular container, or the net_cls handle associated with a container to configure the right TC rules. However, at present, there is no clean interface for the slave to retrieve the state of a container from the Containerizer for any of the launched containers. Thus, we need to introduce a `status` interface in the `Containerizer` base class, in order for the slave to expose container state information in its state.json.   ",2.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2807017543859649,0.39285714285714285,0.1282051282051282,0.3090909090909091,0.3090909090909091,0.0
Task,Implement AuthN handling on the scheduler library,"Currently, we do not have the ability of passing {{Credentials}} via the scheduler library. Once the master supports AuthN handling for the {{/scheduler}} endpoint, we would need to add this support to the library.",3.0,0,0.0,0.5701357466063348,0.0,0.0,0.0,0.0,0.0,0.6842105263157894,0.5642857142857143,0.3846153846153846,0.4181818181818182,0.4121212121212121,0.0
Task,Implement tests for the new Executor library,We need to add tests for the executor library {{src/executor/executor.cpp}}. One possible approach would be to use the existing tests in {{src/tests/scheduler_tests.cpp}} and make them use the new executor library.,3.0,0,0.0,0.5701357466063348,0.0,0.0,0.0,0.0,0.0,0.6842105263157894,0.5642857142857143,0.3846153846153846,0.4181818181818182,0.4121212121212121,0.0
Bug,Create common sha512 compute utility function.,Add common utility function for computing digests. Start with `sha512` since its immediately needed by appc image fetcher. ,2.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3357142857142857,0.14102564102564102,0.2424242424242424,0.2424242424242424,0.0
Documentation,"Improve documentation around roles, principals, authz, and reservations","* What is the difference between a role and a principal?
* Why do some ACL entities reference ""roles"" but others reference ""principals""? In a typical organization, what real-world entities would my roles vs. principals map to? The ACL documentation could use more information about the motivation of ACLs and examples of configuring ACLs to meet real-world security policies.
* We should give some examples of making reservations when the role and principal are different, and why you would want to do that
* We should add an example to the ACL page that includes setting ACLs for reservations and/or persistent volumes",2.0,0,0.5,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.48717948717948717,0.4303030303030303,0.4303030303030303,0.0
Bug,SegFault on agent during executor startup,"When repeatedly performing our system tests we have found that we get a segfault on one of the agents. It probably occurs about one time in ten. I have attached the full log from that agent. I've attached the log from the agent that failed and the master (although I think this is less helpful).

To reproduce
- I have no idea. It seems to occur at certain times. E.g. like if a packet is created right on a minute boundary or something. But I don't think it's something caused by our code because the timestamps are stamped by mesos. I was surprised not to find a bug already open.",1.0,0.26.0,0.5,0.5701357466063348,0.0,0.0,0.0,0.0,0.0,0.6842105263157894,0.5642857142857143,0.02564102564102564,0.006060606060606061,0.006060606060606061,0.2385321100917431
Task,Fix appc CachedImage image validation,"Currently image validation is done assuming that the image's filename will have  digest (SHA-512) information. This is not part of the spec
    (https://github.com/appc/spec/blob/master/spec/discovery.md).
    
    The spec specifies the tuple <image name, labels> as unique identifier for  discovering an image.
",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3357142857142857,0.14102564102564102,0.2424242424242424,0.2424242424242424,0.0
Task,Add 'dependency' message to 'AppcImageManifest' protobuf.,AppcImageManifest protobuf currently lacks 'dependencies' which is necessary for image discovery.,1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3357142857142857,0.14102564102564102,0.2424242424242424,0.2424242424242424,0.0
Task,Disable the test RegistryClientTest.BadTokenServerAddress.,"As we are retiring registry client, disable this test which looks flaky.",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3357142857142857,0.14102564102564102,0.2424242424242424,0.2424242424242424,0.0
Task,Update `Master::Http::stateSummary` to use `jsonify`.,Update {{state-summary}} to use {{jsonify}} to stay consistent with {{state}} HTTP endpoint.,3.0,0,0.0,0.6726998491704375,0.0,0.0,0.0,0.1,0.6000000000000001,0.2807017543859649,0.29285714285714287,0.21794871794871795,0.26666666666666666,0.26666666666666666,0.0
Bug,"Install 3rdparty package boost, glog, protobuf and picojson when installing Mesos",Mesos modules depend on having these packages installed with the exact version as Mesos was compiled with.,3.0,0,0.0,0.3453996983408748,0.0,0.0,0.0,0.0,0.2,0.38596491228070173,0.35714285714285715,0.28205128205128205,0.29090909090909095,0.28484848484848485,0.0
Task,Implement a callback testing interface for the Executor Library,"Currently, we do not have a mocking based callback interface for the executor library. This should look similar to the ongoing work for MESOS-3339 i.e. the corresponding issue for the scheduler library.

The interface should allow us to set expectations like we do for the driver. An example:

{code}
EXPECT_CALL(executor, connected())
  .Times(1)
{code}",3.0,0,0.0,0.5701357466063348,0.0,0.0,0.0,0.0,0.0,0.6842105263157894,0.5642857142857143,0.3846153846153846,0.4181818181818182,0.4121212121212121,0.0
Bug,Introduce filtering test abstractions for HTTP events to libprocess,"We need a test abstraction for {{HttpEvent}} similar to the already existing one's for {{DispatchEvent}}, {{MessageEvent}} in libprocess.

The abstraction can look similar in semantics to the already existing {{FUTURE_DISPATCH}}/{{FUTURE_MESSAGE}}.",3.0,0,0.0,0.5701357466063348,0.0,0.0,0.0,0.0,0.0,0.6842105263157894,0.5642857142857143,0.3846153846153846,0.4181818181818182,0.4121212121212121,0.0
Task,"Document that /reserve, /create-volumes endpoints can return misleading ""success""","The docs for the {{/reserve}} endpoint say:

{noformat}
200 OK: Success (the requested resources have been reserved).
{noformat}

This is not true: the master returns {{200}} when the request has been validated and a {{CheckpointResourcesMessage}} has been sent to the agent, but the master does not attempt to verify that the message has been received or that the agent successfully checkpointed. Same behavior applies to {{/unreserve}}, {{/create-volumes}}, and {{/destroy-volumes}}.

We should _either_:

1. Accurately document what {{200}} return code means.
2. Change the implementation to wait for the agent's next checkpoint to succeed (and to include the effect of the operation) before returning success to the HTTP client.",3.0,0,0.0,1.0,0.0,0.0,0.0,0.0,0.8,0.5438596491228069,0.39999999999999997,0.48717948717948717,0.4303030303030303,0.4303030303030303,0.0
Bug,Prevent allocator from crashing on successful recovery.,"There might be a bug that may crash the master as pointed out by [~bmahler] in https://reviews.apache.org/r/42222/:
{noformat}
It looks like if we trip the resume call in addSlave, this delayed resume will crash the master 
due to the CHECK(paused) that currently resides in resume.
{noformat}",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.717948717948718,0.8242424242424242,0.7212121212121212,0.0
Bug,Traverse all roles for quota allocation.,There might be a bug in how resources are allocated to multiple quota'ed roles if one role's quota is met. We need to investigate this behavior.,3.0,0,0.5,0.3137254901960784,0.0,0.0,0.0,0.0,0.4,0.19298245614035087,0.14285714285714285,0.717948717948718,0.8242424242424242,0.7212121212121212,0.0
Improvement,Introduce protobuf for quota set request.,"To document quota request JSON schema and simplify request processing, introduce a {{QuotaRequest}} protobuf wrapper.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.717948717948718,0.8242424242424242,0.7212121212121212,0.0
Task,Check paths in DiskInfo.Source.Path exist during slave initialization.,We have two options here. We can either check and fail if it does not exists. Or we can create if it does not exist like we did for slave.work_dir.,2.0,0,0.5,0.42533936651583715,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3142857142857143,0.7948717948717948,1.0,1.0,0.0
Task,Update filesystem isolators to look for persistent volume directories from the correct location.,"This is related to MESOS-4400.

Since persistent volume directories can be created from non root disk now. We need to adjust both posix and linux filesystem isolator to look for volumes from the correct location based on the information in DiskInfo.Source.

See relevant code in:
{code}
Future<Nothing> PosixFilesystemIsolatorProcess::update(..);
Future<Nothing> LinuxFilesystemIsolatorProcess::update(..);
{code}",2.0,0,0.5,0.42533936651583715,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3142857142857143,0.7948717948717948,1.0,1.0,0.0
Task,Create persistent volume directories based on DiskInfo.Source.,"Currently, we always create persistent volumes from root disk, and the persistent volumes are directories. With DiskInfo.Source being added, we should create the persistent volume accordingly based on the information in DiskInfo.Source.

This ticket handles the case where DiskInfo.Source.type is PATH. In that case, we should create sub-directories and use the same layout as slave.work_dir.

See the relevant code here:
{code}
void Slave::checkpointResources(...)
{
  // Creates persistent volumes that do not exist and schedules
  // releasing those persistent volumes that are no longer needed.
  //
  // TODO(jieyu): Consider introducing a volume manager once we start
  // to support multiple disks, or raw disks. Depending on the
  // DiskInfo, we may want to create either directories under a root
  // directory, or LVM volumes from a given device.
  Resources volumes = newCheckpointedResources.persistentVolumes();

  foreach (const Resource& volume, volumes) {
    // This is validated in master.
    CHECK_NE(volume.role(), ""*"");

    string path = paths::getPersistentVolumePath(
        flags.work_dir,
        volume.role(),
        volume.disk().persistence().id());

    if (!os::exists(path)) {
      CHECK_SOME(os::mkdir(path, true))
        << ""Failed to create persistent volume at '"" << path << ""'"";
    }
  }
}
{code}",2.0,0,0.5,0.42533936651583715,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3142857142857143,0.7948717948717948,1.0,1.0,0.0
Bug,Add persistent volume endpoint tests with no principal,There are currently no persistent volume endpoint tests that do not use a principal; they should be added.,1.0,0,0.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Task,Shared Volumes Design Doc,Review & Approve design doc,3.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.2,0.0,0.007142857142857143,0.05128205128205128,0.1515151515151515,0.15757575757575756,0.0
Bug,Offers and InverseOffers cannot be accepted in the same ACCEPT call,"*Problem*
* In {{Master::accept}}, {{validation::offer::validate}} returns an error when an {{InverseOffer}} is included in the list of {{OfferIDs}} in an {{ACCEPT}} call.
* If an {{Offer}} is part of the same {{ACCEPT}}, the master sees {{error.isSome()}} and returns a {{TASK_LOST}} for normal offers.  (https://github.com/apache/mesos/blob/fafbdca610d0a150b9fa9cb62d1c63cb7a6fdaf3/src/master/master.cpp#L3117)

Here's a regression test:
https://reviews.apache.org/r/42092/

*Proprosal*
The question is whether we want to allow the mixing of {{Offers}} and {{InverseOffers}}.

Arguments for mixing:
* The design/structure of the maintenance originally intended to overload {{ACCEPT}} and {{DECLINE}} to take inverse offers.
* Enforcing non-mixing may require breaking changes to {{scheduler.proto}}.

Arguments against mixing:
* Some semantics are difficult to explain.  What does it mean to supply {{InverseOffers}} with {{Offer::Operations}}?  What about {{DECLINE}} with {{Offers}} and {{InverseOffers}}, including a ""reason""?
* What happens if we presumably add a third type of offer?
* Does it make sense to {{TASK_LOST}} valid normal offers if {{InverseOffers}} are invalid?",2.0,0.25.0,0.0,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.2293577981651376
Improvement,Change the `principal` in `ReservationInfo` to optional,"With the addition of HTTP endpoints for {{/reserve}} and {{/unreserve}}, it is now desirable to allow dynamic reservations without a principal, in the case where HTTP authentication is disabled. To allow for this, we will change the {{principal}} field in {{ReservationInfo}} from required to optional. For backwards-compatibility, however, the master should currently invalidate any {{ReservationInfo}} messages that do not have this field set.",1.0,0,0.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Documentation,Improve upgrade compatibility documentation.,Investigate and document upgrade compatibility for 0.27 release.,3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12280701754385964,0.32857142857142857,0.11538461538461538,0.23636363636363636,0.23636363636363636,0.0
Task,Adjust Resource arithmetics for DiskInfo.Source.,"Since we added the Source for DiskInfo, we need to adjust the Resource arithmetics for that. That includes equality check, addable check, subtractable check, etc.",2.0,0,0.5,0.42533936651583715,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3142857142857143,0.7948717948717948,1.0,1.0,0.0
Task,Add Source to Resource.DiskInfo.,"Source is used to describe the extra information about the source of a Disk resource. We will support 'PATH' type first and then 'BLOCK' later.

{noformat}
message Source {
      enum Type {
        PATH = 1;
        BLOCK = 2,
      }

      message Path {
        // Path to the folder (e.g., /mnt/raid/disk0).
        required string root = 1;
        required double total_size = 2;
      }

      message Block {
        // Path to the device file (e.g., /dev/sda1, /dev/vg/v1).
        // It can be a physical partition, or a logical volume (LVM).
        required string device = 1;
      }

      required Type type = 1;
      optional Path path = 2;
      optional Block block = 3;
    }
}
{noformat}",1.0,0,0.5,0.42533936651583715,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3142857142857143,0.7948717948717948,1.0,1.0,0.0
Documentation,Document units associated with resource types,We should document the units associated with memory and disk resources.,1.0,0,0.0,1.0,0.0,0.0,0.0,0.0,0.8,0.5438596491228069,0.39999999999999997,0.48717948717948717,0.4303030303030303,0.4303030303030303,0.0
Improvement,Make HierarchicalAllocatorProcess set a Resource's active role during allocation,The concrete implementation here depends on the implementation strategy used to solve MESOS-4367.,3.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.2,0.2631578947368421,0.36428571428571427,1.0,0.8121212121212121,0.8121212121212121,0.0
Improvement,Formating issues and broken links in documentation.,"The online documentation has a number of bad formatting issues and broken links (e.g., mesos-provider.md).",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12280701754385964,0.32857142857142857,0.11538461538461538,0.23636363636363636,0.23636363636363636,0.0
Bug,Create common tar/untar utility function.,"As part of refactoring and creating a common place to add all command utilities, add *tar* and *untar* as the first POC.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3357142857142857,0.14102564102564102,0.2424242424242424,0.2424242424242424,0.0
Task,Expose net_cls network handles in agent's state endpoint,"We need to expose net_cls network handles, associated with containers, to operators and network utilities that would use these network handles to enforce network policy. 

In order to achieve the above we need to add a new field in the `NetworkInfo` protobuf (say NetHandles) and update this field when a container gets assigned to a net_cls cgroup. The `ContainerStatus` protobuf already has the `NetworkInfo` protobuf as a nested message, and the `ContainerStatus` itself is exposed to operators as part of TaskInfo (for tasks associated with the container) in an agent's state.json. ",2.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2807017543859649,0.39285714285714285,0.1282051282051282,0.3090909090909091,0.3090909090909091,0.0
Bug,GMock warning in RoleTest.ImplicitRoleStaticReservation,"{noformat}
[ RUN      ] RoleTest.ImplicitRoleStaticReservation

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: shutdown(0x7fe37a4752f0)
Stack trace:
[       OK ] RoleTest.ImplicitRoleStaticReservation (52 ms)
{noformat}",1.0,0,0.0,1.0,0.0,0.0,0.0,0.0,0.8,0.5438596491228069,0.39999999999999997,0.48717948717948717,0.4303030303030303,0.4303030303030303,0.0
Improvement,Limit the number of processes created by libprocess,"Currently libprocess will create {{max(8, number of CPU cores)}} processes during the initialization, see https://github.com/apache/mesos/blob/0.26.0/3rdparty/libprocess/src/process.cpp#L2146 for details. This should be OK for a normal machine which has no much cores (e.g., 16, 32), but for a powerful machine which may have a large number of cores (e.g., an IBM Power machine may have 192 cores), this will cause too much worker threads which are not necessary.

And since libprocess is widely used in Mesos (master, agent, scheduler, executor), it may also cause some performance issue. For example, when user creates a Docker container via Mesos in a Mesos agent which is running on a powerful machine with 192 cores, the DockerContainerizer in Mesos agent will create a dedicated executor for the container, and there will be 192 worker threads in that executor. And if user creates 1000 Docker containers in that machine, then there will be 1000 executors, i.e., 1000 * 192 worker threads which is a large number and may thrash the OS.

",1.0,0,0.5,0.0030165912518853697,0.0,0.0,0.0,0.0,0.0,0.0,0.007142857142857143,0.23076923076923075,0.296969696969697,0.296969696969697,0.0
Bug,GMock warning in SlaveTest.ContainerUpdatedBeforeTaskReachesExecutor,"{noformat}
[ RUN      ] SlaveTest.ContainerUpdatedBeforeTaskReachesExecutor

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: shutdown(0x7fe189cae850)
Stack trace:
[       OK ] SlaveTest.ContainerUpdatedBeforeTaskReachesExecutor (51 ms)
{noformat}

Occurs non-deterministically for me on OSX 10.10, perhaps one run in ten.",1.0,0,0.0,1.0,0.0,0.0,0.0,0.0,0.8,0.5438596491228069,0.39999999999999997,0.48717948717948717,0.4303030303030303,0.4303030303030303,0.0
Bug,"GMock warning in HookTest.VerifySlaveRunTaskHook, HookTest.VerifySlaveTaskStatusDecorator","{noformat}
[ RUN      ] HookTest.VerifySlaveRunTaskHook

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: shutdown(0x7ff079cb2420)
Stack trace:
[       OK ] HookTest.VerifySlaveRunTaskHook (51 ms)
[ RUN      ] HookTest.VerifySlaveTaskStatusDecorator

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: shutdown(0x7ff079cbb790)
Stack trace:
[       OK ] HookTest.VerifySlaveTaskStatusDecorator (54 ms)
{noformat}

Occurs non-deterministically for me. OSX 10.10.",1.0,0,0.5,0.5701357466063348,0.0,0.0,0.0,0.0,0.0,0.6842105263157894,0.5642857142857143,0.48717948717948717,0.4303030303030303,0.4303030303030303,0.0
Bug,GMock warning in ReservationTest.ACLMultipleOperations,"{noformat}
[ RUN      ] ReservationTest.ACLMultipleOperations

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: shutdown(0x7fa2a311b300)
Stack trace:
[       OK ] ReservationTest.ACLMultipleOperations (174 ms)
[----------] 1 test from ReservationTest (174 ms total)
{noformat}

Seems to occur non-deterministically for me, maybe once per 50 runs or so. OSX 10.10",1.0,0,0.5,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.48717948717948717,0.4303030303030303,0.4303030303030303,0.0
Task,Implement a network-handle manager for net_cls cgroup subsystem,"As part of implementing the net_cls cgroup isolator we need a mechanism to manage the minor handles that will be allocated to containers when they are associated with a net_cls cgroup. The network-handle manager needs to provide the following functionality:

a) During normal operation keep track of the free and allocated network handles. There can be a total of 64K such network handles.
b) On startup, learn the allocated network handle by walking the net_cls cgroup tree for mesos and build a map of free network handles available to the agent. ",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2807017543859649,0.39285714285714285,0.1282051282051282,0.3090909090909091,0.3090909090909091,0.0
Improvement,Allow operators to assign net_cls major handles to mesos agents,"The net_cls cgroup associates a 16-bit major and 16-bit minor network handle to packets originating from tasks associated with a specific net_cls cgroup. In mesos we need to give the operator the ability to fix the 16-bit major handle used in an agent (the minor handle will be allocated by the agent. See MESOS-4345). Fixing the parent handle on the agent allows operators to install default firewall rules using the parent handle to enforce a default policy (say DENY ALL) for all container traffic till the container is allocated a minor handle. 

A simple way to achieve this requirement is to pass the major handle as a flag to the agent at startup. ",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2807017543859649,0.39285714285714285,0.1282051282051282,0.3090909090909091,0.3090909090909091,0.0
Bug,Add parameters to apply patches quiet,Added a parameters to apply the patches quiet; so it's easy for contributor to apply patches with -c.,1.0,0,0.0,0.03469079939668175,0.0,0.0,0.0,0.0,0.0,0.07017543859649122,0.04285714285714286,0.01282051282051282,0.0,0.0,0.0
Documentation,Document supported file types for archive extraction by fetcher,"The Mesos fetcher extracts specified URIs if requested to do so by the scheduler. However, the documentation at http://mesos.apache.org/documentation/latest/fetcher/ doesn't list the file types /extensions that will be extracted by the fetcher.

[The relevant code|https://github.com/apache/mesos/blob/master/src/launcher/fetcher.cpp#L63] specifies an exhaustive list of extensions that will be extracted, the documentation should be updated to match.",1.0,0,0.5,0.06184012066365008,0.0,0.0,0.0,0.0,0.2,0.10526315789473684,0.12857142857142856,0.0,0.006060606060606061,0.006060606060606061,0.0
Improvement,Refactor Appc provisioner tests  ,Current tests can be refactored so that we can reuse some common tasks like test image creation. This will benefit future tests like appc image puller tests.,2.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3357142857142857,0.14102564102564102,0.2424242424242424,0.2424242424242424,0.0
Bug,git commit-msg hook completely breaks fixup commits.,"https://reviews.apache.org/r/41586/ added a git hook to check the commit message format. This completely breaks fixup commits which can be created with
{code}
% git commit --fixup=$SHA1 
{code}
The resulting commit message will then be the one of {{$SHA1}}, but prefixed with {{fixup!}} (followed by a literal space). Tools like {{git rebase}} can automatically use these to e.g., squash matching commits like
{code}
% git rebase -i origin/master --autosquash
{code}
Here all commits for e.g., {{$SHA1}} would be grouped together and squash automatically which is valuable when working on reviews.

We should find a way to reenable such functionality; otherwise we risk that developers completely disable this hook.",1.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.8,0.19298245614035087,0.09999999999999999,1.0,0.8121212121212121,0.8121212121212121,0.0
Documentation,Publish Quota Documentation,Publish and finish the operator guide draft  for quota which describes basic usage of the endpoints and few basic and advanced usage cases.,3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12280701754385964,0.32857142857142857,0.11538461538461538,0.23636363636363636,0.23636363636363636,0.0
Bug,hdfs operations fail due to prepended / on path for non-hdfs hadoop clients.,"This bug was resolved for the hdfs protocol for MESOS-3602 but since the process checks for the ""hdfs"" protocol at the beginning of the URI, the fix does not extend itself to non-hdfs hadoop clients.
{code}
I0107 01:22:01.259490 17678 logging.cpp:172] INFO level logging started!
I0107 01:22:01.259856 17678 fetcher.cpp:422] Fetcher Info: {""cache_directory"":""\/tmp\/mesos\/fetch\/slaves\/530dda5a-481a-4117-8154-3aee637d3b38-S3\/root"",""items"":[{""action"":""BYPASS_CACHE"",""uri"":{""extract"":true,""value"":""maprfs:\/\/\/mesos\/storm-mesos-0.9.3.tgz""}},{""action"":""BYPASS_CACHE"",""uri"":{""extract"":true,""value"":""http:\/\/s0121.stag.urbanairship.com:36373\/conf\/storm.yaml""}}],""sandbox_directory"":""\/mnt\/data\/mesos\/slaves\/530dda5a-481a-4117-8154-3aee637d3b38-S3\/frameworks\/530dda5a-481a-4117-8154-3aee637d3b38-0000\/executors\/word-count-1-1452129714\/runs\/4443d5ac-d034-49b3-bf12-08fb9b0d92d0"",""user"":""root""}
I0107 01:22:01.262171 17678 fetcher.cpp:377] Fetching URI 'maprfs:///mesos/storm-mesos-0.9.3.tgz'
I0107 01:22:01.262212 17678 fetcher.cpp:248] Fetching directly into the sandbox directory
I0107 01:22:01.262243 17678 fetcher.cpp:185] Fetching URI 'maprfs:///mesos/storm-mesos-0.9.3.tgz'
I0107 01:22:01.671777 17678 fetcher.cpp:110] Downloading resource with Hadoop client from 'maprfs:///mesos/storm-mesos-0.9.3.tgz' to '/mnt/data/mesos/slaves/530dda5a-481a-4117-8154-3aee637d3b38-S3/frameworks/530dda5a-481a-4117-8154-3aee637d3b38-0000/executors/word-count-1-1452129714/runs/4443d5ac-d034-49b3-bf12-08fb9b0d92d0/storm-mesos-0.9.3.tgz'
copyToLocal: java.net.URISyntaxException: Expected scheme-specific part at index 7: maprfs:
Usage: java FsShell [-copyToLocal [-ignoreCrc] [-crc] <src> <localdst>]
E0107 01:22:02.435556 17678 shell.hpp:90] Command 'hadoop fs -copyToLocal '/maprfs:///mesos/storm-mesos-0.9.3.tgz' '/mnt/data/mesos/slaves/530dda5a-481a-4117-8154-3aee637d3b38-S3/frameworks/530dda5a-481a-4117-8154-3aee637d3b38-0000/executors/word-count-1-1452129714/runs/4443d5ac-d034-49b3-bf12-08fb9b0d92d0/storm-mesos-0.9.3.tgz'' failed; this is the output:
Failed to fetch 'maprfs:///mesos/storm-mesos-0.9.3.tgz': HDFS copyToLocal failed: Failed to execute 'hadoop fs -copyToLocal '/maprfs:///mesos/storm-mesos-0.9.3.tgz' '/mnt/data/mesos/slaves/530dda5a-481a-4117-8154-3aee637d3b38-S3/frameworks/530dda5a-481a-4117-8154-3aee637d3b38-0000/executors/word-count-1-1452129714/runs/4443d5ac-d034-49b3-bf12-08fb9b0d92d0/storm-mesos-0.9.3.tgz''; the command was either not found or exited with a non-zero exit status: 255
Failed to synchronize with slave (it's probably exited)
{code}

After a brief chat with [~jieyu], it was recommended to fix the current hdfs client code because the new hadoop fetcher plugin is slated to use it.",1.0,0.26.0,0.5,0.06184012066365008,0.0,0.0,0.0,0.0,0.2,0.10526315789473684,0.12857142857142856,0.01282051282051282,0.0,0.0,0.2385321100917431
Bug,Accepting an inverse offer prints misleading logs,"Whenever a scheduler accepts an inverse offer, Mesos will print a line like this in the master logs:
{code}
W1125 10:05:53.155109 29362 master.cpp:2897] ACCEPT call used invalid offers '[ 932f7d7b-f2d4-42c7-9391-222c19b9d35b-O2 ]': Offer 932f7d7b-f2d4-42c7-9391-222c19b9d35b-O2 is no longer valid
{code}

Inverse offers should not trigger this warning.",1.0,0.25.0,0.0,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.2293577981651376
Task,Add AuthN and AuthZ to maintenance endpoints.,Maintenance endpoints are currently only restricted by firewall settings.  They should also support authentication/authorization like other HTTP endpoints.,3.0,0.25.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.2293577981651376
Bug,Sync up configuration.md and flags.cpp,"The https://reviews.apache.org/r/39923/ made some clean up for configuration.md but the related flags.cpp was not updated, we should update those files as well.",1.0,0,0.5,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.10256410256410256,0.06666666666666667,0.06666666666666667,0.0
Task,"Change documentation links to ""*.md""","Right now, links either use the form {noformat}[label](/documentation/latest/foo/){noformat} or {noformat}[label](foo.md){noformat}. We should probably switch to using the latter form consistently -- it previews better on Github, and it will make it easier to have multiple versions of the docs on the website at once in the future.",3.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.12280701754385964,0.32857142857142857,0.48717948717948717,0.4303030303030303,0.4303030303030303,0.0
Task,Tests for quota with implicit roles.,"With the introduction of implicit roles (MESOS-3988), we should make sure quota can be set for an inactive role (unknown to the master) and maybe transition it to the active state.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.717948717948718,0.8242424242424242,0.7212121212121212,0.0
Bug,Mesos command task doesn't support volumes with image,Currently volumes are stripped when an image is specified running a command task with Mesos containerizer. ,3.0,0,0.0,0.277526395173454,0.0,0.0,0.0,0.0,0.4,0.08771929824561403,0.10714285714285714,0.02564102564102564,0.09090909090909091,0.09090909090909091,0.0
Bug,Correctly handle disk quota usage when volumes are bind mounted into the container.,In its current implementation disk quota enforcement on the task sandbox will not work correctly when disk volumes are bind mounted into the task sandbox (this happens when Linux filesystem isolator is used).,3.0,0,0.0,0.07088989441930618,0.0,0.0,0.0,0.0,0.2,0.14035087719298245,0.10714285714285714,0.11538461538461538,0.11515151515151516,0.11515151515151516,0.0
Bug,ExamplesTest.NoExecutorFramework runs forever.,"{noformat: title=Good Run}_x000D_
[ RUN      ] ExamplesTest.NoExecutorFramework_x000D_
I1221 23:10:02.721617 32528 exec.cpp:444] Ignoring exited event because the driver is aborted!_x000D_
Using temporary directory '/tmp/ExamplesTest_NoExecutorFramework_fCmFLn'_x000D_
I1221 23:10:02.721675 32539 exec.cpp:444] Ignoring exited event because the driver is aborted!_x000D_
I1221 23:10:02.722024 32554 exec.cpp:444] Ignoring exited event because the driver is aborted!_x000D_
WARNING: Logging before InitGoogleLogging() is written to STDERR_x000D_
I1221 23:10:05.179466 32569 resources.cpp:478] Parsing resources as JSON failed: cpus:0.1;mem:32;disk:32_x000D_
Trying semicolon-delimited string format instead_x000D_
I1221 23:10:05.180269 32569 logging.cpp:172] Logging to STDERR_x000D_
I1221 23:10:05.185768 32569 process.cpp:998] libprocess is initialized on 172.17.0.2:40874 for 16 cpus_x000D_
I1221 23:10:05.200728 32569 leveldb.cpp:174] Opened db in 4.184362ms_x000D_
I1221 23:10:05.202234 32569 leveldb.cpp:181] Compacted db in 1.459268ms_x000D_
I1221 23:10:05.202353 32569 leveldb.cpp:196] Created db iterator in 73761ns_x000D_
I1221 23:10:05.202383 32569 leveldb.cpp:202] Seeked to beginning of db in 3382ns_x000D_
I1221 23:10:05.202405 32569 leveldb.cpp:271] Iterated through 0 keys in the db in 633ns_x000D_
I1221 23:10:05.202674 32569 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned_x000D_
I1221 23:10:05.205301 32604 recover.cpp:447] Starting replica recovery_x000D_
I1221 23:10:05.206414 32569 local.cpp:239] Using 'local' authorizer_x000D_
I1221 23:10:05.206405 32604 recover.cpp:473] Replica is in EMPTY status_x000D_
I1221 23:10:05.209595 32594 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (4)@172.17.0.2:40874_x000D_
I1221 23:10:05.210916 32596 recover.cpp:193] Received a recover response from a replica in EMPTY status_x000D_
I1221 23:10:05.211515 32597 master.cpp:365] Master 3931c1a8-1cd6-49eb-94c8-d01b33bb008e (6ccf2ee56b13) started on 172.17.0.2:40874_x000D_
I1221 23:10:05.211699 32605 recover.cpp:564] Updating replica status to STARTING_x000D_
I1221 23:10:05.211539 32597 master.cpp:367] Flags at startup: --acls=""permissive: false_x000D_
register_frameworks {_x000D_
  principals {_x000D_
    type: SOME_x000D_
    values: ""test-principal""_x000D_
  }_x000D_
  roles {_x000D_
    type: SOME_x000D_
    values: ""*""_x000D_
  }_x000D_
}_x000D_
run_tasks {_x000D_
  principals {_x000D_
    type: SOME_x000D_
    values: ""test-principal""_x000D_
  }_x000D_
  users {_x000D_
    type: SOME_x000D_
    values: ""mesos""_x000D_
  }_x000D_
}_x000D_
"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_slaves=""false"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/ExamplesTest_NoExecutorFramework_fCmFLn/credentials"" --framework_sorter=""drf"" --help=""true"" --hostname_lookup=""true"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""5secs"" --registry_strict=""false"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.27.0/src/webui"" --work_dir=""/tmp/mesos-otpdch"" --zk_session_timeout=""10secs""_x000D_
I1221 23:10:05.212323 32597 master.cpp:412] Master only allowing authenticated frameworks to register_x000D_
I1221 23:10:05.212337 32597 master.cpp:419] Master allowing unauthenticated slaves to register_x000D_
I1221 23:10:05.212347 32597 credentials.hpp:35] Loading credentials for authentication from '/tmp/ExamplesTest_NoExecutorFramework_fCmFLn/credentials'_x000D_
W1221 23:10:05.212442 32597 credentials.hpp:50] Permissions on credentials file '/tmp/ExamplesTest_NoExecutorFramework_fCmFLn/credentials' are too open. It is recommended that your credentials file is NOT accessible by others._x000D_
I1221 23:10:05.212606 32600 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 656857ns_x000D_
I1221 23:10:05.212620 32597 master.cpp:456] Using default 'crammd5' authenticator_x000D_
I1221 23:10:05.212631 32600 replica.cpp:320] Persisted replica status to STARTING_x000D_
I1221 23:10:05.212893 32597 authenticator.cpp:518] Initializing server SASL_x000D_
I1221 23:10:05.213091 32608 recover.cpp:473] Replica is in STARTING status_x000D_
I1221 23:10:05.213958 32595 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (5)@172.17.0.2:40874_x000D_
I1221 23:10:05.214323 32594 recover.cpp:193] Received a recover response from a replica in STARTING status_x000D_
I1221 23:10:05.214689 32595 recover.cpp:564] Updating replica status to VOTING_x000D_
I1221 23:10:05.215353 32596 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 487419ns_x000D_
I1221 23:10:05.215384 32596 replica.cpp:320] Persisted replica status to VOTING_x000D_
I1221 23:10:05.215481 32605 recover.cpp:578] Successfully joined the Paxos group_x000D_
I1221 23:10:05.215867 32605 recover.cpp:462] Recover process terminated_x000D_
I1221 23:10:05.216111 32569 containerizer.cpp:141] Using isolation: filesystem/posix,posix/cpu,posix/mem_x000D_
W1221 23:10:05.217021 32569 backend.cpp:48] Failed to create 'bind' backend: BindBackend requires root privileges_x000D_
I1221 23:10:05.221482 32608 slave.cpp:191] Slave started on 1)@172.17.0.2:40874_x000D_
I1221 23:10:05.221521 32608 slave.cpp:192] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""auth.docker.io"" --docker_auth_server_port=""443"" --docker_kill_orphans=""true"" --docker_local_archives_dir=""/tmp/mesos/images/docker"" --docker_puller=""local"" --docker_puller_timeout=""60"" --docker_registry=""registry-1.docker.io"" --docker_registry_port=""443"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/mesos/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""filesystem/posix,posix/cpu,posix/mem"" --launcher=""posix"" --launcher_dir=""/mesos/mesos-0.27.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""1secs"" --resources=""cpus:2;mem:10240"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/mesos-otpdch/0""_x000D_
I1221 23:10:05.222578 32608 resources.cpp:478] Parsing resources as JSON failed: cpus:2;mem:10240_x000D_
Trying semicolon-delimited string format instead_x000D_
I1221 23:10:05.223465 32608 slave.cpp:392] Slave resources: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]_x000D_
I1221 23:10:05.223621 32569 containerizer.cpp:141] Using isolation: filesystem/posix,posix/cpu,posix/mem_x000D_
I1221 23:10:05.223610 32608 slave.cpp:400] Slave attributes: [  ]_x000D_
I1221 23:10:05.223677 32608 slave.cpp:405] Slave hostname: 6ccf2ee56b13_x000D_
I1221 23:10:05.223697 32608 slave.cpp:410] Slave checkpoint: true_x000D_
W1221 23:10:05.224143 32569 backend.cpp:48] Failed to create 'bind' backend: BindBackend requires root privileges_x000D_
I1221 23:10:05.226668 32604 slave.cpp:191] Slave started on 2)@172.17.0.2:40874_x000D_
I1221 23:10:05.226692 32604 slave.cpp:192] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""auth.docker.io"" --docker_auth_server_port=""443"" --docker_kill_orphans=""true"" --docker_local_archives_dir=""/tmp/mesos/images/docker"" --docker_puller=""local"" --docker_puller_timeout=""60"" --docker_registry=""registry-1.docker.io"" --docker_registry_port=""443"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/mesos/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""filesystem/posix,posix/cpu,posix/mem"" --launcher=""posix"" --launcher_dir=""/mesos/mesos-0.27.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""1secs"" --resources=""cpus:2;mem:10240"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/mesos-otpdch/1""_x000D_
I1221 23:10:05.227520 32604 resources.cpp:478] Parsing resources as JSON failed: cpus:2;mem:10240_x000D_
Trying semicolon-delimited string format instead_x000D_
I1221 23:10:05.228037 32604 slave.cpp:392] Slave resources: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]_x000D_
I1221 23:10:05.228148 32604 slave.cpp:400] Slave attributes: [  ]_x000D_
I1221 23:10:05.228169 32604 slave.cpp:405] Slave hostname: 6ccf2ee56b13_x000D_
I1221 23:10:05.228184 32604 slave.cpp:410] Slave checkpoint: true_x000D_
I1221 23:10:05.229123 32569 containerizer.cpp:141] Using isolation: filesystem/posix,posix/cpu,posix/mem_x000D_
I1221 23:10:05.229641 32605 state.cpp:58] Recovering state from '/tmp/mesos-otpdch/0/meta'_x000D_
W1221 23:10:05.229645 32569 backend.cpp:48] Failed to create 'bind' backend: BindBackend requires root privileges_x000D_
I1221 23:10:05.229636 32595 state.cpp:58] Recovering state from '/tmp/mesos-otpdch/1/meta'_x000D_
I1221 23:10:05.230242 32605 status_update_manager.cpp:200] Recovering status update manager_x000D_
I1221 23:10:05.230254 32598 status_update_manager.cpp:200] Recovering status update manager_x000D_
I1221 23:10:05.230515 32601 containerizer.cpp:383] Recovering containerizer_x000D_
I1221 23:10:05.230562 32602 containerizer.cpp:383] Recovering containerizer_x000D_
I1221 23:10:05.232681 32597 auxprop.cpp:71] Initialized in-memory auxiliary property plugin_x000D_
I1221 23:10:05.232803 32597 master.cpp:493] Authorization enabled_x000D_
I1221 23:10:05.232867 32600 slave.cpp:4427] Finished recovery_x000D_
I1221 23:10:05.232980 32598 slave.cpp:191] Slave started on 3)@172.17.0.2:40874_x000D_
I1221 23:10:05.233039 32594 slave.cpp:4427] Finished recovery_x000D_
I1221 23:10:05.233376 32599 whitelist_watcher.cpp:77] No whitelist given_x000D_
I1221 23:10:05.233428 32601 hierarchical.cpp:147] Initialized hierarchical allocator process_x000D_
I1221 23:10:05.233003 32598 slave.cpp:192] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""auth.docker.io"" --docker_auth_server_port=""443"" --docker_kill_orphans=""true"" --docker_local_archives_dir=""/tmp/mesos/images/docker"" --docker_puller=""local"" --docker_puller_timeout=""60"" --docker_registry=""registry-1.docker.io"" --docker_registry_port=""443"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/mesos/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""filesystem/posix,posix/cpu,posix/mem"" --launcher=""posix"" --launcher_dir=""/mesos/mesos-0.27.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""1secs"" --resources=""cpus:2;mem:10240"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/mesos-otpdch/2""_x000D_
I1221 23:10:05.233744 32600 slave.cpp:4599] Querying resource estimator for oversubscribable resources_x000D_
I1221 23:10:05.233749 32598 resources.cpp:478] Parsing resources as JSON failed: cpus:2;mem:10240_x000D_
Trying semicolon-delimited string format instead_x000D_
I1221 23:10:05.234222 32598 slave.cpp:392] Slave resources: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]_x000D_
I1221 23:10:05.234284 32598 slave.cpp:400] Slave attributes: [  ]_x000D_
I1221 23:10:05.234299 32598 slave.cpp:405] Slave hostname: 6ccf2ee56b13_x000D_
I1221 23:10:05.234311 32598 slave.cpp:410] Slave checkpoint: true_x000D_
I1221 23:10:05.234338 32600 slave.cpp:729] New master detected at master@172.17.0.2:40874_x000D_
I1221 23:10:05.234376 32604 status_update_manager.cpp:174] Pausing sending status updates_x000D_
I1221 23:10:05.234424 32600 slave.cpp:754] No credentials provided. Attempting to register without authentication_x000D_
I1221 23:10:05.234522 32600 slave.cpp:765] Detecting new master_x000D_
I1221 23:10:05.234616 32569 sched.cpp:164] Version: 0.27.0_x000D_
I1221 23:10:05.234658 32600 slave.cpp:4613] Received oversubscribable resources  from the resource estimator_x000D_
I1221 23:10:05.234671 32594 slave.cpp:4599] Querying resource estimator for oversubscribable resources_x000D_
I1221 23:10:05.234884 32606 slave.cpp:4613] Received oversubscribable resources  from the resource estimator_x000D_
I1221 23:10:05.235038 32595 status_update_manager.cpp:174] Pausing sending status updates_x000D_
I1221 23:10:05.235043 32606 slave.cpp:729] New master detected at master@172.17.0.2:40874_x000D_
I1221 23:10:05.235111 32606 slave.cpp:754] No credentials provided. Attempting to register without authentication_x000D_
I1221 23:10:05.235147 32606 slave.cpp:765] Detecting new master_x000D_
I1221 23:10:05.235240 32594 state.cpp:58] Recovering state from '/tmp/mesos-otpdch/2/meta'_x000D_
I1221 23:10:05.235443 32608 status_update_manager.cpp:200] Recovering status update manager_x000D_
I1221 23:10:05.235625 32594 containerizer.cpp:383] Recovering containerizer_x000D_
I1221 23:10:05.236549 32599 slave.cpp:4427] Finished recovery_x000D_
I1221 23:10:05.236984 32593 sched.cpp:262] New master detected at master@172.17.0.2:40874_x000D_
I1221 23:10:05.237004 32599 slave.cpp:4599] Querying resource estimator for oversubscribable resources_x000D_
I1221 23:10:05.237221 32593 sched.cpp:318] Authenticating with master master@172.17.0.2:40874_x000D_
I1221 23:10:05.237277 32593 sched.cpp:325] Using default CRAM-MD5 authenticatee_x000D_
I1221 23:10:05.237285 32604 status_update_manager.cpp:174] Pausing sending status updates_x000D_
I1221 23:10:05.237288 32599 slave.cpp:729] New master detected at master@172.17.0.2:40874_x000D_
I1221 23:10:05.237361 32599 slave.cpp:754] No credentials provided. Attempting to register without authentication_x000D_
I1221 23:10:05.237433 32599 slave.cpp:765] Detecting new master_x000D_
I1221 23:10:05.237565 32599 slave.cpp:4613] Received oversubscribable resources  from the resource estimator_x000D_
I1221 23:10:05.238154 32605 authenticatee.cpp:97] Initializing client SASL_x000D_
I1221 23:10:05.238315 32605 authenticatee.cpp:121] Creating new client SASL connection_x000D_
I1221 23:10:05.239640 32597 master.cpp:1200] Dropping 'mesos.internal.AuthenticateMessage' message since not elected yet_x000D_
I1221 23:10:05.239765 32597 master.cpp:1629] The newly elected leader is master@172.17.0.2:40874 with id 3931c1a8-1cd6-49eb-94c8-d01b33bb008e_x000D_
I1221 23:10:05.239794 32597 master.cpp:1642] Elected as the leading master!_x000D_
I1221 23:10:05.239843 32597 master.cpp:1387] Recovering from registrar_x000D_
I1221 23:10:05.240056 32600 registrar.cpp:307] Recovering registrar_x000D_
I1221 23:10:05.241477 32608 log.cpp:659] Attempting to start the writer_x000D_
I1221 23:10:05.244540 32600 replica.cpp:493] Replica received implicit promise request from (39)@172.17.0.2:40874 with proposal 1_x000D_
I1221 23:10:05.245358 32600 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 776937ns_x000D_
I1221 23:10:05.245393 32600 replica.cpp:342] Persisted promised to 1_x000D_
I1221 23:10:05.246625 32601 coordinator.cpp:238] Coordinator attempting to fill missing positions_x000D_
I1221 23:10:05.248757 32605 replica.cpp:388] Replica received explicit promise request from (40)@172.17.0.2:40874 for position 0 with proposal 2_x000D_
I1221 23:10:05.249214 32605 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 366567ns_x000D_
I1221 23:10:05.249246 32605 replica.cpp:712] Persisted action at 0_x000D_
I1221 23:10:05.250998 32599 replica.cpp:537] Replica received write request for position 0 from (41)@172.17.0.2:40874_x000D_
I1221 23:10:05.251111 32599 leveldb.cpp:436] Reading position from leveldb took 66773ns_x000D_
I1221 23:10:05.251734 32599 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 379612ns_x000D_
I1221 23:10:05.251759 32599 replica.cpp:712] Persisted action at 0_x000D_
I1221 23:10:05.252555 32601 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0_x000D_
I1221 23:10:05.253010 32601 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 381858ns_x000D_
I1221 23:10:05.253036 32601 replica.cpp:712] Persisted action at 0_x000D_
I1221 23:10:05.253068 32601 replica.cpp:697] Replica learned NOP action at position 0_x000D_
I1221 23:10:05.254043 32595 log.cpp:675] Writer started with ending position 0_x000D_
I1221 23:10:05.256741 32595 leveldb.cpp:436] Reading position from leveldb took 48607ns_x000D_
I1221 23:10:05.260617 32601 registrar.cpp:340] Successfully fetched the registry (0B) in 20.47616ms_x000D_
I1221 23:10:05.260988 32601 registrar.cpp:439] Applied 1 operations in 103123ns; attempting to update the 'registry'_x000D_
I1221 23:10:05.264700 32604 log.cpp:683] Attempting to append 170 bytes to the log_x000D_
I1221 23:10:05.265138 32601 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1_x000D_
I1221 23:10:05.266208 32603 replica.cpp:537] Replica received write request for position 1 from (42)@172.17.0.2:40874_x000D_
I1221 23:10:05.266829 32603 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 551087ns_x000D_
I1221 23:10:05.266861 32603 replica.cpp:712] Persisted action at 1_x000D_
I1221 23:10:05.267918 32605 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0_x000D_
I1221 23:10:05.268442 32605 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 453416ns_x000D_
I1221 23:10:05.268470 32605 replica.cpp:712] Persisted action at 1_x000D_
I1221 23:10:05.268506 32605 replica.cpp:697] Replica learned APPEND action at position 1_x000D_
I1221 23:10:05.270512 32606 registrar.cpp:484] Successfully updated the 'registry' in 9.375232ms_x000D_
I1221 23:10:05.270705 32606 registrar.cpp:370] Successfully recovered registrar_x000D_
I1221 23:10:05.271045 32602 log.cpp:702] Attempting to truncate the log to 1_x000D_
I1221 23:10:05.271178 32603 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2_x000D_
I1221 23:10:05.271695 32605 master.cpp:1439] Recovered 0 slaves from the Registry (131B) ; allowing 10mins for slaves to re-register_x000D_
I1221 23:10:05.271751 32603 hierarchical.cpp:165] Skipping recovery of hierarchical allocator: nothing to recover_x000D_
I1221 23:10:05.272274 32596 replica.cpp:537] Replica received write request for position 2 from (43)@172.17.0.2:40874_x000D_
I1221 23:10:05.272838 32596 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 451122ns_x000D_
I1221 23:10:05.272867 32596 replica.cpp:712] Persisted action at 2_x000D_
I1221 23:10:05.273483 32607 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0_x000D_
I1221 23:10:05.273919 32607 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 405444ns_x000D_
I1221 23:10:05.273977 32607 leveldb.cpp:399] Deleting ~1 keys from leveldb took 33953ns_x000D_
I1221 23:10:05.273998 32607 replica.cpp:712] Persisted action at 2_x000D_
I1221 23:10:05.274024 32607 replica.cpp:697] Replica learned TRUNCATE action at position 2_x000D_
I1221 23:10:05.288020 32593 slave.cpp:1254] Will retry registration in 1.091568855secs if necessary_x000D_
I1221 23:10:05.288321 32605 master.cpp:4132] Registering slave at slave(3)@172.17.0.2:40874 (6ccf2ee56b13) with id 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S0_x000D_
I1221 23:10:05.289105 32607 registrar.cpp:439] Applied 1 operations in 104776ns; attempting to update the 'registry'_x000D_
I1221 23:10:05.291494 32605 log.cpp:683] Attempting to append 339 bytes to the log_x000D_
I1221 23:10:05.291594 32595 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3_x000D_
I1221 23:10:05.292273 32602 replica.cpp:537] Replica received write request for position 3 from (44)@172.17.0.2:40874_x000D_
I1221 23:10:05.292811 32602 leveldb.cpp:341] Persisting action (358 bytes) to leveldb took 499449ns_x000D_
I1221 23:10:05.292845 32602 replica.cpp:712] Persisted action at 3_x000D_
I1221 23:10:05.293373 32594 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0_x000D_
I1221 23:10:05.293776 32594 leveldb.cpp:341] Persisting action (360 bytes) to leveldb took 376184ns_x000D_
I1221 23:10:05.293799 32594 replica.cpp:712] Persisted action at 3_x000D_
I1221 23:10:05.293833 32594 replica.cpp:697] Replica learned APPEND action at position 3_x000D_
I1221 23:10:05.295142 32603 registrar.cpp:484] Successfully updated the 'registry' in 5.945088ms_x000D_
I1221 23:10:05.295403 32602 log.cpp:702] Attempting to truncate the log to 3_x000D_
I1221 23:10:05.295513 32603 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4_x000D_
I1221 23:10:05.296146 32598 replica.cpp:537] Replica received write request for position 4 from (45)@172.17.0.2:40874_x000D_
I1221 23:10:05.296443 32608 slave.cpp:3371] Received ping from slave-observer(1)@172.17.0.2:40874_x000D_
I1221 23:10:05.296552 32598 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 368428ns_x000D_
I1221 23:10:05.296604 32598 replica.cpp:712] Persisted action at 4_x000D_
I1221 23:10:05.296744 32594 master.cpp:4200] Registered slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S0 at slave(3)@172.17.0.2:40874 (6ccf2ee56b13) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]_x000D_
I1221 23:10:05.297025 32597 slave.cpp:904] Registered with master master@172.17.0.2:40874; given slave ID 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S0_x000D_
I1221 23:10:05.297056 32597 fetcher.cpp:81] Clearing fetcher cache_x000D_
I1221 23:10:05.297175 32599 status_update_manager.cpp:181] Resuming sending status updates_x000D_
I1221 23:10:05.297184 32600 hierarchical.cpp:465] Added slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S0 (6ccf2ee56b13) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000] (allocated: )_x000D_
I1221 23:10:05.297473 32597 slave.cpp:927] Checkpointing SlaveInfo to '/tmp/mesos-otpdch/2/meta/slaves/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S0/slave.info'_x000D_
I1221 23:10:05.297618 32600 hierarchical.cpp:1329] No resources available to allocate!_x000D_
I1221 23:10:05.297688 32600 hierarchical.cpp:1101] Performed allocation for slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S0 in 454887ns_x000D_
I1221 23:10:05.298058 32597 slave.cpp:963] Forwarding total oversubscribed resources _x000D_
I1221 23:10:05.298235 32597 master.cpp:4542] Received update of slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S0 at slave(3)@172.17.0.2:40874 (6ccf2ee56b13) with total oversubscribed resources _x000D_
I1221 23:10:05.298396 32604 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0_x000D_
I1221 23:10:05.298765 32597 hierarchical.cpp:521] Slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S0 (6ccf2ee56b13) updated with oversubscribed resources  (total: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: )_x000D_
I1221 23:10:05.298907 32597 hierarchical.cpp:1329] No resources available to allocate!_x000D_
I1221 23:10:05.298933 32597 hierarchical.cpp:1101] Performed allocation for slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S0 in 134328ns_x000D_
I1221 23:10:05.298965 32604 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 505213ns_x000D_
I1221 23:10:05.299031 32604 leveldb.cpp:399] Deleting ~2 keys from leveldb took 37007ns_x000D_
I1221 23:10:05.299054 32604 replica.cpp:712] Persisted action at 4_x000D_
I1221 23:10:05.299103 32604 replica.cpp:697] Replica learned TRUNCATE action at position 4_x000D_
I1221 23:10:05.350281 32598 slave.cpp:1254] Will retry registration in 1.181298785secs if necessary_x000D_
I1221 23:10:05.350510 32608 master.cpp:4132] Registering slave at slave(1)@172.17.0.2:40874 (6ccf2ee56b13) with id 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1_x000D_
I1221 23:10:05.351222 32607 registrar.cpp:439] Applied 1 operations in 118623ns; attempting to update the 'registry'_x000D_
I1221 23:10:05.352174 32604 log.cpp:683] Attempting to append 505 bytes to the log_x000D_
I1221 23:10:05.352375 32595 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 5_x000D_
I1221 23:10:05.353365 32601 replica.cpp:537] Replica received write request for position 5 from (46)@172.17.0.2:40874_x000D_
I1221 23:10:05.353960 32601 leveldb.cpp:341] Persisting action (524 bytes) to leveldb took 552132ns_x000D_
I1221 23:10:05.353986 32601 replica.cpp:712] Persisted action at 5_x000D_
I1221 23:10:05.354867 32606 replica.cpp:691] Replica received learned notice for position 5 from @0.0.0.0:0_x000D_
I1221 23:10:05.355370 32606 leveldb.cpp:341] Persisting action (526 bytes) to leveldb took 456354ns_x000D_
I1221 23:10:05.355399 32606 replica.cpp:712] Persisted action at 5_x000D_
I1221 23:10:05.355432 32606 replica.cpp:697] Replica learned APPEND action at position 5_x000D_
I1221 23:10:05.357318 32595 registrar.cpp:484] Successfully updated the 'registry' in 6.016768ms_x000D_
I1221 23:10:05.357708 32595 log.cpp:702] Attempting to truncate the log to 5_x000D_
I1221 23:10:05.357805 32606 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 6_x000D_
I1221 23:10:05.358273 32602 slave.cpp:3371] Received ping from slave-observer(2)@172.17.0.2:40874_x000D_
I1221 23:10:05.358331 32599 master.cpp:4200] Registered slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 at slave(1)@172.17.0.2:40874 (6ccf2ee56b13) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]_x000D_
I1221 23:10:05.358405 32602 slave.cpp:904] Registered with master master@172.17.0.2:40874; given slave ID 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1_x000D_
I1221 23:10:05.358428 32602 fetcher.cpp:81] Clearing fetcher cache_x000D_
I1221 23:10:05.358722 32599 status_update_manager.cpp:181] Resuming sending status updates_x000D_
I1221 23:10:05.358736 32606 hierarchical.cpp:465] Added slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 (6ccf2ee56b13) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000] (allocated: )_x000D_
I1221 23:10:05.358813 32599 replica.cpp:537] Replica received write request for position 6 from (47)@172.17.0.2:40874_x000D_
I1221 23:10:05.358952 32602 slave.cpp:927] Checkpointing SlaveInfo to '/tmp/mesos-otpdch/0/meta/slaves/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1/slave.info'_x000D_
I1221 23:10:05.358969 32606 hierarchical.cpp:1329] No resources available to allocate!_x000D_
I1221 23:10:05.358996 32606 hierarchical.cpp:1101] Performed allocation for slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 in 217083ns_x000D_
I1221 23:10:05.359350 32599 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 464454ns_x000D_
I1221 23:10:05.359374 32599 replica.cpp:712] Persisted action at 6_x000D_
I1221 23:10:05.359591 32602 slave.cpp:963] Forwarding total oversubscribed resources _x000D_
I1221 23:10:05.359740 32606 master.cpp:4542] Received update of slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 at slave(1)@172.17.0.2:40874 (6ccf2ee56b13) with total oversubscribed resources _x000D_
I1221 23:10:05.360227 32605 replica.cpp:691] Replica received learned notice for position 6 from @0.0.0.0:0_x000D_
I1221 23:10:05.360288 32606 hierarchical.cpp:521] Slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 (6ccf2ee56b13) updated with oversubscribed resources  (total: cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000], allocated: )_x000D_
I1221 23:10:05.360539 32606 hierarchical.cpp:1329] No resources available to allocate!_x000D_
I1221 23:10:05.360591 32606 hierarchical.cpp:1101] Performed allocation for slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S1 in 256841ns_x000D_
I1221 23:10:05.360702 32605 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 444736ns_x000D_
I1221 23:10:05.360759 32605 leveldb.cpp:399] Deleting ~2 keys from leveldb took 30869ns_x000D_
I1221 23:10:05.360777 32605 replica.cpp:712] Persisted action at 6_x000D_
I1221 23:10:05.360800 32605 replica.cpp:697] Replica learned TRUNCATE action at position 6_x000D_
I1221 23:10:05.957257 32601 slave.cpp:1254] Will retry registration in 627.120173ms if necessary_x000D_
I1221 23:10:05.957504 32597 master.cpp:4132] Registering slave at slave(2)@172.17.0.2:40874 (6ccf2ee56b13) with id 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S2_x000D_
I1221 23:10:05.958284 32607 registrar.cpp:439] Applied 1 operations in 174321ns; attempting to update the 'registry'_x000D_
I1221 23:10:05.959336 32594 log.cpp:683] Attempting to append 671 bytes to the log_x000D_
I1221 23:10:05.959477 32606 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 7_x000D_
I1221 23:10:05.960484 32604 replica.cpp:537] Replica received write request for position 7 from (48)@172.17.0.2:40874_x000D_
I1221 23:10:05.960891 32604 leveldb.cpp:341] Persisting action (690 bytes) to leveldb took 362101ns_x000D_
I1221 23:10:05.960917 32604 replica.cpp:712] Persisted action at 7_x000D_
I1221 23:10:05.961642 32597 replica.cpp:691] Replica received learned notice for position 7 from @0.0.0.0:0_x000D_
I1221 23:10:05.962502 32597 leveldb.cpp:341] Persisting action (692 bytes) to leveldb took 828414ns_x000D_
I1221 23:10:05.962532 32597 replica.cpp:712] Persisted action at 7_x000D_
I1221 23:10:05.962563 32597 replica.cpp:697] Replica learned APPEND action at position 7_x000D_
I1221 23:10:05.964241 32598 registrar.cpp:484] Successfully updated the 'registry' in 5.87392ms_x000D_
I1221 23:10:05.964552 32593 log.cpp:702] Attempting to truncate the log to 7_x000D_
I1221 23:10:05.964643 32606 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 8_x000D_
I1221 23:10:05.964964 32593 slave.cpp:3371] Received ping from slave-observer(3)@172.17.0.2:40874_x000D_
I1221 23:10:05.965152 32602 slave.cpp:904] Registered with master master@172.17.0.2:40874; given slave ID 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S2_x000D_
I1221 23:10:05.965111 32600 master.cpp:4200] Registered slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S2 at slave(2)@172.17.0.2:40874 (6ccf2ee56b13) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000]_x000D_
I1221 23:10:05.965178 32602 fetcher.cpp:81] Clearing fetcher cache_x000D_
I1221 23:10:05.965293 32607 status_update_manager.cpp:181] Resuming sending status updates_x000D_
I1221 23:10:05.965293 32593 replica.cpp:537] Replica received write request for position 8 from (49)@172.17.0.2:40874_x000D_
I1221 23:10:05.965637 32603 hierarchical.cpp:465] Added slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S2 (6ccf2ee56b13) with cpus(*):2; mem(*):10240; disk(*):3.70122e+06; ports(*):[31000-32000] (allocated: )_x000D_
I1221 23:10:05.965734 32602 slave.cpp:927] Checkpointing SlaveInfo to '/tmp/mesos-otpdch/1/meta/slaves/3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S2/slave.info'_x000D_
I1221 23:10:05.965751 32593 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 424617ns_x000D_
I1221 23:10:05.965772 32593 replica.cpp:712] Persisted action at 8_x000D_
I1221 23:10:05.965847 32603 hierarchical.cpp:1329] No resources available to allocate!_x000D_
I1221 23:10:05.965880 32603 hierarchical.cpp:1101] Performed allocation for slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S2 in 209313ns_x000D_
I1221 23:10:05.966145 32602 slave.cpp:963] Forwarding total oversubscribed resources _x000D_
I1221 23:10:05.966325 32601 replica.cpp:691] Replica received learned notice for position 8 from @0.0.0.0:0_x000D_
I1221 23:10:05.966342 32596 master.cpp:4542] Received update of slave 3931c1a8-1cd6-49eb-94c8-d01b33bb008e-S2 at slave(2)@172.17.0.2:40874 (6ccf2ee56b13) with total ove",3.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.12280701754385964,0.32857142857142857,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.0
Improvement,Consolidate docker store slave flags,"Currently there are too many slave flags for configuring the docker store/puller.
We can remove the following flags:

docker_auth_server_port
docker_local_archives_dir
docker_registry_port
docker_puller

And consolidate them into the existing flags.",3.0,0,0.0,0.277526395173454,0.0,0.0,0.0,0.0,0.4,0.08771929824561403,0.10714285714285714,0.02564102564102564,0.09090909090909091,0.09090909090909091,0.0
Bug,Docker containers left running on disk after reviewbot builds,"The Mesos Reviewbot builds recently failed due to Docker containers being left running on the disk, eventually leading to a full disk: https://issues.apache.org/jira/browse/INFRA-10984

These containers should be automatically cleaned up to avoid this problem in the future.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Documentation,Document containerizer from user perspective.,"Add documentation that covers:

* Purpose of containerizers from a use case perspective.
* What purpose does each containerizer (mesos. docker, compose) serve.
* What criteria could be used to choose a containerizer.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3357142857142857,0.14102564102564102,0.2424242424242424,0.2424242424242424,0.0
Documentation,"Document ""how to program with dynamic reservations and persistent volumes""","Specifically, some of the gotchas around:

* Retrying reservation attempts after a timeout
* Fuzzy-matching resources to determine whether a reservation/PV is successful
* Represent client state as a state machine and repeatedly move ""toward"" successful terminate stats

Should also point to persistent volume example framework. We should also ask Gabriel and others (Arango?) who have built frameworks with PVs/DRs for feedback.",3.0,0,0.0,1.0,0.0,0.0,0.0,0.0,0.8,0.5438596491228069,0.39999999999999997,0.48717948717948717,0.4303030303030303,0.4303030303030303,0.0
Bug,PersistentVolumeTest.BadACLDropCreateAndDestroy is flaky,"{noformat}
[ RUN      ] PersistentVolumeTest.BadACLDropCreateAndDestroy
I1219 09:51:32.623245 31878 leveldb.cpp:174] Opened db in 4.393596ms
I1219 09:51:32.624084 31878 leveldb.cpp:181] Compacted db in 709447ns
I1219 09:51:32.624186 31878 leveldb.cpp:196] Created db iterator in 21252ns
I1219 09:51:32.624290 31878 leveldb.cpp:202] Seeked to beginning of db in 11391ns
I1219 09:51:32.624378 31878 leveldb.cpp:271] Iterated through 0 keys in the db in 611ns
I1219 09:51:32.624505 31878 replica.cpp:779] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I1219 09:51:32.625195 31904 recover.cpp:447] Starting replica recovery
I1219 09:51:32.625641 31904 recover.cpp:473] Replica is in EMPTY status
I1219 09:51:32.627305 31904 replica.cpp:673] Replica in EMPTY status received a broadcasted recover request from (6740)@172.17.0.3:36408
I1219 09:51:32.627749 31904 recover.cpp:193] Received a recover response from a replica in EMPTY status
I1219 09:51:32.628330 31904 recover.cpp:564] Updating replica status to STARTING
I1219 09:51:32.629068 31906 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 410494ns
I1219 09:51:32.629169 31906 replica.cpp:320] Persisted replica status to STARTING
I1219 09:51:32.629598 31906 recover.cpp:473] Replica is in STARTING status
I1219 09:51:32.630782 31912 replica.cpp:673] Replica in STARTING status received a broadcasted recover request from (6741)@172.17.0.3:36408
I1219 09:51:32.631166 31901 recover.cpp:193] Received a recover response from a replica in STARTING status
I1219 09:51:32.632467 31902 recover.cpp:564] Updating replica status to VOTING
I1219 09:51:32.633600 31907 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 311370ns
I1219 09:51:32.633627 31907 replica.cpp:320] Persisted replica status to VOTING
I1219 09:51:32.633719 31907 recover.cpp:578] Successfully joined the Paxos group
I1219 09:51:32.633874 31907 recover.cpp:462] Recover process terminated
I1219 09:51:32.636409 31909 master.cpp:365] Master bded856d-1c7f-4fad-a8bc-3629ba8c59d3 (60ab6e727501) started on 172.17.0.3:36408
I1219 09:51:32.636593 31909 master.cpp:367] Flags at startup: --acls=""create_volumes {
  principals {
    values: ""creator-principal""
  }
  volume_types {
    type: ANY
  }
}
create_volumes {
  principals {
    type: ANY
  }
  volume_types {
    type: NONE
  }
}
"" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""false"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/SpPF7B/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""25secs"" --registry_strict=""true"" --roles=""role1"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.27.0/_inst/share/mesos/webui"" --work_dir=""/tmp/SpPF7B/master"" --zk_session_timeout=""10secs""
I1219 09:51:32.637055 31909 master.cpp:414] Master allowing unauthenticated frameworks to register
I1219 09:51:32.637068 31909 master.cpp:417] Master only allowing authenticated slaves to register
I1219 09:51:32.637094 31909 credentials.hpp:35] Loading credentials for authentication from '/tmp/SpPF7B/credentials'
I1219 09:51:32.637403 31909 master.cpp:456] Using default 'crammd5' authenticator
I1219 09:51:32.637555 31909 master.cpp:493] Authorization enabled
W1219 09:51:32.637575 31909 master.cpp:553] The '--roles' flag is deprecated. This flag will be removed in the future. See the Mesos 0.27 upgrade notes for more information
I1219 09:51:32.637806 31897 whitelist_watcher.cpp:77] No whitelist given
I1219 09:51:32.637820 31910 hierarchical.cpp:147] Initialized hierarchical allocator process
I1219 09:51:32.639677 31909 master.cpp:1629] The newly elected leader is master@172.17.0.3:36408 with id bded856d-1c7f-4fad-a8bc-3629ba8c59d3
I1219 09:51:32.639768 31909 master.cpp:1642] Elected as the leading master!
I1219 09:51:32.639892 31909 master.cpp:1387] Recovering from registrar
I1219 09:51:32.640136 31907 registrar.cpp:307] Recovering registrar
I1219 09:51:32.640929 31901 log.cpp:659] Attempting to start the writer
I1219 09:51:32.642199 31912 replica.cpp:493] Replica received implicit promise request from (6742)@172.17.0.3:36408 with proposal 1
I1219 09:51:32.642719 31912 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 445876ns
I1219 09:51:32.642755 31912 replica.cpp:342] Persisted promised to 1
I1219 09:51:32.643478 31904 coordinator.cpp:238] Coordinator attempting to fill missing positions
I1219 09:51:32.645009 31909 replica.cpp:388] Replica received explicit promise request from (6743)@172.17.0.3:36408 for position 0 with proposal 2
I1219 09:51:32.645356 31909 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 310064ns
I1219 09:51:32.645382 31909 replica.cpp:712] Persisted action at 0
I1219 09:51:32.646662 31909 replica.cpp:537] Replica received write request for position 0 from (6744)@172.17.0.3:36408
I1219 09:51:32.646721 31909 leveldb.cpp:436] Reading position from leveldb took 29298ns
I1219 09:51:32.647047 31909 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 283424ns
I1219 09:51:32.647073 31909 replica.cpp:712] Persisted action at 0
I1219 09:51:32.647722 31909 replica.cpp:691] Replica received learned notice for position 0 from @0.0.0.0:0
I1219 09:51:32.648052 31909 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 300825ns
I1219 09:51:32.648077 31909 replica.cpp:712] Persisted action at 0
I1219 09:51:32.648095 31909 replica.cpp:697] Replica learned NOP action at position 0
I1219 09:51:32.655295 31899 log.cpp:675] Writer started with ending position 0
I1219 09:51:32.656543 31905 leveldb.cpp:436] Reading position from leveldb took 32788ns
I1219 09:51:32.658164 31905 registrar.cpp:340] Successfully fetched the registry (0B) in 0ns
I1219 09:51:32.658604 31905 registrar.cpp:439] Applied 1 operations in 38183ns; attempting to update the 'registry'
I1219 09:51:32.660102 31905 log.cpp:683] Attempting to append 170 bytes to the log
I1219 09:51:32.660538 31906 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I1219 09:51:32.661872 31906 replica.cpp:537] Replica received write request for position 1 from (6745)@172.17.0.3:36408
I1219 09:51:32.662719 31906 leveldb.cpp:341] Persisting action (189 bytes) to leveldb took 483018ns
I1219 09:51:32.663054 31906 replica.cpp:712] Persisted action at 1
I1219 09:51:32.664008 31902 replica.cpp:691] Replica received learned notice for position 1 from @0.0.0.0:0
I1219 09:51:32.664330 31902 leveldb.cpp:341] Persisting action (191 bytes) to leveldb took 287310ns
I1219 09:51:32.664355 31902 replica.cpp:712] Persisted action at 1
I1219 09:51:32.664376 31902 replica.cpp:697] Replica learned APPEND action at position 1
I1219 09:51:32.665365 31902 registrar.cpp:484] Successfully updated the 'registry' in 0ns
I1219 09:51:32.665493 31902 registrar.cpp:370] Successfully recovered registrar
I1219 09:51:32.665894 31902 master.cpp:1439] Recovered 0 slaves from the Registry (131B) ; allowing 10mins for slaves to re-register
I1219 09:51:32.665990 31902 hierarchical.cpp:165] Skipping recovery of hierarchical allocator: nothing to recover
I1219 09:51:32.666266 31902 log.cpp:702] Attempting to truncate the log to 1
I1219 09:51:32.666424 31902 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I1219 09:51:32.667181 31907 replica.cpp:537] Replica received write request for position 2 from (6746)@172.17.0.3:36408
I1219 09:51:32.667768 31907 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 335947ns
I1219 09:51:32.668067 31907 replica.cpp:712] Persisted action at 2
I1219 09:51:32.668942 31906 replica.cpp:691] Replica received learned notice for position 2 from @0.0.0.0:0
I1219 09:51:32.669240 31906 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 266566ns
I1219 09:51:32.669292 31906 leveldb.cpp:399] Deleting ~1 keys from leveldb took 27852ns
I1219 09:51:32.669314 31906 replica.cpp:712] Persisted action at 2
I1219 09:51:32.669334 31906 replica.cpp:697] Replica learned TRUNCATE action at position 2
I1219 09:51:32.691251 31878 containerizer.cpp:141] Using isolation: posix/cpu,posix/mem,filesystem/posix
W1219 09:51:32.691759 31878 backend.cpp:48] Failed to create 'bind' backend: BindBackend requires root privileges
I1219 09:51:32.697428 31901 slave.cpp:191] Slave started on 228)@172.17.0.3:36408
I1219 09:51:32.697459 31901 slave.cpp:192] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/PersistentVolumeTest_BadACLDropCreateAndDestroy_gWLtnc/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""auth.docker.io"" --docker_auth_server_port=""443"" --docker_kill_orphans=""true"" --docker_local_archives_dir=""/tmp/mesos/images/docker"" --docker_puller=""local"" --docker_puller_timeout=""60"" --docker_registry=""registry-1.docker.io"" --docker_registry_port=""443"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/PersistentVolumeTest_BadACLDropCreateAndDestroy_gWLtnc/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.27.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk(role1):2048"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/PersistentVolumeTest_BadACLDropCreateAndDestroy_gWLtnc""
I1219 09:51:32.697963 31901 credentials.hpp:83] Loading credential for authentication from '/tmp/PersistentVolumeTest_BadACLDropCreateAndDestroy_gWLtnc/credential'
I1219 09:51:32.698210 31901 slave.cpp:322] Slave using credential for: test-principal
I1219 09:51:32.698449 31901 resources.cpp:478] Parsing resources as JSON failed: cpus:2;mem:1024;disk(role1):2048
Trying semicolon-delimited string format instead
I1219 09:51:32.699065 31901 slave.cpp:392] Slave resources: cpus(*):2; mem(*):1024; disk(role1):2048; ports(*):[31000-32000]
I1219 09:51:32.699137 31901 slave.cpp:400] Slave attributes: [  ]
I1219 09:51:32.699151 31901 slave.cpp:405] Slave hostname: 60ab6e727501
I1219 09:51:32.699161 31901 slave.cpp:410] Slave checkpoint: true
I1219 09:51:32.699364 31878 sched.cpp:164] Version: 0.27.0
I1219 09:51:32.700614 31911 sched.cpp:262] New master detected at master@172.17.0.3:36408
I1219 09:51:32.700703 31911 sched.cpp:272] No credentials provided. Attempting to register without authentication
I1219 09:51:32.700724 31911 sched.cpp:714] Sending SUBSCRIBE call to master@172.17.0.3:36408
I1219 09:51:32.700839 31911 sched.cpp:747] Will retry registration in 620.399428ms if necessary
I1219 09:51:32.701244 31903 master.cpp:2197] Received SUBSCRIBE call for framework 'default' at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408
I1219 09:51:32.701313 31903 master.cpp:1668] Authorizing framework principal 'test-principal' to receive offers for role 'role1'
I1219 09:51:32.701625 31903 master.cpp:2268] Subscribing framework default with checkpointing disabled and capabilities [  ]
I1219 09:51:32.702308 31903 hierarchical.cpp:260] Added framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000
I1219 09:51:32.702386 31903 hierarchical.cpp:1329] No resources available to allocate!
I1219 09:51:32.702422 31903 hierarchical.cpp:1423] No inverse offers to send out!
I1219 09:51:32.702448 31903 hierarchical.cpp:1079] Performed allocation for 0 slaves in 114358ns
I1219 09:51:32.702638 31903 sched.cpp:641] Framework registered with bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000
I1219 09:51:32.702688 31903 sched.cpp:655] Scheduler::registered took 25558ns
I1219 09:51:32.703553 31901 state.cpp:58] Recovering state from '/tmp/PersistentVolumeTest_BadACLDropCreateAndDestroy_gWLtnc/meta'
I1219 09:51:32.704118 31897 status_update_manager.cpp:200] Recovering status update manager
I1219 09:51:32.704407 31907 containerizer.cpp:383] Recovering containerizer
I1219 09:51:32.705373 31907 slave.cpp:4427] Finished recovery
I1219 09:51:32.705991 31907 slave.cpp:4599] Querying resource estimator for oversubscribable resources
I1219 09:51:32.706277 31907 slave.cpp:4613] Received oversubscribable resources  from the resource estimator
I1219 09:51:32.706666 31907 slave.cpp:729] New master detected at master@172.17.0.3:36408
I1219 09:51:32.706738 31907 slave.cpp:792] Authenticating with master master@172.17.0.3:36408
I1219 09:51:32.706760 31907 slave.cpp:797] Using default CRAM-MD5 authenticatee
I1219 09:51:32.706886 31899 status_update_manager.cpp:174] Pausing sending status updates
I1219 09:51:32.706941 31907 slave.cpp:765] Detecting new master
I1219 09:51:32.707036 31899 authenticatee.cpp:121] Creating new client SASL connection
I1219 09:51:32.707291 31910 master.cpp:5423] Authenticating slave(228)@172.17.0.3:36408
I1219 09:51:32.707479 31910 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(510)@172.17.0.3:36408
I1219 09:51:32.707849 31910 authenticator.cpp:98] Creating new server SASL connection
I1219 09:51:32.708082 31910 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I1219 09:51:32.708112 31910 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I1219 09:51:32.708196 31910 authenticator.cpp:203] Received SASL authentication start
I1219 09:51:32.708395 31910 authenticator.cpp:325] Authentication requires more steps
I1219 09:51:32.708611 31902 authenticatee.cpp:258] Received SASL authentication step
I1219 09:51:32.708773 31910 authenticator.cpp:231] Received SASL authentication step
I1219 09:51:32.708889 31910 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '60ab6e727501' server FQDN: '60ab6e727501' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I1219 09:51:32.708976 31910 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I1219 09:51:32.709096 31910 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I1219 09:51:32.709200 31910 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '60ab6e727501' server FQDN: '60ab6e727501' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I1219 09:51:32.709285 31910 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I1219 09:51:32.709363 31910 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I1219 09:51:32.709452 31910 authenticator.cpp:317] Authentication success
I1219 09:51:32.709707 31910 authenticatee.cpp:298] Authentication success
I1219 09:51:32.710252 31910 slave.cpp:860] Successfully authenticated with master master@172.17.0.3:36408
I1219 09:51:32.710525 31910 slave.cpp:1254] Will retry registration in 17.44437ms if necessary
I1219 09:51:32.709839 31908 master.cpp:5453] Successfully authenticated principal 'test-principal' at slave(228)@172.17.0.3:36408
I1219 09:51:32.710985 31908 master.cpp:4132] Registering slave at slave(228)@172.17.0.3:36408 (60ab6e727501) with id bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0
I1219 09:51:32.711645 31908 registrar.cpp:439] Applied 1 operations in 83191ns; attempting to update the 'registry'
I1219 09:51:32.709908 31912 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(510)@172.17.0.3:36408
I1219 09:51:32.713407 31908 log.cpp:683] Attempting to append 343 bytes to the log
I1219 09:51:32.713646 31912 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I1219 09:51:32.714884 31911 replica.cpp:537] Replica received write request for position 3 from (6758)@172.17.0.3:36408
I1219 09:51:32.715221 31911 leveldb.cpp:341] Persisting action (362 bytes) to leveldb took 288909ns
I1219 09:51:32.715250 31911 replica.cpp:712] Persisted action at 3
I1219 09:51:32.716145 31912 replica.cpp:691] Replica received learned notice for position 3 from @0.0.0.0:0
I1219 09:51:32.716689 31912 leveldb.cpp:341] Persisting action (364 bytes) to leveldb took 512217ns
I1219 09:51:32.716716 31912 replica.cpp:712] Persisted action at 3
I1219 09:51:32.716737 31912 replica.cpp:697] Replica learned APPEND action at position 3
I1219 09:51:32.718426 31911 registrar.cpp:484] Successfully updated the 'registry' in 0ns
I1219 09:51:32.719441 31902 slave.cpp:3371] Received ping from slave-observer(228)@172.17.0.3:36408
I1219 09:51:32.719843 31909 log.cpp:702] Attempting to truncate the log to 3
I1219 09:51:32.719908 31911 master.cpp:4200] Registered slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 at slave(228)@172.17.0.3:36408 (60ab6e727501) with cpus(*):2; mem(*):1024; disk(role1):2048; ports(*):[31000-32000]
I1219 09:51:32.720064 31911 slave.cpp:904] Registered with master master@172.17.0.3:36408; given slave ID bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0
I1219 09:51:32.720088 31911 fetcher.cpp:81] Clearing fetcher cache
I1219 09:51:32.720491 31911 slave.cpp:927] Checkpointing SlaveInfo to '/tmp/PersistentVolumeTest_BadACLDropCreateAndDestroy_gWLtnc/meta/slaves/bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0/slave.info'
I1219 09:51:32.720844 31909 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I1219 09:51:32.720929 31911 slave.cpp:963] Forwarding total oversubscribed resources 
I1219 09:51:32.721017 31903 status_update_manager.cpp:181] Resuming sending status updates
I1219 09:51:32.721099 31911 master.cpp:4542] Received update of slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 at slave(228)@172.17.0.3:36408 (60ab6e727501) with total oversubscribed resources 
I1219 09:51:32.721141 31905 hierarchical.cpp:465] Added slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 (60ab6e727501) with cpus(*):2; mem(*):1024; disk(role1):2048; ports(*):[31000-32000] (allocated: )
I1219 09:51:32.721879 31911 replica.cpp:537] Replica received write request for position 4 from (6759)@172.17.0.3:36408
I1219 09:51:32.722293 31905 hierarchical.cpp:1423] No inverse offers to send out!
I1219 09:51:32.722337 31905 hierarchical.cpp:1101] Performed allocation for slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 in 1.155563ms
I1219 09:51:32.722681 31905 hierarchical.cpp:521] Slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 (60ab6e727501) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(role1):2048; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):2048)
I1219 09:51:32.722713 31909 master.cpp:5252] Sending 1 offers to framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408
I1219 09:51:32.723031 31905 hierarchical.cpp:1329] No resources available to allocate!
I1219 09:51:32.723073 31905 hierarchical.cpp:1423] No inverse offers to send out!
I1219 09:51:32.723095 31905 hierarchical.cpp:1101] Performed allocation for slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 in 368889ns
I1219 09:51:32.723191 31909 sched.cpp:811] Scheduler::resourceOffers took 113921ns
I1219 09:51:32.723410 31911 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 1.418243ms
I1219 09:51:32.723497 31911 replica.cpp:712] Persisted action at 4
I1219 09:51:32.724326 31907 replica.cpp:691] Replica received learned notice for position 4 from @0.0.0.0:0
I1219 09:51:32.724758 31907 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 329678ns
I1219 09:51:32.724917 31907 leveldb.cpp:399] Deleting ~2 keys from leveldb took 58317ns
I1219 09:51:32.725025 31907 replica.cpp:712] Persisted action at 4
I1219 09:51:32.725127 31907 replica.cpp:697] Replica learned TRUNCATE action at position 4
I1219 09:51:32.731515 31910 hierarchical.cpp:1329] No resources available to allocate!
I1219 09:51:32.731564 31910 hierarchical.cpp:1423] No inverse offers to send out!
I1219 09:51:32.731591 31910 hierarchical.cpp:1079] Performed allocation for 1 slaves in 239271ns
I1219 09:51:32.741710 31910 master.cpp:3055] Processing ACCEPT call for offers: [ bded856d-1c7f-4fad-a8bc-3629ba8c59d3-O0 ] on slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 at slave(228)@172.17.0.3:36408 (60ab6e727501) for framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408
I1219 09:51:32.741770 31910 master.cpp:2843] Authorizing principal 'test-principal' to create volumes
E1219 09:51:32.742707 31910 master.cpp:1737] Dropping CREATE offer operation from framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408: Not authorized to create persistent volumes as 'test-principal'
I1219 09:51:32.743219 31910 hierarchical.cpp:880] Recovered cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):2048 (total: cpus(*):2; mem(*):1024; disk(role1):2048; ports(*):[31000-32000], allocated: ) on slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 from framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000
I1219 09:51:32.752542 31908 hierarchical.cpp:1423] No inverse offers to send out!
I1219 09:51:32.752590 31908 hierarchical.cpp:1079] Performed allocation for 1 slaves in 888401ns
I1219 09:51:32.753018 31908 master.cpp:5252] Sending 1 offers to framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408
I1219 09:51:32.753435 31908 sched.cpp:811] Scheduler::resourceOffers took 92252ns
I1219 09:51:32.761533 31878 sched.cpp:164] Version: 0.27.0
I1219 09:51:32.761931 31897 master.cpp:3570] Processing DECLINE call for offers: [ bded856d-1c7f-4fad-a8bc-3629ba8c59d3-O1 ] for framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408
I1219 09:51:32.762373 31897 sched.cpp:262] New master detected at master@172.17.0.3:36408
I1219 09:51:32.762451 31897 sched.cpp:272] No credentials provided. Attempting to register without authentication
I1219 09:51:32.762470 31897 sched.cpp:714] Sending SUBSCRIBE call to master@172.17.0.3:36408
I1219 09:51:32.762543 31897 sched.cpp:747] Will retry registration in 465.481193ms if necessary
I1219 09:51:32.762572 31898 hierarchical.cpp:880] Recovered cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):2048 (total: cpus(*):2; mem(*):1024; disk(role1):2048; ports(*):[31000-32000], allocated: ) on slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 from framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000
I1219 09:51:32.762722 31898 master.cpp:2197] Received SUBSCRIBE call for framework 'creator-framework' at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408
I1219 09:51:32.762785 31898 master.cpp:1668] Authorizing framework principal 'creator-principal' to receive offers for role 'role1'
I1219 09:51:32.763036 31897 master.cpp:2268] Subscribing framework creator-framework with checkpointing disabled and capabilities [  ]
I1219 09:51:32.763464 31898 hierarchical.cpp:260] Added framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001
I1219 09:51:32.763562 31897 sched.cpp:641] Framework registered with bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001
I1219 09:51:32.763605 31897 sched.cpp:655] Scheduler::registered took 20669ns
I1219 09:51:32.763804 31908 master.cpp:2650] Processing SUPPRESS call for framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408
I1219 09:51:32.764343 31898 hierarchical.cpp:1423] No inverse offers to send out!
I1219 09:51:32.764382 31898 hierarchical.cpp:1079] Performed allocation for 1 slaves in 893765ns
I1219 09:51:32.764428 31898 hierarchical.cpp:953] Suppressed offers for framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000
I1219 09:51:32.764746 31898 master.cpp:5252] Sending 1 offers to framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 (creator-framework) at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408
I1219 09:51:32.765127 31898 sched.cpp:811] Scheduler::resourceOffers took 83608ns
I1219 09:51:32.773298 31900 hierarchical.cpp:1329] No resources available to allocate!
I1219 09:51:32.773339 31900 hierarchical.cpp:1423] No inverse offers to send out!
I1219 09:51:32.773365 31900 hierarchical.cpp:1079] Performed allocation for 1 slaves in 201759ns
I1219 09:51:32.782901 31898 master.cpp:3055] Processing ACCEPT call for offers: [ bded856d-1c7f-4fad-a8bc-3629ba8c59d3-O2 ] on slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 at slave(228)@172.17.0.3:36408 (60ab6e727501) for framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 (creator-framework) at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408
I1219 09:51:32.782961 31898 master.cpp:2843] Authorizing principal 'creator-principal' to create volumes
I1219 09:51:32.784190 31904 master.cpp:3362] Applying CREATE operation for volumes disk(role1)[id1:path1]:128 from framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 (creator-framework) at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408 to slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 at slave(228)@172.17.0.3:36408 (60ab6e727501)
I1219 09:51:32.784548 31904 master.cpp:6486] Sending checkpointed resources disk(role1)[id1:path1]:128 to slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 at slave(228)@172.17.0.3:36408 (60ab6e727501)
I1219 09:51:32.786471 31904 hierarchical.cpp:642] Updated allocation of framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 on slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 from cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):2048 to cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):1920; disk(role1)[id1:path1]:128
I1219 09:51:32.786929 31904 hierarchical.cpp:880] Recovered cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):1920; disk(role1)[id1:path1]:128 (total: cpus(*):2; mem(*):1024; disk(role1):1920; ports(*):[31000-32000]; disk(role1)[id1:path1]:128, allocated: ) on slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 from framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001
I1219 09:51:32.788035 31904 slave.cpp:2277] Updated checkpointed resources from  to disk(role1)[id1:path1]:128
I1219 09:51:32.795177 31902 hierarchical.cpp:1423] No inverse offers to send out!
I1219 09:51:32.795250 31902 hierarchical.cpp:1079] Performed allocation for 1 slaves in 1.357898ms
I1219 09:51:32.795897 31902 master.cpp:5252] Sending 1 offers to framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 (creator-framework) at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408
I1219 09:51:32.796540 31897 sched.cpp:811] Scheduler::resourceOffers took 138880ns
I1219 09:51:32.803026 31902 master.cpp:3570] Processing DECLINE call for offers: [ bded856d-1c7f-4fad-a8bc-3629ba8c59d3-O3 ] for framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 (creator-framework) at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408
I1219 09:51:32.804143 31902 hierarchical.cpp:880] Recovered cpus(*):2; mem(*):1024; ports(*):[31000-32000]; disk(role1):1920; disk(role1)[id1:path1]:128 (total: cpus(*):2; mem(*):1024; disk(role1):1920; ports(*):[31000-32000]; disk(role1)[id1:path1]:128, allocated: ) on slave bded856d-1c7f-4fad-a8bc-3629ba8c59d3-S0 from framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001
I1219 09:51:32.804622 31907 master.cpp:2650] Processing SUPPRESS call for framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 (creator-framework) at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408
I1219 09:51:32.804729 31907 hierarchical.cpp:953] Suppressed offers for framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001
I1219 09:51:32.805140 31897 master.cpp:3649] Processing REVIVE call for framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408
I1219 09:51:32.805250 31897 hierarchical.cpp:973] Removed offer filters for framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000
I1219 09:51:32.806507 31897 hierarchical.cpp:1423] No inverse offers to send out!
I1219 09:51:32.806562 31897 hierarchical.cpp:1079] Performed allocation for 1 slaves in 1.284779ms
I1219 09:51:32.807067 31897 master.cpp:5252] Sending 1 offers to framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0000 (default) at scheduler-0333dddc-4b41-40ed-8853-a1aadf1f1879@172.17.0.3:36408
../../src/tests/persistent_volume_tests.cpp:1336: Failure
Mock function called more times than expected - returning directly.
    Function call: resourceOffers(0x7ffff9edb3a0, @0x7f71079798f0 { 144-byte object <F0-1B 42-14 71-7F 00-00 00-00 00-00 00-00 00-00 D0-96 02-F0 70-7F 00-00 50-97 02-F0 70-7F 00-00 20-A1 02-F0 70-7F 00-00 50-E0 01-F0 70-7F 00-00 B0-9F 02-F0 70-7F 00-00 00-32 01-F0 70-7F 00-00 ... 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 70-7F 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 1F-00 00-00> })
         Expected: to be called once
           Actual: called twice - over-saturated and active
I1219 09:51:32.807899 31897 sched.cpp:811] Scheduler::resourceOffers took 406435ns
I1219 09:51:32.820523 31909 hierarchical.cpp:1329] No resources available to allocate!
I1219 09:51:32.820611 31909 hierarchical.cpp:1423] No inverse offers to send out!
I1219 09:51:32.820642 31909 hierarchical.cpp:1079] Performed allocation for 1 slaves in 448034ns
2015-12-19 09:51:33,146:31878(0x7f6ff6ffd700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:39991] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-12-19 09:51:36,482:31878(0x7f6ff6ffd700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:39991] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-12-19 09:51:39,818:31878(0x7f6ff6ffd700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:39991] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-12-19 09:51:43,155:31878(0x7f6ff6ffd700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:39991] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-12-19 09:51:46,490:31878(0x7f6ff6ffd700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:39991] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
../../src/tests/persistent_volume_tests.cpp:1411: Failure
Failed to wait 15secs for offers
I1219 09:51:47.829073 31909 master.cpp:1130] Framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 (creator-framework) at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408 disconnected
I1219 09:51:47.829169 31909 master.cpp:2493] Disconnecting framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 (creator-framework) at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408
I1219 09:51:47.829200 31909 master.cpp:2517] Deactivating framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 (creator-framework) at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408
I1219 09:51:47.829366 31909 master.cpp:1154] Giving framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 (creator-framework) at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408 0ns to failover
I1219 09:51:47.829720 31909 hierarchical.cpp:366] Deactivated framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001
I1219 09:51:47.831614 31907 master.cpp:5100] Framework failover timeout, removing framework bded856d-1c7f-4fad-a8bc-3629ba8c59d3-0001 (creator-framework) at scheduler-d2c3fe43-4283-4153-8ee5-533ebc1ac5ce@172.17.0.3:36408
I1219 09:51:47.",1.0,0,0.5,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.7948717948717948,1.0,1.0,0.0
Documentation,Add an example bug due to a lack of defer() to the defer() documentation,"In the past, some bugs have been introduced into the codebase due to a lack of {{defer()}} where it should have been used. It would be useful to add an example of this to the {{defer()}} documentation.",2.0,0,0.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Documentation,Write new logging-related documentation,"This should include:
* Default logging behavior for master, agent, framework, executor, task.
* Master/agent:
** A summary of log-related flags.
** {{glog}} specific options.
* Separation of master/agent logs from container logs.
* The {{ContainerLogger}} module.",3.0,0,0.5,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.48717948717948717,0.4303030303030303,0.4303030303030303,0.0
Task,Test case(s) for weights + allocation behavior,"As far as I can see, we currently have NO test cases for behavior when weights are defined.",2.0,0,0.5,0.0015082956259426848,0.3333333333333333,0.5,0.14285714285714285,0.2,0.2,0.03508771929824561,0.05714285714285714,0.48717948717948717,0.4303030303030303,0.4303030303030303,0.0
Bug,Disk Resource Reservation is NOT Enforced for Persistent Volumes,"If I create a persistent volume on a reserved disk resource, I am able to write data in excess of my reserved size.

Disk resource reservation should be enforced just as ""cpus"" and ""mem"" reservations are enforced.",3.0,0,0.5,0.07088989441930618,0.0,0.0,0.0,0.0,0.2,0.14035087719298245,0.10714285714285714,0.01282051282051282,0.0,0.0,0.0
Improvement,Add dynamic reservation tests with no principal,"Currently, there exist no dynamic reservation tests that include authorization of a framework that is registered with no principal. This should be added in order to more comprehensively test the dynamic reservation code.",1.0,0,0.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Bug,Add documentation for API Versioning,"Currently, we don't have any documentation for:

- How Mesos implements API versioning ?
- How are protobufs versioned and how does mesos handle them internally ?
- What do contributors need to do when they make a change to a external user facing protobuf ?

The relevant design doc:
https://docs.google.com/document/d/1-iQjo6778H_fU_1Zi_Yk6szg8qj-wqYgVgnx7u3h6OU/edit#heading=h.2gkbjz6amn7b
",3.0,0,0.0,0.5701357466063348,0.0,0.0,0.0,0.0,0.0,0.6842105263157894,0.5642857142857143,0.3846153846153846,0.4181818181818182,0.4121212121212121,0.0
Documentation,Create a Design Doc for dynamic weights.,"A short design doc for dynamic weights, it will focus on /weights API and the changes to the allocator API.",3.0,0,0.0,0.0015082956259426848,0.3333333333333333,0.5,0.14285714285714285,0.2,0.2,0.03508771929824561,0.05714285714285714,0.0,0.006060606060606061,0.006060606060606061,0.0
Documentation,Avoid using absolute URLs in documentation pages,"Links from one documentation page to another should not use absolute URLs (e.g., {{http://mesos.apache.org/documentation/latest/...}}) for several good reasons. For instance, absolute URLs break when the docs are generated/previewed locally.",1.0,0,0.0,1.0,0.0,0.0,0.0,0.0,0.8,0.5438596491228069,0.39999999999999997,0.48717948717948717,0.4303030303030303,0.4303030303030303,0.0
Bug,Jenkins builds for Centos fail with missing 'which' utility and incorrect 'java.home',"Jenkins builds are now consistently failing for centos 7, withe the failure:

checking value of Java system property 'java.home'...
/usr/lib/jvm/java-1.8.0-openjdk-1.8.0.65-3.b17.el7.x86_64/jre
configure: error: could not guess JAVA_HOME

They also fail early on during 'bootstrap' with a missing 'which' command.

The solution is to update support/docker_build.sh to install 'which' as well as make sure the proper versions of java are installed during the installation process.

The problem here is that we install maven BEFORE installing java-1.7.0-openjdk-devel, causing maven to pull in a dependency on java-1.8.0-openjdk. This causes problems with finding the proper java.home in our mesos/configure script because of the mismatch between the most up to date jre (1.8.0) and the most up to date development tools (1.7.0).  We can either update the script to pull in the 1.8 devel tools or move our dependence on maven until AFTER our installation of java-1.7.0-openjdk-devel.  Unclear what the best solution is.",3.0,0,0.0,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.3205128205128205,0.6,0.6,0.0
Improvement,Move operator<< definitions to .cpp files and include <iosfwd> in .hpp where possible.,"We often include complex headers like {{<ostream>}} in "".hpp"" files to define {{operator<<()}} inline (e.g. ""mesos/authorizer/authorizer.hpp""). Instead, we can move definitions to corresponding "".cpp"" files and replace stream headers with {{iosfwd}}, for example, this is partially done for {{URI}} in ""mesos/uri/uri.hpp"".",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.717948717948718,0.8242424242424242,0.7212121212121212,0.0
Bug,Extend `Master` to authorize persistent volumes,"This ticket is the second in a series that adds authorization support for persistent volumes.

Methods {{Master::authorizeCreateVolume()}} and {{Master::authorizeDestroyVolume}} must be added to allow the Master to authorize these operations.",1.0,0,0.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Bug,Add persistent volume support to the Authorizer,"This ticket is the first in a series that adds authorization support for persistent volume creation and destruction.

Persistent volumes should be authorized with the {{principal}} of the reserving entity (framework or master). The idea is to introduce {{Create}} and {{Destroy}} into the ACL.

{code}
  message Create {
    // Subjects.
    required Entity principals = 1;

    // Objects? Perhaps the kind of volume? allowed permissions?
  }

  message Destroy {
    // Subjects.
    required Entity principals = 1;

    // Objects.
    required Entity creator_principals = 2;
  }
{code}

ACLs for volume creation and destruction must be added to {{authorizer.proto}}, and the appropriate function overloads must be added to the Authorizer.",1.0,0,0.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Bug,Create a user doc for Executor HTTP API,We need a user doc similar to the corresponding one for the Scheduler HTTP API.,3.0,0,0.0,0.5701357466063348,0.0,0.0,0.0,0.0,0.0,0.6842105263157894,0.5642857142857143,0.3846153846153846,0.4181818181818182,0.4121212121212121,0.0
Improvement,ContentType/SchedulerTest.Decline is slow.,"The {{ContentType/SchedulerTest.Decline}} test takes more than {{1s}} to finish on my Mac OS 10.10.4:
{code}
ContentType/SchedulerTest.Decline/0 (1022 ms)
{code}",1.0,0,0.5,0.033182503770739065,0.0,0.0,0.0,0.0,0.2,0.017543859649122806,0.06428571428571428,0.717948717948718,0.8242424242424242,0.7212121212121212,0.0
Improvement,HookTest.VerifySlaveLaunchExecutorHook is slow.,"The {{HookTest.VerifySlaveLaunchExecutorHook}} test takes more than {{5s}} to finish on my Mac OS 10.10.4:
{code}
HookTest.VerifySlaveLaunchExecutorHook (5061 ms)
{code}",1.0,0,0.5,0.00904977375565611,0.0,0.0,0.0,0.0,0.0,0.017543859649122806,0.014285714285714285,0.717948717948718,0.8242424242424242,0.7212121212121212,0.0
Improvement,Log recover tests are slow.,"On Mac OS 10.10.4, some tests take longer than {{1s}} to finish:
{code}
RecoverTest.AutoInitialization (1003 ms)
RecoverTest.AutoInitializationRetry (1000 ms)
{code}",1.0,0,0.5,0.033182503770739065,0.0,0.0,0.0,0.0,0.2,0.017543859649122806,0.06428571428571428,0.717948717948718,0.8242424242424242,0.7212121212121212,0.0
Bug,Rename shutdown_frameworks to teardown_frameworks,"The mesos is now using teardown framework to shutdown a framework but the acls are still using shutdown_framework, it is better to rename shutdown_framework to teardown_framework for acl to keep consistent.

This is a post review request for https://reviews.apache.org/r/40829/",2.0,0,0.0,0.3137254901960784,0.0,0.0,0.0,0.0,0.4,0.19298245614035087,0.14285714285714285,0.10256410256410256,0.06666666666666667,0.06666666666666667,0.0
Task,Implement container logger module metadata recovery,"The {{ContainerLoggers}} are intended to be isolated from agent failover, in the same way that executors do not crash when the agent process crashes.

For default {{ContainerLogger}} s, like the {{SandboxContainerLogger}} and the (tentatively named) {{TruncatingSandboxContainerLogger}}, the log files are exposed during agent recovery regardless.

For non-default {{ContainerLogger}} s, the recovery of executor metadata may be necessary to rebuild endpoints that expose the logs.  This can be implemented as part of {{Containerizer::recover}}.",3.0,0,0.0,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.0
Improvement,Clean up authentication implementation for quota,"To authenticate quota requests we allowed {{QuotaHandler}} to call private {{Http::authenticate()}} function. Once MESOS-3231 lands we do not need neither this injection, nor {{authenticate()}} calls in the {{QuotaHandler}}.",1.0,0,0.5,0.004524886877828055,0.0,0.0,0.0,0.0,0.2,0.12280701754385964,0.24285714285714285,0.717948717948718,0.8242424242424242,0.7212121212121212,0.0
Bug,Reserve/UnReserve Dynamic Reservation Endpoints allow reservations on non-existing roles,"When working with Dynamic reservations via the /reserve and /unreserve endpoints, it is possible to reserve resources for roles that have not been specified via the --roles flag on the master.  However, these roles are not usable because the roles have not been defined, nor are they added to the list of roles available. 

Per the mailing list, changing roles after the fact is not possible at this time. (That may be another JIRA), more importantly, the /reserve and /unreserve end points should not allow reservation of roles not specified by --roles.  ",2.0,"0.25.0,0.26.0",0.5,0.0015082956259426848,0.3333333333333333,0.5,0.14285714285714285,0.2,0.2,0.03508771929824561,0.05714285714285714,0.01282051282051282,0.0,0.0,0.23394495412844035
Task,Modularize plain-file logging for executor/task logs launched with the Docker Containerizer,"Adding a hook inside the Docker containerizer is slightly more involved than the Mesos containerizer.

Docker executors/tasks perform plain-file logging in different places depending on whether the agent is in a Docker container itself
|| Agent || Code ||
| Not in container | {{DockerContainerizerProcess::launchExecutorProcess}} |
| In container | {{Docker::run}} in a {{mesos-docker-executor}} process |

This means a {{ContainerLogger}} will need to be loaded or hooked into the {{mesos-docker-executor}}.  Or we will need to change how piping in done in {{mesos-docker-executor}}.",3.0,0,0.0,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.0
Improvement,Add a ContainerLogger module that restrains log sizes,"One of the major problems this logger module aims to solve is overflowing executor/task log files.  Log files are simply written to disk, and are not managed other than via occasional garbage collection by the agent process (and this only deals with terminated executors).

We should add a {{ContainerLogger}} module that truncates logs as it reaches a configurable maximum size.  Additionally, we should determine if the web UI's {{pailer}} needs to be changed to deal with logs that are not append-only.

This will be a non-default module which will also serve as an example for how to implement the module.",3.0,0,0.0,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.0
Documentation,Document how the fetcher can reach across a proxy connection.,"The fetcher uses libcurl for downloading content from HTTP, HTTPS, etc. There is no source code in the pertinent parts of ""net.hpp"" that deals with proxy settings. However, libcurl automatically picks up certain environment variables and adjusts its settings accordingly. See ""man libcurl-tutorial"" for details. See section ""Proxies"", subsection ""Environment Variables"". If you follow this recipe in your Mesos agent startup script, you can use a proxy. 

We should document this in the fetcher (cache) doc (http://mesos.apache.org/documentation/latest/fetcher/).
",1.0,0,0.5,0.033182503770739065,0.0,0.0,0.0,0.0,0.2,0.017543859649122806,0.06428571428571428,0.14102564102564102,0.16969696969696968,0.17575757575757575,0.0
Improvement,Refactor sorter factories in allocator and improve comments around them.,For clarity we want to refactor the factory section in the allocator and explain the purpose (and necessity) of all sorters.,3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.717948717948718,0.8242424242424242,0.7212121212121212,0.0
Improvement,Ensure `Content-Type` field is set for some responses.,"As pointed out by [~anandmazumdar] in https://reviews.apache.org/r/40905/, we should make sure we set the {{Content-Type}} files for some responses.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.717948717948718,0.8242424242424242,0.7212121212121212,0.0
Improvement,Construct the error string in `MethodNotAllowed`.,"Consider constructing the error string in {{MethodNotAllowed}} rather than at the invocation site. Currently we want all error messages follow the same pattern, so instead of writing
{code}
return MethodNotAllowed({""POST""}, ""Expecting 'POST', received '"" + request.method + ""'"");
{code}
we can write something like
{code}
MethodNotAllowed({""POST""}, request.method)
{code}
",3.0,0,0.5,0.030165912518853696,0.3333333333333333,0.5,0.14285714285714285,0.0,0.0,0.0,0.007142857142857143,0.717948717948718,0.8242424242424242,0.7212121212121212,0.0
Wish,Add field VIP to message Port,"We would like to extend the Mesos protocol buffer 'Port' to include an optional repeated string named ""VIP"" - to map it to a well known virtual IP, or virtual hostname for discovery purposes.

We also want this field exposed in DiscoveryInfo in state.json.",2.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.2807017543859649,0.39285714285714285,0.0,0.0,0.0,0.0
Task,Clean up libprocess gtest macros,"This ticket is regarding the libprocess gtest helpers in {{3rdparty/libprocess/include/process/gtest.hpp}}.

The pattern in this file seems to be a set of macros:

* {{AWAIT_ASSERT_<STATE>_FOR}}
* {{AWAIT_ASSERT_<STATE>}} -- default of 15 seconds
* {{AWAIT_<STATE>\_FOR}} -- alias for {{AWAIT_ASSERT_<STATE>_FOR}}
* {{AWAIT_<STATE>}} -- alias for {{AWAIT_ASSERT_<STATE>}}
* {{AWAIT_EXPECT_<STATE>_FOR}}
* {{AWAIT_EXPECT_<STATE>}} -- default of 15 seconds

(1) {{AWAIT_EQ_FOR}} should be added for completeness.

(2) In {{gtest}}, we've got {{EXPECT_EQ}} as well as the {{bool}}-specific versions: {{EXPECT_TRUE}} and {{EXPECT_FALSE}}.

We should adopt this pattern in these helpers as well. Keeping the pattern above in mind, the following are missing:

* {{AWAIT_ASSERT_TRUE_FOR}}
* {{AWAIT_ASSERT_TRUE}}
* {{AWAIT_ASSERT_FALSE_FOR}}
* {{AWAIT_ASSERT_FALSE}}
* {{AWAIT_EXPECT_TRUE_FOR}}
* {{AWAIT_EXPECT_FALSE_FOR}}

(3) There are HTTP response related macros at the bottom of the file, e.g. {{AWAIT_EXPECT_RESPONSE_STATUS_EQ}}, however these are missing their {{ASSERT}} counterparts.

-(4) The reason for (3) presumably is because we reach for {{EXPECT}} over {{ASSERT}} in general due to the test suite crashing behavior of {{ASSERT}}. If this is the case, it would be worthwhile considering whether macros such as {{AWAIT_READY}} should alias {{AWAIT_EXPECT_READY}} rather than {{AWAIT_ASSERT_READY}}.-

(5) There are a few more missing macros, given {{AWAIT_EQ_FOR}} and {{AWAIT_EQ}} which aliases to {{AWAIT_ASSERT_EQ_FOR}} and {{AWAIT_ASSERT_EQ}} respectively, we should also add {{AWAIT_TRUE_FOR}}, {{AWAIT_TRUE}}, {{AWAIT_FALSE_FOR}}, and {{AWAIT_FALSE}} as well.",2.0,0,0.5,0.03167420814479638,0.3333333333333333,0.5,0.14285714285714285,0.0,0.0,0.05263157894736842,0.049999999999999996,0.21794871794871795,0.26666666666666666,0.26666666666666666,0.0
Bug,HTTPConnectionTest.ClosingResponse is flaky,"Output of the test:
{code}
[ RUN      ] HTTPConnectionTest.ClosingResponse
I1210 01:20:27.048532 26671 process.cpp:3077] Handling HTTP event for process '(22)' with path: '/(22)/get'
../../../3rdparty/libprocess/src/tests/http_tests.cpp:919: Failure
Actual function call count doesn't match EXPECT_CALL(*http.process, get(_))...
         Expected: to be called twice
           Actual: called once - unsatisfied and active
[  FAILED  ] HTTPConnectionTest.ClosingResponse (43 ms)
{code}",1.0,0.26.0,0.5,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.2385321100917431
Bug,`os::strerror_r` breaks the Windows build,`os::strerror_r` does not exist on Windows.,1.0,0,0.0,0.44494720965309204,0.0,0.0,0.0,0.0,0.0,0.14035087719298245,0.15,0.14102564102564102,0.1515151515151515,0.16969696969696968,0.0
Bug,parallel make tests does not build all test targets,"When inside 3rdparty/libprocess:
Running {{make -j8 tests}} from a clean build does not yield the {{libprocess-tests}} binary.
Running it a subsequent time triggers more compilation and ends up yielding the {{libprocess-tests}} binary.
This suggests the {{test}} target is not being built correctly.",1.0,0.26.0,0.5,0.3453996983408748,0.0,0.0,0.0,0.0,0.2,0.38596491228070173,0.35714285714285715,0.24358974358974358,0.22424242424242424,0.22424242424242424,0.2385321100917431
Task,Modularize existing plain-file logging for executor/task logs launched with the Mesos Containerizer,"Once a module for executor/task output logging has been introduced, the default module will mirror the existing behavior.  Executor/task stdout/stderr is piped into files within the executor's sandbox directory.

The files are exposed in the web UI, via the {{/files}} endpoint.",2.0,0,0.0,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.0
Bug,ReservationTest.ACLMultipleOperations is flaky,"Observed from the CI: https://builds.apache.org/job/Mesos/COMPILER=gcc,CONFIGURATION=--verbose%20--enable-libevent%20--enable-ssl,OS=ubuntu%3A14.04,label_exp=docker%7C%7CHadoop/1319/changes",2.0,0,0.5,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.21794871794871795,0.26666666666666666,0.26666666666666666,0.0
Task,Add ContainerInfo to internal Task protobuf.,"In what seems like an oversight, when ContainerInfo was added to TaskInfo, it was not added to our internal Task protobuf.

Also, unlike the agent, it appears that the master does not use protobuf::createTask. We should try remove the manual construction in the master in favor of construction through protobuf::createTask.

Partial contents of ContainerInfo should be exposed through state endpoints on the master and the agent.
",3.0,0,0.5,0.07088989441930618,0.0,0.0,0.0,0.0,0.2,0.14035087719298245,0.10714285714285714,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.0
Bug,Investigate remaining flakiness in MasterMaintenanceTest.InverseOffersFilters,"Per comments in MESOS-3916, the fix for that issue decreased the degree of flakiness, but it seems that some intermittent test failures do occur -- should be investigated.

*Flakiness in task acknowledgment*
{code}
I1203 18:25:04.609817 28732 status_update_manager.cpp:392] Received status update acknowledgement (UUID: 6afd012e-8e88-41b2-8239-a9b852d07ca1) for task 26305fdd-edb0-4764-8b8a-2558f2b2d81b of framework c7900911-cc7a-4dde-92e7-48fe82cddd9e-0000
W1203 18:25:04.610076 28732 status_update_manager.cpp:762] Unexpected status update acknowledgement (received 6afd012e-8e88-41b2-8239-a9b852d07ca1, expecting 82fc7a7b-e64a-4f4d-ab74-76abac42b4e6) for update TASK_RUNNING (UUID: 82fc7a7b-e64a-4f4d-ab74-76abac42b4e6) for task 26305fdd-edb0-4764-8b8a-2558f2b2d81b of framework c7900911-cc7a-4dde-92e7-48fe82cddd9e-0000
E1203 18:25:04.610339 28736 slave.cpp:2339] Failed to handle status update acknowledgement (UUID: 6afd012e-8e88-41b2-8239-a9b852d07ca1) for task 26305fdd-edb0-4764-8b8a-2558f2b2d81b of framework c7900911-cc7a-4dde-92e7-48fe82cddd9e-0000: Duplicate acknowledgemen
{code}

This is a race between [launching and acknowledging two tasks|https://github.com/apache/mesos/blob/75aaaacb89fa961b249c9ab7fa0f45dfa9d415a5/src/tests/master_maintenance_tests.cpp#L1486-L1517].  The status updates for each task are not necessarily received in the same order as launching the tasks.

*Flakiness in first inverse offer filter*
See [this comment in MESOS-3916|https://issues.apache.org/jira/browse/MESOS-3916?focusedCommentId=15027478&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-15027478] for the explanation.  The related logs are above the comment.",1.0,0,0.5,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.48717948717948717,0.4303030303030303,0.4303030303030303,0.0
Improvement,Do not use `Resource.role` for resources in quota request.,"To be consistent with other operator endpoints and to adhere to the principal of least surprise, move role from each {{Resource}} in quota set request to the request itself. 

{{Resource.role}} is used for reserved resources. Since quota is not a direct reservation request, to avoid confusion we shall not reuse this field for communicating the role for which quota should be reserved.

Food for thought: Shall we try to keep internal storage protobufs as close as possible to operator's JSON to provide some sort of a schema or decouple those two for the sake of flexibility?",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.717948717948718,0.8242424242424242,0.7212121212121212,0.0
Improvement,Respond with `MethodNotAllowed` if a request uses an unsupported method.,"We are inconsistent right now in how we respond to endpoint requests with unsupported methods: both {{MethodNotAllowed}} and {{BadRequest}} are used. We are also not consistent in the error message we include in the body.

This ticket proposes use {{MethodNotAllowed}} with standardized message text.",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.717948717948718,0.8242424242424242,0.7212121212121212,0.0
Bug,MemoryPressureMesosTest.CGROUPS_ROOT_SlaveRecovery is flaky,"{code:title=Output from passed test}
[----------] 1 test from MemoryPressureMesosTest
1+0 records in
1+0 records out
1048576 bytes (1.0 MB) copied, 0.000430889 s, 2.4 GB/s
[ RUN      ] MemoryPressureMesosTest.CGROUPS_ROOT_SlaveRecovery
I1202 11:09:14.319327  5062 exec.cpp:134] Version: 0.27.0
I1202 11:09:14.333317  5079 exec.cpp:208] Executor registered on slave bea15b35-9aa1-4b57-96fb-29b5f70638ac-S0
Registered executor on ubuntu
Starting task 4e62294c-cfcf-4a13-b699-c6a4b7ac5162
sh -c 'while true; do dd count=512 bs=1M if=/dev/zero of=./temp; done'
Forked command at 5085
I1202 11:09:14.391739  5077 exec.cpp:254] Received reconnect request from slave bea15b35-9aa1-4b57-96fb-29b5f70638ac-S0
I1202 11:09:14.398598  5082 exec.cpp:231] Executor re-registered on slave bea15b35-9aa1-4b57-96fb-29b5f70638ac-S0
Re-registered executor on ubuntu
Shutting down
Sending SIGTERM to process tree at pid 5085
Killing the following process trees:
[ 
-+- 5085 sh -c while true; do dd count=512 bs=1M if=/dev/zero of=./temp; done 
 \--- 5086 dd count=512 bs=1M if=/dev/zero of=./temp 
]
[       OK ] MemoryPressureMesosTest.CGROUPS_ROOT_SlaveRecovery (1096 ms)
{code}

{code:title=Output from failed test}
[----------] 1 test from MemoryPressureMesosTest
1+0 records in
1+0 records out
1048576 bytes (1.0 MB) copied, 0.000404489 s, 2.6 GB/s
[ RUN      ] MemoryPressureMesosTest.CGROUPS_ROOT_SlaveRecovery
I1202 11:09:15.509950  5109 exec.cpp:134] Version: 0.27.0
I1202 11:09:15.568183  5123 exec.cpp:208] Executor registered on slave 88734acc-718e-45b0-95b9-d8f07cea8a9e-S0
Registered executor on ubuntu
Starting task 14b6bab9-9f60-4130-bdc4-44efba262bc6
Forked command at 5132
sh -c 'while true; do dd count=512 bs=1M if=/dev/zero of=./temp; done'
I1202 11:09:15.665498  5129 exec.cpp:254] Received reconnect request from slave 88734acc-718e-45b0-95b9-d8f07cea8a9e-S0
I1202 11:09:15.670995  5123 exec.cpp:381] Executor asked to shutdown
Shutting down
Sending SIGTERM to process tree at pid 5132
../../src/tests/containerizer/memory_pressure_tests.cpp:283: Failure
(usage).failure(): Unknown container: ebe90e15-72fa-4519-837b-62f43052c913
*** Aborted at 1449083355 (unix time) try ""date -d @1449083355"" if you are using GNU date ***
{code}

Notice that in the failed test, the executor is asked to shutdown when it tries to reconnect to the agent.",1.0,0.26.0,0.5,0.004524886877828055,0.0,0.0,0.0,0.0,0.2,0.12280701754385964,0.24285714285714285,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.2385321100917431
Improvement,Install instructions for CentOS 6.6 lead to errors running `perf`.,"After using the current installation instructions in the getting started documentation, {{perf}} will not run on CentOS 6.6 because the version of elfutils included in devtoolset-2 is not compatible with the version of {{perf}} installed by {{yum}}. Installing and using devtoolset-3, however (http://linux.web.cern.ch/linux/scientific6/docs/softwarecollections.shtml) fixes this issue. This could be resolved by updating the getting started documentation to recommend installing devtoolset-3.",1.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Bug,ContentType/SchedulerTest is flaky.,"SSL build, [Ubuntu 14.04|https://github.com/tillt/mesos-vagrant-ci/blob/master/ubuntu14/setup.sh], non-root test run.

{noformat}
[----------] 22 tests from ContentType/SchedulerTest
[ RUN      ] ContentType/SchedulerTest.Subscribe/0
[       OK ] ContentType/SchedulerTest.Subscribe/0 (48 ms)
*** Aborted at 1448928007 (unix time) try ""date -d @1448928007"" if you are using GNU date ***
[ RUN      ] ContentType/SchedulerTest.Subscribe/1
PC: @          0x1451b8e testing::internal::UntypedFunctionMockerBase::UntypedInvokeWith()
*** SIGSEGV (@0x100000030) received by PID 21320 (TID 0x2b549e5d4700) from PID 48; stack trace: ***
    @     0x2b54c95940b7 os::Linux::chained_handler()
    @     0x2b54c9598219 JVM_handle_linux_signal
    @     0x2b5496300340 (unknown)
    @          0x1451b8e testing::internal::UntypedFunctionMockerBase::UntypedInvokeWith()
    @           0xe2ea6d _ZN7testing8internal18FunctionMockerBaseIFvRKSt5queueIN5mesos2v19scheduler5EventESt5dequeIS6_SaIS6_EEEEE10InvokeWithERKSt5tupleIJSC_EE
    @           0xe2b1bc testing::internal::FunctionMocker<>::Invoke()
    @          0x1118aed mesos::internal::tests::SchedulerTest::Callbacks::received()
    @          0x111c453 _ZNKSt7_Mem_fnIMN5mesos8internal5tests13SchedulerTest9CallbacksEFvRKSt5queueINS0_2v19scheduler5EventESt5dequeIS8_SaIS8_EEEEEclIJSE_EvEEvRS4_DpOT_
    @          0x111c001 _ZNSt5_BindIFSt7_Mem_fnIMN5mesos8internal5tests13SchedulerTest9CallbacksEFvRKSt5queueINS1_2v19scheduler5EventESt5dequeIS9_SaIS9_EEEEESt17reference_wrapperIS5_ESt12_PlaceholderILi1EEEE6__callIvJSF_EJLm0ELm1EEEET_OSt5tupleIJDpT0_EESt12_Index_tupleIJXspT1_EEE
    @          0x111b90d _ZNSt5_BindIFSt7_Mem_fnIMN5mesos8internal5tests13SchedulerTest9CallbacksEFvRKSt5queueINS1_2v19scheduler5EventESt5dequeIS9_SaIS9_EEEEESt17reference_wrapperIS5_ESt12_PlaceholderILi1EEEEclIJSF_EvEET0_DpOT_
    @          0x111ae09 std::_Function_handler<>::_M_invoke()
    @     0x2b5493c6da09 std::function<>::operator()()
    @     0x2b5493c688ee process::AsyncExecutorProcess::execute<>()
    @     0x2b5493c6db2a _ZZN7process8dispatchI7NothingNS_20AsyncExecutorProcessERKSt8functionIFvRKSt5queueIN5mesos2v19scheduler5EventESt5dequeIS8_SaIS8_EEEEESC_PvSG_SC_SJ_EENS_6FutureIT_EERKNS_3PIDIT0_EEMSO_FSL_T1_T2_T3_ET4_T5_T6_ENKUlPNS_11ProcessBaseEE_clES11_
    @     0x2b5493c765a4 _ZNSt17_Function_handlerIFvPN7process11ProcessBaseEEZNS0_8dispatchI7NothingNS0_20AsyncExecutorProcessERKSt8functionIFvRKSt5queueIN5mesos2v19scheduler5EventESt5dequeISC_SaISC_EEEEESG_PvSK_SG_SN_EENS0_6FutureIT_EERKNS0_3PIDIT0_EEMSS_FSP_T1_T2_T3_ET4_T5_T6_EUlS2_E_E9_M_invokeERKSt9_Any_dataS2_
    @     0x2b54946b1201 std::function<>::operator()()
    @     0x2b549469960f process::ProcessBase::visit()
    @     0x2b549469d480 process::DispatchEvent::visit()
    @           0x9dc0ba process::ProcessBase::serve()
    @     0x2b54946958cc process::ProcessManager::resume()
    @     0x2b5494692a9c _ZZN7process14ProcessManager12init_threadsEvENKUlRKSt11atomic_boolE_clES3_
    @     0x2b549469ccac _ZNSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS3_EEE6__callIvIEILm0EEEET_OSt5tupleIIDpT0_EESt12_Index_tupleIIXspT1_EEE
    @     0x2b549469cc5c _ZNSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS3_EEEclIIEvEET0_DpOT_
    @     0x2b549469cbee _ZNSt12_Bind_simpleIFSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS4_EEEvEE9_M_invokeIIEEEvSt12_Index_tupleIIXspT_EEE
    @     0x2b549469cb45 _ZNSt12_Bind_simpleIFSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS4_EEEvEEclEv
    @     0x2b549469cade _ZNSt6thread5_ImplISt12_Bind_simpleIFSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS6_EEEvEEE6_M_runEv
    @     0x2b5495b81a40 (unknown)
    @     0x2b54962f8182 start_thread
    @     0x2b549660847d (unknown)
make[3]: *** [check-local] Segmentation fault
make[3]: Leaving directory `/home/vagrant/mesos/build/src'
make[2]: *** [check-am] Error 2
make[2]: Leaving directory `/home/vagrant/mesos/build/src'
make[1]: *** [check] Error 2
make[1]: Leaving directory `/home/vagrant/mesos/build/src'
make: *** [check-recursive] Error 1
{noformat}",3.0,0.26.0,0.5,0.5701357466063348,0.0,0.0,0.0,0.0,0.0,0.6842105263157894,0.5642857142857143,0.3717948717948718,0.24848484848484845,0.24848484848484845,0.2385321100917431
Bug,RegistryClientTest.SimpleRegistryPuller is flaky,"From ASF CI:
https://builds.apache.org/job/Mesos/1289/COMPILER=gcc,CONFIGURATION=--verbose%20--enable-libevent%20--enable-ssl,OS=centos:7,label_exp=docker%7C%7CHadoop/console

{code}
[ RUN      ] RegistryClientTest.SimpleRegistryPuller
I1127 02:51:40.235900   362 registry_client.cpp:511] Response status for url 'https://localhost:57828/v2/library/busybox/manifests/latest': 401 Unauthorized
I1127 02:51:40.249766   360 registry_client.cpp:511] Response status for url 'https://localhost:57828/v2/library/busybox/manifests/latest': 200 OK
I1127 02:51:40.251137   361 registry_puller.cpp:195] Downloading layer '1ce2e90b0bc7224de3db1f0d646fe8e2c4dd37f1793928287f6074bc451a57ea' for image 'busybox:latest'
I1127 02:51:40.258514   354 registry_client.cpp:511] Response status for url 'https://localhost:57828/v2/library/busybox/blobs/sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4': 307 Temporary Redirect
I1127 02:51:40.264171   367 libevent_ssl_socket.cpp:1023] Socket error: Connection reset by peer
../../src/tests/containerizer/provisioner_docker_tests.cpp:1210: Failure
(socket).failure(): Failed accept: connection error: Connection reset by peer
[  FAILED  ] RegistryClientTest.SimpleRegistryPuller (349 ms)
{code}

Logs from a previous run that passed:
{code}
[ RUN      ] RegistryClientTest.SimpleRegistryPuller
I1126 18:49:05.306396   349 registry_client.cpp:511] Response status for url 'https://localhost:53492/v2/library/busybox/manifests/latest': 401 Unauthorized
I1126 18:49:05.321362   347 registry_client.cpp:511] Response status for url 'https://localhost:53492/v2/library/busybox/manifests/latest': 200 OK
I1126 18:49:05.322720   352 registry_puller.cpp:195] Downloading layer '1ce2e90b0bc7224de3db1f0d646fe8e2c4dd37f1793928287f6074bc451a57ea' for image 'busybox:latest'
I1126 18:49:05.331317   350 registry_client.cpp:511] Response status for url 'https://localhost:53492/v2/library/busybox/blobs/sha256:a3ed95caeb02ffe68cdd9fd84406680ae93d633cb16422d00e8a7c22955b46d4': 307 Temporary Redirect
I1126 18:49:05.370625   352 registry_client.cpp:511] Response status for url 'https://127.0.0.1:53492/': 200 OK
I1126 18:49:05.372102   355 registry_puller.cpp:294] Untarring layer '1ce2e90b0bc7224de3db1f0d646fe8e2c4dd37f1793928287f6074bc451a57ea' downloaded from registry to directory 'output_dir'
[       OK ] RegistryClientTest.SimpleRegistryPuller (353 ms)
{code}",4.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3357142857142857,0.3846153846153846,0.4181818181818182,0.4121212121212121,0.0
Improvement,Remove quota from Registry for quota remove request,"When a remove quota requests hits the endpoint and passes validation, quota should be removed from the registry before the allocator is notified about the change.",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.717948717948718,0.8242424242424242,0.7212121212121212,0.0
Improvement,Introduce filter for non-revocable resources in `Resources`,"{{Resources}} class defines some handy filters, like {{revocable()}}, {{unreserved()}}, and so on. This ticket proposes to add one more: {{nonRevocable()}}.",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.717948717948718,0.8242424242424242,0.7212121212121212,0.0
Task,Introduce remove endpoint for quota,This endpoint is for removing quotas via the DELETE method.,3.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.12280701754385964,0.32857142857142857,0.717948717948718,0.8242424242424242,0.7212121212121212,0.0
Bug,RegistryClientTest.SimpleRegistryPuller doesn't compile with GCC 5.1.1,GCC 5.1.1 has {{-Werror=sign-compare}} in {{-Wall}} and stumbles over a comparison between signed and unsigned int in {{provisioner_docker_tests.cpp}}.,1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.2631578947368421,0.36428571428571427,0.28205128205128205,0.296969696969697,0.296969696969697,0.0
Bug,Pass agent work_dir to isolator modules,"Some isolator modules can benefit from access to the agent's {{work_dir}}. For example, the DVD isolator (https://github.com/emccode/mesos-module-dvdi) is currently forced to mount external volumes in a hard-coded directory. Making the {{work_dir}} accessible to the isolator via {{Isolator::recover()}} would allow the isolator to mount volumes within the agent's {{work_dir}}. This can be accomplished by simply adding an overloaded signature for {{Isolator::recover()}} which includes the {{work_dir}} as a parameter.",1.0,0,0.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Bug,ReservationEndpointsTest.UnreserveAvailableAndOfferedResources is flaky,"Showed up on ASF CI: ( test kept looping on and on and ultimately failing the build after 300 minutes )
https://builds.apache.org/job/Mesos/COMPILER=gcc,CONFIGURATION=--verbose,OS=ubuntu%3A14.04,label_exp=docker%7C%7CHadoop/1269/changes

{code}
[ RUN      ] ReservationEndpointsTest.UnreserveAvailableAndOfferedResources
I1124 01:07:20.050729 30260 leveldb.cpp:174] Opened db in 107.434842ms
I1124 01:07:20.099630 30260 leveldb.cpp:181] Compacted db in 48.82312ms
I1124 01:07:20.099722 30260 leveldb.cpp:196] Created db iterator in 29905ns
I1124 01:07:20.099738 30260 leveldb.cpp:202] Seeked to beginning of db in 3145ns
I1124 01:07:20.099750 30260 leveldb.cpp:271] Iterated through 0 keys in the db in 279ns
I1124 01:07:20.099804 30260 replica.cpp:778] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I1124 01:07:20.100637 30292 recover.cpp:447] Starting replica recovery
I1124 01:07:20.100934 30292 recover.cpp:473] Replica is in EMPTY status
I1124 01:07:20.103240 30288 replica.cpp:674] Replica in EMPTY status received a broadcasted recover request from (6305)@172.17.18.107:37993
I1124 01:07:20.103672 30292 recover.cpp:193] Received a recover response from a replica in EMPTY status
I1124 01:07:20.104142 30292 recover.cpp:564] Updating replica status to STARTING
I1124 01:07:20.114534 30284 master.cpp:365] Master ad27bc60-16d1-4239-9a65-235a991f9600 (9f2f81738d5e) started on 172.17.18.107:37993
I1124 01:07:20.114558 30284 master.cpp:367] Flags at startup: --acls="""" --allocation_interval=""1000secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/I60I5f/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""25secs"" --registry_strict=""true"" --roles=""role"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.26.0/_inst/share/mesos/webui"" --work_dir=""/tmp/I60I5f/master"" --zk_session_timeout=""10secs""
I1124 01:07:20.114809 30284 master.cpp:412] Master only allowing authenticated frameworks to register
I1124 01:07:20.114820 30284 master.cpp:417] Master only allowing authenticated slaves to register
I1124 01:07:20.114825 30284 credentials.hpp:35] Loading credentials for authentication from '/tmp/I60I5f/credentials'
I1124 01:07:20.115067 30284 master.cpp:456] Using default 'crammd5' authenticator
I1124 01:07:20.115320 30284 master.cpp:493] Authorization enabled
I1124 01:07:20.115792 30285 hierarchical.cpp:162] Initialized hierarchical allocator process
I1124 01:07:20.115855 30285 whitelist_watcher.cpp:77] No whitelist given
I1124 01:07:20.118755 30285 master.cpp:1625] The newly elected leader is master@172.17.18.107:37993 with id ad27bc60-16d1-4239-9a65-235a991f9600
I1124 01:07:20.118788 30285 master.cpp:1638] Elected as the leading master!
I1124 01:07:20.118809 30285 master.cpp:1383] Recovering from registrar
I1124 01:07:20.119078 30285 registrar.cpp:307] Recovering registrar
I1124 01:07:20.143256 30292 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 38.787419ms
I1124 01:07:20.143347 30292 replica.cpp:321] Persisted replica status to STARTING
I1124 01:07:20.143717 30292 recover.cpp:473] Replica is in STARTING status
I1124 01:07:20.145454 30286 replica.cpp:674] Replica in STARTING status received a broadcasted recover request from (6307)@172.17.18.107:37993
I1124 01:07:20.145979 30292 recover.cpp:193] Received a recover response from a replica in STARTING status
I1124 01:07:20.146654 30292 recover.cpp:564] Updating replica status to VOTING
I1124 01:07:20.182672 30286 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 35.422256ms
I1124 01:07:20.182747 30286 replica.cpp:321] Persisted replica status to VOTING
I1124 01:07:20.182929 30286 recover.cpp:578] Successfully joined the Paxos group
I1124 01:07:20.183115 30286 recover.cpp:462] Recover process terminated
I1124 01:07:20.183831 30286 log.cpp:659] Attempting to start the writer
I1124 01:07:20.185907 30285 replica.cpp:494] Replica received implicit promise request from (6308)@172.17.18.107:37993 with proposal 1
I1124 01:07:20.225256 30285 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 39.291288ms
I1124 01:07:20.225344 30285 replica.cpp:343] Persisted promised to 1
I1124 01:07:20.226850 30286 coordinator.cpp:238] Coordinator attempting to fill missing positions
I1124 01:07:20.228394 30293 replica.cpp:389] Replica received explicit promise request from (6309)@172.17.18.107:37993 for position 0 with proposal 2
I1124 01:07:20.266371 30293 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 37.874181ms
I1124 01:07:20.266456 30293 replica.cpp:713] Persisted action at 0
I1124 01:07:20.267927 30293 replica.cpp:538] Replica received write request for position 0 from (6310)@172.17.18.107:37993
I1124 01:07:20.268002 30293 leveldb.cpp:436] Reading position from leveldb took 37139ns
I1124 01:07:20.308117 30293 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 39.961976ms
I1124 01:07:20.308205 30293 replica.cpp:713] Persisted action at 0
I1124 01:07:20.309033 30290 replica.cpp:692] Replica received learned notice for position 0 from @0.0.0.0:0
I1124 01:07:20.343257 30290 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 34.175337ms
I1124 01:07:20.343343 30290 replica.cpp:713] Persisted action at 0
I1124 01:07:20.343377 30290 replica.cpp:698] Replica learned NOP action at position 0
I1124 01:07:20.344446 30282 log.cpp:675] Writer started with ending position 0
I1124 01:07:20.346143 30291 leveldb.cpp:436] Reading position from leveldb took 56896ns
I1124 01:07:20.347618 30291 registrar.cpp:340] Successfully fetched the registry (0B) in 228.495104ms
I1124 01:07:20.347862 30291 registrar.cpp:439] Applied 1 operations in 41164ns; attempting to update the 'registry'
I1124 01:07:20.348794 30279 log.cpp:683] Attempting to append 178 bytes to the log
I1124 01:07:20.349081 30279 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 1
I1124 01:07:20.350244 30294 replica.cpp:538] Replica received write request for position 1 from (6311)@172.17.18.107:37993
I1124 01:07:20.385246 30294 leveldb.cpp:341] Persisting action (197 bytes) to leveldb took 34.872508ms
I1124 01:07:20.385323 30294 replica.cpp:713] Persisted action at 1
I1124 01:07:20.386814 30294 replica.cpp:692] Replica received learned notice for position 1 from @0.0.0.0:0
I1124 01:07:20.425163 30294 leveldb.cpp:341] Persisting action (199 bytes) to leveldb took 38.282493ms
I1124 01:07:20.425262 30294 replica.cpp:713] Persisted action at 1
I1124 01:07:20.425298 30294 replica.cpp:698] Replica learned APPEND action at position 1
I1124 01:07:20.427994 30287 registrar.cpp:484] Successfully updated the 'registry' in 79.949056ms
I1124 01:07:20.428141 30283 log.cpp:702] Attempting to truncate the log to 1
I1124 01:07:20.428738 30287 registrar.cpp:370] Successfully recovered registrar
I1124 01:07:20.429306 30290 master.cpp:1435] Recovered 0 slaves from the Registry (139B) ; allowing 10mins for slaves to re-register
I1124 01:07:20.429592 30290 hierarchical.cpp:174] Allocator recovery is not supported yet
I1124 01:07:20.430083 30294 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 2
I1124 01:07:20.431411 30294 replica.cpp:538] Replica received write request for position 2 from (6312)@172.17.18.107:37993
I1124 01:07:20.467258 30294 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 35.661978ms
I1124 01:07:20.467342 30294 replica.cpp:713] Persisted action at 2
I1124 01:07:20.468842 30290 replica.cpp:692] Replica received learned notice for position 2 from @0.0.0.0:0
I1124 01:07:20.502264 30290 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 33.367074ms
I1124 01:07:20.502426 30290 leveldb.cpp:399] Deleting ~1 keys from leveldb took 80765ns
I1124 01:07:20.502452 30290 replica.cpp:713] Persisted action at 2
I1124 01:07:20.502488 30290 replica.cpp:698] Replica learned TRUNCATE action at position 2
I1124 01:07:20.510509 30260 containerizer.cpp:141] Using isolation: posix/cpu,posix/mem,filesystem/posix
W1124 01:07:20.511119 30260 backend.cpp:48] Failed to create 'bind' backend: BindBackend requires root privileges
I1124 01:07:20.516801 30288 slave.cpp:189] Slave started on 219)@172.17.18.107:37993
I1124 01:07:20.516839 30288 slave.cpp:190] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/ReservationEndpointsTest_UnreserveAvailableAndOfferedResources_CSzecr/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""auth.docker.io"" --docker_auth_server_port=""443"" --docker_kill_orphans=""true"" --docker_local_archives_dir=""/tmp/mesos/images/docker"" --docker_puller=""local"" --docker_puller_timeout=""60"" --docker_registry=""registry-1.docker.io"" --docker_registry_port=""443"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/ReservationEndpointsTest_UnreserveAvailableAndOfferedResources_CSzecr/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.26.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/ReservationEndpointsTest_UnreserveAvailableAndOfferedResources_CSzecr""
I1124 01:07:20.517670 30288 credentials.hpp:83] Loading credential for authentication from '/tmp/ReservationEndpointsTest_UnreserveAvailableAndOfferedResources_CSzecr/credential'
I1124 01:07:20.517982 30288 slave.cpp:320] Slave using credential for: test-principal
I1124 01:07:20.518334 30288 resources.cpp:472] Parsing resources as JSON failed: cpus:2;mem:1024;disk:1024;ports:[31000-32000]
Trying semicolon-delimited string format instead
I1124 01:07:20.518815 30260 resources.cpp:472] Parsing resources as JSON failed: cpus:1;mem:128
Trying semicolon-delimited string format instead
I1124 01:07:20.518975 30288 slave.cpp:390] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I1124 01:07:20.519104 30288 slave.cpp:398] Slave attributes: [  ]
I1124 01:07:20.519124 30288 slave.cpp:403] Slave hostname: 9f2f81738d5e
I1124 01:07:20.519136 30288 slave.cpp:408] Slave checkpoint: true
I1124 01:07:20.519407 30260 resources.cpp:472] Parsing resources as JSON failed: mem:384
Trying semicolon-delimited string format instead
I1124 01:07:20.522702 30288 state.cpp:52] Recovering state from '/tmp/ReservationEndpointsTest_UnreserveAvailableAndOfferedResources_CSzecr/meta'
I1124 01:07:20.523265 30288 status_update_manager.cpp:200] Recovering status update manager
I1124 01:07:20.523531 30288 containerizer.cpp:383] Recovering containerizer
I1124 01:07:20.524998 30288 slave.cpp:4258] Finished recovery
I1124 01:07:20.525802 30288 slave.cpp:4430] Querying resource estimator for oversubscribable resources
I1124 01:07:20.526753 30288 slave.cpp:727] New master detected at master@172.17.18.107:37993
I1124 01:07:20.527292 30288 slave.cpp:790] Authenticating with master master@172.17.18.107:37993
I1124 01:07:20.528240 30288 slave.cpp:795] Using default CRAM-MD5 authenticatee
I1124 01:07:20.527003 30286 status_update_manager.cpp:174] Pausing sending status updates
I1124 01:07:20.528955 30285 authenticatee.cpp:121] Creating new client SASL connection
I1124 01:07:20.529469 30285 master.cpp:5169] Authenticating slave(219)@172.17.18.107:37993
I1124 01:07:20.529729 30283 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(515)@172.17.18.107:37993
I1124 01:07:20.530287 30283 authenticator.cpp:98] Creating new server SASL connection
I1124 01:07:20.530764 30285 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I1124 01:07:20.530903 30285 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I1124 01:07:20.531096 30285 authenticator.cpp:203] Received SASL authentication start
I1124 01:07:20.531241 30285 authenticator.cpp:325] Authentication requires more steps
I1124 01:07:20.531388 30285 authenticatee.cpp:258] Received SASL authentication step
I1124 01:07:20.531616 30285 authenticator.cpp:231] Received SASL authentication step
I1124 01:07:20.531668 30285 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '9f2f81738d5e' server FQDN: '9f2f81738d5e' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I1124 01:07:20.531690 30285 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I1124 01:07:20.531774 30285 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I1124 01:07:20.531834 30285 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '9f2f81738d5e' server FQDN: '9f2f81738d5e' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I1124 01:07:20.531855 30285 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I1124 01:07:20.531867 30285 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I1124 01:07:20.531903 30285 authenticator.cpp:317] Authentication success
I1124 01:07:20.532016 30283 authenticatee.cpp:298] Authentication success
I1124 01:07:20.532331 30281 master.cpp:5199] Successfully authenticated principal 'test-principal' at slave(219)@172.17.18.107:37993
I1124 01:07:20.532652 30291 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(515)@172.17.18.107:37993
I1124 01:07:20.533113 30288 slave.cpp:763] Detecting new master
I1124 01:07:20.533628 30288 slave.cpp:4444] Received oversubscribable resources  from the resource estimator
I1124 01:07:20.546396 30288 slave.cpp:858] Successfully authenticated with master master@172.17.18.107:37993
I1124 01:07:20.547111 30287 master.cpp:3878] Registering slave at slave(219)@172.17.18.107:37993 (9f2f81738d5e) with id ad27bc60-16d1-4239-9a65-235a991f9600-S0
I1124 01:07:20.547886 30287 registrar.cpp:439] Applied 1 operations in 91121ns; attempting to update the 'registry'
I1124 01:07:20.550647 30287 log.cpp:683] Attempting to append 347 bytes to the log
I1124 01:07:20.550935 30279 coordinator.cpp:348] Coordinator attempting to write APPEND action at position 3
I1124 01:07:20.551534 30288 slave.cpp:1252] Will retry registration in 3.399312ms if necessary
I1124 01:07:20.551868 30291 replica.cpp:538] Replica received write request for position 3 from (6324)@172.17.18.107:37993
I1124 01:07:20.557605 30281 slave.cpp:1252] Will retry registration in 16.296866ms if necessary
I1124 01:07:20.557891 30293 master.cpp:3866] Ignoring register slave message from slave(219)@172.17.18.107:37993 (9f2f81738d5e) as admission is already in progress
I1124 01:07:20.574681 30279 slave.cpp:1252] Will retry registration in 73.52632ms if necessary
I1124 01:07:20.575078 30293 master.cpp:3866] Ignoring register slave message from slave(219)@172.17.18.107:37993 (9f2f81738d5e) as admission is already in progress
I1124 01:07:20.586236 30291 leveldb.cpp:341] Persisting action (366 bytes) to leveldb took 34.301173ms
I1124 01:07:20.586287 30291 replica.cpp:713] Persisted action at 3
I1124 01:07:20.587509 30289 replica.cpp:692] Replica received learned notice for position 3 from @0.0.0.0:0
I1124 01:07:20.611263 30289 leveldb.cpp:341] Persisting action (368 bytes) to leveldb took 23.677211ms
I1124 01:07:20.611352 30289 replica.cpp:713] Persisted action at 3
I1124 01:07:20.611387 30289 replica.cpp:698] Replica learned APPEND action at position 3
I1124 01:07:20.613580 30279 registrar.cpp:484] Successfully updated the 'registry' in 65.490944ms
I1124 01:07:20.613802 30288 log.cpp:702] Attempting to truncate the log to 3
I1124 01:07:20.613993 30288 coordinator.cpp:348] Coordinator attempting to write TRUNCATE action at position 4
I1124 01:07:20.615281 30289 replica.cpp:538] Replica received write request for position 4 from (6325)@172.17.18.107:37993
I1124 01:07:20.615883 30279 master.cpp:3946] Registered slave ad27bc60-16d1-4239-9a65-235a991f9600-S0 at slave(219)@172.17.18.107:37993 (9f2f81738d5e) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I1124 01:07:20.616261 30282 slave.cpp:902] Registered with master master@172.17.18.107:37993; given slave ID ad27bc60-16d1-4239-9a65-235a991f9600-S0
I1124 01:07:20.616883 30282 fetcher.cpp:79] Clearing fetcher cache
I1124 01:07:20.617261 30280 status_update_manager.cpp:181] Resuming sending status updates
I1124 01:07:20.617766 30282 slave.cpp:925] Checkpointing SlaveInfo to '/tmp/ReservationEndpointsTest_UnreserveAvailableAndOfferedResources_CSzecr/meta/slaves/ad27bc60-16d1-4239-9a65-235a991f9600-S0/slave.info'
I1124 01:07:20.616550 30284 hierarchical.cpp:380] Added slave ad27bc60-16d1-4239-9a65-235a991f9600-S0 (9f2f81738d5e) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
I1124 01:07:20.618670 30282 slave.cpp:961] Forwarding total oversubscribed resources 
I1124 01:07:20.618932 30282 slave.cpp:3197] Received ping from slave-observer(216)@172.17.18.107:37993
I1124 01:07:20.619288 30285 master.cpp:4288] Received update of slave ad27bc60-16d1-4239-9a65-235a991f9600-S0 at slave(219)@172.17.18.107:37993 (9f2f81738d5e) with total oversubscribed resources 
I1124 01:07:20.619446 30284 hierarchical.cpp:1066] No resources available to allocate!
I1124 01:07:20.619526 30284 hierarchical.cpp:1159] No inverse offers to send out!
I1124 01:07:20.619568 30284 hierarchical.cpp:977] Performed allocation for slave ad27bc60-16d1-4239-9a65-235a991f9600-S0 in 1.108641ms
I1124 01:07:20.620057 30284 hierarchical.cpp:436] Slave ad27bc60-16d1-4239-9a65-235a991f9600-S0 (9f2f81738d5e) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: )
I1124 01:07:20.620393 30284 hierarchical.cpp:1066] No resources available to allocate!
I1124 01:07:20.620462 30284 hierarchical.cpp:1159] No inverse offers to send out!
I1124 01:07:20.620507 30284 hierarchical.cpp:977] Performed allocation for slave ad27bc60-16d1-4239-9a65-235a991f9600-S0 in 395959ns
I1124 01:07:20.624356 30285 process.cpp:3067] Handling HTTP event for process 'master' with path: '/master/reserve'
I1124 01:07:20.624418 30285 http.cpp:336] HTTP POST for /master/reserve from 172.17.18.107:48995
I1124 01:07:20.626936 30285 master.cpp:6224] Sending checkpointed resources cpus(role, test-principal):1; mem(role, test-principal):512 to slave ad27bc60-16d1-4239-9a65-235a991f9600-S0 at slave(219)@172.17.18.107:37993 (9f2f81738d5e)
I1124 01:07:20.631428 30260 sched.cpp:164] Version: 0.26.0
I1124 01:07:20.632068 30287 sched.cpp:262] New master detected at master@172.17.18.107:37993
I1124 01:07:20.632230 30287 sched.cpp:318] Authenticating with master master@172.17.18.107:37993
I1124 01:07:20.632307 30287 sched.cpp:325] Using default CRAM-MD5 authenticatee
I1124 01:07:20.632693 30287 authenticatee.cpp:121] Creating new client SASL connection
I1124 01:07:20.633275 30287 master.cpp:5169] Authenticating scheduler-2c19aed9-7470-4927-949b-fb23f1775ab4@172.17.18.107:37993
I1124 01:07:20.633519 30287 authenticator.cpp:413] Starting authentication session for crammd5_authenticatee(516)@172.17.18.107:37993
I1124 01:07:20.633965 30287 authenticator.cpp:98] Creating new server SASL connection
I1124 01:07:20.634316 30287 authenticatee.cpp:212] Received SASL authentication mechanisms: CRAM-MD5
I1124 01:07:20.634456 30287 authenticatee.cpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I1124 01:07:20.634605 30287 authenticator.cpp:203] Received SASL authentication start
I1124 01:07:20.634771 30287 authenticator.cpp:325] Authentication requires more steps
I1124 01:07:20.634914 30287 authenticatee.cpp:258] Received SASL authentication step
I1124 01:07:20.635126 30287 authenticator.cpp:231] Received SASL authentication step
I1124 01:07:20.635270 30287 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '9f2f81738d5e' server FQDN: '9f2f81738d5e' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I1124 01:07:20.635347 30287 auxprop.cpp:179] Looking up auxiliary property '*userPassword'
I1124 01:07:20.636262 30287 auxprop.cpp:179] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I1124 01:07:20.636349 30287 auxprop.cpp:107] Request to lookup properties for user: 'test-principal' realm: '9f2f81738d5e' server FQDN: '9f2f81738d5e' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I1124 01:07:20.636415 30287 auxprop.cpp:129] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I1124 01:07:20.636466 30287 auxprop.cpp:129] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I1124 01:07:20.636541 30287 authenticator.cpp:317] Authentication success
I1124 01:07:20.636754 30287 authenticatee.cpp:298] Authentication success
I1124 01:07:20.636831 30286 master.cpp:5199] Successfully authenticated principal 'test-principal' at scheduler-2c19aed9-7470-4927-949b-fb23f1775ab4@172.17.18.107:37993
I1124 01:07:20.636884 30281 authenticator.cpp:431] Authentication session cleanup for crammd5_authenticatee(516)@172.17.18.107:37993
I1124 01:07:20.637516 30286 sched.cpp:407] Successfully authenticated with master master@172.17.18.107:37993
I1124 01:07:20.637622 30286 sched.cpp:714] Sending SUBSCRIBE call to master@172.17.18.107:37993
I1124 01:07:20.637763 30286 sched.cpp:747] Will retry registration in 1.659715229secs if necessary
I1124 01:07:20.637928 30280 master.cpp:2195] Received SUBSCRIBE call for framework 'default' at scheduler-2c19aed9-7470-4927-949b-fb23f1775ab4@172.17.18.107:37993
I1124 01:07:20.638162 30280 master.cpp:1664] Authorizing framework principal 'test-principal' to receive offers for role 'role'
I1124 01:07:20.638510 30280 master.cpp:2266] Subscribing framework default with checkpointing disabled and capabilities [  ]
I1124 01:07:20.639348 30283 sched.cpp:641] Framework registered with ad27bc60-16d1-4239-9a65-235a991f9600-0000
I1124 01:07:20.639452 30283 sched.cpp:655] Scheduler::registered took 18594ns
I1124 01:07:20.639559 30280 hierarchical.cpp:220] Added framework ad27bc60-16d1-4239-9a65-235a991f9600-0000
I1124 01:07:20.640575 30280 hierarchical.cpp:1159] No inverse offers to send out!
I1124 01:07:20.641254 30280 hierarchical.cpp:961] Performed allocation for 1 slaves in 1.618341ms
I1124 01:07:20.641125 30283 master.cpp:4998] Sending 1 offers to framework ad27bc60-16d1-4239-9a65-235a991f9600-0000 (default) at scheduler-2c19aed9-7470-4927-949b-fb23f1775ab4@172.17.18.107:37993

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: resourceOffers(0x7fffe27c3830, @0x2ae03c9ce9d0 { 144-byte object <D0-1C B2-33 E0-2A 00-00 00-00 00-00 00-00 00-00 70-CF 00-48 E0-2A 00-00 10-D0 00-48 E0-2A 00-00 B0-D0 00-48 E0-2A 00-00 50-D1 00-48 E0-2A 00-00 A0-D1 00-48 E0-2A 00-00 40-00 01-48 E0-2A 00-00 ... 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 35-61 39-39 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 63-36 30-2D 00-00 00-00 00-00 00-00 00-00 00-00 1F-00 00-00> })
Stack trace:
I1124 01:07:20.642010 30283 sched.cpp:811] Scheduler::resourceOffers took 257483ns
I1124 01:07:20.642578 30283 master.cpp:3395] Processing REVIVE call for framework ad27bc60-16d1-4239-9a65-235a991f9600-0000 (default) at scheduler-2c19aed9-7470-4927-949b-fb23f1775ab4@172.17.18.107:37993
I1124 01:07:20.642757 30283 hierarchical.cpp:886] Removed offer filters for framework ad27bc60-16d1-4239-9a65-235a991f9600-0000
I1124 01:07:20.644323 30283 hierarchical.cpp:1066] No resources available to allocate!
I1124 01:07:20.644377 30283 hierarchical.cpp:1159] No inverse offers to send out!
I1124 01:07:20.644404 30283 hierarchical.cpp:961] Performed allocation for 1 slaves in 1.616762ms
I1124 01:07:20.644626 30291 slave.cpp:2275] Updated checkpointed resources from  to cpus(role, test-principal):1; mem(role, test-principal):512
I1124 01:07:20.652545 30289 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 37.216951ms
I1124 01:07:20.652601 30289 replica.cpp:713] Persisted action at 4
I1124 01:07:20.667095 30279 replica.cpp:692] Replica received learned notice for position 4 from @0.0.0.0:0
I1124 01:07:20.695262 30279 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 28.08901ms
I1124 01:07:20.695446 30279 leveldb.cpp:399] Deleting ~2 keys from leveldb took 96033ns
I1124 01:07:20.695482 30279 replica.cpp:713] Persisted action at 4
I1124 01:07:20.695523 30279 replica.cpp:698] Replica learned TRUNCATE action at position 4
2015-11-24 01:07:21,415:30260(0x2ae0d6471700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:46930] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-11-24 01:07:24,751:30260(0x2ae0d6471700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:46930] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-11-24 01:07:28,087:30260(0x2ae0d6471700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:46930] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-11-24 01:07:31,424:30260(0x2ae0d6471700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:46930] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-11-24 01:07:34,760:30260(0x2ae0d6471700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:46930] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
I1124 01:07:35.534328 30283 slave.cpp:4430] Querying resource estimator for oversubscribable resources
I1124 01:07:35.534613 30283 slave.cpp:4444] Received oversubscribable resources  from the resource estimator
I1124 01:07:35.616957 30293 slave.cpp:3197] Received ping from slave-observer(216)@172.17.18.107:37993
2015-11-24 01:07:38,096:30260(0x2ae0d6471700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:46930] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-11-24 01:07:41,433:30260(0x2ae0d6471700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:46930] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-11-24 01:07:44,769:30260(0x2ae0d6471700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:46930] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-11-24 01:07:48,105:30260(0x2ae0d6471700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:46930] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
I1124 01:07:50.535526 30284 slave.cpp:4430] Querying resource estimator for oversubscribable resources
I1124 01:07:50.535809 30284 slave.cpp:4444] Received oversubscribable resources  from the resource estimator
I1124 01:07:50.618424 30284 slave.cpp:3197] Received ping from slave-observer(216)@172.17.18.107:37993
2015-11-24 01:07:51,441:30260(0x2ae0d6471700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:46930] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-11-24 01:07:54,778:30260(0x2ae0d6471700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:46930] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-11-24 01:07:58,114:30260(0x2ae0d6471700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:46930] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-11-24 01:08:01,450:30260(0x2ae0d6471700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:46930] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-11-24 01:08:04,785:30260(0x2ae0d6471700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:46930] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
I1124 01:08:05.536947 30293 slave.cpp:4430] Querying resource estimator for oversubscribable resources
I1124 01:08:05.537225 30293 slave.cpp:4444] Received oversubscribable resources  from the resource estimator
I1124 01:08:05.619575 30293 slave.cpp:3197] Received ping from slave-observer(216)@172.17.18.107:37993
2015-11-24 01:08:08,119:30260(0x2ae0d6471700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:46930] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-11-24 01:08:11,456:30260(0x2ae0d6471700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:46930] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-11-24 01:08:14,790:30260(0x2ae0d6471700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:46930] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-11-24 01:08:18,127:30260(0x2ae0d6471700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:46930] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
I1124 01:08:20.520412 30280 slave.cpp:4067] Current disk usage 6.86%. Max allowed age: 5.820034363201435days
I1124 01:08:20.539533 30289 slave.cpp:4430] Querying resource estimator for oversubscribable resources
I1124 01:08:20.539852 30289 slave.cpp:4444] Received oversubscribable resources  from the resource estimator
I1124 01:08:20.620409 30284 slave.cpp:3197] Received ping from slave-observer(216)@172.17.18.107:37993
2015-11-24 01:08:21,463:30260(0x2ae0d6471700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:46930] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-11-24 01:08:24,799:30260(0x2ae0d6471700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:46930] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-11-24 01:08:28,135:30260(0x2ae0d6471700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:46930] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-11-24 01:08:31,471:30260(0x2ae0d6471700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:46930] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-11-24 01:08:34,807:30260(0x2ae0d6471700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:46930] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
I1124 01:08:35.540747 30288 slave.cpp:4430] Querying resource estimator for oversubscribable resources
I1124 01:08:35.541132 30288 slave.cpp:4444] Received oversubscribable resources  from the resource estimator
I1124 01:08:35.621484 30287 slave.cpp:3197] Received ping from slave-observer(216)@172.17.18.107:37993
2015-11-24 01:08:38,143:30260(0x2ae0d6471700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:46930] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-11-24 01:08:41,479:30260(0x2ae0d6471700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:46930] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2015-11-24 01:08:44,815:30260(0x2ae0d6471700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:46930] zk retcode=-4, errno=111(Connection refused): server refused",1.0,0,0.0,0.5701357466063348,0.0,0.0,0.0,0.0,0.0,0.6842105263157894,0.5642857142857143,0.3846153846153846,0.4181818181818182,0.4121212121212121,0.0
Documentation,"libprocess: document when, why defer() is necessary","Current rules around this are pretty confusing and undocumented, as evidenced by some recent bugs in this area.

Some example snippets in the mesos source code that were a result of this confusion and are indeed bugs:

1. https://github.com/apache/mesos/blob/master/src/slave/containerizer/mesos/provisioner/docker/registry_client.cpp#L754
{code}
return doHttpGet(blobURL, None(), true, true, None())
    .then([this, blobURLPath, digest, filePath](
        const http::Response& response) -> Future<size_t> {
      Try<int> fd = os::open(
          filePath.value,
          O_WRONLY | O_CREAT | O_TRUNC | O_CLOEXEC,
          S_IRUSR | S_IWUSR | S_IRGRP | S_IROTH);
{code}
",1.0,0,0.5,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.48717948717948717,0.4303030303030303,0.4303030303030303,0.0
Task,Tests for quota request validation,"Tests should include:
* JSON validation;
* Absence of irrelevant fields;
* Semantic validation.",3.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.12280701754385964,0.32857142857142857,0.717948717948718,0.8242424242424242,0.7212121212121212,0.0
Task,Implement recovery in the Hierarchical allocator,The built-in Hierarchical allocator should implement the recovery (in the presence of quota).,3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.717948717948718,0.8242424242424242,0.7212121212121212,0.0
Improvement,Replace `QuotaInfo` with `Quota` in allocator interface,"After introduction of C++ wrapper `Quota` for `QuotaInfo`, all allocator methods using `QuotaInfo` should be updated.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.717948717948718,0.8242424242424242,0.7212121212121212,0.0
Bug,C++ HTTP Scheduler Library does not work with SSL enabled,"The C++ HTTP scheduler library does not work against Mesos when SSL is enabled (without downgrade).

The fix should be simple:
* The library should detect if SSL is enabled.
* If SSL is enabled, connections should be made with HTTPS instead of HTTP.",3.0,0,0.5,0.5701357466063348,0.0,0.0,0.0,0.0,0.0,0.6842105263157894,0.5642857142857143,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.0
Bug,Ensure resources in `QuotaInfo` protobuf do not contain `role`,"{{QuotaInfo}} protobuf currently stores per-role quotas, including {{Resource}} objects. These resources are neither statically nor dynamically reserved, hence they may not contain {{role}} field. We should ensure this field is unset, as well as update validation routine for {{QuotaInfo}}",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.717948717948718,0.8242424242424242,0.7212121212121212,0.0
Bug,LimitedCpuIsolatorTest.ROOT_CGROUPS_Cfs and LimitedCpuIsolatorTest.ROOT_CGROUPS_Cfs_Big_Quota fail on Debian 8.,"sudo ./bin/mesos-test.sh --gtest_filter=""LimitedCpuIsolatorTest.ROOT_CGROUPS_Cfs""

{noformat}
...
F1119 14:34:52.514742 30706 isolator_tests.cpp:455] CHECK_SOME(isolator): Failed to find 'cpu.cfs_quota_us'. Your kernel might be too old to use the CFS cgroups feature.
{noformat}
",2.0,0.26.0,0.5,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.14102564102564102,0.16969696969696968,0.17575757575757575,0.2385321100917431
Improvement,Standardize quota endpoints,"To be consistent with other operator endpoints, require a single JSON object in the request as opposed to key-value pairs encoded in a string.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.717948717948718,0.8242424242424242,0.7212121212121212,0.0
Bug,User CGroup Isolation tests fail on Centos 6.,"UserCgroupIsolatorTest/0.ROOT_CGROUPS_UserCgroup and UserCgroupIsolatorTest/1.ROOT_CGROUPS_UserCgroup fail on CentOS 6.6 with similar output when libevent and SSL are enabled.

{noformat}
sudo ./bin/mesos-tests.sh --gtest_filter=""UserCgroupIsolatorTest/0.ROOT_CGROUPS_UserCgroup"" --verbose
{noformat}
{noformat}
[==========] Running 1 test from 1 test case.
[----------] Global test environment set-up.
[----------] 1 test from UserCgroupIsolatorTest/0, where TypeParam = mesos::internal::slave::CgroupsMemIsolatorProcess
userdel: user 'mesos.test.unprivileged.user' does not exist
[ RUN      ] UserCgroupIsolatorTest/0.ROOT_CGROUPS_UserCgroup
I1118 16:53:35.273717 30249 mem.cpp:605] Started listening for OOM events for container 867a829e-4a26-43f5-86e0-938bf1f47688
I1118 16:53:35.274538 30249 mem.cpp:725] Started listening on low memory pressure events for container 867a829e-4a26-43f5-86e0-938bf1f47688
I1118 16:53:35.275164 30249 mem.cpp:725] Started listening on medium memory pressure events for container 867a829e-4a26-43f5-86e0-938bf1f47688
I1118 16:53:35.275784 30249 mem.cpp:725] Started listening on critical memory pressure events for container 867a829e-4a26-43f5-86e0-938bf1f47688
I1118 16:53:35.276448 30249 mem.cpp:356] Updated 'memory.soft_limit_in_bytes' to 1GB for container 867a829e-4a26-43f5-86e0-938bf1f47688
I1118 16:53:35.277331 30249 mem.cpp:391] Updated 'memory.limit_in_bytes' to 1GB for container 867a829e-4a26-43f5-86e0-938bf1f47688
-bash: /sys/fs/cgroup/memory/mesos/867a829e-4a26-43f5-86e0-938bf1f47688/cgroup.procs: No such file or directory
mkdir: cannot create directory `/sys/fs/cgroup/memory/mesos/867a829e-4a26-43f5-86e0-938bf1f47688/user': No such file or directory
../../src/tests/containerizer/isolator_tests.cpp:1307: Failure
Value of: os::system( ""su - "" + UNPRIVILEGED_USERNAME + "" -c 'mkdir "" + path::join(flags.cgroups_hierarchy, userCgroup) + ""'"")
  Actual: 256
Expected: 0
-bash: /sys/fs/cgroup/memory/mesos/867a829e-4a26-43f5-86e0-938bf1f47688/user/cgroup.procs: No such file or directory
../../src/tests/containerizer/isolator_tests.cpp:1316: Failure
Value of: os::system( ""su - "" + UNPRIVILEGED_USERNAME + "" -c 'echo $$ >"" + path::join(flags.cgroups_hierarchy, userCgroup, ""cgroup.procs"") + ""'"")
  Actual: 256
Expected: 0
[  FAILED  ] UserCgroupIsolatorTest/0.ROOT_CGROUPS_UserCgroup, where TypeParam = mesos::internal::slave::CgroupsMemIsolatorProcess (149 ms)
{noformat}

{noformat}
sudo ./bin/mesos-tests.sh --gtest_filter=""UserCgroupIsolatorTest/1.ROOT_CGROUPS_UserCgroup"" --verbose
{noformat}
{noformat}
[==========] Running 1 test from 1 test case.
[----------] Global test environment set-up.
[----------] 1 test from UserCgroupIsolatorTest/1, where TypeParam = mesos::internal::slave::CgroupsCpushareIsolatorProcess
userdel: user 'mesos.test.unprivileged.user' does not exist
[ RUN      ] UserCgroupIsolatorTest/1.ROOT_CGROUPS_UserCgroup
I1118 17:01:00.550706 30357 cpushare.cpp:392] Updated 'cpu.shares' to 1024 (cpus 1) for container e57f4343-1a97-4b44-b347-803be47ace80
-bash: /sys/fs/cgroup/cpuacct/mesos/e57f4343-1a97-4b44-b347-803be47ace80/cgroup.procs: No such file or directory
mkdir: cannot create directory `/sys/fs/cgroup/cpuacct/mesos/e57f4343-1a97-4b44-b347-803be47ace80/user': No such file or directory
../../src/tests/containerizer/isolator_tests.cpp:1307: Failure
Value of: os::system( ""su - "" + UNPRIVILEGED_USERNAME + "" -c 'mkdir "" + path::join(flags.cgroups_hierarchy, userCgroup) + ""'"")
  Actual: 256
Expected: 0
-bash: /sys/fs/cgroup/cpuacct/mesos/e57f4343-1a97-4b44-b347-803be47ace80/user/cgroup.procs: No such file or directory
../../src/tests/containerizer/isolator_tests.cpp:1316: Failure
Value of: os::system( ""su - "" + UNPRIVILEGED_USERNAME + "" -c 'echo $$ >"" + path::join(flags.cgroups_hierarchy, userCgroup, ""cgroup.procs"") + ""'"")
  Actual: 256
Expected: 0
-bash: /sys/fs/cgroup/cpu/mesos/e57f4343-1a97-4b44-b347-803be47ace80/cgroup.procs: No such file or directory
mkdir: cannot create directory `/sys/fs/cgroup/cpu/mesos/e57f4343-1a97-4b44-b347-803be47ace80/user': No such file or directory
../../src/tests/containerizer/isolator_tests.cpp:1307: Failure
Value of: os::system( ""su - "" + UNPRIVILEGED_USERNAME + "" -c 'mkdir "" + path::join(flags.cgroups_hierarchy, userCgroup) + ""'"")
  Actual: 256
Expected: 0
-bash: /sys/fs/cgroup/cpu/mesos/e57f4343-1a97-4b44-b347-803be47ace80/user/cgroup.procs: No such file or directory
../../src/tests/containerizer/isolator_tests.cpp:1316: Failure
Value of: os::system( ""su - "" + UNPRIVILEGED_USERNAME + "" -c 'echo $$ >"" + path::join(flags.cgroups_hierarchy, userCgroup, ""cgroup.procs"") + ""'"")
  Actual: 256
Expected: 0
[  FAILED  ] UserCgroupIsolatorTest/1.ROOT_CGROUPS_UserCgroup, where TypeParam = mesos::internal::slave::CgroupsCpushareIsolatorProcess (116 ms)
{noformat}",3.0,0.26.0,0.5,0.004524886877828055,0.0,0.0,0.0,0.0,0.2,0.12280701754385964,0.24285714285714285,0.14102564102564102,0.16969696969696968,0.17575757575757575,0.2385321100917431
Task,Add operator documentation for /weight endpoint,"This JIRA ticket will update the related doc to apply to dynamic weights, and add an new operator guide for dynamic weights which describes basic usage of the /weights endpoint.",2.0,0,0.5,0.0015082956259426848,0.3333333333333333,0.5,0.14285714285714285,0.2,0.2,0.03508771929824561,0.05714285714285714,0.0,0.006060606060606061,0.006060606060606061,0.0
Bug,/reserve and /unreserve should be permissive under a master without authentication.,"Currently, the {{/reserve}} and {{/unreserve}} endpoints do not work without authentication enabled on the master. When authentication is disabled on the master, these endpoints should just be permissive.",1.0,0,0.5,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.21794871794871795,0.26666666666666666,0.26666666666666666,0.0
Bug,ubsan error in net::IP::create(sockaddr const&): misaligned address,"Running ubsan from GCC 5.2 on the current Mesos unit tests yields this, among other problems:

{noformat}
/mesos/3rdparty/libprocess/3rdparty/stout/include/stout/ip.hpp:230:56: runtime error: reference binding to misaligned address 0x00000199629c for type 'const struct sockaddr_storage', which requires 8 byte alignment
0x00000199629c: note: pointer points here
  00 00 00 00 02 00 00 00  ff ff ff 00 00 00 00 00  00 00 00 00 00 00 00 00  00 00 00 00 00 00 00 00
              ^
    #0 0x5950cb in net::IP::create(sockaddr const&) (/home/vagrant/build-mesos-ubsan/3rdparty/libprocess/3rdparty/stout-tests+0x5950cb)
    #1 0x5970cd in net::IPNetwork::fromLinkDevice(std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, int) (/home/vagrant/build-mesos-ubsan/3rdparty/libprocess/3rdparty/stout-tests+0x5970cd)
    #2 0x58e006 in NetTest_LinkDevice_Test::TestBody() (/home/vagrant/build-mesos-ubsan/3rdparty/libprocess/3rdparty/stout-tests+0x58e006)
    #3 0x85abd5 in void testing::internal::HandleSehExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) (/home/vagrant/build-mesos-ubsan/3rdparty/libprocess/3rdparty/stout-tests+0x85abd5)
    #4 0x848abc in void testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void>(testing::Test*, void (testing::Test::*)(), char const*) (/home/vagrant/build-mesos-ubsan/3rdparty/libprocess/3rdparty/stout-tests+0x848abc)
    #5 0x7e2755 in testing::Test::Run() (/home/vagrant/build-mesos-ubsan/3rdparty/libprocess/3rdparty/stout-tests+0x7e2755)
    #6 0x7e44a0 in testing::TestInfo::Run() (/home/vagrant/build-mesos-ubsan/3rdparty/libprocess/3rdparty/stout-tests+0x7e44a0)
    #7 0x7e5ffa in testing::TestCase::Run() (/home/vagrant/build-mesos-ubsan/3rdparty/libprocess/3rdparty/stout-tests+0x7e5ffa)
    #8 0x7ffe21 in testing::internal::UnitTestImpl::RunAllTests() (/home/vagrant/build-mesos-ubsan/3rdparty/libprocess/3rdparty/stout-tests+0x7ffe21)
    #9 0x85d7a5 in bool testing::internal::HandleSehExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool>(testing::internal::UnitTestImpl*, bool (testing::internal::UnitTestImpl::*)(), char const*) (/home/vagrant/build-mesos-ubsan/3rdparty/libprocess/3rdparty/stout-tests+0x85d7a5)
    #10 0x84b37a in bool testing::internal::HandleExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool>(testing::internal::UnitTestImpl*, bool (testing::internal::UnitTestImpl::*)(), char const*) (/home/vagrant/build-mesos-ubsan/3rdparty/libprocess/3rdparty/stout-tests+0x84b37a)
    #11 0x7f8a4a in testing::UnitTest::Run() (/home/vagrant/build-mesos-ubsan/3rdparty/libprocess/3rdparty/stout-tests+0x7f8a4a)
    #12 0x608a96 in RUN_ALL_TESTS() (/home/vagrant/build-mesos-ubsan/3rdparty/libprocess/3rdparty/stout-tests+0x608a96)
    #13 0x60896b in main (/home/vagrant/build-mesos-ubsan/3rdparty/libprocess/3rdparty/stout-tests+0x60896b)
    #14 0x7fd0f0c7fa3f in __libc_start_main (/lib/x86_64-linux-gnu/libc.so.6+0x20a3f)
    #15 0x4145c8 in _start (/home/vagrant/build-mesos-ubsan/3rdparty/libprocess/3rdparty/stout-tests+0x4145c8)
{noformat}",2.0,0,0.0,1.0,0.0,0.0,0.0,0.0,0.8,0.5438596491228069,0.39999999999999997,0.48717948717948717,0.4303030303030303,0.4303030303030303,0.0
Task,Consider allowing setting quotas for the default '*' role.,"Investigate use cases and implications of the possibility to set quota for the '*' role. For example, having quota for '*' set can effectively reduce the scope of the quota capacity heuristic.",2.0,0,0.5,0.0,0.6666666666666666,1.0,0.5714285714285714,0.0,0.0,0.5438596491228069,0.5357142857142857,0.717948717948718,0.8242424242424242,0.7212121212121212,0.0
Task,Libprocess: Unify the initialization of the MetricsProcess and ReaperProcess,"Related to this [TODO|https://github.com/apache/mesos/blob/aa0cd7ed4edf1184cbc592b5caa2429a8373e813/3rdparty/libprocess/src/process.cpp#L949-L950].

The {{MetricsProcess}} and {{ReaperProcess}} are global processes (singletons) which are initialized upon first use.  The two processes could be initialized alongside the {{gc}}, {{help}}, {{logging}}, {{profiler}}, and {{system}} (statistics) processes inside {{process::initialize}}.

This is also necessary for libprocess re-initialization.",3.0,0,0.0,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.0
Bug,ROOT tests fail on Mesos 0.26 on Ubuntu/CentOS,"Running {{0.26.0-rc1}} on both CentOS 7.1 and Ubuntu 14.04 with {{sudo}} privileges, causes segfaults when running Docker tests.

Logs attached.",2.0,0.26.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.1794871794871795,0.15757575757575756,0.17575757575757575,0.2385321100917431
Bug,MasterMaintenanceTest.InverseOffersFilters is flaky,"Verbose Logs:
{code}
[ RUN      ] MasterMaintenanceTest.InverseOffersFilters
I1113 16:43:58.486469  8728 leveldb.cpp:176] Opened db in 2.360405ms
I1113 16:43:58.486935  8728 leveldb.cpp:183] Compacted db in 407105ns
I1113 16:43:58.486995  8728 leveldb.cpp:198] Created db iterator in 16221ns
I1113 16:43:58.487030  8728 leveldb.cpp:204] Seeked to beginning of db in 10935ns
I1113 16:43:58.487046  8728 leveldb.cpp:273] Iterated through 0 keys in the db in 999ns
I1113 16:43:58.487090  8728 replica.cpp:780] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I1113 16:43:58.487735  8747 recover.cpp:449] Starting replica recovery
I1113 16:43:58.488047  8747 recover.cpp:475] Replica is in EMPTY status
I1113 16:43:58.488977  8745 replica.cpp:676] Replica in EMPTY status received a broadcasted recover request from (58)@10.0.2.15:45384
I1113 16:43:58.489452  8746 recover.cpp:195] Received a recover response from a replica in EMPTY status
I1113 16:43:58.489712  8747 recover.cpp:566] Updating replica status to STARTING
I1113 16:43:58.490706  8742 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 745443ns
I1113 16:43:58.490739  8742 replica.cpp:323] Persisted replica status to STARTING
I1113 16:43:58.490859  8742 recover.cpp:475] Replica is in STARTING status
I1113 16:43:58.491786  8747 replica.cpp:676] Replica in STARTING status received a broadcasted recover request from (59)@10.0.2.15:45384
I1113 16:43:58.492542  8749 recover.cpp:195] Received a recover response from a replica in STARTING status
I1113 16:43:58.493221  8743 recover.cpp:566] Updating replica status to VOTING
I1113 16:43:58.493710  8743 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 331874ns
I1113 16:43:58.493767  8743 replica.cpp:323] Persisted replica status to VOTING
I1113 16:43:58.493868  8743 recover.cpp:580] Successfully joined the Paxos group
I1113 16:43:58.494119  8743 recover.cpp:464] Recover process terminated
I1113 16:43:58.504369  8749 master.cpp:367] Master d59449fc-5462-43c5-b935-e05563fdd4b6 (vagrant-ubuntu-wily-64) started on 10.0.2.15:45384
I1113 16:43:58.504438  8749 master.cpp:369] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""false"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/ZB7csS/credentials"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""25secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/local/share/mesos/webui"" --work_dir=""/tmp/ZB7csS/master"" --zk_session_timeout=""10secs""
I1113 16:43:58.504717  8749 master.cpp:416] Master allowing unauthenticated frameworks to register
I1113 16:43:58.504889  8749 master.cpp:419] Master only allowing authenticated slaves to register
I1113 16:43:58.504922  8749 credentials.hpp:37] Loading credentials for authentication from '/tmp/ZB7csS/credentials'
I1113 16:43:58.505497  8749 master.cpp:458] Using default 'crammd5' authenticator
I1113 16:43:58.505759  8749 master.cpp:495] Authorization enabled
I1113 16:43:58.507638  8746 master.cpp:1606] The newly elected leader is master@10.0.2.15:45384 with id d59449fc-5462-43c5-b935-e05563fdd4b6
I1113 16:43:58.507693  8746 master.cpp:1619] Elected as the leading master!
I1113 16:43:58.507720  8746 master.cpp:1379] Recovering from registrar
I1113 16:43:58.507946  8749 registrar.cpp:309] Recovering registrar
I1113 16:43:58.508561  8749 log.cpp:661] Attempting to start the writer
I1113 16:43:58.510282  8747 replica.cpp:496] Replica received implicit promise request from (60)@10.0.2.15:45384 with proposal 1
I1113 16:43:58.510867  8747 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 475696ns
I1113 16:43:58.510946  8747 replica.cpp:345] Persisted promised to 1
I1113 16:43:58.511912  8745 coordinator.cpp:240] Coordinator attempting to fill missing positions
I1113 16:43:58.513030  8749 replica.cpp:391] Replica received explicit promise request from (61)@10.0.2.15:45384 for position 0 with proposal 2
I1113 16:43:58.513819  8749 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 739171ns
I1113 16:43:58.513867  8749 replica.cpp:715] Persisted action at 0
I1113 16:43:58.522002  8745 replica.cpp:540] Replica received write request for position 0 from (62)@10.0.2.15:45384
I1113 16:43:58.522114  8745 leveldb.cpp:438] Reading position from leveldb took 33549ns
I1113 16:43:58.522599  8745 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 435729ns
I1113 16:43:58.522652  8745 replica.cpp:715] Persisted action at 0
I1113 16:43:58.523291  8746 replica.cpp:694] Replica received learned notice for position 0 from @0.0.0.0:0
I1113 16:43:58.523901  8746 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 538894ns
I1113 16:43:58.523983  8746 replica.cpp:715] Persisted action at 0
I1113 16:43:58.524060  8746 replica.cpp:700] Replica learned NOP action at position 0
I1113 16:43:58.524775  8747 log.cpp:677] Writer started with ending position 0
I1113 16:43:58.525902  8745 leveldb.cpp:438] Reading position from leveldb took 39685ns
I1113 16:43:58.526852  8745 registrar.cpp:342] Successfully fetched the registry (0B) in 18.832896ms
I1113 16:43:58.527084  8745 registrar.cpp:441] Applied 1 operations in 24930ns; attempting to update the 'registry'
I1113 16:43:58.528020  8745 log.cpp:685] Attempting to append 189 bytes to the log
I1113 16:43:58.528323  8748 coordinator.cpp:350] Coordinator attempting to write APPEND action at position 1
I1113 16:43:58.529465  8744 replica.cpp:540] Replica received write request for position 1 from (63)@10.0.2.15:45384
I1113 16:43:58.530081  8744 leveldb.cpp:343] Persisting action (208 bytes) to leveldb took 552812ns
I1113 16:43:58.530128  8744 replica.cpp:715] Persisted action at 1
I1113 16:43:58.530781  8745 replica.cpp:694] Replica received learned notice for position 1 from @0.0.0.0:0
I1113 16:43:58.531121  8745 leveldb.cpp:343] Persisting action (210 bytes) to leveldb took 271774ns
I1113 16:43:58.531162  8745 replica.cpp:715] Persisted action at 1
I1113 16:43:58.531188  8745 replica.cpp:700] Replica learned APPEND action at position 1
I1113 16:43:58.532064  8743 registrar.cpp:486] Successfully updated the 'registry' in 4.9152ms
I1113 16:43:58.532402  8743 registrar.cpp:372] Successfully recovered registrar
I1113 16:43:58.532768  8742 log.cpp:704] Attempting to truncate the log to 1
I1113 16:43:58.532891  8743 master.cpp:1416] Recovered 0 slaves from the Registry (150B) ; allowing 10mins for slaves to re-register
I1113 16:43:58.532968  8742 coordinator.cpp:350] Coordinator attempting to write TRUNCATE action at position 2
I1113 16:43:58.534010  8742 replica.cpp:540] Replica received write request for position 2 from (64)@10.0.2.15:45384
I1113 16:43:58.534488  8742 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 420186ns
I1113 16:43:58.534533  8742 replica.cpp:715] Persisted action at 2
I1113 16:43:58.535081  8748 replica.cpp:694] Replica received learned notice for position 2 from @0.0.0.0:0
I1113 16:43:58.535482  8748 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 360618ns
I1113 16:43:58.535550  8748 leveldb.cpp:401] Deleting ~1 keys from leveldb took 23693ns
I1113 16:43:58.535575  8748 replica.cpp:715] Persisted action at 2
I1113 16:43:58.535611  8748 replica.cpp:700] Replica learned TRUNCATE action at position 2
I1113 16:43:58.550834  8746 slave.cpp:191] Slave started on 5)@10.0.2.15:45384
I1113 16:43:58.550834  8746 slave.cpp:192] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/MasterMaintenanceTest_InverseOffersFilters_2zc09g/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""auth.docker.io"" --docker_auth_server_port=""443"" --docker_kill_orphans=""true"" --docker_local_archives_dir=""/tmp/mesos/images/docker"" --docker_puller=""local"" --docker_puller_timeout=""60"" --docker_registry=""registry-1.docker.io"" --docker_registry_port=""443"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/MasterMaintenanceTest_InverseOffersFilters_2zc09g/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname=""maintenance-host"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/home/vagrant/build-mesos/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/MasterMaintenanceTest_InverseOffersFilters_2zc09g""
I1113 16:43:58.551501  8746 credentials.hpp:85] Loading credential for authentication from '/tmp/MasterMaintenanceTest_InverseOffersFilters_2zc09g/credential'
I1113 16:43:58.551703  8746 slave.cpp:322] Slave using credential for: test-principal
I1113 16:43:58.552422  8746 slave.cpp:392] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I1113 16:43:58.552510  8746 slave.cpp:400] Slave attributes: [  ]
I1113 16:43:58.552532  8746 slave.cpp:405] Slave hostname: maintenance-host
I1113 16:43:58.552547  8746 slave.cpp:410] Slave checkpoint: true
I1113 16:43:58.553520  8746 state.cpp:54] Recovering state from '/tmp/MasterMaintenanceTest_InverseOffersFilters_2zc09g/meta'
I1113 16:43:58.553938  8746 status_update_manager.cpp:202] Recovering status update manager
I1113 16:43:58.554251  8746 slave.cpp:4230] Finished recovery
I1113 16:43:58.555016  8746 slave.cpp:729] New master detected at master@10.0.2.15:45384
I1113 16:43:58.555166  8746 slave.cpp:792] Authenticating with master master@10.0.2.15:45384
I1113 16:43:58.555207  8746 slave.cpp:797] Using default CRAM-MD5 authenticatee
I1113 16:43:58.555589  8746 slave.cpp:765] Detecting new master
I1113 16:43:58.555076  8749 status_update_manager.cpp:176] Pausing sending status updates
I1113 16:43:58.555719  8742 authenticatee.cpp:123] Creating new client SASL connection
I1113 16:43:58.560645  8744 master.cpp:5150] Authenticating slave(5)@10.0.2.15:45384
I1113 16:43:58.561305  8744 authenticator.cpp:100] Creating new server SASL connection
I1113 16:43:58.566682  8744 authenticatee.cpp:214] Received SASL authentication mechanisms: CRAM-MD5
I1113 16:43:58.566779  8744 authenticatee.cpp:240] Attempting to authenticate with mechanism 'CRAM-MD5'
I1113 16:43:58.566872  8744 authenticator.cpp:205] Received SASL authentication start
I1113 16:43:58.566936  8744 authenticator.cpp:327] Authentication requires more steps
I1113 16:43:58.567602  8744 authenticatee.cpp:260] Received SASL authentication step
I1113 16:43:58.567775  8744 authenticator.cpp:233] Received SASL authentication step
I1113 16:43:58.568128  8744 authenticator.cpp:319] Authentication success
I1113 16:43:58.568282  8742 authenticatee.cpp:300] Authentication success
I1113 16:43:58.568320  8749 master.cpp:5180] Successfully authenticated principal 'test-principal' at slave(5)@10.0.2.15:45384
I1113 16:43:58.568701  8742 slave.cpp:860] Successfully authenticated with master master@10.0.2.15:45384
I1113 16:43:58.569272  8747 master.cpp:3859] Registering slave at slave(5)@10.0.2.15:45384 (maintenance-host) with id d59449fc-5462-43c5-b935-e05563fdd4b6-S0
I1113 16:43:58.570096  8747 registrar.cpp:441] Applied 1 operations in 59195ns; attempting to update the 'registry'
I1113 16:43:58.570772  8748 log.cpp:685] Attempting to append 362 bytes to the log
I1113 16:43:58.570772  8749 coordinator.cpp:350] Coordinator attempting to write APPEND action at position 3
I1113 16:43:58.572155  8745 replica.cpp:540] Replica received write request for position 3 from (69)@10.0.2.15:45384
I1113 16:43:58.572801  8745 leveldb.cpp:343] Persisting action (381 bytes) to leveldb took 563073ns
I1113 16:43:58.572854  8745 replica.cpp:715] Persisted action at 3
I1113 16:43:58.573707  8745 replica.cpp:694] Replica received learned notice for position 3 from @0.0.0.0:0
I1113 16:43:58.574255  8745 leveldb.cpp:343] Persisting action (383 bytes) to leveldb took 485234ns
I1113 16:43:58.574311  8745 replica.cpp:715] Persisted action at 3
I1113 16:43:58.574342  8745 replica.cpp:700] Replica learned APPEND action at position 3
I1113 16:43:58.575857  8747 master.cpp:3847] Ignoring register slave message from slave(5)@10.0.2.15:45384 (maintenance-host) as admission is already in progress
I1113 16:43:58.576217  8744 log.cpp:704] Attempting to truncate the log to 3
I1113 16:43:58.575887  8748 registrar.cpp:486] Successfully updated the 'registry' in 5.682176ms
I1113 16:43:58.576400  8744 coordinator.cpp:350] Coordinator attempting to write TRUNCATE action at position 4
I1113 16:43:58.577169  8746 master.cpp:3927] Registered slave d59449fc-5462-43c5-b935-e05563fdd4b6-S0 at slave(5)@10.0.2.15:45384 (maintenance-host) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I1113 16:43:58.577287  8745 hierarchical.cpp:344] Added slave d59449fc-5462-43c5-b935-e05563fdd4b6-S0 (maintenance-host) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
I1113 16:43:58.577472  8744 slave.cpp:904] Registered with master master@10.0.2.15:45384; given slave ID d59449fc-5462-43c5-b935-e05563fdd4b6-S0
I1113 16:43:58.577999  8745 status_update_manager.cpp:183] Resuming sending status updates
I1113 16:43:58.578279  8748 replica.cpp:540] Replica received write request for position 4 from (70)@10.0.2.15:45384
I1113 16:43:58.578346  8744 slave.cpp:963] Forwarding total oversubscribed resources
I1113 16:43:58.578734  8744 master.cpp:4269] Received update of slave d59449fc-5462-43c5-b935-e05563fdd4b6-S0 at slave(5)@10.0.2.15:45384 (maintenance-host) with total oversubscribed resources
I1113 16:43:58.578846  8748 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 304993ns
I1113 16:43:58.578889  8748 replica.cpp:715] Persisted action at 4
I1113 16:43:58.578897  8744 hierarchical.cpp:400] Slave d59449fc-5462-43c5-b935-e05563fdd4b6-S0 (maintenance-host) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: )
I1113 16:43:58.579463  8744 replica.cpp:694] Replica received learned notice for position 4 from @0.0.0.0:0
I1113 16:43:58.579888  8744 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 384596ns
I1113 16:43:58.579952  8744 leveldb.cpp:401] Deleting ~2 keys from leveldb took 27011ns
I1113 16:43:58.579977  8744 replica.cpp:715] Persisted action at 4
I1113 16:43:58.580001  8744 replica.cpp:700] Replica learned TRUNCATE action at position 4
I1113 16:43:58.584300  8743 slave.cpp:191] Slave started on 6)@10.0.2.15:45384
I1113 16:43:58.584398  8743 slave.cpp:192] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/MasterMaintenanceTest_InverseOffersFilters_CDFgvt/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_auth_server=""auth.docker.io"" --docker_auth_server_port=""443"" --docker_kill_orphans=""true"" --docker_local_archives_dir=""/tmp/mesos/images/docker"" --docker_puller=""local"" --docker_puller_timeout=""60"" --docker_registry=""registry-1.docker.io"" --docker_registry_port=""443"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --docker_store_dir=""/tmp/mesos/store/docker"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/MasterMaintenanceTest_InverseOffersFilters_CDFgvt/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --hostname=""maintenance-host-2"" --hostname_lookup=""true"" --image_provisioner_backend=""copy"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/home/vagrant/build-mesos/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --systemd_runtime_directory=""/run/systemd/system"" --version=""false"" --work_dir=""/tmp/MasterMaintenanceTest_InverseOffersFilters_CDFgvt""
I1113 16:43:58.584731  8743 credentials.hpp:85] Loading credential for authentication from '/tmp/MasterMaintenanceTest_InverseOffersFilters_CDFgvt/credential'
I1113 16:43:58.584915  8743 slave.cpp:322] Slave using credential for: test-principal
I1113 16:43:58.585309  8743 slave.cpp:392] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I1113 16:43:58.585482  8743 slave.cpp:400] Slave attributes: [  ]
I1113 16:43:58.585566  8743 slave.cpp:405] Slave hostname: maintenance-host-2
I1113 16:43:58.585619  8743 slave.cpp:410] Slave checkpoint: true
I1113 16:43:58.586431  8743 state.cpp:54] Recovering state from '/tmp/MasterMaintenanceTest_InverseOffersFilters_CDFgvt/meta'
I1113 16:43:58.586890  8745 status_update_manager.cpp:202] Recovering status update manager
I1113 16:43:58.587136  8745 slave.cpp:4230] Finished recovery
I1113 16:43:58.587817  8745 slave.cpp:729] New master detected at master@10.0.2.15:45384
I1113 16:43:58.587836  8747 status_update_manager.cpp:176] Pausing sending status updates
I1113 16:43:58.587908  8745 slave.cpp:792] Authenticating with master master@10.0.2.15:45384
I1113 16:43:58.587934  8745 slave.cpp:797] Using default CRAM-MD5 authenticatee
I1113 16:43:58.588043  8745 slave.cpp:765] Detecting new master
I1113 16:43:58.588170  8745 authenticatee.cpp:123] Creating new client SASL connection
I1113 16:43:58.592891  8745 master.cpp:5150] Authenticating slave(6)@10.0.2.15:45384
I1113 16:43:58.594146  8745 authenticator.cpp:100] Creating new server SASL connection
I1113 16:43:58.599606  8749 authenticatee.cpp:214] Received SASL authentication mechanisms: CRAM-MD5
I1113 16:43:58.599684  8749 authenticatee.cpp:240] Attempting to authenticate with mechanism 'CRAM-MD5'
I1113 16:43:58.599774  8749 authenticator.cpp:205] Received SASL authentication start
I1113 16:43:58.599830  8749 authenticator.cpp:327] Authentication requires more steps
I1113 16:43:58.599895  8749 authenticatee.cpp:260] Received SASL authentication step
I1113 16:43:58.599966  8749 authenticator.cpp:233] Received SASL authentication step
I1113 16:43:58.600042  8749 authenticator.cpp:319] Authentication success
I1113 16:43:58.600183  8749 authenticatee.cpp:300] Authentication success
I1113 16:43:58.600304  8749 master.cpp:5180] Successfully authenticated principal 'test-principal' at slave(6)@10.0.2.15:45384
I1113 16:43:58.600749  8745 slave.cpp:860] Successfully authenticated with master master@10.0.2.15:45384
I1113 16:43:58.601652  8745 master.cpp:3859] Registering slave at slave(6)@10.0.2.15:45384 (maintenance-host-2) with id d59449fc-5462-43c5-b935-e05563fdd4b6-S1
I1113 16:43:58.602469  8745 registrar.cpp:441] Applied 1 operations in 62055ns; attempting to update the 'registry'
I1113 16:43:58.603483  8742 log.cpp:685] Attempting to append 534 bytes to the log
I1113 16:43:58.603664  8747 coordinator.cpp:350] Coordinator attempting to write APPEND action at position 5
I1113 16:43:58.604473  8748 replica.cpp:540] Replica received write request for position 5 from (75)@10.0.2.15:45384
I1113 16:43:58.605144  8748 leveldb.cpp:343] Persisting action (553 bytes) to leveldb took 512473ns
I1113 16:43:58.605190  8748 replica.cpp:715] Persisted action at 5
I1113 16:43:58.606076  8742 replica.cpp:694] Replica received learned notice for position 5 from @0.0.0.0:0
I1113 16:43:58.606385  8742 leveldb.cpp:343] Persisting action (555 bytes) to leveldb took 264699ns
I1113 16:43:58.606427  8742 replica.cpp:715] Persisted action at 5
I1113 16:43:58.606456  8742 replica.cpp:700] Replica learned APPEND action at position 5
I1113 16:43:58.607722  8749 registrar.cpp:486] Successfully updated the 'registry' in 4.815104ms
I1113 16:43:58.607897  8748 log.cpp:704] Attempting to truncate the log to 5
I1113 16:43:58.608088  8748 coordinator.cpp:350] Coordinator attempting to write TRUNCATE action at position 6
I1113 16:43:58.608280  8749 master.cpp:3927] Registered slave d59449fc-5462-43c5-b935-e05563fdd4b6-S1 at slave(6)@10.0.2.15:45384 (maintenance-host-2) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I1113 16:43:58.608378  8742 hierarchical.cpp:344] Added slave d59449fc-5462-43c5-b935-e05563fdd4b6-S1 (maintenance-host-2) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
I1113 16:43:58.608783  8747 slave.cpp:904] Registered with master master@10.0.2.15:45384; given slave ID d59449fc-5462-43c5-b935-e05563fdd4b6-S1
I1113 16:43:58.609323  8746 status_update_manager.cpp:183] Resuming sending status updates
I1113 16:43:58.609797  8747 slave.cpp:963] Forwarding total oversubscribed resources
I1113 16:43:58.610152  8749 master.cpp:4269] Received update of slave d59449fc-5462-43c5-b935-e05563fdd4b6-S1 at slave(6)@10.0.2.15:45384 (maintenance-host-2) with total oversubscribed resources
I1113 16:43:58.610436  8749 hierarchical.cpp:400] Slave d59449fc-5462-43c5-b935-e05563fdd4b6-S1 (maintenance-host-2) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: )
I1113 16:43:58.610265  8744 replica.cpp:540] Replica received write request for position 6 from (76)@10.0.2.15:45384
I1113 16:43:58.611052  8744 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 346656ns
I1113 16:43:58.611089  8744 replica.cpp:715] Persisted action at 6
I1113 16:43:58.611934  8744 http.cpp:338] HTTP POST for /master/maintenance/schedule from 10.0.2.15:60984
I1113 16:43:58.612453  8744 replica.cpp:694] Replica received learned notice for position 6 from @0.0.0.0:0
I1113 16:43:58.612859  8744 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 353053ns
I1113 16:43:58.612920  8744 leveldb.cpp:401] Deleting ~2 keys from leveldb took 25904ns
I1113 16:43:58.612957  8744 replica.cpp:715] Persisted action at 6
I1113 16:43:58.613082  8744 replica.cpp:700] Replica learned TRUNCATE action at position 6
I1113 16:43:58.612925  8742 registrar.cpp:441] Applied 1 operations in 184286ns; attempting to update the 'registry'
I1113 16:43:58.614495  8742 log.cpp:685] Attempting to append 749 bytes to the log
I1113 16:43:58.614680  8744 coordinator.cpp:350] Coordinator attempting to write APPEND action at position 7
I1113 16:43:58.615454  8745 replica.cpp:540] Replica received write request for position 7 from (77)@10.0.2.15:45384
I1113 16:43:58.615761  8745 leveldb.cpp:343] Persisting action (768 bytes) to leveldb took 261919ns
I1113 16:43:58.615790  8745 replica.cpp:715] Persisted action at 7
I1113 16:43:58.616392  8745 replica.cpp:694] Replica received learned notice for position 7 from @0.0.0.0:0
I1113 16:43:58.617075  8745 leveldb.cpp:343] Persisting action (770 bytes) to leveldb took 639439ns
I1113 16:43:58.617111  8745 replica.cpp:715] Persisted action at 7
I1113 16:43:58.617137  8745 replica.cpp:700] Replica learned APPEND action at position 7
I1113 16:43:58.618403  8745 registrar.cpp:486] Successfully updated the 'registry' in 4.761344ms
I1113 16:43:58.618563  8742 log.cpp:704] Attempting to truncate the log to 7
I1113 16:43:58.618996  8745 coordinator.cpp:350] Coordinator attempting to write TRUNCATE action at position 8
I1113 16:43:58.618693  8746 master.cpp:4325] Updating unavailability of slave d59449fc-5462-43c5-b935-e05563fdd4b6-S1 at slave(6)@10.0.2.15:45384 (maintenance-host-2), starting at 2393.24256053228weeks
I1113 16:43:58.619581  8746 master.cpp:4325] Updating unavailability of slave d59449fc-5462-43c5-b935-e05563fdd4b6-S0 at slave(5)@10.0.2.15:45384 (maintenance-host), starting at 2393.24256053228weeks
I1113 16:43:58.619680  8746 master.cpp:4325] Updating unavailability of slave d59449fc-5462-43c5-b935-e05563fdd4b6-S0 at slave(5)@10.0.2.15:45384 (maintenance-host), starting at 2393.24256053228weeks
I1113 16:43:58.619757  8746 master.cpp:4325] Updating unavailability of slave d59449fc-5462-43c5-b935-e05563fdd4b6-S1 at slave(6)@10.0.2.15:45384 (maintenance-host-2), starting at 2393.24256053228weeks
I1113 16:43:58.619964  8746 replica.cpp:540] Replica received write request for position 8 from (78)@10.0.2.15:45384
I1113 16:43:58.620584  8746 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 574141ns
I1113 16:43:58.620633  8746 replica.cpp:715] Persisted action at 8
I1113 16:43:58.621672  8746 replica.cpp:694] Replica received learned notice for position 8 from @0.0.0.0:0
I1113 16:43:58.622351  8728 scheduler.cpp:156] Version: 0.26.0
I1113 16:43:58.622827  8746 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 1.097466ms
I1113 16:43:58.622943  8746 leveldb.cpp:401] Deleting ~2 keys from leveldb took 47195ns
I1113 16:43:58.623087  8746 replica.cpp:715] Persisted action at 8
I1113 16:43:58.623191  8746 replica.cpp:700] Replica learned TRUNCATE action at position 8
I1113 16:43:58.639466  8747 http.cpp:338] HTTP POST for /master/api/v1/scheduler from 10.0.2.15:60986
I1113 16:43:58.639690  8747 master.cpp:1868] Received subscription request for HTTP framework 'default'
I1113 16:43:58.639804  8747 master.cpp:1645] Authorizing framework principal 'test-principal' to receive offers for role '*'
I1113 16:43:58.641152  8745 master.cpp:1960] Subscribing framework 'default' with checkpointing disabled and capabilities [  ]
I1113 16:43:58.642168  8745 hierarchical.cpp:195] Added framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000
I1113 16:43:58.644707  8749 master.cpp:4979] Sending 2 offers to framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000 (default)
I1113 16:43:58.645970  8749 master.cpp:5069] Sending 2 inverse offers to framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000 (default)
I1113 16:44:00.693930  8742 http.cpp:338] HTTP POST for /master/api/v1/scheduler from 10.0.2.15:60988
I1113 16:44:00.694870  8742 master.cpp:2915] Processing ACCEPT call for offers: [ d59449fc-5462-43c5-b935-e05563fdd4b6-O0 ] on slave d59449fc-5462-43c5-b935-e05563fdd4b6-S1 at slave(6)@10.0.2.15:45384 (maintenance-host-2) for framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000 (default)
I1113 16:44:00.695387  8742 master.cpp:2711] Authorizing framework principal 'test-principal' to launch task a2f7263f-17aa-4849-a0dc-7c8453b3b81f as user 'vagrant'
W1113 16:44:00.698230  8745 validation.cpp:422] Executor executor-1 for task a2f7263f-17aa-4849-a0dc-7c8453b3b81f uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases.
W1113 16:44:00.698603  8745 validation.cpp:434] Executor executor-1 for task a2f7263f-17aa-4849-a0dc-7c8453b3b81f uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases.
I1113 16:44:00.699178  8745 master.hpp:176] Adding task a2f7263f-17aa-4849-a0dc-7c8453b3b81f with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave d59449fc-5462-43c5-b935-e05563fdd4b6-S1 (maintenance-host-2)
I1113 16:44:00.699599  8745 master.cpp:3245] Launching task a2f7263f-17aa-4849-a0dc-7c8453b3b81f of framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000 (default) with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave d59449fc-5462-43c5-b935-e05563fdd4b6-S1 at slave(6)@10.0.2.15:45384 (maintenance-host-2)
I1113 16:44:00.703222  8745 http.cpp:338] HTTP POST for /master/api/v1/scheduler from 10.0.2.15:60990
I1113 16:44:00.704217  8745 master.cpp:2915] Processing ACCEPT call for offers: [ d59449fc-5462-43c5-b935-e05563fdd4b6-O1 ] on slave d59449fc-5462-43c5-b935-e05563fdd4b6-S0 at slave(5)@10.0.2.15:45384 (maintenance-host) for framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000 (default)
I1113 16:44:00.704463  8745 master.cpp:2711] Authorizing framework principal 'test-principal' to launch task a6ef5d55-4c84-4bc6-bd4a-a1f07de3fcc8 as user 'vagrant'
I1113 16:44:00.703451  8744 slave.cpp:1294] Got assigned task a2f7263f-17aa-4849-a0dc-7c8453b3b81f for framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000
I1113 16:44:00.705693  8744 slave.cpp:1410] Launching task a2f7263f-17aa-4849-a0dc-7c8453b3b81f for framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000
I1113 16:44:00.706001  8744 paths.cpp:436] Trying to chown '/tmp/MasterMaintenanceTest_InverseOffersFilters_CDFgvt/slaves/d59449fc-5462-43c5-b935-e05563fdd4b6-S1/frameworks/d59449fc-5462-43c5-b935-e05563fdd4b6-0000/executors/executor-1/runs/49d1654a-180d-406c-82f4-ab1ebd168451' to user 'vagrant'
W1113 16:44:00.707725  8745 validation.cpp:422] Executor executor-2 for task a6ef5d55-4c84-4bc6-bd4a-a1f07de3fcc8 uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases.
W1113 16:44:00.707792  8745 validation.cpp:434] Executor executor-2 for task a6ef5d55-4c84-4bc6-bd4a-a1f07de3fcc8 uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases.
I1113 16:44:00.708001  8745 master.hpp:176] Adding task a6ef5d55-4c84-4bc6-bd4a-a1f07de3fcc8 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave d59449fc-5462-43c5-b935-e05563fdd4b6-S0 (maintenance-host)
I1113 16:44:00.708151  8745 master.cpp:3245] Launching task a6ef5d55-4c84-4bc6-bd4a-a1f07de3fcc8 of framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000 (default) with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave d59449fc-5462-43c5-b935-e05563fdd4b6-S0 at slave(5)@10.0.2.15:45384 (maintenance-host)
I1113 16:44:00.708411  8745 slave.cpp:1294] Got assigned task a6ef5d55-4c84-4bc6-bd4a-a1f07de3fcc8 for framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000
I1113 16:44:00.708694  8745 slave.cpp:1410] Launching task a6ef5d55-4c84-4bc6-bd4a-a1f07de3fcc8 for framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000
I1113 16:44:00.708943  8745 paths.cpp:436] Trying to chown '/tmp/MasterMaintenanceTest_InverseOffersFilters_2zc09g/slaves/d59449fc-5462-43c5-b935-e05563fdd4b6-S0/frameworks/d59449fc-5462-43c5-b935-e05563fdd4b6-0000/executors/executor-2/runs/e3cf520d-b4ef-47bb-ab6a-bd1d7fd0d0d9' to user 'vagrant'
I1113 16:44:00.709643  8744 slave.cpp:4999] Launching executor executor-1 of framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000 with resources  in work directory '/tmp/MasterMaintenanceTest_InverseOffersFilters_CDFgvt/slaves/d59449fc-5462-43c5-b935-e05563fdd4b6-S1/frameworks/d59449fc-5462-43c5-b935-e05563fdd4b6-0000/executors/executor-1/runs/49d1654a-180d-406c-82f4-ab1ebd168451'
I1113 16:44:00.710903  8744 exec.cpp:136] Version: 0.26.0
I1113 16:44:00.711484  8744 slave.cpp:1628] Queuing task 'a2f7263f-17aa-4849-a0dc-7c8453b3b81f' for executor 'executor-1' of framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000
I1113 16:44:00.711753  8745 slave.cpp:4999] Launching executor executor-2 of framework d59449fc-5462-43c5-b935-e05563fdd4b6-0000 with resources  in work directory '/tmp/MasterMaintenanceTest_InverseOffersFilters_2zc09g/slaves/d",3.0,0,0.0,1.0,0.0,0.0,0.0,0.0,0.8,0.5438596491228069,0.39999999999999997,0.48717948717948717,0.4303030303030303,0.4303030303030303,0.0
Task,Rescind offers in order to satisfy quota,"When a quota request comes in, we may need to rescind a certain amount of outstanding offers in order to satisfy it. Because resources are allocated in the allocator, there can be a race between rescinding and allocating. This race makes it hard to determine the exact amount of offers that should be rescinded in the master.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.717948717948718,0.8242424242424242,0.7212121212121212,0.0
Task,Add a `--force` flag to disable sanity check in quota,"There are use cases when an operator may want to disable the sanity check for quota endpoints (MESOS-3074), even if this renders the cluster under quota. For example, an operator sets quota before adding more agents in order to make sure that no non-quota allocations from new agents are made. ",1.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.12280701754385964,0.32857142857142857,0.717948717948718,0.8242424242424242,0.7212121212121212,0.0
Bug,isolator module headers depend on picojson headers,"When trying to build an isolator module, stout headers end up depending on {{picojson.hpp}} which is not installed.

{code}
In file included from /opt/mesos/include/mesos/module/isolator.hpp:25:
In file included from /opt/mesos/include/mesos/slave/isolator.hpp:30:
In file included from /opt/mesos/include/process/dispatch.hpp:22:
In file included from /opt/mesos/include/process/process.hpp:26:
In file included from /opt/mesos/include/process/event.hpp:21:
In file included from /opt/mesos/include/process/http.hpp:39:
/opt/mesos/include/stout/json.hpp:23:10: fatal error: 'picojson.h' file not found
#include <picojson.h>
         ^
8 warnings and 1 error generated.
{code}",3.0,0,0.0,0.5444947209653093,0.0,0.0,0.0,0.5,1.0,0.05263157894736842,0.03571428571428571,0.11538461538461538,0.05454545454545455,0.05454545454545455,0.0
Documentation,Five new docker-related slave flags are not covered by the configuration documentation.,"These flags were added to ""slave/flags.cpp"", but are not mentioned in ""docs/configuration.md"":

  add(&Flags::docker_auth_server,
      ""docker_auth_server"",
      ""Docker authentication server"",
      ""auth.docker.io"");

  add(&Flags::docker_auth_server_port,
      ""docker_auth_server_port"",
      ""Docker authentication server port"",
      ""443"");
  add(&Flags::docker_puller_timeout_secs,
      ""docker_puller_timeout"",
      ""Timeout value in seconds for pulling images from Docker registry"",
      ""60"");

  add(&Flags::docker_registry,
      ""docker_registry"",
      ""Default Docker image registry server host"",
      ""registry-1.docker.io"");
  add(&Flags::docker_registry_port,
      ""docker_registry_port"",
      ""Default Docker registry server port"",
      ""443"");
",1.0,0.26.0,0.0,0.06184012066365008,0.0,0.0,0.0,0.0,0.2,0.10526315789473684,0.12857142857142856,0.14102564102564102,0.16969696969696968,0.17575757575757575,0.2385321100917431
Improvement,Add authorization for '/create-volume' and '/destroy-volume' HTTP endpoints,"This is the fourth in a series of tickets that adds authorization support for persistent volumes.

We need to add ACL authorization for the '/create-volume' and '/destroy-volume' HTTP endpoints. In other complementary work, authorization for frameworks performing {{CREATE}} and {{DESTROY}} operations is being added by MESOS-3065.

This will consist of adding authorization calls into the HTTP endpoint code in {{src/master/http.cpp}}, as well as tests for both failed & successful calls to '/create-volumes' and '/destroy-volumes' with authorization. We also must ensure that the {{principal}} field of {{Resource.DiskInfo.Persistence}} is being populated correctly.",2.0,0,0.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Task,Enable mesos-reviewbot project on jenkins to use docker,"As a first step to adding capability for building multiple configurations on reviewbot, we need to change the build scripts to use docker. ",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3357142857142857,0.14102564102564102,0.2424242424242424,0.2424242424242424,0.0
Bug,Wrong syntax and inconsistent formatting of JSON examples in flag documentation,"The JSON examples in the documentation of the commandline flags ({{mesos-master.sh --help}} and {{mesos-slave.sh --help}}) don't have a consistent formatting. Furthermore, some examples aren't even compliant JSON because they have trailing commas were they shouldn't.",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.2631578947368421,0.36428571428571427,0.28205128205128205,0.296969696969697,0.296969696969697,0.0
Improvement,Corrected style in hierarchical allocator,"The built-in allocator code has some style issues (namespaces in the .cpp file, unfortunate formatting) which should be corrected for readability.",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.717948717948718,0.8242424242424242,0.7212121212121212,0.0
Task,Libprocess: Implement process::Clock::finalize,"Tracks this [TODO|https://github.com/apache/mesos/blob/aa0cd7ed4edf1184cbc592b5caa2429a8373e813/3rdparty/libprocess/src/process.cpp#L974-L975].

The {{Clock}} is initialized with a callback that, among other things, will dereference the global {{process_manager}} object.

When libprocess is shutting down, the {{process_manager}} is cleaned up.  Between cleanup and termination of libprocess, there is some chance that a {{Timer}} will time out and result in dereferencing {{process_manager}}.

*Proposal* 
* Implement {{Clock::finalize}}.  This would clear:
** existing timers
** process-specific clocks
** ticks
* Change {{process::finalize}}.
*# Resume the clock.  (The clock is only paused during some tests.)  When the clock is not paused, the callback does not dereference {{process_manager}}.
*# Clean up {{process_manager}}.  This terminates all the processes that would potentially interact with {{Clock}}.
*# Call {{Clock::finalize}}.",3.0,0,0.0,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.0
Bug,Incorrect and inconsistent include order for <gmock/gmock.h> and <gtest/gtest.h>.,"We currently have an inconsistent (and mostly incorrect) include order for <gmock/gmock.h> and <gtest/gtest.h> (see below). Some files include them (incorrectly)  between the c and cpp standard header, while other correclt include them afterwards. According to the [Google Styleguide| https://google.github.io/styleguide/cppguide.html#Names_and_Order_of_Includes] the second include order is correct.


{code:title=external_containerizer_test.cpp}
#include <unistd.h>

#include <gmock/gmock.h>

#include <string>
{code}

{code:title=launcher.hpp}
#include <vector>

#include <gmock/gmock.h>
{code}",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12280701754385964,0.32857142857142857,0.11538461538461538,0.23636363636363636,0.23636363636363636,0.0
Task,Account dynamic reservations towards quota.,"Dynamic reservations—whether allocated or not—should be accounted towards role's quota. This requires update in at least two places:
* The built-in allocator, which actually satisfies quota;
* The sanity check in the master.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.717948717948718,0.8242424242424242,0.7212121212121212,0.0
Task,Investigate recovery for the Hierarchical allocator,The built-in Hierarchical allocator should implement the recovery (in the presence of quota).,3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.717948717948718,0.8242424242424242,0.7212121212121212,0.0
Task,Enhance allocator interface with the recovery() method,There are some scenarios (e.g. quota is set for some roles) when it makes sense to notify an allocator about the recovery. Introduce a method into the allocator interface that allows for this.,3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.717948717948718,0.8242424242424242,0.7212121212121212,0.0
Improvement,Make `Resource.DiskInfo.Persistence.principal` a required field,"A `principal` field is being added to the `Resource.DiskInfo.Persistence` message to facilitate authorization of persistent volume creation/deletion. In the long-run it should be a required field, but it's being initially introduced as optional to avoid breaking existing frameworks. The field should be changed to required at the end of a deprecation cycle.",1.0,0,0.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Task,Simplify and/or document the libprocess initialization synchronization logic,"Tracks this [TODO|https://github.com/apache/mesos/blob/3bda55da1d0b580a1b7de43babfdc0d30fbc87ea/3rdparty/libprocess/src/process.cpp#L749].

The [synchronization logic of libprocess|https://github.com/apache/mesos/commit/cd757cf75637c92c438bf4cd22f21ba1b5be702f#diff-128d3b56fc8c9ec0176fdbadcfd11fc2] [predates abstractions|https://github.com/apache/mesos/commit/6c3b107e4e02d5ba0673eb3145d71ec9d256a639#diff-0eebc8689450916990abe080d86c2acb] like {{process::Once}}, which is used in almost all other one-time initialization blocks.  

The logic should be documented.  It can also be simplified (see the [review description|https://reviews.apache.org/r/39949/]).  Or it can be replaced with {{process::Once}}.",1.0,0,0.0,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.0
Task,Investigate the requirements of programmatically re-initializing libprocess,"This issue is for investigating what needs to be added/changed in {{process::finalize}} such that {{process::initialize}} will start on a clean slate.  Additional issues will be created once done.  Also see [the parent issue|MESOS-3820].

{{process::finalize}} should cover the following components:
* {{__s__}} (the server socket)
** {{delete}} should be sufficient.  This closes the socket and thereby prevents any further interaction from it.
* {{process_manager}}
** Related prior work: [MESOS-3158]
** Cleans up the garbage collector, help, logging, profiler, statistics, route processes (including [this one|https://github.com/apache/mesos/blob/3bda55da1d0b580a1b7de43babfdc0d30fbc87ea/3rdparty/libprocess/src/process.cpp#L963], which currently leaks a pointer).
** Cleans up any other {{spawn}} 'd process.
** Manages the {{EventLoop}}.
* {{Clock}}
** The goal here is to clear any timers so that nothing can deference {{process_manager}} while we're finalizing/finalized.  It's probably not important to execute any remaining timers, since we're ""shutting down"" libprocess.  This means:
*** The clock should be {{paused}} and {{settled}} before the clean up of {{process_manager}}.
*** Processes, which might interact with the {{Clock}}, should be cleaned up next.
*** A new {{Clock::finalize}} method would then clear timers, process-specific clocks, and {{tick}} s; and then {{resume}} the clock.
* {{__address__}} (the advertised IP and port)
** Needs to be cleared after {{process_manager}} has been cleaned up.  Processes use this to communicate events.  If cleared prematurely, {{TerminateEvents}} will not be sent correctly, leading to infinite waits.
* {{socket_manager}}
** The idea here is to close all sockets and deallocate any existing {{HttpProxy}} or {{Encoder}} objects.
** All sockets are created via {{__s__}}, so cleaning up the server socket prior will prevent any new activity.
* {{mime}}
** This is effectively a static map.
** It should be possible to statically initialize it.
* Synchronization atomics {{initialized}} & {{initializing}}.
** Once cleanup is done, these should be reset.

*Summary*:
* Implement {{Clock::finalize}}.  [MESOS-3882]
* Implement {{~SocketManager}}.  [MESOS-3910]
* Make sure the {{MetricsProcess}} and {{ReaperProcess}} are reinitialized.  [MESOS-3934]
* (Optional) Clean up {{mime}}.
* Wrap everything up in {{process::finalize}}.",2.0,0,0.0,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.0
Task,Authenticate quota requests,"Quota requests need to be authenticated.
This ticket will authenticate quota requests using credentials provided by the {{Authorization}} field of the HTTP request. This is similar to how authentication is implemented in {{Master::Http}}.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.2631578947368421,0.36428571428571427,0.28205128205128205,0.296969696969697,0.296969696969697,0.0
Task,Draft Design Doc for first Step External Volume MVP,"As part of the overall design doc for global resources we would like to introduce improvements for Docker Volume Driver isolator module (https://github.com/emccode/mesos-module-dvdi).
Currently the isolator module is controlled by setting environment variables as follows: {code} ""env"": {
  ""DVDI_VOLUME_NAME"": ""testing"",
  ""DVDI_VOLUME_DRIVER"": ""platform1"",
  ""DVDI_VOLUME_OPTS"": ""size=5,iops=150,volumetype=io1,newfstype=ext4,overwritefs=false"",
  ""DVDI_VOLUME_NAME1"": ""testing2"",
  ""DVDI_VOLUME_DRIVER1"": ""platform2"",
  ""DVDI_VOLUME_OPTS1"": ""size=6,volumetype=gp2,newfstype=xfs,overwritefs=true""
} {code} We should develop a more structured way for passing these settings to the isolator module which is in line with the overall goal of global resources.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12280701754385964,0.32857142857142857,0.11538461538461538,0.23636363636363636,0.23636363636363636,0.0
Bug,Investigate recent crashes in Command Executor,"Post https://reviews.apache.org/r/38900 i.e. updating CommandExecutor to support rootfs. There seem to be some tests showing frequent crashes due to assert violations.

{{FetcherCacheTest.SimpleEviction}} failed due to the following log:

{code}
I1107 19:36:46.360908 30657 slave.cpp:1793] Sending queued task '3' to executor ''3' of framework 7d94c7fb-8950-4bcf-80c1-46112292dcd6-0000 at executor(1)@172.17.5.200:33871'
I1107 19:36:46.363682  1236 exec.cpp:297] 

I1107 19:36:46.373569  1245 exec.cpp:210] Executor registered on slave 7d94c7fb-8950-4bcf-80c1-46112292dcd6-S0
    @     0x7f9f5a7db3fa  google::LogMessage::Fail()
I1107 19:36:46.394081  1245 exec.cpp:222] Executor::registered took 395411ns
    @     0x7f9f5a7db359  google::LogMessage::SendToLog()
    @     0x7f9f5a7dad6a  google::LogMessage::Flush()
    @     0x7f9f5a7dda9e  google::LogMessageFatal::~LogMessageFatal()
    @           0x48d00a  _CheckFatal::~_CheckFatal()
    @           0x49c99d  mesos::internal::CommandExecutorProcess::launchTask()
    @           0x4b3dd7  _ZZN7process8dispatchIN5mesos8internal22CommandExecutorProcessEPNS1_14ExecutorDriverERKNS1_8TaskInfoES5_S6_EEvRKNS_3PIDIT_EEMSA_FvT0_T1_ET2_T3_ENKUlPNS_11ProcessBaseEE_clESL_
    @           0x4c470c  _ZNSt17_Function_handlerIFvPN7process11ProcessBaseEEZNS0_8dispatchIN5mesos8internal22CommandExecutorProcessEPNS5_14ExecutorDriverERKNS5_8TaskInfoES9_SA_EEvRKNS0_3PIDIT_EEMSE_FvT0_T1_ET2_T3_EUlS2_E_E9_M_invokeERKSt9_Any_dataS2_
    @     0x7f9f5a761b1b  std::function<>::operator()()
    @     0x7f9f5a749935  process::ProcessBase::visit()
    @     0x7f9f5a74d700  process::DispatchEvent::visit()
    @           0x48e004  process::ProcessBase::serve()
    @     0x7f9f5a745d21  process::ProcessManager::resume()
    @     0x7f9f5a742f52  _ZZN7process14ProcessManager12init_threadsEvENKUlRKSt11atomic_boolE_clES3_
    @     0x7f9f5a74cf2c  _ZNSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS3_EEE6__callIvIEILm0EEEET_OSt5tupleIIDpT0_EESt12_Index_tupleIIXspT1_EEE
    @     0x7f9f5a74cedc  _ZNSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS3_EEEclIIEvEET0_DpOT_
    @     0x7f9f5a74ce6e  _ZNSt12_Bind_simpleIFSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS4_EEEvEE9_M_invokeIIEEEvSt12_Index_tupleIIXspT_EEE
    @     0x7f9f5a74cdc5  _ZNSt12_Bind_simpleIFSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS4_EEEvEEclEv
    @     0x7f9f5a74cd5e  _ZNSt6thread5_ImplISt12_Bind_simpleIFSt5_BindIFZN7process14ProcessManager12init_threadsEvEUlRKSt11atomic_boolE_St17reference_wrapperIS6_EEEvEEE6_M_runEv
    @     0x7f9f5624f1e0  (unknown)
    @     0x7f9f564a8df5  start_thread
    @     0x7f9f559b71ad  __clone
I1107 19:36:46.551370 30656 containerizer.cpp:1257] Executor for container '6553a617-6b4a-418d-9759-5681f45ff854' has exited
I1107 19:36:46.551429 30656 containerizer.cpp:1074] Destroying container '6553a617-6b4a-418d-9759-5681f45ff854'
I1107 19:36:46.553869 30656 containerizer.cpp:1257] Executor for container 'd2c1f924-c92a-453e-82b1-c294d09c4873' has exited
{code}

The reason seems to be a race between the executor receiving a {{RunTaskMessage}} before {{ExecutorRegisteredMessage}} leading to the {{CHECK_SOME(executorInfo)}} failure.

Link to complete log: https://issues.apache.org/jira/browse/MESOS-2831?focusedCommentId=14995535&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-14995535

Another related failure from {{ExamplesTest.PersistentVolumeFramework}}

{code}
    @     0x7f4f71529cbd  google::LogMessage::SendToLog()
I1107 13:15:09.949987 31573 slave.cpp:2337] Status update manager successfully handled status update acknowledgement (UUID: 721c7316-5580-4636-a83a-098e3bd4ed1f) for task ad90531f-d3d8-43f6-96f2-c81c4548a12d of framework ac4ea54a-7d19-4e41-9ee3-1a761f8e5b0f-0000
    @     0x7f4f715296ce  google::LogMessage::Flush()
    @     0x7f4f7152c402  google::LogMessageFatal::~LogMessageFatal()
    @           0x48d00a  _CheckFatal::~_CheckFatal()
    @           0x49c99d  mesos::internal::CommandExecutorProcess::launchTask()
    @           0x4b3dd7  _ZZN7process8dispatchIN5mesos8internal22CommandExecutorProcessEPNS1_14ExecutorDriverERKNS1_8TaskInfoES5_S6_EEvRKNS_3PIDIT_EEMSA_FvT0_T1_ET2_T3_ENKUlPNS_11ProcessBaseEE_clESL_
    @           0x4c470c  _ZNSt17_Function_handlerIFvPN7process11ProcessBaseEEZNS0_8dispatchIN5mesos8internal22CommandExecutorProcessEPNS5_14ExecutorDriverERKNS5_8TaskInfoES9_SA_EEvRKNS0_3PIDIT_EEMSE_FvT0_T1_ET2_T3_EUlS2_E_E9_M_invokeERKSt9_Any_dataS2_
    @     0x7f4f714b047f  std::function<>::operator()()
    @     0x7f4f71498299  process::ProcessBase::visit()
    @     0x7f4f7149c064  process::DispatchEvent::visit()
    @           0x48e004  process::ProcessBase::serve()
    @     0x7f4f71494685  process::ProcessManager::resume()
{code}

Full logs at:
https://builds.apache.org/job/Mesos/1191/COMPILER=gcc,CONFIGURATION=--verbose,OS=centos:7,label_exp=docker%7C%7CHadoop/consoleFull",2.0,0,0.0,0.5701357466063348,0.0,0.0,0.0,0.0,0.0,0.6842105263157894,0.5642857142857143,0.3846153846153846,0.4181818181818182,0.4121212121212121,0.0
Bug,Corrected style in Makefiles,Order of files in Makefiles is not strictly alphabetic,1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.717948717948718,0.8242424242424242,0.7212121212121212,0.0
Bug,Root tests for LinuxFilesystemIsolatorTest are broken,"The refactor in [MESOS-3762] ended up exposing some differences in the {{TemporaryDirectoryTest}} classes (one in Stout, one in Mesos-proper).

The tests that broke (during tear down):
{code}
LinuxFilesystemIsolatorTest.ROOT_PersistentVolumeWithRootFilesystem
LinuxFilesystemIsolatorTest.ROOT_PersistentVolumeWithoutRootFilesystem
LinuxFilesystemIsolatorTest.ROOT_MultipleContainers
{code}

As per an offline discussion between [~jvanremoortere] and [~jieyu], the solution is to merge the two {{TemporaryDirectoryTest}} classes and to fix the tear down of {{LinuxFilesystemIsolatorTest}}.",2.0,0,0.0,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.0
Bug,Rootfs in provisioner test doesn't handle symlink directories properly,"Currently Rootfs doesn't fully copy the directory structure over, and also doesn't create the symlinks in the new rootfs and will cause shell and other binaries that rely on the symlinks to no longer function.",4.0,0,0.0,0.277526395173454,0.0,0.0,0.0,0.0,0.4,0.08771929824561403,0.10714285714285714,0.02564102564102564,0.09090909090909091,0.09090909090909091,0.0
Bug,/help endpoints do not work for nested paths,"Mesos displays the list of all supported endpoints starting at a given path prefix using the {{/help}} suffix, e.g. {{master:5050/help}}.

It seems that the {{help}} functionality is broken for URL's having nested paths e.g. {{master:5050/help/master/machine/down}}. The response returned is:
{quote}
Malformed URL, expecting '/help/id/name/'
{quote}",2.0,0,0.5,0.3137254901960784,0.0,0.0,0.0,0.0,0.4,0.19298245614035087,0.14285714285714285,0.3846153846153846,0.4181818181818182,0.4121212121212121,0.0
Documentation,Document operator HTTP endpoints,"These are not exhaustively documented; they probably should be.

Some endpoints have docs: e.g., {{/reserve}} and {{/unreserve}} are described in the reservation doc page. But it would be good to have a single page that lists all the endpoints and their semantics.",3.0,0,0.5,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.48717948717948717,0.4303030303030303,0.4303030303030303,0.0
Epic,Test-only libprocess reinitialization,"*Background*
Libprocess initialization includes the spawning of a variety of global processes and the creation of the server socket which listens for incoming requests.  Some properties of the server socket are configured via environment variables, such as the IP and port or the SSL configuration.

In the case of tests, libprocess is initialized once per test binary.  This means that testing different configurations (SSL in particular) is cumbersome as a separate process would be needed for every test case.

*Proposal*
# Add some optional code between some tests like:
{code}
// Cleanup all of libprocess's state, as if we're starting anew.
process::finalize(); 

// For tests that need to test SSL connections with the Master:
openssl::reinitialize();

process::initialize();
{code}
See [MESOS-3863] for more on {{process::finalize}}.",3.0,0,0.0,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.0
Improvement,"Add documentation explaining ""roles""","Docs currently talk about resources, static/dynamic reservations, but don't explain what a ""role"" concept is to begin with.",2.0,0,0.0,1.0,0.0,0.0,0.0,0.0,0.8,0.5438596491228069,0.39999999999999997,0.48717948717948717,0.4303030303030303,0.4303030303030303,0.0
Bug,Containerizer attempts to create Linux launcher by default ,Mesos containerizer attempts to create a Linux launcher by default without verifying whether the necessary prerequisites (such as availability of cgroups) are met.,3.0,0,0.0,0.07088989441930618,0.0,0.0,0.0,0.0,0.2,0.14035087719298245,0.10714285714285714,0.11538461538461538,0.11515151515151516,0.11515151515151516,0.0
Bug,Cannot start mesos local on a Debian GNU/Linux 8 docker machine,"We updated the mesos version to 0.25.0 in our Marathon docker image, that runs our integration tests.
We use mesos local for those tests. This fails with this message:

{noformat}
root@a06e4b4eb776:/marathon# mesos local
I1022 18:42:26.852485   136 leveldb.cpp:176] Opened db in 6.103258ms
I1022 18:42:26.853302   136 leveldb.cpp:183] Compacted db in 765740ns
I1022 18:42:26.853343   136 leveldb.cpp:198] Created db iterator in 9001ns
I1022 18:42:26.853355   136 leveldb.cpp:204] Seeked to beginning of db in 1287ns
I1022 18:42:26.853366   136 leveldb.cpp:273] Iterated through 0 keys in the db in 1111ns
I1022 18:42:26.853406   136 replica.cpp:744] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I1022 18:42:26.853775   141 recover.cpp:449] Starting replica recovery
I1022 18:42:26.853862   141 recover.cpp:475] Replica is in EMPTY status
I1022 18:42:26.854751   138 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request
I1022 18:42:26.854856   140 recover.cpp:195] Received a recover response from a replica in EMPTY status
I1022 18:42:26.855002   140 recover.cpp:566] Updating replica status to STARTING
I1022 18:42:26.855655   138 master.cpp:376] Master a3f39818-1bda-4710-b96b-2a60ed4d12b8 (a06e4b4eb776) started on 172.17.0.14:5050
I1022 18:42:26.855680   138 master.cpp:378] Flags at startup: --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""false"" --authenticate_slaves=""false"" --authenticators=""crammd5"" --authorizers=""local"" --framework_sorter=""drf"" --help=""false"" --hostname_lookup=""true"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""5secs"" --registry_strict=""false"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/usr/share/mesos/webui"" --work_dir=""/tmp/mesos/local/AK0XpG"" --zk_session_timeout=""10secs""
I1022 18:42:26.855790   138 master.cpp:425] Master allowing unauthenticated frameworks to register
I1022 18:42:26.855803   138 master.cpp:430] Master allowing unauthenticated slaves to register
I1022 18:42:26.855815   138 master.cpp:467] Using default 'crammd5' authenticator
W1022 18:42:26.855829   138 authenticator.cpp:505] No credentials provided, authentication requests will be refused
I1022 18:42:26.855840   138 authenticator.cpp:512] Initializing server SASL
I1022 18:42:26.856442   136 containerizer.cpp:143] Using isolation: posix/cpu,posix/mem,filesystem/posix
I1022 18:42:26.856943   140 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 1.888185ms
I1022 18:42:26.856987   140 replica.cpp:323] Persisted replica status to STARTING
I1022 18:42:26.857115   140 recover.cpp:475] Replica is in STARTING status
I1022 18:42:26.857270   140 replica.cpp:641] Replica in STARTING status received a broadcasted recover request
I1022 18:42:26.857312   140 recover.cpp:195] Received a recover response from a replica in STARTING status
I1022 18:42:26.857368   140 recover.cpp:566] Updating replica status to VOTING
I1022 18:42:26.857781   140 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 371121ns
I1022 18:42:26.857841   140 replica.cpp:323] Persisted replica status to VOTING
I1022 18:42:26.857895   140 recover.cpp:580] Successfully joined the Paxos group
I1022 18:42:26.857928   140 recover.cpp:464] Recover process terminated
I1022 18:42:26.862455   137 master.cpp:1603] The newly elected leader is master@172.17.0.14:5050 with id a3f39818-1bda-4710-b96b-2a60ed4d12b8
I1022 18:42:26.862498   137 master.cpp:1616] Elected as the leading master!
I1022 18:42:26.862511   137 master.cpp:1376] Recovering from registrar
I1022 18:42:26.862560   137 registrar.cpp:309] Recovering registrar
Failed to create a containerizer: Could not create MesosContainerizer: Failed to create launcher: Failed to create Linux launcher: Failed to mount cgroups hierarchy at '/sys/fs/cgroup/freezer': 'freezer' is already attached to another hierarchy
{noformat}

The setup worked with mesos 0.24.0.
The Dockerfile is here: https://github.com/mesosphere/marathon/blob/mv/mesos_0.25/Dockerfile



{noformat}
root@a06e4b4eb776:/marathon# ls /sys/fs/cgroup/
root@a06e4b4eb776:/marathon# 
{noformat}

{noformat}
root@a06e4b4eb776:/marathon# cat /proc/mounts 
none / aufs rw,relatime,si=6e7ac87f36042e03,dio,dirperm1 0 0
proc /proc proc rw,nosuid,nodev,noexec,relatime 0 0
tmpfs /dev tmpfs rw,nosuid,mode=755 0 0
devpts /dev/pts devpts rw,nosuid,noexec,relatime,gid=5,mode=620,ptmxmode=666 0 0
shm /dev/shm tmpfs rw,nosuid,nodev,noexec,relatime,size=65536k 0 0
mqueue /dev/mqueue mqueue rw,nosuid,nodev,noexec,relatime 0 0
sysfs /sys sysfs ro,nosuid,nodev,noexec,relatime 0 0
/dev/sda1 /etc/resolv.conf ext4 rw,relatime,data=ordered 0 0
/dev/sda1 /etc/hostname ext4 rw,relatime,data=ordered 0 0
/dev/sda1 /etc/hosts ext4 rw,relatime,data=ordered 0 0
devpts /dev/console devpts rw,relatime,mode=600,ptmxmode=000 0 0
proc /proc/bus proc ro,nosuid,nodev,noexec,relatime 0 0
proc /proc/fs proc ro,nosuid,nodev,noexec,relatime 0 0
proc /proc/irq proc ro,nosuid,nodev,noexec,relatime 0 0
proc /proc/sys proc ro,nosuid,nodev,noexec,relatime 0 0
proc /proc/sysrq-trigger proc ro,nosuid,nodev,noexec,relatime 0 0
tmpfs /proc/kcore tmpfs rw,nosuid,mode=755 0 0
tmpfs /proc/timer_stats tmpfs rw,nosuid,mode=755 0 0
{noformat}

[~bernd-mesos] Can you please assign to the correct person?",3.0,0.25.0,0.5,0.0,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3357142857142857,0.02564102564102564,0.01212121212121212,0.01212121212121212,0.2293577981651376
Documentation,Backticks are not mentioned in Mesos C++ Style Guide,"As far as I can tell, current practice is to quote code excerpts and object names with backticks when writing comments. For example:

{code}
// You know, `sadPanda` seems extra sad lately.
std::string sadPanda;
sadPanda = ""   :'(   "";
{code}

However, I don't see this documented in our C++ style guide at all. It should be added.",1.0,0,0.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Improvement,MasterAllocatorTest.SlaveLost is slow.,"The {{MasterAllocatorTest.SlaveLost}} takes more that {{5s}} to complete. A brief look into the code hints that the stopped agent does not quit immediately (and hence its resources are not released by the allocator) because [it waits for the executor to terminate|https://github.com/apache/mesos/blob/master/src/tests/master_allocator_tests.cpp#L717]. {{5s}} timeout comes from {{EXECUTOR_SHUTDOWN_GRACE_PERIOD}} agent constant.

Possible solutions:
* Do not wait until the stopped agent quits (can be flaky, needs deeper analysis).
* Decrease the agent's {{executor_shutdown_grace_period}} flag.
* Terminate the executor faster (this may require some refactoring since the executor driver is created in the {{TestContainerizer}} and we do not have direct access to it.
",1.0,0,0.5,0.033182503770739065,0.0,0.0,0.0,0.0,0.2,0.017543859649122806,0.06428571428571428,0.717948717948718,0.8242424242424242,0.7212121212121212,0.0
Bug,RegistryClientTest.SimpleGetBlob is flaky,"{{RegistryClientTest.SimpleGetBlob}} fails about 1/5 times.  This was encountered on OSX.

{code:title=Repro}
bin/mesos-tests.sh --gtest_filter=""*RegistryClientTest.SimpleGetBlob*"" --gtest_repeat=10 --gtest_break_on_failure
{code}

{code:title=Example Failure}
[ RUN      ] RegistryClientTest.SimpleGetBlob
../../src/tests/containerizer/provisioner_docker_tests.cpp:946: Failure
Value of: blobResponse
  Actual: ""2015-10-20 20:58:59.579393024+00:00""
Expected: blob.get()
Which is: ""\x15\x3\x3\00(P~\xCA&\xC6<\x4\x16\xE\xB2\xFF\b1a\xB9Z{\xE0\x80\xDA`\xBCt\x5R\x81x6\xF8 \x8B{\xA8\xA9\x4\xAB\xB6"" ""E\xE6\xDE\xCF\xD9*\xCC!\xC2\x15"" ""2015-10-20 20:58:59.579393024+00:00""
*** Aborted at 1445374739 (unix time) try ""date -d @1445374739"" if you are using GNU date ***
PC: @        0x103144ddc testing::UnitTest::AddTestPartResult()
*** SIGSEGV (@0x0) received by PID 49008 (TID 0x7fff73ca3300) stack trace: ***
    @     0x7fff8c58af1a _sigtramp
    @     0x7fff8386e187 malloc
    @        0x1031445b7 testing::internal::AssertHelper::operator=()
    @        0x1030d32e0 mesos::internal::tests::RegistryClientTest_SimpleGetBlob_Test::TestBody()
    @        0x1030d3562 mesos::internal::tests::RegistryClientTest_SimpleGetBlob_Test::TestBody()
    @        0x1031ac8f3 testing::internal::HandleSehExceptionsInMethodIfSupported<>()
    @        0x103192f87 testing::internal::HandleExceptionsInMethodIfSupported<>()
    @        0x1031533f5 testing::Test::Run()
    @        0x10315493b testing::TestInfo::Run()
    @        0x1031555f7 testing::TestCase::Run()
    @        0x103163df3 testing::internal::UnitTestImpl::RunAllTests()
    @        0x1031af8c3 testing::internal::HandleSehExceptionsInMethodIfSupported<>()
    @        0x103195397 testing::internal::HandleExceptionsInMethodIfSupported<>()
    @        0x1031639f2 testing::UnitTest::Run()
    @        0x1025abd41 RUN_ALL_TESTS()
    @        0x1025a8089 main
    @     0x7fff86b155c9 start
{code}

{code:title=Less common failure}
[ RUN      ] RegistryClientTest.SimpleGetBlob
../../src/tests/containerizer/provisioner_docker_tests.cpp:926: Failure
(socket).failure(): Failed accept: connection error: error:00000000:lib(0):func(0):reason(0)
{code}",4.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3357142857142857,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.0
Bug,Mesos JSON API creates invalid JSON due to lack of binary data / non-ASCII handling,"Spark encodes some binary data into the ExecutorInfo.data field.  This field is sent as a ""bytes"" Protobuf value, which can have arbitrary non-UTF8 data.

If you have such a field, it seems that it is splatted out into JSON without any regards to proper character encoding:

{code}
0006b0b0  2e 73 70 61 72 6b 2e 65  78 65 63 75 74 6f 72 2e  |.spark.executor.|
0006b0c0  4d 65 73 6f 73 45 78 65  63 75 74 6f 72 42 61 63  |MesosExecutorBac|
0006b0d0  6b 65 6e 64 22 7d 2c 22  64 61 74 61 22 3a 22 ac  |kend""},""data"":"".|
0006b0e0  ed 5c 75 30 30 30 30 5c  75 30 30 30 35 75 72 5c  |.\u0000\u0005ur\|
0006b0f0  75 30 30 30 30 5c 75 30  30 30 66 5b 4c 73 63 61  |u0000\u000f[Lsca|
0006b100  6c 61 2e 54 75 70 6c 65  32 3b 2e cc 5c 75 30 30  |la.Tuple2;..\u00|
{code}

I suspect this is because the HTTP api emits the executorInfo.data directly:

{code}
JSON::Object model(const ExecutorInfo& executorInfo)
{
  JSON::Object object;
  object.values[""executor_id""] = executorInfo.executor_id().value();
  object.values[""name""] = executorInfo.name();
  object.values[""data""] = executorInfo.data();
  object.values[""framework_id""] = executorInfo.framework_id().value();
  object.values[""command""] = model(executorInfo.command());
  object.values[""resources""] = model(executorInfo.resources());
  return object;
}
{code}

I think this may be because the custom JSON processing library in stout seems to not have any idea of what a byte array is.  I'm guessing that some implicit conversion makes it get written as a String instead, but:

{code}
inline std::ostream& operator<<(std::ostream& out, const String& string)
{
  // TODO(benh): This escaping DOES NOT handle unicode, it encodes as ASCII.
  // See RFC4627 for the JSON string specificiation.
  return out << picojson::value(string.value).serialize();
}
{code}

Thank you for any assistance here.  Our cluster is currently entirely down -- the frameworks cannot handle parsing the invalid JSON produced (it is not even valid utf-8)
",2.0,"0.24.1,0.26.0",0.5,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.02564102564102564,0.006060606060606061,0.006060606060606061,0.2294036697247706
Task,Need for http::put request method,"As we decided to create a more restful api for managing Quota request.
Therefore we also want to use the HTTP put request and hence need to enable the libprocess/http to send put request besides get and post requests.",1.0,0,0.5,0.0015082956259426848,0.3333333333333333,0.5,0.14285714285714285,0.2,0.2,0.03508771929824561,0.05714285714285714,0.11538461538461538,0.23636363636363636,0.23636363636363636,0.0
Task,Refactor SSLTest fixture such that MesosTest can use the same helpers.,"In order to write tests that exercise SSL with other components of Mesos, such as the HTTP scheduler library, we need to use the setup/teardown logic found in the {{SSLTest}} fixture.

Currently, the test fixtures have separate inheritance structures like this:
{code}
SSLTest <- ::testing::Test
MesosTest <- TemporaryDirectoryTest <- ::testing::Test
{code}
where {{::testing::Test}} is a gtest class.

The plan is the following:
# Change {{SSLTest}} to inherit from {{TemporaryDirectoryTest}}.  This will require moving the setup (generation of keys and certs) from {{SetUpTestCase}} to {{SetUp}}.  At the same time, *some* of the cleanup logic in the SSLTest will not be needed.
# Move the logic of generating keys/certs into helpers, so that individual tests can call them when needed, much like {{MesosTest}}.
# Write a child class of {{SSLTest}} which has the same functionality as the existing {{SSLTest}}, for use by the existing tests that rely on {{SSLTest}} or the {{RegistryClientTest}}.
# Have {{MesosTest}} inherit from {{SSLTest}} (which might be renamed during the refactor).  If Mesos is not compiled with {{--enable-ssl}}, then {{SSLTest}} could be {{#ifdef}}'d into any empty class.

The resulting structure should be like:
{code}
MesosTest <- SSLTest <- TemporaryDirectoryTest <- ::testing::Test
ChildOfSSLTest /
{code}",3.0,0,0.0,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.0
Improvement,Document messages.proto,The messages we pass between Mesos components are largely undocumented.  See this [TODO|https://github.com/apache/mesos/blob/19f14d06bac269b635657960d8ea8b2928b7830c/src/messages/messages.proto#L23].,3.0,0,0.0,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.0
Documentation,CentOS 6 dependency install fails at Maven,"It seems the Apache Maven dependencies have changed such that following the Getting Started docs for CentOS 6.6 will fail at Maven installation:

{code}
---> Package apache-maven.noarch 0:3.3.3-2.el6 will be installed
--> Processing Dependency: java-devel >= 1:1.7.0 for package: apache-maven-3.3.3-2.el6.noarch
--> Finished Dependency Resolution
Error: Package: apache-maven-3.3.3-2.el6.noarch (epel-apache-maven)
           Requires: java-devel >= 1:1.7.0
           Available: java-1.5.0-gcj-devel-1.5.0.0-29.1.el6.x86_64 (base)
               java-devel = 1.5.0
           Available: 1:java-1.6.0-openjdk-devel-1.6.0.35-1.13.7.1.el6_6.x86_64 (base)
               java-devel = 1:1.6.0
           Available: 1:java-1.6.0-openjdk-devel-1.6.0.36-1.13.8.1.el6_7.x86_64 (updates)
               java-devel = 1:1.6.0
 You could try using --skip-broken to work around the problem
 You could try running: rpm -Va --nofiles --nodigest
{code}",1.0,0,0.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Documentation,Configuration docs are missing --enable-libevent and --enable-ssl,"The {{\-\-enable-libevent}} and {{\-\-enable-ssl}} config flags are currently not documented in the ""Configuration"" docs with the rest of the flags. They should be added.",1.0,0.25.0,0.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.2293577981651376
Bug,HTTP scheduler library does not gracefully parse invalid resource identifiers,"If you pass a nonsense string for ""master"" into a framework using the C++ HTTP scheduler library, the framework segfaults.

For example, using the example frameworks:

{code:title=Scheduler Driver}
build/src/test-framework --master=""asdf://127.0.0.1:5050""
{code}
Results in:
{code}
Failed to create a master detector for 'asdf://127.0.0.1:5050': Failed to parse 'asdf://127.0.0.1:5050'
{code}

{code:title=HTTP Scheduler Library}
export DEFAULT_PRINCIPAL=root
build/src/event-call-framework --master=""asdf://127.0.0.1:5050""
{code}
Results in
{code}
I1015 16:18:45.432075 2062201600 scheduler.cpp:157] Version: 0.26.0
Segmentation fault: 11
{code}

{code:title=Stack Trace}
* thread #2: tid = 0x28b6bb, 0x0000000100ad03ca libmesos-0.26.0.dylib`mesos::v1::scheduler::MesosProcess::initialize(this=0x00000001076031a0) + 42 at scheduler.cpp:213, stop reason = EXC_BAD_ACCESS (code=1, address=0x0)
  * frame #0: 0x0000000100ad03ca libmesos-0.26.0.dylib`mesos::v1::scheduler::MesosProcess::initialize(this=0x00000001076031a0) + 42 at scheduler.cpp:213
    frame #1: 0x0000000100ad05f2 libmesos-0.26.0.dylib`virtual thunk to mesos::v1::scheduler::MesosProcess::initialize(this=0x00000001076031a0) + 34 at scheduler.cpp:210
    frame #2: 0x00000001022b60f3 libmesos-0.26.0.dylib`::resume() + 931 at process.cpp:2449
    frame #3: 0x00000001022c131c libmesos-0.26.0.dylib`::operator()() + 268 at process.cpp:2174
    frame #4: 0x00000001022c0fa2 libmesos-0.26.0.dylib`::__thread_proxy<std::__1::tuple<std::__1::__bind<(lambda at ../../../3rdparty/libprocess/src/process.cpp:2158:35), std::__1::reference_wrapper<const std::__1::atomic<bool> > > > >() [inlined] __invoke<(lambda at ../../../3rdparty/libprocess/src/process.cpp:2158:35) &, const std::__1::atomic<bool> &> + 27 at __functional_base:415
    frame #5: 0x00000001022c0f87 libmesos-0.26.0.dylib`::__thread_proxy<std::__1::tuple<std::__1::__bind<(lambda at ../../../3rdparty/libprocess/src/process.cpp:2158:35), std::__1::reference_wrapper<const std::__1::atomic<bool> > > > >() [inlined] __apply_functor<(lambda at ../../../3rdparty/libprocess/src/process.cpp:2158:35), std::__1::tuple<std::__1::reference_wrapper<const std::__1::atomic<bool> > >, 0, std::__1::tuple<> > + 55 at functional:2060
    frame #6: 0x00000001022c0f50 libmesos-0.26.0.dylib`::__thread_proxy<std::__1::tuple<std::__1::__bind<(lambda at ../../../3rdparty/libprocess/src/process.cpp:2158:35), std::__1::reference_wrapper<const std::__1::atomic<bool> > > > >() [inlined] operator()<> + 41 at functional:2123
    frame #7: 0x00000001022c0f27 libmesos-0.26.0.dylib`::__thread_proxy<std::__1::tuple<std::__1::__bind<(lambda at ../../../3rdparty/libprocess/src/process.cpp:2158:35), std::__1::reference_wrapper<const std::__1::atomic<bool> > > > >() [inlined] __invoke<std::__1::__bind<(lambda at ../../../3rdparty/libprocess/src/process.cpp:2158:35), std::__1::reference_wrapper<const std::__1::atomic<bool> > >> + 14 at __functional_base:415
    frame #8: 0x00000001022c0f19 libmesos-0.26.0.dylib`::__thread_proxy<std::__1::tuple<std::__1::__bind<(lambda at ../../../3rdparty/libprocess/src/process.cpp:2158:35), std::__1::reference_wrapper<const std::__1::atomic<bool> > > > >() [inlined] __thread_execute<std::__1::__bind<(lambda at ../../../3rdparty/libprocess/src/process.cpp:2158:35), std::__1::reference_wrapper<const std::__1::atomic<bool> > >> + 25 at thread:337
    frame #9: 0x00000001022c0f00 libmesos-0.26.0.dylib`::__thread_proxy<std::__1::tuple<std::__1::__bind<(lambda at ../../../3rdparty/libprocess/src/process.cpp:2158:35), std::__1::reference_wrapper<const std::__1::atomic<bool> > > > >() + 368 at thread:347
    frame #10: 0x00007fff964c705a libsystem_pthread.dylib`_pthread_body + 131
    frame #11: 0x00007fff964c6fd7 libsystem_pthread.dylib`_pthread_start + 176
    frame #12: 0x00007fff964c43ed libsystem_pthread.dylib`thread_start + 13
{code}",1.0,0.25.0,0.0,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.2293577981651376
Bug,Provide diagnostic output in agent log when fetching fails,"When fetching fails, the fetcher has written log output to stderr in the task sandbox, but it is not easy to get to. It may even be impossible to get to if one only has the agent log available and no more access to the sandbox. This is for instance the case when looking at output from a CI run.

The fetcher actor in the agent detects if the external fetcher program claims to have succeeded or not. When it exits with an error code, we could grab the fetcher log from the stderr file in the sandbox and append it to the agent log.

This is similar to this patch: https://reviews.apache.org/r/37813/

The difference is that the output of the latter is triggered by test failures outside the fetcher, whereas what is proposed here is triggering upon failures inside the fetcher.",2.0,0,0.0,0.06184012066365008,0.0,0.0,0.0,0.0,0.2,0.10526315789473684,0.12857142857142856,0.14102564102564102,0.16969696969696968,0.17575757575757575,0.0
Bug,Incorrect sed syntax for Mac OSX,"The build currently fails on OSX:

{noformat}
../3rdparty/libprocess/3rdparty/protobuf-2.5.0/src/protoc -I../../mesos/include/mesos/containerizer		\
		-I../../mesos/include -I../../mesos/src						\
		--python_out=python/interface/src/mesos/interface ../../mesos/include/mesos/containerizer/containerizer.proto
../../mesos/install-sh -c -d python/interface/src/mesos/v1/interface
sed -i 's/mesos\.mesos_pb2/mesos_pb2/' python/interface/src/mesos/interface/containerizer_pb2.py
sed: 1: ""python/interface/src/me ..."": extra characters at the end of p command
make[1]: *** [python/interface/src/mesos/interface/containerizer_pb2.py] Error 1
{noformat}

This is because the sed command uses the wrong syntax for OSX: you need {code}sed -i """"{code} to instruct sed to not use a backup file.",2.0,0,0.0,1.0,0.0,0.0,0.0,0.0,0.8,0.5438596491228069,0.39999999999999997,0.48717948717948717,0.4303030303030303,0.4303030303030303,0.0
Improvement,Speed up FaultToleranceTest.FrameworkReregister test,"FaultToleranceTest.FrameworkReregister test takes more than one second to complete:
{code}
[ RUN      ] FaultToleranceTest.FrameworkReregister
[       OK ] FaultToleranceTest.FrameworkReregister (1056 ms)
{code}

There must be a {{1s}} timeout somewhere which we should mitigate via {{Clock::advance()}}.",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.717948717948718,0.8242424242424242,0.7212121212121212,0.0
Bug,Update Allocator interface to support quota,"An allocator should be notified when a quota is being set/updated or removed. Also to support master failover in presence of quota, allocator should be notified about the reregistering agents and allocations towards quota.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.717948717948718,0.8242424242424242,0.7212121212121212,0.0
Bug,HTTP Pipelining doesn't keep order of requests,"[HTTP 1.1 Pipelining|https://en.wikipedia.org/wiki/HTTP_pipelining] describes a mechanism by which multiple HTTP request can be performed over a single socket. The requirement here is that responses should be send in the same order as requests are being made.

Libprocess has some mechanisms built in to deal with pipelining when multiple HTTP requests are made, it is still, however, possible to create a situation in which responses are scrambled respected to the requests arrival.

Consider the situation in which there are two libprocess processes, {{processA}} and {{processB}}, each running in a different thread, {{thread2}} and {{thread3}} respectively. The [{{ProcessManager}}|https://github.com/apache/mesos/blob/1d68eed9089659b06a1e710f707818dbcafeec52/3rdparty/libprocess/src/process.cpp#L374] runs in {{thread1}}.

{{processA}} is of type {{ProcessA}} which looks roughly as follows:

{code}
class ProcessA : public ProcessBase<ProcessA>
{
public:
  ProcessA() {}

  Future<http::Response> foo(const http::Request&) {
    // … Do something …
   return http::Ok();
  }

protected:
  virtual void initialize() {
    route(""/foo"", None(), &ProcessA::foo);
  }
}
{code}

{{processB}} is from type {{ProcessB}} which is just like {{ProcessA}} but routes {{""bar""}} instead of {{""foo""}}.

The situation in which the bug arises is the following:

# Two requests, one for {{""http://server_uri/(1)/foo""}} and one for {{""http://server_uri/(2)//bar""}} are made over the same socket.
# The first request arrives to [{{ProcessManager::handle}}|https://github.com/apache/mesos/blob/1d68eed9089659b06a1e710f707818dbcafeec52/3rdparty/libprocess/src/process.cpp#L2202] which is still running in {{thread1}}. This one creates an {{HttpEvent}} and delivers to the handler, in this case {{processA}}.
# [{{ProcessManager::deliver}}|https://github.com/apache/mesos/blob/1d68eed9089659b06a1e710f707818dbcafeec52/3rdparty/libprocess/src/process.cpp#L2361] enqueues the HTTP event in to the {{processA}} queue. This happens in {{thread1}}.
# The second request arrives to [{{ProcessManager::handle}}|https://github.com/apache/mesos/blob/1d68eed9089659b06a1e710f707818dbcafeec52/3rdparty/libprocess/src/process.cpp#L2202] which is still running in {{thread1}}. Another {{HttpEvent}} is created and delivered to the handler, in this case {{processB}}.
# [{{ProcessManager::deliver}}|https://github.com/apache/mesos/blob/1d68eed9089659b06a1e710f707818dbcafeec52/3rdparty/libprocess/src/process.cpp#L2361] enqueues the HTTP event in to the {{processB}} queue. This happens in {{thread1}}.
# {{Thread2}} is blocked, so {{processA}} cannot handle the first request, it is stuck in the queue.
# {{Thread3}} is idle, so it picks up the request to {{processB}} immediately.
# [{{ProcessBase::visit(HttpEvent)}}|https://github.com/apache/mesos/blob/1d68eed9089659b06a1e710f707818dbcafeec52/3rdparty/libprocess/src/process.cpp#L3073] is called in {{thread3}}, this one in turn [dispatches|https://github.com/apache/mesos/blob/1d68eed9089659b06a1e710f707818dbcafeec52/3rdparty/libprocess/src/process.cpp#L3106] the response's future to the {{HttpProxy}} associated with the socket where the request came.

At the last point, the bug is evident, the request to {{processB}} will be send before the request to {{processA}} even if the handler takes a long time and the {{processA::bar()}} actually finishes before. The responses are not send in the order the requests are done.

h1. Reproducer

The following is a test which successfully reproduces the issue:

{code:title=3rdparty/libprocess/src/tests/http_tests.cpp}
#include <process/latch.hpp

using process::Latch;
using testing::InvokeWithoutArgs;

// This tests tries to force a situation in which HTTP Pipelining is scrambled.
// It does so by having two actors to which three requests are made, the first
// two requests to the first actor and a third request to the second actor.
// The first request will block the first actor long enough to allow the second
// actor to process the third request. Since the first actor will not be able to
// handle any event until it is done processing the first request, the third
// request is finished before the second even starts.
// The ultimate goal of the test is to alter the order in which
// `ProcessBase::visit(HttpEvent)` is executed for the different events
// respect to the order in which the requests arrived.
TEST(HTTPConnectionTest, ComplexPipelining)
{
  Http server1, server2;

  Future<http::Request> get1, get2, get3;
  Latch latch;

  EXPECT_CALL(*server1.process, get(_))
    .WillOnce(DoAll(FutureArg<0>(&get1),
                    InvokeWithoutArgs([&latch]() { latch.await(); }),
                    Return(http::OK(""1""))))
    .WillOnce(DoAll(FutureArg<0>(&get2),
                    Return(http::OK(""2""))));

  EXPECT_CALL(*server2.process, get(_))
    .WillOnce(DoAll(FutureArg<0>(&get3),
                    Return(http::OK(""3""))));

    auto url1 = http::URL(
      ""http"",
      server1.process->self().address.ip,
      server1.process->self().address.port,
      server1.process->self().id + ""/get"");
    auto url2 = http::URL(
      ""http"",
      server1.process->self().address.ip,
      server1.process->self().address.port,
      server2.process->self().id + ""/get"");

  // Create a connection to the server for HTTP pipelining.
  Future<http::Connection> connect = http::connect(url1);

  AWAIT_READY(connect);

  http::Connection connection = connect.get();

  http::Request request1;
  request1.method = ""GET"";
  request1.url = url1;
  request1.keepAlive = true;
  request1.body = ""1"";
  Future<http::Response> response1 = connection.send(request1);

  http::Request request2 = request1;
  request2.body = ""2"";
  Future<http::Response> response2 = connection.send(request2);

  http::Request request3;
  request3.method = ""GET"";
  request3.url = url2;
  request3.keepAlive = true;
  request3.body = ""3"";
  Future<http::Response> response3 = connection.send(request3);

  // Verify that request1 arrived at server1 and it is the right request.
  // Now server1 is blocked processing request1 and cannot pick up more events
  // in the queue.
  AWAIT_READY(get1);
  EXPECT_EQ(request1.body, get1->body);

  // Verify that request3 arrived at server2 and it is the right request.
  AWAIT_READY(get3);
  EXPECT_EQ(request3.body, get3->body);

  // Request2 hasn't been picked up since server1 is still blocked serving
  // request1.
  EXPECT_TRUE(get2.isPending());

  // Free server1 so it can serve request2.
  latch.trigger();

  // Verify that request2 arrived at server1 and it is the right request.
  AWAIT_READY(get2);
  EXPECT_EQ(request2.body, get2->body);

  // Wait for all responses.
  AWAIT_READY(response1);
  AWAIT_READY(response2);
  AWAIT_READY(response3);

  // If pipelining works as expected, even though server2 finished processing
  // its request before server1 even began with request2, the responses should
  // arrive in the order they were made.
  EXPECT_EQ(request1.body, response1->body);
  EXPECT_EQ(request2.body, response2->body);
  EXPECT_EQ(request3.body, response3->body);

  AWAIT_READY(connection.disconnect());
  AWAIT_READY(connection.disconnected());
}
{code}",3.0,0.24.0,0.0,0.004524886877828055,0.0,0.0,0.0,0.0,0.2,0.12280701754385964,0.24285714285714285,0.10256410256410256,0.11515151515151516,0.11515151515151516,0.2201834862385321
Bug,Deprecate resource_monitoring_interval flag,This parameter should be deprecated after 0.23.0 release as it has no use now. ,1.0,0,0.0,0.3137254901960784,0.0,0.0,0.0,0.0,0.4,0.19298245614035087,0.14285714285714285,0.10256410256410256,0.06666666666666667,0.06666666666666667,0.0
Bug,JSON parsing allows non-whitespace trailing characters,"Picojson supports a streaming mode in which a stream containing a series of JSON values can be repeatedly parsed. For this reason, it does not return an error when passed a string containing a valid JSON value followed by non-whitespace trailing characters.

However, in addition to the four-argument {{picojson::parse()}} that we're using, picojson contains a two-argument {{parse()}} function (https://github.com/kazuho/picojson/blob/master/picojson.h#L938-L942) which accepts a {{std::string}} and should probably validate its input to ensure it doesn't contain trailing characters. A pull request has been filed for this change at https://github.com/kazuho/picojson/pull/70 and if it's merged, we can switch to the two-argument function call. In the meantime, we should provide such input validation ourselves in {{JSON::parse()}}.",1.0,0,0.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Bug,Enable building mesos.apache.org locally in a Docker container.,We should make it easy for everyone to modify the website and be able to generate it locally before pushing to upstream. ,3.0,0,0.0,0.07088989441930618,0.0,0.0,0.0,0.0,0.2,0.14035087719298245,0.10714285714285714,0.11538461538461538,0.11515151515151516,0.11515151515151516,0.0
Documentation,Clarify error message 'could not chown work directory',"When deploying a framework I encountered the error message 'could not chown work directory'.

It took me a while to figure out that this happened because my framework was registered as a user on my host machine which did not exist on the Docker container and the agent was running as root.

I suggest to clarify this message by pointing out to either set {{--switch-user}}  to {{false}} or to run the framework as the same user as the agent.",1.0,0.25.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.15789473684210525,0.20714285714285713,0.0,0.0,0.0,0.2293577981651376
Task,Add support for github and variable base URLs to apply-reviews.py,"From Adam's email on dev@ list:

I have used the '-g' feature for github PRs in the past, and we should
continue to support that model, so that new Mesos contributors don't have
to create new RB accounts and learn a new process just for quick
documentation changes, etc.

As a side note, now that the Myriad incubator project has migrated to
Apache git and we can no longer merge PRs directly, we were hoping to take
advantage of a tool like apply-reviews to apply our PR patches. It looks
like apply-reviews.sh only specifies 'mesos' in the GITHUB_URL/API_URL.
Would apply-reviews.py be just as easy to reuse for another project (i.e.
Myriad)?",3.0,0,0.0,0.07088989441930618,0.0,0.0,0.0,0.0,0.2,0.14035087719298245,0.10714285714285714,0.11538461538461538,0.11515151515151516,0.11515151515151516,0.0
Task,Port slave/containerizer/mesos/launch.cpp to Windows,"Important subset of the dependency tree follows:

slave/containerizer/mesos/launch.cpp: os, protobuf, launch
launch: subcommand
subcommand: flags
flags.hpp: os.hpp, path.hpp, fetch.hpp",3.0,0,0.0,0.44494720965309204,0.0,0.0,0.0,0.0,0.0,0.14035087719298245,0.15,0.14102564102564102,0.1515151515151515,0.16969696969696968,0.0
Task,Port slave/containerizer/mesos/containerizer.cpp to Windows,"Important subset of the dependency tree follows:

slave/containerizer/mesos/containerizer.cpp: isolator, collect, defer, io, metrics, reap, subprocess, fs, os, path, protobuf_utils, paths, slave, containerizer, fetcher, launcher, posix, disk, containerizer, launch, provisioner",3.0,0,0.0,0.44494720965309204,0.0,0.0,0.0,0.0,0.0,0.14035087719298245,0.15,0.14102564102564102,0.1515151515151515,0.16969696969696968,0.0
Task,Create slave/containerizer/isolators/filesystem/windows.cpp,"Should look a lot like the posix.cpp flavor. Important subset of the dependency tree follows for the posix flavor:

slave/containerizer/isolators/filesystem/posix.cpp: filesystem/posix, fs, os, path
filesystem/posix: flags, isolator",3.0,0,0.0,0.44494720965309204,0.0,0.0,0.0,0.0,0.0,0.14035087719298245,0.15,0.14102564102564102,0.1515151515151515,0.16969696969696968,0.0
Task,Port slave/containerizer/isolator.cpp to Windows,"Important subset of the dependency tree follows:

isolator.hpp: dispatch.hpp, path.hpp
isolator: process
dispatch.hpp: process.hpp
",3.0,0,0.0,0.44494720965309204,0.0,0.0,0.0,0.0,0.0,0.14035087719298245,0.15,0.14102564102564102,0.1515151515151515,0.16969696969696968,0.0
Task,Port slave/containerizer/fetcher.cpp,"Important subset of the dependency tree follows:

slave/containerizer/fetcher.cpp: slave, fetcher, collect, dispatch, net
collect: future, defer, process
fetcher: type_utils, future, process, subprocess
dispatch.hpp: process.hpp
net.hpp: ip, networking stuff
future.hpp: pid.hpp
defer.hpp: deferred.hpp, dispatch.hpp
deferred.hpp: dispatch.hpp, pid.hpp
type_utils.hpp: uuid.hpp
subprocess: os, future",3.0,0,0.0,0.44494720965309204,0.0,0.0,0.0,0.0,0.0,0.14035087719298245,0.15,0.14102564102564102,0.1515151515151515,0.16969696969696968,0.0
Task,Port slave/state.cpp,"Important subset of changes this depends on:

slave/state.cpp: pid, os, path, protobuf, paths, state
pid.hpp: address.hpp, ip.hpp
address.hpp: ip.hpp, net.hpp
net.hpp: ip, networking stuff
state: type_utils, pid, os, path, protobuf, uuid
type_utils.hpp: uuid.hpp",3.0,0,0.0,0.44494720965309204,0.0,0.0,0.0,0.0,0.0,0.14035087719298245,0.15,0.14102564102564102,0.1515151515151515,0.16969696969696968,0.0
Task,Port slave/paths.cpp to Windows,"Important subset of dependency tree of changes necessary:

slave/paths.cpp: os, path",1.0,0,0.0,0.44494720965309204,0.0,0.0,0.0,0.0,0.0,0.14035087719298245,0.15,0.14102564102564102,0.1515151515151515,0.16969696969696968,0.0
Bug,ExamplesTest.PersistentVolumeFramework does not work in OS X El Capitan,"The example persistent volume framework test does not pass in OS X El Capitan. It seems to be executing the {{<build_dir>/src/.libs/mesos-executor}} directly while it should be executing the wrapper script at {{<build_dir>/src/mesos-executor}} instead. The no-executor framework passes however, which seem to have a very similar configuration with the persistent volume framework. The following is the output that shows the {{dyld}} load error:

{noformat}
I1008 01:22:52.280140 4284416 launcher.cpp:132] Forked child with pid '1706' for contain
er 'b6d3bd96-2ebd-47b1-a16a-a22ffba992aa'
I1008 01:22:52.280300 4284416 containerizer.cpp:873] Checkpointing executor's forked pid
 1706 to '/var/folders/p6/nfxknpz52dzfc6zqnz23tq180000gn/T/mesos-XXXXXX.5OZ3locB/0/meta/
slaves/34d6329e-69cb-4a72-aee4-fe892bf1c70b-S2/frameworks/34d6329e-69cb-4a72-aee4-fe892b
f1c70b-0000/executors/dec188d4-d2dc-40c5-ac4d-881adc3d81c0/runs/b6d3bd96-2ebd-47b1-a16a-
a22ffba992aa/pids/forked.pid'
dyld: Library not loaded: /usr/local/lib/libmesos-0.26.0.dylib
  Referenced from: /Users/mpark/Projects/mesos/build/src/.libs/mesos-executor
  Reason: image not found
dyld: Library not loaded: /usr/local/lib/libmesos-0.26.0.dylib
  Referenced from: /Users/mpark/Projects/mesos/build/src/.libs/mesos-executor
  Reason: image not found
dyld: Library not loaded: /usr/local/lib/libmesos-0.26.0.dylib
  Referenced from: /Users/mpark/Projects/mesos/build/src/.libs/mesos-executor
  Reason: image not found
I1008 01:22:52.365397 3211264 containerizer.cpp:1284] Executor for container '06b649be-88c8-4047-8fb5-e89bdd096b66' has exited
I1008 01:22:52.365433 3211264 containerizer.cpp:1097] Destroying container '06b649be-88c8-4047-8fb5-e89bdd096b66'
{noformat}",3.0,0,0.0,0.6726998491704375,0.0,0.0,0.0,0.1,0.6000000000000001,0.2807017543859649,0.29285714285714287,0.21794871794871795,0.26666666666666666,0.26666666666666666,0.0
Bug,Test build failure due to comparison between signed and unsigned integers,"Compilation fails on OpenSUSE Tumbleweed (Linux 4.1.6, gcc 5.1.1, glibc 2.22) with the following errors:

{code}
In file included from ../../src/tests/values_tests.cpp:22:0: 
../3rdparty/libprocess/3rdparty/gmock-1.7.0/gtest/include/gtest/gtest.h: In instantiatio
n of ‘testing::AssertionResult testing::internal::CmpHelperEQ(const char*, const char*, 
const T1&, const T2&) [with T1 = int; T2 = long unsigned int]’: 
../3rdparty/libprocess/3rdparty/gmock-1.7.0/gtest/include/gtest/gtest.h:1484:23:   requi
red from ‘static testing::AssertionResult testing::internal::EqHelper<lhs_is_null_litera
l>::Compare(const char*, const char*, const T1&, const T2&) [with T1 = int; T2 = long un
signed int; bool lhs_is_null_literal = false]’ 
../../src/tests/values_tests.cpp:287:3:   required from here 
../3rdparty/libprocess/3rdparty/gmock-1.7.0/gtest/include/gtest/gtest.h:1448:16: error: 
comparison between signed and unsigned integer expressions [-Werror=sign-compare] 
  if (expected == actual) { 
               ^ 
 CXX      tests/containerizer/mesos_tests-provisioner_docker_tests.o 
^CMakefile:6779: recipe for target 'tests/mesos_tests-values_tests.o' failed 
make[3]: *** [tests/mesos_tests-values_tests.o] Interrupt
{code}",1.0,0,0.0,0.3453996983408748,0.0,0.0,0.0,0.0,0.2,0.38596491228070173,0.35714285714285715,0.28205128205128205,0.29090909090909095,0.28484848484848485,0.0
Bug,Framework process hangs after master failover when number frameworks > libprocess thread pool size,"When running multi framework instances per process, if the number of framework created exceeds the libprocess threads then during master failover the zookeeper updates can cause deadlock. E.g. On a machine with 24 cpus, if the framework instance count exceeds 24 ( per process)  then when the master fails over all the libprocess threads block updating the cache ( GroupProcess) leading to deadlock. Below is the stack trace of one the libprocess thread :

{code}
Thread 101 (Thread 0x7f42821f1700 (LWP 5974)):
#0  0x000000314100b5bc in pthread_cond_wait@@GLIBC_2.3.2 () from /lib64/libpthread.so.0
#1  0x00007f42870d1637 in Gate::arrive(long) () from /Users/mchadha/venv/lib/python2.7/site-packages/mesos.native-0.22.1003-py2.7-linux-x86_64.egg/mesos/native/_mesos.so
#2  0x00007f42870be87c in process::ProcessManager::wait(process::UPID const&) () from /Users/mchadha/venv/lib/python2.7/site-packages/mesos.native-0.22.1003-py2.7-linux-x86_64.eg
g/mesos/native/_mesos.so
#3  0x00007f42870c25f7 in process::wait(process::UPID const&, Duration const&) () from /Users/mchadha/venv/lib/python2.7/site-packages/mesos.native-0.22.1003-py2.7-linux-x86_64.e
gg/mesos/native/_mesos.so
#4  0x00007f428708e294 in process::Latch::await(Duration const&) () from /Users/mchadha/venv/lib/python2.7/site-packages/mesos.native-0.22.1003-py2.7-linux-x86_64.egg/mesos/nativ
e/_mesos.so
#5  0x00007f4286b67dea in process::Future<int>::await(Duration const&) const () from /Users/mchadha/venv/lib/python2.7/site-packages/mesos.native-0.22.1003-py2.7-linux-x86_64.egg
/mesos/native/_mesos.so
#6  0x00007f4286b5a0df in process::Future<int>::get() const () from /Users/mchadha/venv/lib/python2.7/site-packages/mesos.native-0.22.1003-py2.7-linux-x86_64.egg/mesos/native/_me
sos.so
#7  0x00007f4286ff0508 in ZooKeeper::getChildren(std::basic_string<char, std::char_traits<char>, std::allocator<char> > const&, bool, std::vector<std::basic_string<char, std::cha
r_traits<char>, std::allocator<char> >, std::allocator<std::basic_string<char, std::char_traits<char>, std::allocator<char> > > >*) () from /Users/mchadha/venv/lib/python2.7/site
-packages/mesos.native-0.22.1003-py2.7-linux-x86_64.egg/mesos/native/_mesos.so
#8  0x00007f4286cb394e in zookeeper::GroupProcess::cache() () from /Users/mchadha/venv/lib/python2.7/site-packages/mesos.native-0.22.1003-py2.7-linux-x86_64.egg/mesos/native/_mes
os.so
#9  0x00007f4286cb1e63 in zookeeper::GroupProcess::updated(long, std::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) () from /Users/mchadha/venv/lib/py
thon2.7/site-packages/mesos.native-0.22.1003-py2.7-linux-x86_64.egg/mesos/native/_mesos.so
#10 0x00007f4286ce027a in std::tr1::_Mem_fn<void (zookeeper::GroupProcess::*)(long, std::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)>::operator()(zo
okeeper::GroupProcess*, long, std::basic_string<char, std::char_traits<char>, std::allocator<char> > const&) const () from /Users/mchadha/venv/lib/python2.7/site-packages/mesos.n
ative-0.22.1003-py2.7-linux-x86_64.egg/mesos/native/_mesos.so
#11 0x00007f4286ce0067 in std::tr1::result_of<std::tr1::_Mem_fn<void (zookeeper::GroupProcess::*)(long, std::basic_string<char, std::char_traits<char>, std::allocator<char> > con
st&)> ()(std::tr1::result_of<std::tr1::_Mu<std::tr1::_Placeholder<1>, false, true> ()(std::tr1::_Placeholder<1>, std::tr1::tuple<zookeeper::GroupProcess*&>)>::type, std::tr1::res
ult_of<std::tr1::_Mu<long, false, false> ()(long, std::tr1::_Mu<std::tr1::_Placeholder<1>, false, true> ()(std::tr1::_Placeholder<1>, std::tr1::tuple<zookeeper::GroupProcess*&>))
>::type, std::tr1::result_of<std::tr1::_Mu<std::basic_string<char, std::char_traits<char>, std::allocator<char> >, false, false> ()(std::basic_string<char, std::char_traits<char>
, std::allocator<char> >, std::tr1::_Mu<std::tr1::_Placeholder<1>, false, true> ()(std::tr1::_Placeholder<1>, std::tr1::tuple<zookeeper::GroupProcess*&>))>::type)>::type std::tr1
::_Bind<std::tr1::_Mem_fn<void (zookeeper::GroupProcess::*)(long, std::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)> ()(std::tr1::_Placeholder<1>, lo
ng, std::basic_string<char, std::char_traits<char>, std::allocator<char> >)>::__call<zookeeper::GroupProcess*&, 0, 1, 2>(std::tr1::_Mu<std::tr1::_Placeholder<1>, false, true> ( c
onst&)(std::tr1::_Placeholder<1>, std::tr1::tuple<zookeeper::GroupProcess*&>), std::tr1::_Index_tuple<0, 1, 2>) () from /Users/mchadha/venv/lib/python2.7/site-packages/mesos.nati
ve-0.22.1003-py2.7-linux-x86_64.egg/mesos/native/_mesos.so
#12 0x00007f4286cdfd16 in std::tr1::result_of<std::tr1::_Mem_fn<void (zookeeper::GroupProcess::*)(long, std::basic_string<char, std::char_traits<char>, std::allocator<char> > con
st&)> ()(std::tr1::result_of<std::tr1::_Mu<std::tr1::_Placeholder<1>, false, true> ()(std::tr1::_Placeholder<1>, std::tr1::tuple<zookeeper::GroupProcess*>)>::type, std::tr1::resu
lt_of<std::tr1::_Mu<long, false, false> ()(long, std::tr1::_Mu<std::tr1::_Placeholder<1>, false, true> ()(std::tr1::_Placeholder<1>, std::tr1::tuple<zookeeper::GroupProcess*>))>:
:type, std::tr1::result_of<std::tr1::_Mu<std::basic_string<char, std::char_traits<char>, std::allocator<char> >, false, false> ()(std::basic_string<char, std::char_traits<char>,
std::allocator<char> >, std::tr1::_Mu<std::tr1::_Placeholder<1>, false, true> ()(std::tr1::_Placeholder<1>, std::tr1::tuple<zookeeper::GroupProcess*>))>::type)>::type std::tr1::_
Bind<std::tr1::_Mem_fn<void (zookeeper::GroupProcess::*)(long, std::basic_string<char, std::char_traits<char>, std::allocator<char> > const&)> ()(std::tr1::_Placeholder<1>, long,
 std::basic_string<char, std::char_traits<char>, std::allocator<char> >)>::operator()<zookeeper::GroupProcess*>(zookeeper::GroupProcess*&) () from /Users/mchadha/venv/lib/python2
.7/site-packages/mesos.native-0.22.1003-py2.7-linux-x86_64.egg/mesos/native/_mesos.so
#13 0x00007f4286cdf8be in std::tr1::_Function_handler<void ()(zookeeper::GroupProcess*), std::tr1::_Bind<std::tr1::_Mem_fn<void (zookeeper::GroupProcess::*)(long, std::basic_stri
ng<char, std::char_traits<char>, std::allocator<char> > const&)> ()(std::tr1::_Placeholder<1>, long, std::basic_string<char, std::char_traits<char>, std::allocator<char> >)> >::_
M_invoke(std::tr1::_Any_data const&, zookeeper::GroupProcess*) () from /Users/mchadha/venv/lib/python2.7/site-packages/mesos.native-0.22.1003-py2.7-linux-x86_64.egg/mesos/native/
_mesos.so
#14 0x00007f4286cc2394 in std::tr1::function<void ()(zookeeper::GroupProcess*)>::operator()(zookeeper::GroupProcess*) const () from /Users/mchadha/venv/lib/python2.7/site-package
s/mesos.native-0.22.1003-py2.7-linux-x86_64.egg/mesos/native/_mesos.so
#15 0x00007f4286cbc3a2 in void process::internal::vdispatcher<zookeeper::GroupProcess>(process::ProcessBase*, std::tr1::shared_ptr<std::tr1::function<void ()(zookeeper::GroupProc
ess*)> >) () from /Users/mchadha/venv/lib/python2.7/site-packages/mesos.native-0.22.1003-py2.7-linux-x86_64.egg/mesos/native/_mesos.so
#16 0x00007f4286ccdca5 in std::tr1::result_of<void (*()(std::tr1::result_of<std::tr1::_Mu<std::tr1::_Placeholder<1>, false, true> ()(std::tr1::_Placeholder<1>, std::tr1::tuple<pr
ocess::ProcessBase*&>)>::type, std::tr1::result_of<std::tr1::_Mu<std::tr1::shared_ptr<std::tr1::function<void ()(zookeeper::GroupProcess*)> >, false, false> ()(std::tr1::shared_p
tr<std::tr1::function<void ()(zookeeper::GroupProcess*)> >, std::tr1::_Mu<std::tr1::_Placeholder<1>, false, true> ()(std::tr1::_Placeholder<1>, std::tr1::tuple<process::ProcessBa
se*&>))>::type))(process::ProcessBase*, std::tr1::shared_ptr<std::tr1::function<void ()(zookeeper::GroupProcess*)> >)>::type std::tr1::_Bind<void (*()(std::tr1::_Placeholder<1>,
std::tr1::shared_ptr<std::tr1::function<void ()(zookeeper::GroupProcess*)> >))(process::ProcessBase*, std::tr1::shared_ptr<std::tr1::function<void ()(zookeeper::GroupProcess*)> >
)>::__call<process::ProcessBase*&, 0, 1>(std::tr1::_Mu<std::tr1::_Placeholder<1>, false, true> ( const&)(std::tr1::_Placeholder<1>, std::tr1::tuple<process::ProcessBase*&>), std:
:tr1::_Index_tuple<0, 1>) () from /Users/mchadha/venv/lib/python2.7/site-packages/mesos.native-0.22.1003-py2.7-linux-x86_64.egg/mesos/native/_mesos.so
#17 0x00007f4286cc7a5a in std::tr1::result_of<void (*()(std::tr1::result_of<std::tr1::_Mu<std::tr1::_Placeholder<1>, false, true> ()(std::tr1::_Placeholder<1>, std::tr1::tuple<pr
ocess::ProcessBase*>)>::type, std::tr1::result_of<std::tr1::_Mu<std::tr1::shared_ptr<std::tr1::function<void ()(zookeeper::GroupProcess*)> >, false, false> ()(std::tr1::shared_pt
r<std::tr1::function<void ()(zookeeper::GroupProcess*)> >, std::tr1::_Mu<std::tr1::_Placeholder<1>, false, true> ()(std::tr1::_Placeholder<1>, std::tr1::tuple<process::ProcessBas
e*>))>::type))(process::ProcessBase*, std::tr1::shared_ptr<std::tr1::function<void ()(zookeeper::GroupProcess*)> >)>::type std::tr1::_Bind<void (*()(std::tr1::_Placeholder<1>, st
d::tr1::shared_ptr<std::tr1::function<void ()(zookeeper::GroupProcess*)> >))(process::ProcessBase*, std::tr1::shared_ptr<std::tr1::function<void ()(zookeeper::GroupProcess*)> >)>
::operator()<process::ProcessBase*>(process::ProcessBase*&) () from /Users/mchadha/venv/lib/python2.7/site-packages/mesos.native-0.22.1003-py2.7-linux-x86_64.egg/mesos/native/_me
sos.so
#18 0x00007f4286cc2480 in std::tr1::_Function_handler<void ()(process::ProcessBase*), std::tr1::_Bind<void (*()(std::tr1::_Placeholder<1>, std::tr1::shared_ptr<std::tr1::function
<void ()(zookeeper::GroupProcess*)> >))(process::ProcessBase*, std::tr1::shared_ptr<std::tr1::function<void ()(zookeeper::GroupProcess*)> >)> >::_M_invoke(std::tr1::_Any_data con
st&, process::ProcessBase*) () from /Users/mchadha/venv/lib/python2.7/site-packages/mesos.native-0.22.1003-py2.7-linux-x86_64.egg/mesos/native/_mesos.so
#19 0x00007f42870db546 in std::tr1::function<void ()(process::ProcessBase*)>::operator()(process::ProcessBase*) const () from /Users/mchadha/venv/lib/python2.7/site-packages/meso
s.native-0.22.1003-py2.7-linux-x86_64.egg/mesos/native/_mesos.so
#20 0x00007f42870c1013 in process::ProcessBase::visit(process::DispatchEvent const&) () from /Users/mchadha/venv/lib/python2.7/site-packages/mesos.native-0.22.1003-py2.7-linux-x8
6_64.egg/mesos/native/_mesos.so
#21 0x00007f42870c5582 in process::DispatchEvent::visit(process::EventVisitor*) const () from /Users/mchadha/venv/lib/python2.7/site-packages/mesos.native-0.22.1003-py2.7-linux-x
86_64.egg/mesos/native/_mesos.so
#22 0x00007f428666680e in process::ProcessBase::serve(process::Event const&) () from /Users/mchadha/venv/lib/python2.7/site-packages/mesos.native-0.22.1003-py2.7-linux-x86_64.egg
/mesos/native/_mesos.so
#23 0x00007f42870bd88f in process::ProcessManager::resume(process::ProcessBase*) () from /Users/mchadha/venv/lib/python2.7/site-packages/mesos.native-0.22.1003-py2.7-linux-x86_64
.egg/mesos/native/_mesos.so
#24 0x00007f42870b1cb9 in process::schedule(void*) () from /Users/mchadha/venv/lib/python2.7/site-packages/mesos.native-0.22.1003-py2.7-linux-x86_64.egg/mesos/native/_mesos.so
#25 0x00000031410079d1 in start_thread () from /lib64/libpthread.so.0
#26 0x00000031408e88fd in clone () from /lib64/libc.so.6
{code}

Solution: 
 Create master detector per url instead of per framework.
Will send the review request. 

",3.0,0.24.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.017543859649122806,0.014285714285714285,0.01282051282051282,0.006060606060606061,0.006060606060606061,0.22027522935779814
Task,Propagate Isolator::prepare() failures to the framework,"Currently, if {{Isolator::prepare}} fails for some isolator(s), we simply return a generic message about container being destroyed during launch.

It would be especially helpful if a third-party isolator modules could report the error back to the framework.",2.0,0,0.0,0.3453996983408748,0.0,0.0,0.0,0.0,0.2,0.38596491228070173,0.35714285714285715,0.28205128205128205,0.29090909090909095,0.28484848484848485,0.0
Bug,MemoryPressureMesosTest.CGROUPS_ROOT_Statistics and CGROUPS_ROOT_SlaveRecovery are flaky,"I am install Mesos 0.24.0 on 4 servers which have very similar hardware and software configurations. 

After performing {{../configure}}, {{make}}, and {{make check}} some servers have completed successfully and other failed on test {{[ RUN      ] MemoryPressureMesosTest.CGROUPS_ROOT_Statistics}}.

Is there something I should check in this test? 

{code}
PERFORMED MAKE CHECK NODE-001
[ RUN      ] MemoryPressureMesosTest.CGROUPS_ROOT_Statistics
I1005 14:37:35.585067 38479 exec.cpp:133] Version: 0.24.0
I1005 14:37:35.593789 38497 exec.cpp:207] Executor registered on slave 20151005-143735-2393768202-35106-27900-S0
Registered executor on svdidac038.techlabs.accenture.com
Starting task 010b2fe9-4eac-4136-8a8a-6ce7665488b0
Forked command at 38510
sh -c 'while true; do dd count=512 bs=1M if=/dev/zero of=./temp; done'


PERFORMED MAKE CHECK NODE-002
[ RUN      ] MemoryPressureMesosTest.CGROUPS_ROOT_Statistics
I1005 14:38:58.794112 36997 exec.cpp:133] Version: 0.24.0
I1005 14:38:58.802851 37022 exec.cpp:207] Executor registered on slave 20151005-143857-2360213770-50427-26325-S0
Registered executor on svdidac039.techlabs.accenture.com
Starting task 9bb317ba-41cb-44a4-b507-d1c85ceabc28
sh -c 'while true; do dd count=512 bs=1M if=/dev/zero of=./temp; done'
Forked command at 37028
../../src/tests/containerizer/memory_pressure_tests.cpp:145: Failure
Expected: (usage.get().mem_medium_pressure_counter()) >= (usage.get().mem_critical_pressure_counter()), actual: 5 vs 6
2015-10-05 14:39:00,130:26325(0x2af08cc78700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:37198] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
[  FAILED  ] MemoryPressureMesosTest.CGROUPS_ROOT_Statistics (4303 ms)
{code}",1.0,"0.24.0,0.26.0",0.5,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.02564102564102564,0.006060606060606061,0.006060606060606061,0.2293577981651376
Task,Add a test module for ip-per-container support,"With the addition of {{NetworkInfo}} to allow frameworks to request IP-per-container for their tasks, we should add a simple module that mimics the behavior of a real network-isolation module for testing purposes. We can then add this module in {{src/examples}} and write some tests against it.

This module can also serve as a template module for third-party network isolation provides for building their own network isolator modules.",3.0,0,0.0,0.3453996983408748,0.0,0.0,0.0,0.0,0.2,0.38596491228070173,0.35714285714285715,0.28205128205128205,0.29090909090909095,0.28484848484848485,0.0
Bug,"rename libprocess tests to ""libprocess-tests""","Stout tests are in a binary named {{stout-tests}}, Mesos tests are in {{mesos-tests}}, but libprocess tests are just {{tests}}. It would be helpful to name them {{libprocess-tests}} ",1.0,0,0.5,0.3453996983408748,0.0,0.0,0.0,0.0,0.2,0.38596491228070173,0.35714285714285715,0.11538461538461538,0.05454545454545455,0.05454545454545455,0.0
Bug,V1 API java/python protos are not generated,"The java/python protos for the V1 api should be generated according to the Makefile; however, they do not show up in the generated build directory.",2.0,0,0.5,0.5701357466063348,0.0,0.0,0.0,0.0,0.0,0.6842105263157894,0.5642857142857143,0.24358974358974358,0.22424242424242424,0.22424242424242424,0.0
Bug,JSON-based credential files do not work correctly,"Specifying the following credentials file:
{code}
{
  “credentials”: [
    {
      “principal”: “user”,
      “secret”: “password”
    }
  ]
}
{code}

Then hitting a master endpoint with:
{code}
curl -i -u “user:password” ...
{code}

Does not work. This is contrary to the text-based credentials file which works:
{code}
user password
{code}

Currently, the password in a JSON-based credentials file needs to be base64-encoded in order for it to work:
{code}
{
  “credentials”: [
    {
      “principal”: “user”,
      “secret”: “cGFzc3dvcmQ=”
    }
  ]
}
{code}",1.0,0,0.5,0.0784313725490196,0.3333333333333333,0.5,0.14285714285714285,0.0,0.0,0.05263157894736842,0.09285714285714286,0.21794871794871795,0.26666666666666666,0.26666666666666666,0.0
Task,Make the Command Scheduler use the HTTP Scheduler Library,We should make the Command Scheduler in {{src/cli/executor.cpp}} use the Scheduler Library {{src/scheduler/scheduler.cpp}} instead of the Scheduler Driver.,3.0,0,0.0,0.5701357466063348,0.0,0.0,0.0,0.0,0.0,0.6842105263157894,0.5642857142857143,0.3846153846153846,0.4181818181818182,0.4121212121212121,0.0
Bug,mesos.cli broken in 0.24.x,"The issue was initially reported on the mailing list: http://www.mail-archive.com/user@mesos.apache.org/msg04670.html

The format of the master data stored in zookeeper has changed but the mesos.cli does not reflect these changes causing tools like {{mesos-tail}} and {{mesos-ps}} to fail.

Example error from {{mesos-tail}}:

{noformat}
mesos-master ~$ mesos tail -f -n 50 service
Traceback (most recent call last):
  File ""/usr/local/bin/mesos-tail"", line 11, in <module>
    sys.exit(main())
  File ""/usr/local/lib/python2.7/dist-packages/mesos/cli/cli.py"", line 61, in 
wrapper
    return fn(*args, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/mesos/cli/cmds/tail.py"", line 
55, in main
    args.task, args.file, fail=(not args.follow)):
  File ""/usr/local/lib/python2.7/dist-packages/mesos/cli/cluster.py"", line 27, 
in files
    tlist = MASTER.tasks(fltr)
  File ""/usr/local/lib/python2.7/dist-packages/mesos/cli/master.py"", line 174, 
in tasks
    self._task_list(active_only))))
  File ""/usr/local/lib/python2.7/dist-packages/mesos/cli/master.py"", line 153, 
in _task_list
    *[util.merge(x, *keys) for x in self.frameworks(active_only)])
  File ""/usr/local/lib/python2.7/dist-packages/mesos/cli/master.py"", line 185, 
in frameworks
    return util.merge(self.state, *keys)
  File ""/usr/local/lib/python2.7/dist-packages/mesos/cli/util.py"", line 58, in 
__get__
    value = self.fget(inst)
  File ""/usr/local/lib/python2.7/dist-packages/mesos/cli/master.py"", line 123, 
in state
    return self.fetch(""/master/state.json"").json()
  File ""/usr/local/lib/python2.7/dist-packages/mesos/cli/master.py"", line 64, 
in fetch
    return requests.get(urlparse.urljoin(self.host, url), **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/requests/api.py"", line 69, in get
    return request('get', url, params=params, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/requests/api.py"", line 50, in 
request
    response = session.request(method=method, url=url, **kwargs)
  File ""/usr/local/lib/python2.7/dist-packages/requests/sessions.py"", line 451, 
in request
    prep = self.prepare_request(req)
  File ""/usr/local/lib/python2.7/dist-packages/requests/sessions.py"", line 382, 
in prepare_request
    hooks=merge_hooks(request.hooks, self.hooks),
  File ""/usr/local/lib/python2.7/dist-packages/requests/models.py"", line 304, 
in prepare
    self.prepare_url(url, params)
  File ""/usr/local/lib/python2.7/dist-packages/requests/models.py"", line 357, 
in prepare_url
    raise InvalidURL(*e.args)
requests.exceptions.InvalidURL: Failed to parse: 
10.100.1.100:5050"",""port"":5050,""version"":""0.24.1""}
{noformat}

The problem exists in https://github.com/mesosphere/mesos-cli/blob/master/mesos/cli/master.py#L107. The code should be along the lines of:

{noformat}
            try:
                parsed =  json.loads(val)
                return parsed[""address""][""ip""] + "":"" + str(parsed[""address""][""port""])
            except Exception:
                return val.split(""@"")[-1]
{noformat}

This causes the master address to come back correctly.",1.0,"0.24.0,0.24.1",0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.15789473684210525,0.20714285714285713,0.01282051282051282,0.0,0.0,0.2202293577981651
Improvement,Allocator changes trigger large re-compiles,"Due to the templatized nature of the allocator, even small changes trigger large recompiles of the code-base. This make iterating on changes expensive for developers.",3.0,0,0.0,0.42533936651583715,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3142857142857143,0.24358974358974358,0.22424242424242424,0.22424242424242424,0.0
Bug,LIBPROCESS_IP not passed when executor's environment is specified,"When the executor's environment is specified explicitly via {{\-\-executor_environment_variables}}, {{LIBPROCESS_IP}} will not be passed, leading to errors in some cases - for example, when no DNS is available.",2.0,0.24.1,0.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.22027522935779814
Task,CHECK failure due to floating point precision on reservation request,"result.cpus() == cpus() check is failing due to ( double == double ) comparison problem. 


Root Cause : 

Framework requested 0.1 cpu reservation for the first task. So far so good. Next Reserve operation — lead to double operations resulting in following double values :

 results.cpus() : 23.9999999999999964472863211995 cpus() : 24

And the check ( result.cpus() == cpus() ) failed. 

 The double arithmetic operations caused results.cpus() value to be :  23.9999999999999964472863211995 and hence ( 23.9999999999999964472863211995 == 24 ) failed.


",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.017543859649122806,0.014285714285714285,0.01282051282051282,0.006060606060606061,0.006060606060606061,0.0
Bug,Libevent termination triggers Broken Pipe,"When the libevent loop terminates and we unblock the {{SIGPIPE}} signal, the pending {{SIGPIPE}} instantly triggers and causes a broken pipe when the test binary stops running.
{code}
Program received signal SIGPIPE, Broken pipe.
[Switching to Thread 0x7ffff18b4700 (LWP 16270)]
pthread_sigmask (how=1, newmask=<optimized out>, oldmask=0x7ffff18b3d80) at ../sysdeps/unix/sysv/linux/pthread_sigmask.c:53
53	../sysdeps/unix/sysv/linux/pthread_sigmask.c: No such file or directory.
(gdb) bt
#0  pthread_sigmask (how=1, newmask=<optimized out>, oldmask=0x7ffff18b3d80) at ../sysdeps/unix/sysv/linux/pthread_sigmask.c:53
#1  0x00000000006fd9a4 in unblock () at ../../../3rdparty/libprocess/3rdparty/stout/include/stout/os/posix/signals.hpp:90
#2  0x00000000007d7915 in run () at ../../../3rdparty/libprocess/src/libevent.cpp:125
#3  0x00000000007950cb in _M_invoke<>(void) () at /usr/include/c++/4.9/functional:1700
#4  0x0000000000795000 in operator() () at /usr/include/c++/4.9/functional:1688
#5  0x0000000000794f6e in _M_run () at /usr/include/c++/4.9/thread:115
#6  0x00007ffff668de30 in ?? () from /usr/lib/x86_64-linux-gnu/libstdc++.so.6
#7  0x00007ffff79a16aa in start_thread (arg=0x7ffff18b4700) at pthread_create.c:333
#8  0x00007ffff5df1eed in clone () at ../sysdeps/unix/sysv/linux/x86_64/clone.S:109
{code}",2.0,0,0.0,0.42533936651583715,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3142857142857143,0.24358974358974358,0.22424242424242424,0.22424242424242424,0.0
Task,Figure out how to enforce 64-bit builds on Windows.,"We need to make sure people don't try to compile Mesos on 32-bit architectures. We don't want a Windows repeat of something like this:

https://issues.apache.org/jira/browse/MESOS-267",3.0,0,0.0,0.44494720965309204,0.0,0.0,0.0,0.0,0.0,0.14035087719298245,0.15,0.14102564102564102,0.1515151515151515,0.16969696969696968,0.0
Bug,Cgroups Test Filters aborts tests on Centos 6.6 ,"Running make check on centos 6.6 causes all tests to abort due to CHECK_SOME test in CgroupsFIlter:

{code}
Build directory: /home/jenkins/workspace/mesos-config-centos6/build
F0923 23:00:49.748896 27362 environment.cpp:132] CHECK_SOME(hierarchies_): Failed to determine canonical path of /sys/fs/cgroup/freezer: No such file or directory 
*** Check failure stack trace: ***
    @     0x7fb786ca0c4d  google::LogMessage::Fail()
    @     0x7fb786ca298c  google::LogMessage::SendToLog()
    @     0x7fb786ca083c  google::LogMessage::Flush()
    @     0x7fb786ca3289  google::LogMessageFatal::~LogMessageFatal()
    @           0x58e66c  mesos::internal::tests::CgroupsFilter::CgroupsFilter()
    @           0x58712f  mesos::internal::tests::Environment::Environment()
    @           0x4c882f  main
    @     0x7fb782767d5d  __libc_start_main
    @           0x4d6331  (unknown)
make[3]: *** [check-local] Aborted
{code}",1.0,0,0.0,0.06334841628959276,0.0,0.0,0.0,0.0,0.0,0.2982456140350877,0.19999999999999998,0.21794871794871795,0.1515151515151515,0.1515151515151515,0.0
Bug,Don't retry close() on EINTR.,"On Linux, retrying close on EINTR is dangerous because the fd is already released and we may accidentally close a newly opened fd (from another thread), see:

http://ewontfix.com/4/
http://lwn.net/Articles/576478/
http://lwn.net/Articles/576591/

It appears that other OSes, like HPUX, require a retry of close on EINTR. The Austin Group recently proposed changes to POSIX to require that the EINTR case need a retry, but EINPROGRESS be used for when a retry should not occur:

http://austingroupbugs.net/view.php?id=529

However, Linux does not follow this and so we need to remove our EINTR retries.

Some more links for posterity:

https://github.com/wahern/cqueues/issues/56#issuecomment-108656004
https://code.google.com/p/chromium/issues/detail?id=269623
https://codereview.chromium.org/23455051/
",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.0
Bug,Build instructions for CentOS 6.6 should include `sudo yum update`,Neglecting to run {{sudo yum update}} on CentOS 6.6 currently causes the build to break when building {{mesos-0.25.0.jar}}. The build instructions for this platform on the Getting Started page should be changed accordingly.,1.0,0.25.0,0.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.2293577981651376
Bug,configure cannot find libevent headers in CentOS 6,"If libevent is installed via {{sudo yum install libevent-headers}}, running {{../configure --enable-libevent}} will fail to discover the libevent headers:

{code}
checking event2/event.h usability... no
checking event2/event.h presence... no
checking for event2/event.h... no
configure: error: cannot find libevent headers
-------------------------------------------------------------------
libevent is required for libprocess to build.
-------------------------------------------------------------------
{code}",2.0,0.25.0,0.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.2293577981651376
Task,Add implementation for sha256 based file content verification.,https://reviews.apache.org/r/38747/,3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3357142857142857,0.14102564102564102,0.2424242424242424,0.2424242424242424,0.0
Task,Create interface for digest verifier,"Add interface for digest verifier so that we can add implementations for digest types like sha256, sha512 etc",2.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3357142857142857,0.14102564102564102,0.2424242424242424,0.2424242424242424,0.0
Task,Expose maintenance user doc via the documentation home page,"The committed docs can be found here:
http://mesos.apache.org/documentation/latest/maintenance/

We need to add a link to {{docs/home.md}}
Also, the doc needs some minor formatting tweaks.",1.0,0,0.0,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.0
Task,Enable ubuntu builds in ASF CI,"I've disabled ubuntu:14.04 builds on ASF CI because the job randomly fails on fetching packages.

{code}
Get:406 http://archive.ubuntu.com/ubuntu/ trusty-updates/main gdisk amd64 0.8.8-1ubuntu0.1 [185 kB]
Err http://archive.ubuntu.com/ubuntu/ trusty-security/main libldap-2.4-2 amd64 2.4.31-1+nmu2ubuntu8.1
  404  Not Found [IP: 91.189.91.15 80]
Err http://archive.ubuntu.com/ubuntu/ trusty-security/main libfreetype6 amd64 2.5.2-1ubuntu2.4
  404  Not Found [IP: 91.189.91.15 80]
Err http://archive.ubuntu.com/ubuntu/ trusty-security/main libicu52 amd64 52.1-3ubuntu0.3
  404  Not Found [IP: 91.189.91.15 80]
Fetched 213 MB in 1min 57s (1812 kB/s)
 [91mE [0m [91m: Failed to fetch http://archive.ubuntu.com/ubuntu/pool/main/o/openldap/libldap-2.4-2_2.4.31-1+nmu2ubuntu8.1_amd64.deb  404  Not Found [IP: 91.189.91.15 80]

E: Failed to fetch http://archive.ubuntu.com/ubuntu/pool/main/f/freetype/libfreetype6_2.5.2-1ubuntu2.4_amd64.deb  404  Not Found [IP: 91.189.91.15 80]

E: Failed to fetch http://archive.ubuntu.com/ubuntu/pool/main/i/icu/libicu52_52.1-3ubuntu0.3_amd64.deb  404  Not Found [IP: 91.189.91.15 80]

E: Failed to fetch http://archive.ubuntu.com/ubuntu/pool/main/g/gvfs/gvfs-common_1.20.3-0ubuntu1.1_all.deb  404  Not Found [IP: 91.189.91.15 80]

E: Failed to fetch http://archive.ubuntu.com/ubuntu/pool/main/g/gvfs/gvfs-libs_1.20.3-0ubuntu1.1_amd64.deb  404  Not Found [IP: 91.189.91.15 80]

E: Failed to fetch http://archive.ubuntu.com/ubuntu/pool/main/g/gvfs/gvfs-daemons_1.20.3-0ubuntu1.1_amd64.deb  404  Not Found [IP: 91.189.91.15 80]

E: Failed to fetch http://archive.ubuntu.com/ubuntu/pool/main/g/gvfs/gvfs_1.20.3-0ubuntu1.1_amd64.deb  404  Not Found [IP: 91.189.91.15 80]

E: Unable to fetch some archives, maybe run apt-get update or try with --fix-missing?
 [0mThe command '/bin/sh -c apt-get -y install build-essential clang git maven autoconf libtool' returned a non-zero code: 100
{code}

We need to figure out what the problem is and fix it before enabling testing on ubuntu.",1.0,0,0.5,0.0784313725490196,0.3333333333333333,0.5,0.14285714285714285,0.0,0.0,0.05263157894736842,0.09285714285714286,0.8846153846153846,0.9878787878787878,0.9878787878787878,0.0
Bug,Mesos UI fails to represent JSON entities,"The Mesos UI is broken, it seems to fail to represent JSON from /state.
This may have been introduced with https://reviews.apache.org/r/38028 ",1.0,0,0.5,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.038461538461538464,0.048484848484848485,0.048484848484848485,0.0
Bug,Add support for exposing Accept/Decline responses for inverse offers,"Current implementation of maintenance primitives does not support exposing Accept/Decline responses of frameworks to the cluster operators. 

This functionality is necessary to provide visibility to operators into whether a given framework is ready to comply with the posted maintenance schedule.",2.0,0,0.5,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.11538461538461538,0.11515151515151516,0.11515151515151516,0.0
Task,Refactor Executor struct in Slave to handle HTTP based executors,"Currently, the {{struct Executor}} in slave only supports executors connected via message passing (driver). We should refactor it to add support for HTTP based Executors similar to what was done for the Scheduler API {{struct Framework}} in {{src/master/master.hpp}}",3.0,0,0.0,0.5701357466063348,0.0,0.0,0.0,0.0,0.0,0.6842105263157894,0.5642857142857143,0.3846153846153846,0.4181818181818182,0.4121212121212121,0.0
Bug,RegistryTokenTest.ExpiredToken test is flaky,"RegistryTokenTest.ExpiredToken test is flaky. Here is the error I got on OSX after running it for several times:

{noformat}
[ RUN      ] RegistryTokenTest.ExpiredToken
../../src/tests/containerizer/provisioner_docker_tests.cpp:167: Failure
Value of: token.isError()
  Actual: false
Expected: true
libc++abi.dylib: terminating with uncaught exception of type testing::internal::GoogleTestFailureException: ../../src/tests/containerizer/provisioner_docker_tests.cpp:167: Failure
Value of: token.isError()
  Actual: false
Expected: true
*** Aborted at 1442708631 (unix time) try ""date -d @1442708631"" if you are using GNU date ***
PC: @     0x7fff925fd286 __pthread_kill
*** SIGABRT (@0x7fff925fd286) received by PID 7082 (TID 0x7fff7d7ad300) stack trace: ***
    @     0x7fff9041af1a _sigtramp
    @     0x7fff59759968 (unknown)
    @     0x7fff9bb429b3 abort
    @     0x7fff90ce1a21 abort_message
    @     0x7fff90d099b9 default_terminate_handler()
    @     0x7fff994767eb _objc_terminate()
    @     0x7fff90d070a1 std::__terminate()
    @     0x7fff90d06d48 __cxa_rethrow
    @        0x10781bb16 testing::internal::HandleExceptionsInMethodIfSupported<>()
    @        0x1077e9d30 testing::UnitTest::Run()
    @        0x106d59a91 RUN_ALL_TESTS()
    @        0x106d55d47 main
    @     0x7fff8fc395c9 start
    @                0x3 (unknown)
Abort trap: 6
~/src/mesos/build ((3ee82e3...)) $
{noformat}",3.0,0,0.5,1.0,0.0,0.0,0.0,0.0,0.8,0.5438596491228069,0.39999999999999997,0.11538461538461538,0.11515151515151516,0.11515151515151516,0.0
Task,Add metrics for filesystem isolation and image provisioning.,"We need to know about:

1) Errors encountered while provisioning root filesystems
2) Errors encountered while cleaning up root filesystems
3) Number of containers changing root filesystem
...",2.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.15789473684210525,0.15,0.7948717948717948,1.0,1.0,0.0
Task,Change /machine/up and /machine/down endpoints to take an array,"With [MESOS-3312] committed, the {{/machine/up}} and {{/machine/down}} endpoints should also take an input as an array.

It is important to change this before maintenance primitives are released:
https://reviews.apache.org/r/38011/

Also, a minor change to the error message from these endpoints:
https://reviews.apache.org/r/37969/",1.0,0,0.0,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.0
Bug,Segfault when accepting or declining inverse offers,"Discovered while writing a test for filters (in regards to inverse offers).

Fix here: https://reviews.apache.org/r/38470/",1.0,0,0.0,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.0
Improvement,Add flag to disable hostname lookup,"In testing / buildinging DCOS we've found that we need to set --hostname explicitly on the masters. For our uses IP and `hostname` must always be the same thing. 

More in general, under certain circumstances, dynamic lookup of {{hostname}}, while successful, provides undesirable results; we would also like, in those circumstances, be able to just set the hostname to the chosen
IP address (possibly set via the {{\-\- ip_discovery_command}} method).

We suggest adding a {{\-\-no-hostname-lookup}}. 
Note that we can introduce this flag as {{--hostname-lookup}} with a default to 'true' (which is the current semantics) and that way someone can do {{\-\-no-hostname-lookup}} or {{\-\-hostname-lookup=false}}.
",3.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.15789473684210525,0.20714285714285713,0.08974358974358974,0.0787878787878788,0.17575757575757575,0.0
Task,Windows: Port protobuf_tests.hpp,"We have ported `stout/protobuf.hpp`, but to make the `protobuf_tests.cpp` file to work, we need to port `stout/uuid.hpp`.",2.0,0,0.0,0.44494720965309204,0.0,0.0,0.0,0.0,0.0,0.14035087719298245,0.15,0.14102564102564102,0.1515151515151515,0.16969696969696968,0.0
Task,Unmount irrelevant host mounts in the new container's mount namespace.,"As described in this [TODO|https://github.com/apache/mesos/blob/e601e469c64594dd8339352af405cbf26a574ea8/src/slave/containerizer/isolators/filesystem/linux.cpp#L418]:
{noformat:title=}
  // TODO(jieyu): Try to unmount work directory mounts and persistent
  // volume mounts for other containers to release the extra
  // references to those mounts.
{noformat}

This will a best effort attempt to alleviate the race condition between provisioner's container cleanup and new containers copying host mount table.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15789473684210525,0.15,0.20512820512820512,0.13939393939393938,0.13939393939393938,0.0
Bug,process::collect and process::await do not perform discard propagation.,"When aggregating futures with collect, one may discard the outer future:

{code}
Promise<int> p1;
Promise<string> p2;

Future<int, string> collect = process::collect(p1.future(), p2.future());

collect.discard();

// collect will transition to DISCARDED

// However, p{1,2}.future().hasDiscard() remains false
// as there is no discard propagation!
{code}

Discard requests should propagate down into the inner futures being collected.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.0
Bug,Perf event isolator stops performing sampling if a single timeout occurs.,"Currently the perf event isolator times out a sample after a fixed extra time of 2 seconds on top of the sample time elapses:

{code}
    Duration timeout = flags.perf_duration + Seconds(2);
{code}

This should be based on the reap interval maximum.

Also, the code stops sampling altogether when a single timeout occurs. We've observed time outs during normal operation, so it would be better for the isolator to continue performing perf sampling in the case of timeouts. It may also make sense to continue sampling in the case of errors, since these may be transient.",3.0,0.24.0,0.5,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.8846153846153846,0.9878787878787878,0.9878787878787878,0.2201834862385321
Improvement,Factor out V1 API test helper functions,"We currently have some helper functionality for V1 API tests. This is copied in a few test files.
Factor this out into a common place once the API is stabilized.
{code}
// Helper class for using EXPECT_CALL since the Mesos scheduler API
  // is callback based.
  class Callbacks
  {
  public:
    MOCK_METHOD0(connected, void(void));
    MOCK_METHOD0(disconnected, void(void));
    MOCK_METHOD1(received, void(const std::queue<Event>&));
  };
{code}
{code}
// Enqueues all received events into a libprocess queue.
// TODO(jmlvanre): Factor this common code out of tests into V1
// helper.
ACTION_P(Enqueue, queue)
{
  std::queue<Event> events = arg0;
  while (!events.empty()) {
    // Note that we currently drop HEARTBEATs because most of these tests
    // are not designed to deal with heartbeats.
    // TODO(vinod): Implement DROP_HTTP_CALLS that can filter heartbeats.
    if (events.front().type() == Event::HEARTBEAT) {
      VLOG(1) << ""Ignoring HEARTBEAT event"";
    } else {
      queue->put(events.front());
    }
    events.pop();
  }
}
{code}

We can also update the helpers in {{/tests/mesos.hpp}} to support the V1 API.  This would let us get ride of lines like:
{code}
v1::TaskInfo taskInfo = evolve(createTask(devolve(offer), """", DEFAULT_EXECUTOR_ID));
{code}
In favor of:
{code}
v1::TaskInfo taskInfo = createTask(offer, """", DEFAULT_EXECUTOR_ID);
{code}",2.0,0,0.5,0.3137254901960784,0.0,0.0,0.0,0.0,0.4,0.19298245614035087,0.14285714285714285,0.24358974358974358,0.22424242424242424,0.22424242424242424,0.0
Improvement,Log source address replicated log recieved broadcasts,"Currently Mesos doesn't log what machine a replicated log status broadcast was recieved from:
{code}
Sep 11 21:41:14 master-01 mesos-master[15625]: I0911 21:41:14.320164 15637 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request
Sep 11 21:41:14 master-01 mesos-dns[15583]: I0911 21:41:14.321097   15583 detect.go:118] ignoring children-changed event, leader has not changed: /mesos
Sep 11 21:41:14 master-01 mesos-master[15625]: I0911 21:41:14.353914 15639 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request
Sep 11 21:41:14 master-01 mesos-master[15625]: I0911 21:41:14.479132 15639 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request
{code}

It would be really useful for debugging replicated log startup issues to have info about where the message came from (libprocess address, ip, or hostname) the message came from",2.0,"0.23.0,0.24.0",0.5,1.0,0.0,0.0,0.0,0.0,0.8,0.5438596491228069,0.39999999999999997,0.08974358974358974,0.0787878787878788,0.07272727272727272,0.21559633027522934
Bug,mesos-execute does not support credentials,mesos-execute does not appear to support passing credentials. This makes it impossible to use on a cluster where framework authentication is required.,2.0,0,0.5,0.0030165912518853697,0.0,0.0,0.0,0.0,0.0,0.017543859649122806,0.007142857142857143,0.01282051282051282,0.0,0.0,0.0
Task,Remove unused executor protobuf,"The executor protobuf definition living outside the v1/ directory is unused, it should  be removed to avoid confusion.",1.0,1.0.0,0.0,0.0784313725490196,0.3333333333333333,0.5,0.14285714285714285,0.0,0.0,0.05263157894736842,0.09285714285714286,0.038461538461538464,0.048484848484848485,0.048484848484848485,0.9174311926605504
Documentation,Document a test pattern for expediting event firing,We use {{Clock::advance()}} extensively in tests to expedite event firing and minimize overall {{make check}} time. Document this pattern for posterity.,3.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.717948717948718,0.8242424242424242,0.09696969696969697,0.0
Task,Add executor protobuf to v1,"A new protobuf for Executor was introduced in Mesos for the HTTP API, it needs to be added to /v1 so it reflects changes made on v1/mesos.proto. This protobuf is ought to be changed as the executor HTTP API design evolves.",1.0,1.0.0,0.0,0.0784313725490196,0.3333333333333333,0.5,0.14285714285714285,0.0,0.0,0.05263157894736842,0.09285714285714286,0.038461538461538464,0.048484848484848485,0.048484848484848485,0.9174311926605504
Task,Add device support in cgroups abstraction,"Add support for [device cgroups|https://www.kernel.org/doc/Documentation/cgroup-v1/devices.txt] to aid isolators controlling access to devices.

In the future, we could think about how to numerate and control access to devices as resource or task/container policy",3.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.02857142857142857,0.01282051282051282,0.06666666666666667,0.06666666666666667,0.0
Improvement,Allow resources/attributes discovery,"In heterogeneous clusters, tasks sometimes have strong constraints on the type of hardware they need to execute on. The current solution is to use custom resources and attributes on the agents. Detecting non-standard resources/attributes requires wrapping the ""mesos-slave"" binary behind a script and use custom code to probe the agent. Unfortunately, this approach doesn't allow composition. The solution would be to provide a hook/module mechanism to allow users to use custom code performing resources/attributes discovery.

Please review the detailed document below:
https://docs.google.com/document/d/15OkebDezFxzeyLsyQoU0upB0eoVECAlzEkeg0HQAX9w

Feel free to express comments/concerns by annotating the document or by replying to this issue.
",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.006060606060606061,0.006060606060606061,0.0
Documentation,Update quota design doc based on user comments and offline syncs,"We got plenty of feedback from different parties, which we would like to persist in the design doc for posterity.",3.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.717948717948718,0.8242424242424242,0.09696969696969697,0.0
Improvement,Command-line flags should take precedence over OS Env variables,"Currently, it appears that re-defining a flag on the command-line that was already defined via a OS Env var ({{MESOS_*}}) causes the Master to fail with a not very helpful message.

For example, if one has {{MESOS_QUORUM}} defined, this happens:
{noformat}
$ ./mesos-master --zk=zk://192.168.1.4/mesos --quorum=1 --hostname=192.168.1.4 --ip=192.168.1.4
Duplicate flag 'quorum' on command line
{noformat}

which is not very helpful.

Ideally, we would parse the flags with a ""well-known"" priority (command-line first, environment last) - but at the very least, the error message should be more helpful in explaining what the issue is.",2.0,0.24.0,0.5,0.03469079939668175,0.0,0.0,0.0,0.0,0.0,0.07017543859649122,0.04285714285714286,0.1794871794871795,0.15757575757575756,0.17575757575757575,0.2201834862385321
Task,Implement filtering mechanism for (Scheduler API Events) Testing,"Currently, our testing infrastructure does not have a mechanism of filtering/dropping HTTP events of a particular type from the Scheduler API response stream.  We need a {{DROP_HTTP_CALLS}} abstraction that can help us to filter a particular event type.

{code}
// Enqueues all received events into a libprocess queue.
ACTION_P(Enqueue, queue)
{
  std::queue<Event> events = arg0;
  while (!events.empty()) {
    // Note that we currently drop HEARTBEATs because most of these tests
    // are not designed to deal with heartbeats.
    // TODO(vinod): Implement DROP_HTTP_CALLS that can filter heartbeats.
    if (events.front().type() == Event::HEARTBEAT) {
      VLOG(1) << ""Ignoring HEARTBEAT event"";
    } else {
      queue->put(events.front());
    }
    events.pop();
  }
}
{code}

This helper code is duplicated in at least two places currently, Scheduler Library/Maintenance Primitives tests. 
- The solution can be as trivial as moving this helper function to a common test-header.
- Implement a {{DROP_HTTP_CALLS}} similar to what we do for other protobufs via {{DROP_CALLS}}.",3.0,0,0.0,0.5701357466063348,0.0,0.0,0.0,0.0,0.0,0.6842105263157894,0.5642857142857143,0.3846153846153846,0.4181818181818182,0.4121212121212121,0.0
Bug,Dynamic reservations are not counted as used resources in the master,"Dynamically reserved resources should be considered used or allocated and hence reflected in Mesos bookkeeping structures and {{state.json}}.

I expanded the {{ReservationTest.ReserveThenUnreserve}} test with the following section:
{code}
  // Check that the Master counts the reservation as a used resource.
  {
    Future<process::http::Response> response =
      process::http::get(master.get(), ""state.json"");
    AWAIT_READY(response);

    Try<JSON::Object> parse = JSON::parse<JSON::Object>(response.get().body);
    ASSERT_SOME(parse);

    Result<JSON::Number> cpus =
      parse.get().find<JSON::Number>(""slaves[0].used_resources.cpus"");

    ASSERT_SOME_EQ(JSON::Number(1), cpus);
  }
{code}
and got
{noformat}
../../../src/tests/reservation_tests.cpp:168: Failure
Value of: (cpus).get()
  Actual: 0
Expected: JSON::Number(1)
Which is: 1
{noformat}

Idea for new resources states: https://docs.google.com/drawings/d/1aquVIqPY8D_MR-cQjZu-wz5nNn3cYP3jXqegUHl-Kzc/edit",3.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.717948717948718,0.8242424242424242,0.09696969696969697,0.0
Task,Refactored libprocess SSL tests., Refactor SSL test fixture to be available for reuse by other projects. Currently the fixture class and its the symbols it depends on are not present in libproces's include files.,3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3357142857142857,0.14102564102564102,0.2424242424242424,0.2424242424242424,0.0
Bug,Make use of C++11 atomics,"Now that we require C++11, we can make use of std::atomic. For example:

* libprocess/process.cpp uses a bare int + __sync_synchronize() for ""running""
* __sync_synchronize() is used in logging.hpp in libprocess and fork.hpp in stout
* sched/sched.cpp uses a volatile int for ""running"" -- this is wrong, ""volatile"" is not sufficient to ensure safe concurrent access
* ""volatile"" is used in a few other places -- most are probably dubious but I haven't looked closely",2.0,0,0.0,1.0,0.0,0.0,0.0,0.0,0.8,0.5438596491228069,0.39999999999999997,0.48717948717948717,0.4303030303030303,0.4303030303030303,0.0
Bug,Spurious fetcher message about extracting an archive,"The fetcher emits a spurious log message about not extracting an archive with "".tgz"" extension, even though the tarball is extracted correctly.

{code}
I0826 19:02:08.304914  2109 logging.cpp:172] INFO level logging started!
I0826 19:02:08.305253  2109 fetcher.cpp:413] Fetcher Info: {""cache_directory"":""\/tmp\/mesos\/fetch\/slaves\/20150826-185716-251662764-5050-1-S0\/root"",""items"":[{""action"":""BYPASS_CACHE"",""uri"":{""extract"":true,""value"":""file:\/\/\/mesos\/sampleflaskapp.tgz""}}],""sandbox_directory"":""\/tmp\/mesos\/slaves\/20150826-185716-251662764-5050-1-S0\/frameworks\/20150826-185716-251662764-5050-1-0000\/executors\/sample-flask-app.f222d202-4c24-11e5-a628-0242ac110011\/runs\/e71f50b8-816d-46d5-bcc6-f9850a0402ed"",""user"":""root""}
I0826 19:02:08.306834  2109 fetcher.cpp:368] Fetching URI 'file:///mesos/sampleflaskapp.tgz'
I0826 19:02:08.306864  2109 fetcher.cpp:242] Fetching directly into the sandbox directory
I0826 19:02:08.306884  2109 fetcher.cpp:179] Fetching URI 'file:///mesos/sampleflaskapp.tgz'
I0826 19:02:08.306900  2109 fetcher.cpp:159] Copying resource with command:cp '/mesos/sampleflaskapp.tgz' '/tmp/mesos/slaves/20150826-185716-251662764-5050-1-S0/frameworks/20150826-185716-251662764-5050-1-0000/executors/sample-flask-app.f222d202-4c24-11e5-a628-0242ac110011/runs/e71f50b8-816d-46d5-bcc6-f9850a0402ed/sampleflaskapp.tgz'
I0826 19:02:08.309063  2109 fetcher.cpp:76] Extracting with command: tar -C '/tmp/mesos/slaves/20150826-185716-251662764-5050-1-S0/frameworks/20150826-185716-251662764-5050-1-0000/executors/sample-flask-app.f222d202-4c24-11e5-a628-0242ac110011/runs/e71f50b8-816d-46d5-bcc6-f9850a0402ed' -xf '/tmp/mesos/slaves/20150826-185716-251662764-5050-1-S0/frameworks/20150826-185716-251662764-5050-1-0000/executors/sample-flask-app.f222d202-4c24-11e5-a628-0242ac110011/runs/e71f50b8-816d-46d5-bcc6-f9850a0402ed/sampleflaskapp.tgz'
I0826 19:02:08.315313  2109 fetcher.cpp:84] Extracted '/tmp/mesos/slaves/20150826-185716-251662764-5050-1-S0/frameworks/20150826-185716-251662764-5050-1-0000/executors/sample-flask-app.f222d202-4c24-11e5-a628-0242ac110011/runs/e71f50b8-816d-46d5-bcc6-f9850a0402ed/sampleflaskapp.tgz' into '/tmp/mesos/slaves/20150826-185716-251662764-5050-1-S0/frameworks/20150826-185716-251662764-5050-1-0000/executors/sample-flask-app.f222d202-4c24-11e5-a628-0242ac110011/runs/e71f50b8-816d-46d5-bcc6-f9850a0402ed'
W0826 19:02:08.315381  2109 fetcher.cpp:264] Copying instead of extracting resource from URI with 'extract' flag, because it does not seem to be an archive: file:///mesos/sampleflaskapp.tgz
I0826 19:02:08.315604  2109 fetcher.cpp:445] Fetched 'file:///mesos/sampleflaskapp.tgz' to '/tmp/mesos/slaves/20150826-185716-251662764-5050-1-S0/frameworks/20150826-185716-251662764-5050-1-0000/executors/sample-flask-app.f222d202-4c24-11e5-a628-0242ac110011/runs/e71f50b8-816d-46d5-bcc6-f9850a0402ed/sampleflaskapp.tgz'
{code}",1.0,0,0.5,0.06184012066365008,0.0,0.0,0.0,0.0,0.2,0.10526315789473684,0.12857142857142856,0.28205128205128205,0.29090909090909095,0.28484848484848485,0.0
Bug,Mesos will not build when configured with gperftools enabled,"Mesos configured with {{--enable-perftools}} currently will not build on OSX 10.10.4 or Ubuntu 14.04, possibly because the bundled gperftools-2.0 is not current. The stable release is now 2.4, which builds successfully on both of these platforms.

This issue is resolved when Mesos will build successfully out of the box with gperftools enabled. After this ticket is resolved, the libprocess profiler should be tested to confirm that it still works and if not, it should be fixed.",2.0,0,0.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Improvement,Factor out JSON to repeated protobuf conversion,"In general, we have the collection of protobuf messages as another protobuf message, which makes JSON -> protobuf conversion straightforward. This is not always the case, for example, {{Resources}} class is not a protobuf, though protobuf-convertible.

To facilitate conversions like JSON -> {{Resources}} and avoid writing code for each particular case, we propose to introduce {{JSON::Array}} -> {{repeated protobuf}} conversion. With this in place, {{JSON::Array}} -> {{Resources}} boils down to {{JSON::Array}} -> {{repeated Resource}} -> (extra c-tor call) -> {{Resources}}.",2.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.717948717948718,0.8242424242424242,0.09696969696969697,0.0
Bug,SlaveTest.HTTPSchedulerSlaveRestart,"Observed on ASF CI

{code}
[ RUN      ] SlaveTest.HTTPSchedulerSlaveRestart
Using temporary directory '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_CXyDrA'
I0825 22:07:36.809872 27610 leveldb.cpp:176] Opened db in 3.751801ms
I0825 22:07:36.811115 27610 leveldb.cpp:183] Compacted db in 1.2194ms
I0825 22:07:36.811175 27610 leveldb.cpp:198] Created db iterator in 30669ns
I0825 22:07:36.811197 27610 leveldb.cpp:204] Seeked to beginning of db in 7829ns
I0825 22:07:36.811208 27610 leveldb.cpp:273] Iterated through 0 keys in the db in 6017ns
I0825 22:07:36.811245 27610 replica.cpp:744] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0825 22:07:36.811722 27638 recover.cpp:449] Starting replica recovery
I0825 22:07:36.811980 27638 recover.cpp:475] Replica is in EMPTY status
I0825 22:07:36.813033 27641 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request
I0825 22:07:36.813355 27635 recover.cpp:195] Received a recover response from a replica in EMPTY status
I0825 22:07:36.813756 27628 recover.cpp:566] Updating replica status to STARTING
I0825 22:07:36.814434 27636 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 570160ns
I0825 22:07:36.814471 27636 replica.cpp:323] Persisted replica status to STARTING
I0825 22:07:36.814743 27642 recover.cpp:475] Replica is in STARTING status
I0825 22:07:36.814965 27638 master.cpp:378] Master 20150825-220736-234885548-51219-27610 (09c6504e3a31) started on 172.17.0.14:51219
I0825 22:07:36.814999 27638 master.cpp:380] Flags at startup: --acls="""" --allocation_interval=""1secs"" --allocator=""HierarchicalDRF"" --authenticate=""true"" --authenticate_slaves=""true"" --authenticators=""crammd5"" --authorizers=""local"" --credentials=""/tmp/SlaveTest_HTTPSchedulerSlaveRestart_CXyDrA/credentials"" --framework_sorter=""drf"" --help=""false"" --initialize_driver_logging=""true"" --log_auto_initialize=""true"" --logbufsecs=""0"" --logging_level=""INFO"" --max_slave_ping_timeouts=""5"" --quiet=""false"" --recovery_slave_removal_limit=""100%"" --registry=""replicated_log"" --registry_fetch_timeout=""1mins"" --registry_store_timeout=""25secs"" --registry_strict=""true"" --root_submissions=""true"" --slave_ping_timeout=""15secs"" --slave_reregister_timeout=""10mins"" --user_sorter=""drf"" --version=""false"" --webui_dir=""/mesos/mesos-0.25.0/_inst/share/mesos/webui"" --work_dir=""/tmp/SlaveTest_HTTPSchedulerSlaveRestart_CXyDrA/master"" --zk_session_timeout=""10secs""
I0825 22:07:36.815347 27638 master.cpp:425] Master only allowing authenticated frameworks to register
I0825 22:07:36.815371 27638 master.cpp:430] Master only allowing authenticated slaves to register
I0825 22:07:36.815402 27638 credentials.hpp:37] Loading credentials for authentication from '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_CXyDrA/credentials'
I0825 22:07:36.815634 27632 replica.cpp:641] Replica in STARTING status received a broadcasted recover request
I0825 22:07:36.815752 27638 master.cpp:469] Using default 'crammd5' authenticator
I0825 22:07:36.815904 27638 master.cpp:506] Authorization enabled
I0825 22:07:36.815979 27643 recover.cpp:195] Received a recover response from a replica in STARTING status
I0825 22:07:36.816185 27637 whitelist_watcher.cpp:79] No whitelist given
I0825 22:07:36.816186 27641 hierarchical.hpp:346] Initialized hierarchical allocator process
I0825 22:07:36.816519 27630 recover.cpp:566] Updating replica status to VOTING
I0825 22:07:36.817258 27639 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 475231ns
I0825 22:07:36.817296 27639 replica.cpp:323] Persisted replica status to VOTING
I0825 22:07:36.817420 27637 master.cpp:1525] The newly elected leader is master@172.17.0.14:51219 with id 20150825-220736-234885548-51219-27610
I0825 22:07:36.817467 27637 master.cpp:1538] Elected as the leading master!
I0825 22:07:36.817483 27637 master.cpp:1308] Recovering from registrar
I0825 22:07:36.817509 27635 recover.cpp:580] Successfully joined the Paxos group
I0825 22:07:36.817708 27633 registrar.cpp:311] Recovering registrar
I0825 22:07:36.817844 27635 recover.cpp:464] Recover process terminated
I0825 22:07:36.818439 27631 log.cpp:661] Attempting to start the writer
I0825 22:07:36.819694 27636 replica.cpp:477] Replica received implicit promise request with proposal 1
I0825 22:07:36.820133 27636 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 421255ns
I0825 22:07:36.820168 27636 replica.cpp:345] Persisted promised to 1
I0825 22:07:36.820804 27630 coordinator.cpp:231] Coordinator attemping to fill missing position
I0825 22:07:36.822105 27638 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2
I0825 22:07:36.822597 27638 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 468065ns
I0825 22:07:36.822625 27638 replica.cpp:679] Persisted action at 0
I0825 22:07:36.823737 27637 replica.cpp:511] Replica received write request for position 0
I0825 22:07:36.823796 27637 leveldb.cpp:438] Reading position from leveldb took 39603ns
I0825 22:07:36.824267 27637 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 446655ns
I0825 22:07:36.824296 27637 replica.cpp:679] Persisted action at 0
I0825 22:07:36.824961 27634 replica.cpp:658] Replica received learned notice for position 0
I0825 22:07:36.825340 27634 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 362236ns
I0825 22:07:36.825369 27634 replica.cpp:679] Persisted action at 0
I0825 22:07:36.825388 27634 replica.cpp:664] Replica learned NOP action at position 0
I0825 22:07:36.825975 27642 log.cpp:677] Writer started with ending position 0
I0825 22:07:36.826997 27628 leveldb.cpp:438] Reading position from leveldb took 56us
I0825 22:07:36.829946 27639 registrar.cpp:344] Successfully fetched the registry (0B) in 12.187136ms
I0825 22:07:36.830077 27639 registrar.cpp:443] Applied 1 operations in 40874ns; attempting to update the 'registry'
I0825 22:07:36.832870 27635 log.cpp:685] Attempting to append 174 bytes to the log
I0825 22:07:36.833088 27641 coordinator.cpp:341] Coordinator attempting to write APPEND action at position 1
I0825 22:07:36.833845 27636 replica.cpp:511] Replica received write request for position 1
I0825 22:07:36.834293 27636 leveldb.cpp:343] Persisting action (193 bytes) to leveldb took 425175ns
I0825 22:07:36.834324 27636 replica.cpp:679] Persisted action at 1
I0825 22:07:36.835077 27643 replica.cpp:658] Replica received learned notice for position 1
I0825 22:07:36.835500 27643 leveldb.cpp:343] Persisting action (195 bytes) to leveldb took 404831ns
I0825 22:07:36.835532 27643 replica.cpp:679] Persisted action at 1
I0825 22:07:36.835574 27643 replica.cpp:664] Replica learned APPEND action at position 1
I0825 22:07:36.836545 27643 registrar.cpp:488] Successfully updated the 'registry' in 6.393088ms
I0825 22:07:36.836707 27643 registrar.cpp:374] Successfully recovered registrar
I0825 22:07:36.836874 27639 log.cpp:704] Attempting to truncate the log to 1
I0825 22:07:36.837174 27632 master.cpp:1335] Recovered 0 slaves from the Registry (135B) ; allowing 10mins for slaves to re-register
I0825 22:07:36.837291 27634 coordinator.cpp:341] Coordinator attempting to write TRUNCATE action at position 2
I0825 22:07:36.838249 27639 replica.cpp:511] Replica received write request for position 2
I0825 22:07:36.838685 27639 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 412214ns
I0825 22:07:36.838716 27639 replica.cpp:679] Persisted action at 2
I0825 22:07:36.839735 27628 replica.cpp:658] Replica received learned notice for position 2
I0825 22:07:36.840304 27628 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 547841ns
I0825 22:07:36.840375 27628 leveldb.cpp:401] Deleting ~1 keys from leveldb took 51256ns
I0825 22:07:36.840401 27628 replica.cpp:679] Persisted action at 2
I0825 22:07:36.840428 27628 replica.cpp:664] Replica learned TRUNCATE action at position 2
I0825 22:07:36.849371 27610 containerizer.cpp:143] Using isolation: posix/cpu,posix/mem,filesystem/posix
I0825 22:07:36.856500 27633 slave.cpp:190] Slave started on 286)@172.17.0.14:51219
I0825 22:07:36.856541 27633 slave.cpp:191] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docker_remove_delay=""6hrs"" --docker_socket=""/var/run/docker.sock"" --docker_stop_timeout=""0ns"" --enforce_container_disk_quota=""false"" --executor_registration_timeout=""1mins"" --executor_shutdown_grace_period=""5secs"" --fetcher_cache_dir=""/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/fetch"" --fetcher_cache_size=""2GB"" --frameworks_home="""" --gc_delay=""1weeks"" --gc_disk_headroom=""0.1"" --hadoop_home="""" --help=""false"" --initialize_driver_logging=""true"" --isolation=""posix/cpu,posix/mem"" --launcher_dir=""/mesos/mesos-0.25.0/_build/src"" --logbufsecs=""0"" --logging_level=""INFO"" --oversubscribed_resources_interval=""15secs"" --perf_duration=""10secs"" --perf_interval=""1mins"" --qos_correction_interval_min=""0ns"" --quiet=""false"" --recover=""reconnect"" --recovery_timeout=""15mins"" --registration_backoff_factor=""10ms"" --resource_monitoring_interval=""1secs"" --resources=""cpus:2;mem:1024;disk:1024;ports:[31000-32000]"" --revocable_cpu_low_priority=""true"" --sandbox_directory=""/mnt/mesos/sandbox"" --strict=""true"" --switch_user=""true"" --version=""false"" --work_dir=""/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L""
I0825 22:07:36.857074 27633 credentials.hpp:85] Loading credential for authentication from '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/credential'
I0825 22:07:36.857275 27633 slave.cpp:321] Slave using credential for: test-principal
I0825 22:07:36.857822 27633 slave.cpp:354] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0825 22:07:36.857936 27633 slave.cpp:384] Slave hostname: 09c6504e3a31
I0825 22:07:36.857959 27633 slave.cpp:389] Slave checkpoint: true
I0825 22:07:36.858886 27637 state.cpp:54] Recovering state from '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/meta'
I0825 22:07:36.859130 27638 status_update_manager.cpp:202] Recovering status update manager
I0825 22:07:36.859465 27636 containerizer.cpp:379] Recovering containerizer
I0825 22:07:36.860631 27634 slave.cpp:4069] Finished recovery
I0825 22:07:36.861034 27634 slave.cpp:4226] Querying resource estimator for oversubscribable resources
I0825 22:07:36.861239 27643 status_update_manager.cpp:176] Pausing sending status updates
I0825 22:07:36.861240 27634 slave.cpp:684] New master detected at master@172.17.0.14:51219
I0825 22:07:36.861322 27634 slave.cpp:747] Authenticating with master master@172.17.0.14:51219
I0825 22:07:36.861343 27634 slave.cpp:752] Using default CRAM-MD5 authenticatee
I0825 22:07:36.861450 27634 slave.cpp:720] Detecting new master
I0825 22:07:36.861495 27628 authenticatee.cpp:115] Creating new client SASL connection
I0825 22:07:36.861569 27634 slave.cpp:4240] Received oversubscribable resources  from the resource estimator
I0825 22:07:36.861716 27632 master.cpp:4694] Authenticating slave(286)@172.17.0.14:51219
I0825 22:07:36.861799 27629 authenticator.cpp:407] Starting authentication session for crammd5_authenticatee(665)@172.17.0.14:51219
I0825 22:07:36.862045 27642 authenticator.cpp:92] Creating new server SASL connection
I0825 22:07:36.862308 27635 authenticatee.cpp:206] Received SASL authentication mechanisms: CRAM-MD5
I0825 22:07:36.862337 27635 authenticatee.cpp:232] Attempting to authenticate with mechanism 'CRAM-MD5'
I0825 22:07:36.862421 27629 authenticator.cpp:197] Received SASL authentication start
I0825 22:07:36.862478 27629 authenticator.cpp:319] Authentication requires more steps
I0825 22:07:36.862579 27633 authenticatee.cpp:252] Received SASL authentication step
I0825 22:07:36.862679 27628 authenticator.cpp:225] Received SASL authentication step
I0825 22:07:36.862707 27628 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: '09c6504e3a31' server FQDN: '09c6504e3a31' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0825 22:07:36.862717 27628 auxprop.cpp:174] Looking up auxiliary property '*userPassword'
I0825 22:07:36.862754 27628 auxprop.cpp:174] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0825 22:07:36.862785 27628 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: '09c6504e3a31' server FQDN: '09c6504e3a31' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0825 22:07:36.862797 27628 auxprop.cpp:124] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0825 22:07:36.862802 27628 auxprop.cpp:124] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0825 22:07:36.862817 27628 authenticator.cpp:311] Authentication success
I0825 22:07:36.862884 27629 authenticatee.cpp:292] Authentication success
I0825 22:07:36.862921 27630 master.cpp:4724] Successfully authenticated principal 'test-principal' at slave(286)@172.17.0.14:51219
I0825 22:07:36.862969 27642 authenticator.cpp:425] Authentication session cleanup for crammd5_authenticatee(665)@172.17.0.14:51219
I0825 22:07:36.863139 27639 slave.cpp:815] Successfully authenticated with master master@172.17.0.14:51219
I0825 22:07:36.863256 27639 slave.cpp:1209] Will retry registration in 15.028678ms if necessary
I0825 22:07:36.863382 27643 master.cpp:3636] Registering slave at slave(286)@172.17.0.14:51219 (09c6504e3a31) with id 20150825-220736-234885548-51219-27610-S0
I0825 22:07:36.863899 27610 sched.cpp:164] Version: 0.25.0
I0825 22:07:36.863940 27636 registrar.cpp:443] Applied 1 operations in 94492ns; attempting to update the 'registry'
I0825 22:07:36.864670 27632 sched.cpp:262] New master detected at master@172.17.0.14:51219
I0825 22:07:36.864790 27632 sched.cpp:318] Authenticating with master master@172.17.0.14:51219
I0825 22:07:36.864821 27632 sched.cpp:325] Using default CRAM-MD5 authenticatee
I0825 22:07:36.865095 27637 authenticatee.cpp:115] Creating new client SASL connection
I0825 22:07:36.865453 27643 master.cpp:4694] Authenticating scheduler-6c5ddcdb-9dd1-4b38-b051-5f714d3c1c55@172.17.0.14:51219
I0825 22:07:36.865603 27629 authenticator.cpp:407] Starting authentication session for crammd5_authenticatee(666)@172.17.0.14:51219
I0825 22:07:36.865840 27638 authenticator.cpp:92] Creating new server SASL connection
I0825 22:07:36.866217 27630 authenticatee.cpp:206] Received SASL authentication mechanisms: CRAM-MD5
I0825 22:07:36.866260 27630 authenticatee.cpp:232] Attempting to authenticate with mechanism 'CRAM-MD5'
I0825 22:07:36.866433 27639 authenticator.cpp:197] Received SASL authentication start
I0825 22:07:36.866513 27639 authenticator.cpp:319] Authentication requires more steps
I0825 22:07:36.866710 27630 authenticatee.cpp:252] Received SASL authentication step
I0825 22:07:36.866999 27638 authenticator.cpp:225] Received SASL authentication step
I0825 22:07:36.867051 27638 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: '09c6504e3a31' server FQDN: '09c6504e3a31' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0825 22:07:36.867077 27638 auxprop.cpp:174] Looking up auxiliary property '*userPassword'
I0825 22:07:36.867130 27638 auxprop.cpp:174] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0825 22:07:36.867162 27638 auxprop.cpp:102] Request to lookup properties for user: 'test-principal' realm: '09c6504e3a31' server FQDN: '09c6504e3a31' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0825 22:07:36.867175 27638 auxprop.cpp:124] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0825 22:07:36.867183 27638 auxprop.cpp:124] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0825 22:07:36.867202 27638 authenticator.cpp:311] Authentication success
I0825 22:07:36.867426 27636 authenticatee.cpp:292] Authentication success
I0825 22:07:36.867434 27633 authenticator.cpp:425] Authentication session cleanup for crammd5_authenticatee(666)@172.17.0.14:51219
I0825 22:07:36.867627 27630 master.cpp:4724] Successfully authenticated principal 'test-principal' at scheduler-6c5ddcdb-9dd1-4b38-b051-5f714d3c1c55@172.17.0.14:51219
I0825 22:07:36.867951 27641 sched.cpp:407] Successfully authenticated with master master@172.17.0.14:51219
I0825 22:07:36.867986 27641 sched.cpp:713] Sending SUBSCRIBE call to master@172.17.0.14:51219
I0825 22:07:36.868114 27641 sched.cpp:746] Will retry registration in 1.352726078secs if necessary
I0825 22:07:36.868233 27634 log.cpp:685] Attempting to append 344 bytes to the log
I0825 22:07:36.868268 27638 master.cpp:2094] Received SUBSCRIBE call for framework 'default' at scheduler-6c5ddcdb-9dd1-4b38-b051-5f714d3c1c55@172.17.0.14:51219
I0825 22:07:36.868305 27638 master.cpp:1564] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0825 22:07:36.868373 27631 coordinator.cpp:341] Coordinator attempting to write APPEND action at position 3
I0825 22:07:36.868614 27642 master.cpp:2164] Subscribing framework default with checkpointing enabled and capabilities [  ]
I0825 22:07:36.868999 27643 hierarchical.hpp:391] Added framework 20150825-220736-234885548-51219-27610-0000
I0825 22:07:36.869030 27643 hierarchical.hpp:1010] No resources available to allocate!
I0825 22:07:36.869046 27643 hierarchical.hpp:910] Performed allocation for 0 slaves in 34654ns
I0825 22:07:36.869215 27631 sched.cpp:640] Framework registered with 20150825-220736-234885548-51219-27610-0000
I0825 22:07:36.869215 27643 replica.cpp:511] Replica received write request for position 3
I0825 22:07:36.869268 27631 sched.cpp:654] Scheduler::registered took 29976ns
I0825 22:07:36.869453 27643 leveldb.cpp:343] Persisting action (363 bytes) to leveldb took 181689ns
I0825 22:07:36.869477 27643 replica.cpp:679] Persisted action at 3
I0825 22:07:36.870075 27629 replica.cpp:658] Replica received learned notice for position 3
I0825 22:07:36.870542 27629 leveldb.cpp:343] Persisting action (365 bytes) to leveldb took 469081ns
I0825 22:07:36.870589 27629 replica.cpp:679] Persisted action at 3
I0825 22:07:36.870622 27629 replica.cpp:664] Replica learned APPEND action at position 3
I0825 22:07:36.872133 27632 registrar.cpp:488] Successfully updated the 'registry' in 8.113152ms
I0825 22:07:36.872354 27639 log.cpp:704] Attempting to truncate the log to 3
I0825 22:07:36.872470 27632 coordinator.cpp:341] Coordinator attempting to write TRUNCATE action at position 4
I0825 22:07:36.872879 27637 slave.cpp:3058] Received ping from slave-observer(274)@172.17.0.14:51219
I0825 22:07:36.873015 27636 master.cpp:3699] Registered slave 20150825-220736-234885548-51219-27610-S0 at slave(286)@172.17.0.14:51219 (09c6504e3a31) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0825 22:07:36.873180 27637 slave.cpp:859] Registered with master master@172.17.0.14:51219; given slave ID 20150825-220736-234885548-51219-27610-S0
I0825 22:07:36.873219 27637 fetcher.cpp:77] Clearing fetcher cache
I0825 22:07:36.873410 27634 status_update_manager.cpp:183] Resuming sending status updates
I0825 22:07:36.873379 27628 hierarchical.hpp:542] Added slave 20150825-220736-234885548-51219-27610-S0 (09c6504e3a31) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (allocated: )
I0825 22:07:36.873482 27642 replica.cpp:511] Replica received write request for position 4
I0825 22:07:36.873661 27637 slave.cpp:882] Checkpointing SlaveInfo to '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/meta/slaves/20150825-220736-234885548-51219-27610-S0/slave.info'
I0825 22:07:36.874042 27642 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 538208ns
I0825 22:07:36.874078 27642 replica.cpp:679] Persisted action at 4
I0825 22:07:36.874196 27628 hierarchical.hpp:928] Performed allocation for slave 20150825-220736-234885548-51219-27610-S0 in 739900ns
I0825 22:07:36.874204 27637 slave.cpp:918] Forwarding total oversubscribed resources 
I0825 22:07:36.874824 27635 master.cpp:4613] Sending 1 offers to framework 20150825-220736-234885548-51219-27610-0000 (default) at scheduler-6c5ddcdb-9dd1-4b38-b051-5f714d3c1c55@172.17.0.14:51219
I0825 22:07:36.874958 27639 replica.cpp:658] Replica received learned notice for position 4
I0825 22:07:36.875074 27635 master.cpp:3998] Received update of slave 20150825-220736-234885548-51219-27610-S0 at slave(286)@172.17.0.14:51219 (09c6504e3a31) with total oversubscribed resources 
I0825 22:07:36.875485 27636 sched.cpp:803] Scheduler::resourceOffers took 243089ns
I0825 22:07:36.875450 27638 hierarchical.hpp:602] Slave 20150825-220736-234885548-51219-27610-S0 (09c6504e3a31) updated with oversubscribed resources  (total: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000], allocated: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000])
I0825 22:07:36.875495 27639 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 462264ns
I0825 22:07:36.875643 27639 leveldb.cpp:401] Deleting ~2 keys from leveldb took 109856ns
I0825 22:07:36.875682 27639 replica.cpp:679] Persisted action at 4
I0825 22:07:36.875717 27639 replica.cpp:664] Replica learned TRUNCATE action at position 4
I0825 22:07:36.876045 27638 hierarchical.hpp:1010] No resources available to allocate!
I0825 22:07:36.876072 27638 hierarchical.hpp:928] Performed allocation for slave 20150825-220736-234885548-51219-27610-S0 in 541099ns
I0825 22:07:36.879416 27639 master.cpp:2739] Processing ACCEPT call for offers: [ 20150825-220736-234885548-51219-27610-O0 ] on slave 20150825-220736-234885548-51219-27610-S0 at slave(286)@172.17.0.14:51219 (09c6504e3a31) for framework 20150825-220736-234885548-51219-27610-0000 (default) at scheduler-6c5ddcdb-9dd1-4b38-b051-5f714d3c1c55@172.17.0.14:51219
I0825 22:07:36.879475 27639 master.cpp:2570] Authorizing framework principal 'test-principal' to launch task b89d1df8-f2fb-44be-8f60-9352cf32a79d as user 'mesos'
I0825 22:07:36.880975 27639 master.hpp:170] Adding task b89d1df8-f2fb-44be-8f60-9352cf32a79d with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20150825-220736-234885548-51219-27610-S0 (09c6504e3a31)
I0825 22:07:36.881124 27639 master.cpp:3069] Launching task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 (default) at scheduler-6c5ddcdb-9dd1-4b38-b051-5f714d3c1c55@172.17.0.14:51219 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20150825-220736-234885548-51219-27610-S0 at slave(286)@172.17.0.14:51219 (09c6504e3a31)
I0825 22:07:36.882314 27636 slave.cpp:1249] Got assigned task b89d1df8-f2fb-44be-8f60-9352cf32a79d for framework 20150825-220736-234885548-51219-27610-0000
I0825 22:07:36.882470 27636 slave.cpp:4720] Checkpointing FrameworkInfo to '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/meta/slaves/20150825-220736-234885548-51219-27610-S0/frameworks/20150825-220736-234885548-51219-27610-0000/framework.info'
I0825 22:07:36.882984 27636 slave.cpp:4731] Checkpointing framework pid '@0.0.0.0:0' to '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/meta/slaves/20150825-220736-234885548-51219-27610-S0/frameworks/20150825-220736-234885548-51219-27610-0000/framework.pid'
I0825 22:07:36.884068 27636 slave.cpp:1365] Launching task b89d1df8-f2fb-44be-8f60-9352cf32a79d for framework 20150825-220736-234885548-51219-27610-0000
I0825 22:07:36.895586 27636 slave.cpp:5156] Checkpointing ExecutorInfo to '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/meta/slaves/20150825-220736-234885548-51219-27610-S0/frameworks/20150825-220736-234885548-51219-27610-0000/executors/b89d1df8-f2fb-44be-8f60-9352cf32a79d/executor.info'
I0825 22:07:36.896765 27636 slave.cpp:4799] Launching executor b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 with resources cpus(*):0.1; mem(*):32 in work directory '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/slaves/20150825-220736-234885548-51219-27610-S0/frameworks/20150825-220736-234885548-51219-27610-0000/executors/b89d1df8-f2fb-44be-8f60-9352cf32a79d/runs/1499299a-93dd-4982-9249-ad0e19d1c06c'
I0825 22:07:36.897374 27643 containerizer.cpp:633] Starting container '1499299a-93dd-4982-9249-ad0e19d1c06c' for executor 'b89d1df8-f2fb-44be-8f60-9352cf32a79d' of framework '20150825-220736-234885548-51219-27610-0000'
I0825 22:07:36.897414 27636 slave.cpp:5179] Checkpointing TaskInfo to '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/meta/slaves/20150825-220736-234885548-51219-27610-S0/frameworks/20150825-220736-234885548-51219-27610-0000/executors/b89d1df8-f2fb-44be-8f60-9352cf32a79d/runs/1499299a-93dd-4982-9249-ad0e19d1c06c/tasks/b89d1df8-f2fb-44be-8f60-9352cf32a79d/task.info'
I0825 22:07:36.897974 27636 slave.cpp:1583] Queuing task 'b89d1df8-f2fb-44be-8f60-9352cf32a79d' for executor b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework '20150825-220736-234885548-51219-27610-0000
I0825 22:07:36.898123 27636 slave.cpp:637] Successfully attached file '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/slaves/20150825-220736-234885548-51219-27610-S0/frameworks/20150825-220736-234885548-51219-27610-0000/executors/b89d1df8-f2fb-44be-8f60-9352cf32a79d/runs/1499299a-93dd-4982-9249-ad0e19d1c06c'
I0825 22:07:36.902439 27641 launcher.cpp:131] Forked child with pid '2326' for container '1499299a-93dd-4982-9249-ad0e19d1c06c'
I0825 22:07:36.902752 27641 containerizer.cpp:855] Checkpointing executor's forked pid 2326 to '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/meta/slaves/20150825-220736-234885548-51219-27610-S0/frameworks/20150825-220736-234885548-51219-27610-0000/executors/b89d1df8-f2fb-44be-8f60-9352cf32a79d/runs/1499299a-93dd-4982-9249-ad0e19d1c06c/pids/forked.pid'
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0825 22:07:37.029348  2340 process.cpp:1012] libprocess is initialized on 172.17.0.14:42774 for 16 cpus
I0825 22:07:37.030342  2340 logging.cpp:177] Logging to STDERR
I0825 22:07:37.032822  2340 exec.cpp:133] Version: 0.25.0
I0825 22:07:37.038837  2355 exec.cpp:183] Executor started at: executor(1)@172.17.0.14:42774 with pid 2340
I0825 22:07:37.041252 27638 slave.cpp:2358] Got registration for executor 'b89d1df8-f2fb-44be-8f60-9352cf32a79d' of framework 20150825-220736-234885548-51219-27610-0000 from executor(1)@172.17.0.14:42774
I0825 22:07:37.041371 27638 slave.cpp:2444] Checkpointing executor pid 'executor(1)@172.17.0.14:42774' to '/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/meta/slaves/20150825-220736-234885548-51219-27610-S0/frameworks/20150825-220736-234885548-51219-27610-0000/executors/b89d1df8-f2fb-44be-8f60-9352cf32a79d/runs/1499299a-93dd-4982-9249-ad0e19d1c06c/pids/libprocess.pid'
I0825 22:07:37.044067 27634 slave.cpp:1739] Sending queued task 'b89d1df8-f2fb-44be-8f60-9352cf32a79d' to executor 'b89d1df8-f2fb-44be-8f60-9352cf32a79d' of framework 20150825-220736-234885548-51219-27610-0000
I0825 22:07:37.044256  2358 exec.cpp:207] Executor registered on slave 20150825-220736-234885548-51219-27610-S0
I0825 22:07:37.046058  2358 exec.cpp:219] Executor::registered took 239083ns
Registered executor on 09c6504e3a31
Starting task b89d1df8-f2fb-44be-8f60-9352cf32a79d
I0825 22:07:37.046394  2358 exec.cpp:294] Executor asked to run task 'b89d1df8-f2fb-44be-8f60-9352cf32a79d'
I0825 22:07:37.046493  2358 exec.cpp:303] Executor::launchTask took 84034ns
sh -c 'sleep 1000'
Forked command at 2371
I0825 22:07:37.049942  2366 exec.cpp:516] Executor sending status update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000
I0825 22:07:37.050977 27635 slave.cpp:2696] Handling status update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 from executor(1)@172.17.0.14:42774
I0825 22:07:37.051316 27632 status_update_manager.cpp:322] Received status update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000
I0825 22:07:37.051379 27632 status_update_manager.cpp:499] Creating StatusUpdate stream for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000
I0825 22:07:37.052251 27632 status_update_manager.cpp:826] Checkpointing UPDATE for status update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000
I0825 22:07:37.053840 27632 status_update_manager.cpp:376] Forwarding update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 to the slave
I0825 22:07:37.054127 27642 slave.cpp:2975] Forwarding the update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 to master@172.17.0.14:51219
I0825 22:07:37.054364 27642 slave.cpp:2899] Status update manager successfully handled status update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000
I0825 22:07:37.054407 27642 slave.cpp:2905] Sending acknowledgement for status update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 to executor(1)@172.17.0.14:42774
I0825 22:07:37.054469 27635 master.cpp:4069] Status update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 from slave 20150825-220736-234885548-51219-27610-S0 at slave(286)@172.17.0.14:51219 (09c6504e3a31)
I0825 22:07:37.054519 27635 master.cpp:4108] Forwarding status update TASK_RUNNING (UUID: 98c4a799-ad82-497d-be1e-6dfb56a0894e) for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000
I0825 22:07:37.054743 27635 master.cpp:5576] Updating the latest state of task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 to TASK_RUNNING
I0825 22:07:37.055011 27641 sched.cpp:910] Scheduler::statusUpdate took 169426ns
I0825 22:07:37.055639 27634 master.cpp:3398] Processing ACKNOWLEDGE call 98c4a799-ad82-497d-be1e-6dfb56a0894e for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000 (default) at scheduler-6c5ddcdb-9dd1-4b38-b051-5f714d3c1c55@172.17.0.14:51219 on slave 20150825-220736-234885548-51219-27610-S0
I0825 22:07:37.055665  2359 exec.cpp:340] Executor received status update acknowledgement 98c4a799-ad82-497d-be1e-6dfb56a0894e for task b89d1df8-f2fb-44be-8f60-9352cf32a79d of framework 20150825-220736-234885548-51219-27610-0000
I0825 22:07:37.055886 27640 slave.cpp:564] Slave terminating
I0825 22:07:37.056210 27634 master.cpp:1012] Slave 20150825-220736-234885548-51219-27610-S0 at slave(286)@172.17.0.14:51219 (09c6504e3a31) disconnected
I0825 22:07:37.056257 27634 master.cpp:2415] Disconnecting slave 20150825-220736-234885548-51219-27610-S0 at slave(286)@172.17.0.14:51219 (09c6504e3a31)
I0825 22:07:37.056339 27634 master.cpp:2434] Deactivating slave 20150825-220736-234885548-51219-27610-S0 at slave(286)@172.17.0.14:51219 (09c6504e3a31)
I0825 22:07:37.056675 27643 hierarchical.hpp:635] Slave 20150825-220736-234885548-51219-27610-S0 deactivated
I0825 22:07:37.059391 27610 containerizer.cpp:143] Using isolation: posix/cpu,posix/mem,filesystem/posix
I0825 22:07:37.066619 27641 slave.cpp:190] Slave started on 287)@172.17.0.14:51219
I0825 22:07:37.066668 27641 slave.cpp:191] Flags at startup: --appc_store_dir=""/tmp/mesos/store/appc"" --authenticatee=""crammd5"" --cgroups_cpu_enable_pids_and_tids_count=""false"" --cgroups_enable_cfs=""false"" --cgroups_hierarchy=""/sys/fs/cgroup"" --cgroups_limit_swap=""false"" --cgroups_root=""mesos"" --container_disk_watch_interval=""15secs"" --containerizers=""mesos"" --credential=""/tmp/SlaveTest_HTTPSchedulerSlaveRestart_ukkA8L/credential"" --default_role=""*"" --disk_watch_interval=""1mins"" --docker=""docker"" --docker_kill_orphans=""true"" --docke",2.0,0.24.0,0.5,0.5701357466063348,0.0,0.0,0.0,0.0,0.0,0.6842105263157894,0.5642857142857143,0.8846153846153846,0.9878787878787878,0.9878787878787878,0.2201834862385321
Task,Define the container rootfs directories within the slave work_dir.,"A few motivations:

1) Given the design in MESOS-3004 it became apparent that we need to support multiple images in a container and these images can be of different image types. (There are no sufficient reasons or major obstacles that force us not to allow it and it obviously gives the users more flexibility).

2) Also, even though we currently allow only one backend for each provisioner, when we update a running slave there can be multiple backends left in each container that we need to launch tasks with, or at least recover. We should evaluate in the future whether to support multiple backends and choose among them dynamically based on image characteristics.

3) Since the rootfs' lifecycle tie with the running containers and should be cleaned up after containers die, it fits into the pattern of {{word_dir}} and we can manage them inside the work dir without needing to ask the operator to specify more flags.
",2.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15789473684210525,0.15,0.20512820512820512,0.13939393939393938,0.13939393939393938,0.0
Bug,Configurable size of completed task / framework history,"We try to make Mesos work with multiple frameworks and mesos-dns at the same time. The goal is to have set of frameworks per team / project on a single Mesos cluster.

At this point our mesos state.json is at 4mb and it takes a while to assembly. 5 mesos-dns instances hit state.json every 5 seconds, effectively pushing mesos-master CPU usage through the roof. It's at 100%+ all the time.

Here's the problem:

{noformat}
mesos λ curl -s http://mesos-master:5050/master/state.json | jq .frameworks[].completed_tasks[].framework_id | sort | uniq -c | sort -n
   1 ""20150606-001827-252388362-5050-5982-0003""
  16 ""20150606-001827-252388362-5050-5982-0005""
  18 ""20150606-001827-252388362-5050-5982-0029""
  73 ""20150606-001827-252388362-5050-5982-0007""
 141 ""20150606-001827-252388362-5050-5982-0009""
 154 ""20150820-154817-302720010-5050-15320-0000""
 289 ""20150606-001827-252388362-5050-5982-0004""
 510 ""20150606-001827-252388362-5050-5982-0012""
 666 ""20150606-001827-252388362-5050-5982-0028""
 923 ""20150116-002612-269165578-5050-32204-0003""
1000 ""20150606-001827-252388362-5050-5982-0001""
1000 ""20150606-001827-252388362-5050-5982-0006""
1000 ""20150606-001827-252388362-5050-5982-0010""
1000 ""20150606-001827-252388362-5050-5982-0011""
1000 ""20150606-001827-252388362-5050-5982-0027""

mesos λ fgrep 1000 -r src/master
src/master/constants.cpp:const size_t MAX_REMOVED_SLAVES = 100000;
src/master/constants.cpp:const uint32_t MAX_COMPLETED_TASKS_PER_FRAMEWORK = 1000;
{noformat}

Active tasks are just 6% of state.json response:

{noformat}
mesos λ cat ~/temp/mesos-state.json | jq -c . | wc
       1   14796 4138942
mesos λ cat ~/temp/mesos-state.json | jq .frameworks[].tasks | jq -c . | wc
      16      37  252774
{noformat}

I see four options that can improve the situation:

1. Add query string param to exclude completed tasks from state.json and use it in mesos-dns and similar tools. There is no need for mesos-dns to know about completed tasks, it's just extra load on master and mesos-dns.

2. Make history size configurable.

3. Make JSON serialization faster. With 10000s of tasks even without history it would take a lot of time to serialize tasks for mesos-dns. Doing it every 60 seconds instead of every 5 seconds isn't really an option.

4. Create event bus for mesos master. Marathon has it and it'd be nice to have it in Mesos. This way mesos-dns could avoid polling master state and switch to listening for events.

All can be done independently.

Note to mesosphere folks: please start distributing debug symbols with your distribution. I was asking for it for a while and it is really helpful: https://github.com/mesosphere/marathon/issues/1497#issuecomment-104182501

Perf report for leading master: 

!http://i.imgur.com/iz7C3o0.png!

I'm on 0.23.0.",3.0,0,0.5,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.02564102564102564,0.006060606060606061,0.006060606060606061,0.0
Improvement,Remove remnants of LIBPROCESS_STATISTICS_WINDOW,"As seen in MESOS-1283, LIBPROCESS_STATISTICS_WINDOW is no longer needed since metrics now require specification of a window size, and default to no history if not provided.

Some commented-out code remnants associated with this environment variable still remain and should be removed.",1.0,0,0.0,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.6025641025641025,0.8242424242424242,0.8181818181818181,0.0
Task,Add a protobuf to represent time with integer precision.,"Existing timestamps in the protobufs use {{double}} to encode time.  Generally, the field represents seconds (with the decimal component to represent smaller denominations of time).  This is less than ideal.

Instead, we should use integers, so as to not lose data (and to be able to compare value reliably).

Something like:
{code}
message Time {
  int64 seconds;
  int32 nanoseconds;
}
{code}",1.0,0,0.0,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.0
Bug,downloadWithHadoop tries to access Error() for a valid Try<bool>,"This was reported while trying to install Hadoop / Mesos integration:
{noformat}
I0818 05:36:35.058688 24428 fetcher.cpp:409] Fetcher Info: {""cache_directory"":""\/tmp\/mesos\/fetch\/slaves\/20150706-075218-1611773194-5050-28439-S473\/hadoop"",""items"":[{""action"":""BYPASS_CACHE"",""uri"":{""extract"":true,""value"":""hdfs:\/\/hdfs.prod:54310\/user\/ashwanth\/hadoop-with-mesos-2.6.0-cdh5.4.4.tar.gz""}}],""sandbox_directory"":""\/var\/lib\/mesos\/slaves\/20150706-075218-1611773194-5050-28439-S473\/frameworks\/20150706-075218-1611773194-5050-28439-4532\/executors\/executor_Task_Tracker_4129\/runs\/c26f52d4-4055-46fa-b999-11d73f2096dd"",""user"":""hadoop""}
I0818 05:36:35.059806 24428 fetcher.cpp:364] Fetching URI 'hdfs://hdfs.prod:54310/user/ashwanth/hadoop-with-mesos-2.6.0-cdh5.4.4.tar.gz'
I0818 05:36:35.059821 24428 fetcher.cpp:238] Fetching directly into the sandbox directory
I0818 05:36:35.059835 24428 fetcher.cpp:176] Fetching URI 'hdfs://hdfs.prod:54310/user/ashwanth/hadoop-with-mesos-2.6.0-cdh5.4.4.tar.gz'
mesos-fetcher: /tmp/mesos-build/mesos-repo/3rdparty/libprocess/3rdparty/stout/include/stout/try.hpp:90: const string& Try<T>::error() const [with T = bool; std::string = std::basic_string<char>]: Assertion `data.isNone()' failed.
{noformat}

This is, however, a genuine bug in {{src/launcher/fetcher.cpp#L99}}:

{code}
  Try<bool> available = hdfs.available();

  if (available.isError() || !available.get()) {
    return Error(""Skipping fetch with Hadoop Client as""
                 "" Hadoop Client not available: "" + available.error());
  }
{code}
The root cause is that (probably) the HDFS client is not available on the slave; however, we do not {{error()}} but rather return a {{false}} result.

The bug is exposed in the {{return}} line, where we try to retrieve {{available.error()}} (which is not there - it's just `false`).

This was a 'latent' bug that has been exposed by (my) recent refactoring of {{os::shell}} which is used by {{hdfs.available()}} under the covers.",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15789473684210525,0.20714285714285713,0.1794871794871795,0.15757575757575756,0.17575757575757575,0.0
Bug,JSON representation of Protobuf should use base64 encoding for 'bytes' fields.,"Currently we encode 'bytes' fields as UTF-8 strings, which is lossy for binary data due to invalid byte sequences! In order to encode binary data in a lossless fashion, we can encode 'bytes' fields in base64.

Note that this is also how proto3 does its encoding (see [here|https://developers.google.com/protocol-buffers/docs/proto3?hl=en#json]), so this would make migration easier as well.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.0
Bug,Cgroup CHECK fails test harness,"CHECK in clean up of ContainerizerTest causes test harness to abort rather than fail or skip only perf related tests.

[ RUN      ] SlaveRecoveryTest/0.RestartBeforeContainerizerLaunch
[       OK ] SlaveRecoveryTest/0.RestartBeforeContainerizerLaunch (628 ms)
[----------] 24 tests from SlaveRecoveryTest/0 (38986 ms total)

[----------] 4 tests from MesosContainerizerSlaveRecoveryTest
[ RUN      ] MesosContainerizerSlaveRecoveryTest.ResourceStatistics
../../src/tests/mesos.cpp:720: Failure
cgroups::mount(hierarchy, subsystem): 'perf_event' is already attached to another hierarchy
-------------------------------------------------------------
We cannot run any cgroups tests that require
a hierarchy with subsystem 'perf_event'
because we failed to find an existing hierarchy
or create a new one (tried '/tmp/mesos_test_cgroup/perf_event').
You can either remove all existing
hierarchies, or disable this test case
(i.e., --gtest_filter=-MesosContainerizerSlaveRecoveryTest.*).
-------------------------------------------------------------
F0811 17:23:43.874696 12955 mesos.cpp:774] CHECK_SOME(cgroups): '/tmp/mesos_test_cgroup/perf_event' is not a valid hierarchy 
*** Check failure stack trace: ***
    @     0x7fb2fb4835fd  google::LogMessage::Fail()
    @     0x7fb2fb48543d  google::LogMessage::SendToLog()
    @     0x7fb2fb4831ec  google::LogMessage::Flush()
    @     0x7fb2fb485d39  google::LogMessageFatal::~LogMessageFatal()
    @           0x4e3f98  _CheckFatal::~_CheckFatal()
    @           0x82f25a  mesos::internal::tests::ContainerizerTest<>::TearDown()
    @           0xc030e3  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @           0xbf9050  testing::Test::Run()
    @           0xbf912e  testing::TestInfo::Run()
    @           0xbf9235  testing::TestCase::Run()
    @           0xbf94e8  testing::internal::UnitTestImpl::RunAllTests()
    @           0xbf97a4  testing::UnitTest::Run()
    @           0x4a9df3  main
    @     0x7fb2f9371ec5  (unknown)
    @           0x4b63ee  (unknown)
Build step 'Execute shell' marked build as failure",2.0,0,0.0,0.06334841628959276,0.0,0.0,0.0,0.0,0.0,0.2982456140350877,0.19999999999999998,0.21794871794871795,0.1515151515151515,0.1515151515151515,0.0
Bug,Ignore no statistics condition for containers with no qdisc,"In PortMappingStatistics::execute, we log the following errors to stderr if the egress rate limiting qdiscs are not configured inside the container.

{code}
Failed to get the network statistics for the htb qdisc on eth0
Failed to get the network statistics for the fq_codel qdisc on eth0
{code}

This can occur because of an error reading the qdisc (statistics function return an error) or because the qdisc does not exist (function returns none).  

We should not log an error when the qdisc does not exist since this is normal behaviour if the container is created without rate limiting.  We do not want to gate this function on the slave rate limiting flag since we would have to compare the behaviour against the flag value at the time the container was created.",2.0,0,0.0,0.06334841628959276,0.0,0.0,0.0,0.0,0.0,0.2982456140350877,0.19999999999999998,0.21794871794871795,0.1515151515151515,0.1515151515151515,0.0
Bug,"http::get API evaluates ""host"" wrongly","Currently libprocess http API sets the ""Host"" header field from the peer socket address (IP:port). The problem is that socket address might not be right HTTP server and might be just a proxy. ",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3357142857142857,0.14102564102564102,0.2424242424242424,0.2424242424242424,0.0
Task,Updated slave task label decorator hook to pass in ExecutorInfo.,"If that task being launched has a command executor, there is no way for
the hook to determine the executor-id for that task. The executor-id is sometimes required by the label decorators for accounting purposes and for preparing ground for executor-environment-decorator (which is not passed the TaskInfo).",1.0,0,0.0,0.3453996983408748,0.0,0.0,0.0,0.0,0.2,0.38596491228070173,0.35714285714285715,0.28205128205128205,0.29090909090909095,0.28484848484848485,0.0
Bug,some variables in version.hpp use `Type &var` instead of `Type& var`,"Some variables in 
3rdparty/libprocess/3rdparty/stout/include/stout/version.hpp violate Mesos code style of biding '&' and '*' to the type name  (as opposed to binding to the variable name).",1.0,0,0.5,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.11538461538461538,0.11515151515151516,0.11515151515151516,0.0
Task,Implement token manager for docker registry,"Implement the following:
- A component that fetches JSON web authorization token from a given registry.
- Caches the token keyed on registry, service and scope
- Validates the cache for expiry date

Nice to have:
- Cache gets pruned as tokens are aged beyond expiration time. ",4.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3357142857142857,0.14102564102564102,0.2424242424242424,0.2424242424242424,0.0
Task,Design doc for docker registry token manager,Create design document for describing the component and interaction between Docker Registry Client and remote Docker Registry for token based authorization.,2.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3357142857142857,0.14102564102564102,0.2424242424242424,0.2424242424242424,0.0
Story,As a Java developer I want a simple way to obtain information about Master from ZooKeeper,"With the new JSON {{MasterInfo}} published to ZK, we want to provide a simple library class for Java Framework developers to retrieve info about the masters and the leader.",2.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15789473684210525,0.20714285714285713,0.1794871794871795,0.15757575757575756,0.17575757575757575,0.0
Story,As a Python developer I want a simple way to obtain information about Master from ZooKeeper,"With the new JSON {{MasterInfo}} published to ZK, we want to provide a simple library class for Python developers to retrieve info about the masters and the leader.",2.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15789473684210525,0.20714285714285713,0.1794871794871795,0.15757575757575756,0.17575757575757575,0.0
Bug,C++ style guide is not rendered correctly (code section syntax disregarded),"Some paragraphs at the bottom of docs/mesos-c++-style-guide.md containing code sections are not rendered correctly by the web site generator. It looks fine in a github gist and apparently the syntax used is correct. 

",1.0,0.23.0,0.5,0.06184012066365008,0.0,0.0,0.0,0.0,0.2,0.10526315789473684,0.12857142857142856,0.3846153846153846,0.4181818181818182,0.17575757575757575,0.2110091743119266
Bug,Libev handle_async can deadlock with run_in_event_loop,"Due to the arbitrary nature of the functions that are executed in handle_async, invoking them under the (A) {{watchers_mutex}} can lead to deadlocks if (B) is acquired before calling {{run_in_event_loop}} and (B) is also acquired within the arbitrary function.
{code}
==82679== Thread #10: lock order ""0x60774F8 before 0x60768C0"" violated
==82679== 
==82679== Observed (incorrect) order is: acquisition of lock at 0x60768C0
==82679==    at 0x4C32145: pthread_mutex_lock (in /usr/lib/valgrind/vgpreload_helgrind-amd64-linux.so)
==82679==    by 0x692C9B: __gthread_mutex_lock(pthread_mutex_t*) (gthr-default.h:748)
==82679==    by 0x6950BF: std::mutex::lock() (mutex:134)
==82679==    by 0x696219: Synchronized<std::mutex> synchronize<std::mutex>(std::mutex*)::{lambda(std::mutex*)#1}::operator()(std::mutex*) const (synchronized.hpp:58)
==82679==    by 0x696238: Synchronized<std::mutex> synchronize<std::mutex>(std::mutex*)::{lambda(std::mutex*)#1}::_FUN(std::mutex*) (synchronized.hpp:58)
==82679==    by 0x6984CF: Synchronized<std::mutex>::Synchronized(std::mutex*, void (*)(std::mutex*), void (*)(std::mutex*)) (synchronized.hpp:35)
==82679==    by 0x6962DE: Synchronized<std::mutex> synchronize<std::mutex>(std::mutex*) (synchronized.hpp:60)
==82679==    by 0x728FE1: process::handle_async(ev_loop*, ev_async*, int) (libev.cpp:48)
==82679==    by 0x761384: ev_invoke_pending (ev.c:2994)
==82679==    by 0x7643C4: ev_run (ev.c:3394)
==82679==    by 0x728E37: ev_loop (ev.h:826)
==82679==    by 0x729469: process::EventLoop::run() (libev.cpp:135)
==82679== 
==82679==  followed by a later acquisition of lock at 0x60774F8
==82679==    at 0x4C32145: pthread_mutex_lock (in /usr/lib/valgrind/vgpreload_helgrind-amd64-linux.so)
==82679==    by 0x4C6F9D: __gthread_mutex_lock(pthread_mutex_t*) (gthr-default.h:748)
==82679==    by 0x4C6FED: __gthread_recursive_mutex_lock(pthread_mutex_t*) (gthr-default.h:810)
==82679==    by 0x4F5D3D: std::recursive_mutex::lock() (mutex:175)
==82679==    by 0x516513: Synchronized<std::recursive_mutex> synchronize<std::recursive_mutex>(std::recursive_mutex*)::{lambda(std::recursive_mutex*)#1}::operator()(std::recursive_mutex*) const (synchronized.hpp:58)
==82679==    by 0x516532: Synchronized<std::recursive_mutex> synchronize<std::recursive_mutex>(std::recursive_mutex*)::{lambda(std::recursive_mutex*)#1}::_FUN(std::recursive_mutex*) (synchronized.hpp:58)
==82679==    by 0x52E619: Synchronized<std::recursive_mutex>::Synchronized(std::recursive_mutex*, void (*)(std::recursive_mutex*), void (*)(std::recursive_mutex*)) (synchronized.hpp:35)
==82679==    by 0x5165D4: Synchronized<std::recursive_mutex> synchronize<std::recursive_mutex>(std::recursive_mutex*) (synchronized.hpp:60)
==82679==    by 0x6BF4E1: process::ProcessManager::use(process::UPID const&) (process.cpp:2127)
==82679==    by 0x6C2B8C: process::ProcessManager::terminate(process::UPID const&, bool, process::ProcessBase*) (process.cpp:2604)
==82679==    by 0x6C6C3C: process::terminate(process::UPID const&, bool) (process.cpp:3107)
==82679==    by 0x692B65: process::Latch::trigger() (latch.cpp:53)
{code}

This was introduced in https://github.com/apache/mesos/commit/849fc4d361e40062073324153ba97e98e294fdf2",3.0,0.23.0,0.0,0.42533936651583715,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3142857142857143,0.24358974358974358,0.22424242424242424,0.22424242424242424,0.2110091743119266
Task,Remove unused 'fatal' and 'fatalerror' macros,"There exist {{fatal}} and {{fatalerror}} macros in both {{libprocess}} and {{stout}}. None of them are currently used as we favor {{glog}}'s {{LOG(FATAL)}}, and therefore should be removed.",1.0,0.23.0,0.0,0.6726998491704375,0.0,0.0,0.0,0.1,0.6000000000000001,0.2807017543859649,0.29285714285714287,0.21794871794871795,0.26666666666666666,0.26666666666666666,0.2110091743119266
Task,Validate Quota Requests.,We need to validate quota requests in terms of syntactical and semantical correctness.,3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12280701754385964,0.32857142857142857,0.11538461538461538,0.23636363636363636,0.23636363636363636,0.0
Bug,"MemIsolatorTest/{0,1}.MemUsage fails on OS X","Looks like this is due to {{mlockall}} being unimplemented on OS X.

{noformat}
[----------] 1 test from MemIsolatorTest/0, where TypeParam = N5mesos8internal5slave23PosixMemIsolatorProcessE
[ RUN      ] MemIsolatorTest/0.MemUsage
Failed to allocate RSS memory: Failed to make pages to be mapped unevictable: Function not implemented../../src/tests/containerizer/isolator_tests.cpp:812: Failure
helper.increaseRSS(allocation): Failed to sync with the subprocess
../../src/tests/containerizer/isolator_tests.cpp:815: Failure
(usage).failure(): Failed to get usage: No process found at 40558
[  FAILED  ] MemIsolatorTest/0.MemUsage, where TypeParam = N5mesos8internal5slave23PosixMemIsolatorProcessE (56 ms)
[----------] 1 test from MemIsolatorTest/0 (57 ms total)

[----------] 1 test from MemIsolatorTest/1, where TypeParam = N5mesos8internal5tests6ModuleINS_5slave8IsolatorELNS1_8ModuleIDE0EEE
[ RUN      ] MemIsolatorTest/1.MemUsage
Failed to allocate RSS memory: Failed to make pages to be mapped unevictable: Function not implemented../../src/tests/containerizer/isolator_tests.cpp:812: Failure
helper.increaseRSS(allocation): Failed to sync with the subprocess
../../src/tests/containerizer/isolator_tests.cpp:815: Failure
(usage).failure(): Failed to get usage: No process found at 40572
[  FAILED  ] MemIsolatorTest/1.MemUsage, where TypeParam = N5mesos8internal5tests6ModuleINS_5slave8IsolatorELNS1_8ModuleIDE0EEE (50 ms)
[----------] 1 test from MemIsolatorTest/1 (50 ms total)
{noformat}",2.0,0.23.0,0.5,0.07088989441930618,0.0,0.0,0.0,0.0,0.2,0.14035087719298245,0.10714285714285714,0.21794871794871795,0.26666666666666666,0.26666666666666666,0.2110091743119266
Task,Always set TaskStatus.executor_id when sending a status update message from Executor,"Currently, the Executor doesn't always set TaskStatus.executor_id. This prevents the Slave TaskStatus label decorator hook from knowing the executor id.

An appropriate place to automatically fill in the executor_id is ExecutorProcesS::sendStatusUpdate() since we are already filling in some other information here.",1.0,0,0.0,0.3453996983408748,0.0,0.0,0.0,0.0,0.2,0.38596491228070173,0.35714285714285715,0.28205128205128205,0.29090909090909095,0.28484848484848485,0.0
Bug,ContainerInfo::Image::AppC::id should be optional,"As I commented here: https://reviews.apache.org/r/34136/

Currently ContainerInfo::Image::Appc is defined as the following

{noformat:title=}
    message AppC {
      required string name = 1;
      required string id = 2;
      optional Labels labels = 3;
    }
{noformat}

In which the {{id}} is a required field. When users specify the image in tasks they likely will not use an image id (much like when you use docker or rkt to launch containers, you often use {{ubuntu}} or {{ubuntu:latest}} and seldom a SHA512 ID) and we should change it to be optional.

The motivating scenario is that: if the frameworks in the Mesos use something like {{image=ubuntu:14.04""}} to run a task and {{image=ubuntu}} defaults to {{image=ubuntu:latest}}, the operator can swap the latest version for all new tasks requesting {{image=ubuntu}}. If they allow users to specify {{image=ubuntu:live}}, they can swap the live version under the covers as well. This allows the operator to release important image updates (e.g., security patches) and have it picked up by new tasks in the cluster without asking the users to update their job/task configs.",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15789473684210525,0.15,0.20512820512820512,0.13939393939393938,0.13939393939393938,0.0
Task,Implement a utility for computing hash,It is useful for both appc and docker to compute and verify image hash.,2.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3357142857142857,0.20512820512820512,0.13939393939393938,0.13939393939393938,0.0
Bug,TimeTest.Now fails with --enable-libevent,"[ RUN      ] TimeTest.Now
../../../3rdparty/libprocess/src/tests/time_tests.cpp:50: Failure
Expected: (Microseconds(10)) < (Clock::now() - t1), actual: 8-byte object <10-27 00-00 00-00 00-00> vs 0ns
[  FAILED  ] TimeTest.Now (0 ms)",2.0,0.23.0,0.0,0.42533936651583715,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3142857142857143,0.24358974358974358,0.22424242424242424,0.22424242424242424,0.2110091743119266
Bug,Refactor Subprocess logic in linux/perf.cpp to use common subroutine,"MESOS-2834 will enhance the perf isolator to support the different output formats provided by difference kernel versions.  In order to achieve this, it requires to execute the ""perf --version"" command. 

We should decompose the existing Subcommand processing in perf so that we can share the implementation between the multiple uses of perf.",3.0,0,0.0,0.06334841628959276,0.0,0.0,0.0,0.0,0.0,0.2982456140350877,0.19999999999999998,0.21794871794871795,0.1515151515151515,0.1515151515151515,0.0
Documentation,Documentation images do not load,"Any images which are referenced from the generated docs ({{docs/*.md}}) do not show up on the website.  For example:
* [Architecture|http://mesos.apache.org/documentation/latest/architecture/]
* [External Containerizer|http://mesos.apache.org/documentation/latest/external-containerizer/]
* [Fetcher Cache Internals|http://mesos.apache.org/documentation/latest/fetcher-cache-internals/]
* [Maintenance|http://mesos.apache.org/documentation/latest/maintenance/] 	
* [Oversubscription|http://mesos.apache.org/documentation/latest/oversubscription/]
",3.0,0.24.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2201834862385321
Bug,Fetcher logs erroneous message when successfully extracting an archive,"When fetching an asset while not using the cache, the fetcher may erroneously report this: ""Copying instead of extracting resource from URI with 'extract' flag, because it does not seem to be an archive: "".

This message appears in the stderr log in the sandbox no matter whether extraction succeeded or not. It should be absent after successful extraction.
",1.0,0.23.0,0.5,0.3137254901960784,0.0,0.0,0.0,0.0,0.4,0.19298245614035087,0.14285714285714285,0.14102564102564102,0.16969696969696968,0.17575757575757575,0.2110091743119266
Improvement,"Mark Path::basename, Path::dirname as const functions.","The functions Path::basename and Path::dirname in stout/path.hpp are not marked const, although they could. Marking them const would remove some ambiguities in the usage of these functions.",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.2631578947368421,0.36428571428571427,0.28205128205128205,0.296969696969697,0.296969696969697,0.0
Bug,Fetcher Tests use EXPECT while subsequent logic relies on the outcome.,The fetcher tests use EXPECT validation for critical measures (e.g. non-empty results) and the subsequent logic releis on this (i.e. by accessing the first element). In such cases we should use ASSERT/CHECK.,1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12280701754385964,0.32857142857142857,0.11538461538461538,0.23636363636363636,0.23636363636363636,0.0
Bug,FrameworkInfo should only be updated if the re-registration is valid,"See Ben Mahler's comment in https://reviews.apache.org/r/32961/
FrameworkInfo should not be updated if the re-registration is invalid. This can happen in a few cases under the branching logic, so this requires some refactoring.
Notice that a {code}FrameworkErrorMessage{code} can be generated  both inside {code}else if (from != framework->pid){code} as well as from inside {code}failoverFramework(framework, from);{code}",2.0,0.25.0,0.5,0.3137254901960784,0.0,0.0,0.0,0.0,0.4,0.19298245614035087,0.14285714285714285,0.24358974358974358,0.22424242424242424,0.22424242424242424,0.2293577981651376
Bug,Design doc for docker image registry client,Create design document for the docker registry Authenticator component so that we have a baseline for the implementation. ,3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3357142857142857,0.14102564102564102,0.2424242424242424,0.2424242424242424,0.0
Task,Introduce QuotaInfo message,A {{QuotaInfo}} protobuf message is internal representation for quota related information (e.g. for persisting quota). The protobuf message should be extendable for future needs and allows for easy aggregation across roles and operator principals. It may also be used to pass quota information to allocators.,3.0,0,1.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12280701754385964,0.32857142857142857,0.717948717948718,0.8242424242424242,0.09696969696969697,0.0
Task,Provide a means to check http connection equality for streaming connections.,"If one uses an http::Pipe::Writer to stream a response, one cannot compare the writer with another to see if the connection has changed.

This is useful for example, in the master's http api when there is asynchronous disconnection logic. When we handle the disconnection, it's possible for the scheduler to have re-subscribed, and so the master needs to tell if the disconnection event is relevant for the current connection before taking action.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.0
Improvement,Libprocess Process: Join runqueue workers during finalization,"The lack of synchronization between ProcessManager destruction and the thread pool threads running the queued processes means that the shared state that is part of the ProcessManager gets destroyed prematurely.
Synchronizing the ProcessManager destructor with draining the work queues and stopping the workers will allow us to not require leaking the shared state to avoid use beyond destruction.",3.0,0,0.5,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.24358974358974358,0.22424242424242424,0.22424242424242424,0.0
Story,"Enable Mesos Agent Node to use arbitrary script / module to figure out IP, HOSTNAME","Following from MESOS-2902 we want to enable the same functionality in the Mesos Agents too.

This is probably best done once we implement the new {{os::shell}} semantics, as described in MESOS-3142.",1.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.15789473684210525,0.20714285714285713,0.02564102564102564,0.0303030303030303,0.17575757575757575,0.0
Improvement,Need for HTTP delete requests,"As we decided to create a more restful api for managing Quota request.
Therefore we also want to use the HTTP Delete request and hence need to enable the libprocess/http to send delete request besides get and post requests.",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12280701754385964,0.32857142857142857,0.11538461538461538,0.23636363636363636,0.23636363636363636,0.0
Bug,Resolve issue with hanging tests with Zookeeper,"See MESOS-2736 for the original issue;
the submitted [Review|https://reviews.apache.org/r/36663] currently has no tests, the one posted in the subsequent [r/3687|https://reviews.apache.org/r/36807] currently hangs when ran after the other {{TEST_F(MasterZooKeeperTest, LostZooKeeperCluster)}}.

The issue is around the {{await()}} in {{StartMaster()}} ({{cluster.hpp #430}}) that waits indefinitely for the master recovery.
",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15789473684210525,0.20714285714285713,0.1794871794871795,0.15757575757575756,0.17575757575757575,0.0
Task,Using a unresolvable hostname crashes the framework on registration,"The following commands trigger the crash:

{noformat}
$ sudo hostname foo  # an unresolvable hostname
$ sudo ./bin/mesos-master.sh --ip=127.0.0.1 --work_dir=/var/lib/mesos
$ LIBPROCESS_IP=127.0.0.1 ./src/mesos-execute --master=127.0.0.1:5050 --name=bar --command=""while true; do sleep 100; done""
{noformat}

The crash output:

{noformat}
WARNING: Logging before InitGoogleLogging() is written to STDERR
W0724 14:20:39.960733 1925993216 sched.cpp:1487]
**************************************************
Scheduler driver bound to loopback interface! Cannot communicate with remote master(s). You might want to set 'LIBPROCESS_IP' environment variable to use a routable IP address.
**************************************************
ABORT: (../../3rdparty/libprocess/3rdparty/stout/include/stout/try.hpp:85): Try::get() but state == ERROR: nodename nor servname provided, or not known[1]    24560 abort      LIBPROCESS_IP=127.0.0.1 ./src/mesos-execute --master=127.0.0.1:5050
{noformat}",1.0,0,0.0,0.6726998491704375,0.0,0.0,0.0,0.1,0.6000000000000001,0.2807017543859649,0.29285714285714287,0.21794871794871795,0.26666666666666666,0.26666666666666666,0.0
Task,Update Homebrew formula for Mesos (Mac OSX),"We have pushed a [pull request|https://github.com/Homebrew/homebrew/pull/42099] to Homebrew for the new 0.23 formula.

Once accepted, we must verify that this works on a Mac OSX device.
This would also be a great time to ensure our documentation is up-to-date.

Currently, the Homebrew check fails, as they have deprecated SHA-1 checksums:
{noformat}
Error Message

failed: brew audit mesos
Stacktrace

        Error: 7 problems in 1 formula
mesos:
 * Stable resource ""protobuf"": SHA1 checksums are deprecated, please use SHA256
 * Stable resource ""python-gflags"": SHA1 checksums are deprecated, please use SHA256
 * Stable resource ""six"": SHA1 checksums are deprecated, please use SHA256
 * Stable resource ""google-apputils"": SHA1 checksums are deprecated, please use SHA256
 * Stable resource ""python-dateutil"": SHA1 checksums are deprecated, please use SHA256
 * Stable resource ""boto"": SHA1 checksums are deprecated, please use SHA256
 * Stable resource ""pytz"": SHA1 checksums are deprecated, please use SHA256
{noformat}

Don't know enough about Homebrew to really figure out what is going on here; nor how to fix this.
The Mesos SHA-256 has been correctly entered and computed via the [Online SHA/MD5 calculator|https://md5file.com/calculator].

I guess, we should go download the packages and compute their SHA-256 and/or research from the respective download sites whether they publish the SHA.",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15789473684210525,0.20714285714285713,0.1794871794871795,0.15757575757575756,0.17575757575757575,0.0
Story,As a Developer I want a better way to run shell commands,"When reviewing the code in [r/36425|https://reviews.apache.org/r/36425/] [~benjaminhindman] noticed that there is a better abstraction that is possible to introduce for {{os::shell()}} that will simplify the caller's life.

Instead of having to handle all possible outcomes, we propose to refactor {{os::shell()}} as follows:

{code}
/**
 * Returns the output from running the specified command with the shell.
 */
Try<std::string> shell(const string& command)
{
  // Actually handle the WIFEXITED, WIFSIGNALED here!
}
{code}

where the returned string is {{stdout}} and, should the program be signaled, or exit with a non-zero exit code, we will simply return a {{Failure}} with an error message that will encapsulate both the returned/signaled state, and, possibly {{stderr}}.

And some test driven development:
{code}
EXPECT_ERROR(os::shell(""false""));
EXPECT_SOME(os::shell(""true""));

EXPECT_SOME_EQ(""hello world"", os::shell(""echo hello world""));
{code}

Alternatively, the caller can ask to have {{stderr}} conflated with {{stdout}}:
{code}
Try<string> outAndErr = os::shell(""myCmd --foo 2>&1"");
{code}

However, {{stderr}} will be ignored by default:
{code}
// We don't read standard error by default.
EXPECT_SOME_EQ("""", os::shell(""echo hello world 1>&2""));

// We don't even read stderr if something fails (to return in Try::error).
Try<string> output = os::shell(""echo hello world 1>&2 && false"");
EXPECT_ERROR(output);
EXPECT_FALSE(strings::contains(output.error(), ""hello world""));
{code}

An analysis of existing usage shows that in almost all cases, the caller only cares {{if not error}}; in fact, the actual exit code is read only once, and even then, in a test case.

We believe this will simplify the API to the caller, and will significantly reduce the length and complexity at the calling sites (<6 LOC against the current 20+).",2.0,0.23.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.15789473684210525,0.20714285714285713,0.02564102564102564,0.0303030303030303,0.17575757575757575,0.2110091743119266
Story,Publish MasterInfo to ZK using JSON,"Following from MESOS-2340, which now allows Master to correctly decode JSON information ({{MasterInfo}}) published to Zookeeper, we can now enable the Master Leader Contender to serialize it too in JSON.",2.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15789473684210525,0.20714285714285713,0.1794871794871795,0.15757575757575756,0.17575757575757575,0.0
Task,Isolator::prepare() should return Executor environment vars as well,"Sometimes the Isolators need to pass on some environment variables for the Executor that is being launched. For example, to successfully launch an executor inside a network namespace, one needs to set LIBPROCESS_IP to point to the container IP, otherwise the executor tries to bind to the Slave IP which may be invalid inside the namespace. Another example is where the file system isolator should be able to specify the WORK_DIR depending on if a new rootfs is used.",2.0,0,0.0,0.3453996983408748,0.0,0.0,0.0,0.0,0.2,0.38596491228070173,0.35714285714285715,0.28205128205128205,0.29090909090909095,0.28484848484848485,0.0
Improvement,Improve task reconciliation documentation.,Include additional information about task reconciliation that explain why the master may not return the states of all tasks immediately and why an explicit task reconciliation algorithm is necessary.,1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.2631578947368421,0.36428571428571427,0.28205128205128205,0.296969696969697,0.296969696969697,0.0
Improvement,Add configurable UNIMPLEMENTED macro to stout,"During the transition to support for windows, it would be great if we had the ability to use a macro that marks functions as un-implemented.
To support being able to find all the unimplemented functions easily at compile time, while also being able to run the tests at the same time, we can add a configuration flag that controls whether this macro aborts or expands to a static assertion.",2.0,0,0.0,0.42533936651583715,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3142857142857143,0.24358974358974358,0.22424242424242424,0.22424242424242424,0.0
Bug,Always disable SSLV2,"The SSL protocol mismatch tests are failing on Centos7 when matching SSLV2 with SSLV2. Since this version of the protocol is highly discouraged anyway, let's disable it completely unless requested otherwise.",2.0,0,0.0,0.42533936651583715,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3142857142857143,0.24358974358974358,0.22424242424242424,0.22424242424242424,0.0
Task,Pass ExecutorInfo argument into Isolator::isolate().,"Some isolators need to lookup the executor environment variables to customize their isolation needs. Currently, one has to use the ""prepare()"" call to cache the executor-info to use it later during isolate() call.",2.0,0,0.0,0.3453996983408748,0.0,0.0,0.0,0.0,0.2,0.38596491228070173,0.35714285714285715,0.28205128205128205,0.29090909090909095,0.28484848484848485,0.0
Task,"Convert mesos::slave::{Limitation,ExecutorRunState} into protobufs.",Published RR: https://reviews.apache.org/r/36718/,1.0,0,0.0,0.3453996983408748,0.0,0.0,0.0,0.0,0.2,0.38596491228070173,0.35714285714285715,0.28205128205128205,0.29090909090909095,0.28484848484848485,0.0
Task,Expand CMake build system to support building the containerizer and associated components,"In other tasks in epic MESOS-898, we implement a CMake-based build system that allows us to build process library, the process tests, and the stout tests.

For the CMake build system MVP, it's important that we expand this to build the containerizer, associated modules, and all related tests.",3.0,0,0.0,0.44494720965309204,0.0,0.0,0.0,0.0,0.0,0.14035087719298245,0.15,0.14102564102564102,0.1515151515151515,0.16969696969696968,0.0
Task,Define CMake style guide,"The short story is that it is important to be principled about how the CMake build system is maintained, because there CMake language makes it difficult to statically verify that a configuration is correct. It is not unique in this regard, but (make is arguably even worse) but it is something that's important to make sure we get right.

The longer story is, CMake's language is dynamically scoped and often has somewhat odd defaults for variable values (_e.g._, IIRC, target names passed to ExternalProject_Add default to ""PREFIX"" instead of erroring out). This means that it is rare to get a configuration-time error (_i.e._, CMake usually doesn't say something like ""hey this variable isn't defined""), and in large projects, this can make it very difficult to know where definitions come from, or whether it's important that one config routine runs before another. Dynamic scoping also makes it particularly easy to write spaghetti code, which is clearly undesirable for something as important as a build system.

Thus, it is particularly important that we lay down our expectations for how the CMake system is to be structured. This might include:

* Function naming (_e.g._, making it easy to tell whether a function was defined by us, and where it was defined; so we might say that we want our functions to have an underscore to start, and start with the package the come from, like libprocess, so that we know where to look for the definition.)
* What assertions we want to check variable values against, so that we can replace subtle errors (_e.g._, a library is accidentally named something silly like ""PREFIX.0.0.1"") with an obvious ones (_e.g._, ""You have failed to define your target name, so CMake has defaulted to 'PREFIX'; please check your configuration routines"")
* Decisions of what goes where. (_e.g._, the most complex parts of the CMake MVPs is in the configuration routines, like `MesosConfigure.cmake`; to curb this, we should have strict rules about what goes in that file vs other files, and how we know what is to be run before what. Part of this should probably be prominent comments explaining the structure of the project, so that people aren't confused!)
* And so on.",3.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.8,0.19298245614035087,0.09999999999999999,0.14102564102564102,0.1515151515151515,0.16969696969696968,0.0
Task,Standardize separation of Windows/Linux-specific OS code,"There are 50+ files that must be touched to separate OS-specific code.

First, we will standardize the changes by using stout/abort.hpp as an example.
The review/discussion can be found here:
https://reviews.apache.org/r/36625/",3.0,0,0.0,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.0
Improvement,PoC running command executor with image provisioner,This is to implement a PoC of the alternative design choices with MESOS-3004,3.0,0,0.0,0.277526395173454,0.0,0.0,0.0,0.0,0.4,0.08771929824561403,0.10714285714285714,0.02564102564102564,0.09090909090909091,0.09090909090909091,0.0
Improvement,Support HTTPS requests in libprocess,"In order to pull images from Docker registries, https calls are needed to securely communicate with the registry hosts. Currently, only http requests are supported through libprocess. Now that SSL sockets are available through libprocess, support for https can be added.",3.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3357142857142857,0.0,0.024242424242424242,0.024242424242424242,0.0
Bug,`sudo make distcheck` fails on Ubuntu 14.04 (and possibly other OSes too),"Running tests as root causes a large number of failures.
{noformat}
$ lsb_release -a
LSB Version:    core-2.0-amd64:core-2.0-noarch:core-3.0-amd64:core-3.0-noarch:core-3.1-amd64:core-3.1-noarch:core-3.2-amd64:core-3.2-noarch:core-4.0-amd64:core-4.0-noarch:core-4.1-amd64:core-4.1-noarch:cxx-3.0-amd64:cxx-3.0-noarch:cxx-3.1-amd64:cxx-3.1-noarch:cxx-3.2-amd64:cxx-3.2-noarch:cxx-4.0-amd64:cxx-4.0-noarch:cxx-4.1-amd64:cxx-4.1-noarch:desktop-3.1-amd64:desktop-3.1-noarch:desktop-3.2-amd64:desktop-3.2-noarch:desktop-4.0-amd64:desktop-4.0-noarch:desktop-4.1-amd64:desktop-4.1-noarch:graphics-2.0-amd64:graphics-2.0-noarch:graphics-3.0-amd64:graphics-3.0-noarch:graphics-3.1-amd64:graphics-3.1-noarch:graphics-3.2-amd64:graphics-3.2-noarch:graphics-4.0-amd64:graphics-4.0-noarch:graphics-4.1-amd64:graphics-4.1-noarch:languages-3.2-amd64:languages-3.2-noarch:languages-4.0-amd64:languages-4.0-noarch:languages-4.1-amd64:languages-4.1-noarch:multimedia-3.2-amd64:multimedia-3.2-noarch:multimedia-4.0-amd64:multimedia-4.0-noarch:multimedia-4.1-amd64:multimedia-4.1-noarch:printing-3.2-amd64:printing-3.2-noarch:printing-4.0-amd64:printing-4.0-noarch:printing-4.1-amd64:printing-4.1-noarch:qt4-3.1-amd64:qt4-3.1-noarch:security-4.0-amd64:security-4.0-noarch:security-4.1-amd64:security-4.1-noarch
Distributor ID: Ubuntu
Description:    Ubuntu 14.04.2 LTS
Release:        14.04
Codename:       trusty

$ sudo make -j12 V=0 check

[==========] 712 tests from 116 test cases ran. (318672 ms total)
[  PASSED  ] 676 tests.
[  FAILED  ] 36 tests, listed below:
[  FAILED  ] PerfEventIsolatorTest.ROOT_CGROUPS_Sample
[  FAILED  ] UserCgroupIsolatorTest/2.ROOT_CGROUPS_UserCgroup, where TypeParam = mesos::internal::slave::CgroupsPerfEventIsolatorProcess
[  FAILED  ] SlaveRecoveryTest/0.RecoverSlaveState, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.RecoverStatusUpdateManager, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.ReconnectExecutor, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.RecoverUnregisteredExecutor, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.RecoverTerminatedExecutor, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.RecoverCompletedExecutor, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.CleanupExecutor, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.RemoveNonCheckpointingFramework, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.NonCheckpointingFramework, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.KillTask, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.Reboot, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.GCExecutor, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.ShutdownSlave, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.ShutdownSlaveSIGUSR1, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.RegisterDisconnectedSlave, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.ReconcileKillTask, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.ReconcileShutdownFramework, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.ReconcileTasksMissingFromSlave, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.SchedulerFailover, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.PartitionedSlave, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.MasterFailover, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.MultipleFrameworks, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.MultipleSlaves, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] SlaveRecoveryTest/0.RestartBeforeContainerizerLaunch, where TypeParam = mesos::internal::slave::MesosContainerizer
[  FAILED  ] MesosContainerizerSlaveRecoveryTest.ResourceStatistics
[  FAILED  ] MesosContainerizerSlaveRecoveryTest.CGROUPS_ROOT_PerfRollForward
[  FAILED  ] MesosContainerizerSlaveRecoveryTest.CGROUPS_ROOT_PidNamespaceForward
[  FAILED  ] MesosContainerizerSlaveRecoveryTest.CGROUPS_ROOT_PidNamespaceBackward
[  FAILED  ] CgroupsAnyHierarchyWithPerfEventTest.ROOT_CGROUPS_Perf
[  FAILED  ] MemoryPressureMesosTest.CGROUPS_ROOT_Statistics
[  FAILED  ] MemoryPressureMesosTest.CGROUPS_ROOT_SlaveRecovery
[  FAILED  ] NsTest.ROOT_setns
[  FAILED  ] PerfTest.ROOT_Events
[  FAILED  ] PerfTest.ROOT_SamplePid

36 FAILED TESTS
{noformat}

Full log attached.",2.0,0.23.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15789473684210525,0.20714285714285713,0.1794871794871795,0.15757575757575756,0.17575757575757575,0.2110091743119266
Task,Add Labels to TaskStatus and expose them via state.json,"This would allow the executors and Slave modules to expose some meta-data to frameworks and Mesos-DNS via state.json.

A typical use case is to allow the containers to expose their IP to framework/Mesos-DNS.",2.0,0,0.0,0.3453996983408748,0.0,0.0,0.0,0.0,0.2,0.38596491228070173,0.35714285714285715,0.28205128205128205,0.29090909090909095,0.28484848484848485,0.0
Improvement,Add capacity heuristic for quota requests in Master,"We need to to validate quota requests in the Mesos Master as outlined in the Design Doc: https://docs.google.com/document/d/16iRNmziasEjVOblYp5bbkeBZ7pnjNlaIzPQqMTHQ-9I

This ticket aims to validate satisfiability (in terms of available resources) of a quota request using a heuristic algorithm in the Mesos Master, rather than validating the syntax of the request.",3.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.11538461538461538,0.23636363636363636,0.23636363636363636,0.0
Epic,Introduce HTTP endpoints for Quota,"We need to implement the HTTP endpoints for Quota as outlined in the Design Doc: (https://docs.google.com/document/d/16iRNmziasEjVOblYp5bbkeBZ7pnjNlaIzPQqMTHQ-9I).
",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12280701754385964,0.32857142857142857,0.11538461538461538,0.23636363636363636,0.23636363636363636,0.0
Improvement,Unify initialization of modularized components,"h1.Introduction

As it stands right now, default implementations of modularized components are required to have a non parametrized {{create()}} static method. This allows to write tests which can cover default implementations and modules based on these default implementations on a uniform way.

For example, with the interface {{Foo}}:

{code}
class Foo {
public:
  virtual ~Foo() {}

  virtual Future<int> hello() = 0;

protected:
  Foo() {}
};
{code}

With a default implementation:

{code}
class LocalFoo {
public:
  Try<Foo*> create() {
    return new Foo;
  }

  virtual Future<int> hello() {
    return 1;
  }
};
{code}

This allows to create typed tests which look as following:

{code}
typedef ::testing::Types<LocalFoo,
                         tests::Module<Foo, TestLocalFoo>>
  FooTestTypes;

TYPED_TEST_CASE(FooTest, FooTestTypes);

TYPED_TEST(FooTest, ATest)
{
  Try<Foo*> foo = TypeParam::create();
  ASSERT_SOME(foo);

  AWAIT_CHECK_EQUAL(foo.get()->hello(), 1);
}
{code}

The test will be applied to each of types in the template parameters of {{FooTestTypes}}. This allows to test different implementation of an interface. In our code, it tests default implementations and a module which uses the same default implementation.

The class {{tests::Module<typename T, ModuleID N>}} needs a little explanation, it is a wrapper around {{ModuleManager}} which allows the tests to encode information about the requested module in the type itself instead of passing a string to the factory method. The wrapper around create, the real important method looks as follows:

{code}
template<typename T, ModuleID N>
static Try<T*> test::Module<T, N>::create()
{
  Try<std::string> moduleName = getModuleName(N);
  if (moduleName.isError()) {
    return Error(moduleName.error());
  }
  return mesos::modules::ModuleManager::create<T>(moduleName.get());
}
{code}

h1.The Problem

Consider the following implementation of {{Foo}}:

{code}
class ParameterFoo {
public:
  Try<Foo*> create(int i) {
    return new ParameterFoo(i);
  }

  ParameterFoo(int i) : i_(i) {}

  virtual Future<int> hello() {
    return i;
  }

private:
  int i_;
};
{code}

As it can be seen, this implementation cannot be used as a default implementation since its create API does not match the one of {{test::Module<>}}: {{create()}} has a different signature for both types. It is still a common situation to require initialization parameters for objects, however this constraint (keeping both interfaces alike) forces default implementations of modularized components to have default constructors, therefore the tests are forcing the design of the interfaces.

Implementations which are supposed to be used as modules only, i.e. non default implementations are allowed to have constructor parameters, since the actual signature of their factory method is, this factory method's function is to decode the parameters and call the appropriate constructor:

{code}
template<typename T>
T* Module<T>::create(const Parameters& params);
{code}

where parameters is just an array of key-value string pairs whose interpretation is left to the specific module. Sadly, this call is wrapped by 
{{ModuleManager}} which only allows module parameters to be passed from the command line and does not offer a programmatic way to feed construction parameters to modules.

h1.The Ugly Workaround

With the requirement of a default constructor and parameters devoid {{create()}} factory function, a common pattern (see [Authenticator|https://github.com/apache/mesos/blob/9d4ac11ed757aa5869da440dfe5343a61b07199a/include/mesos/authentication/authenticator.hpp]) has been introduced to feed construction parameters into default implementation, this leads to adding an {{initialize()}} call to the public interface, which will have {{Foo}} become:

{code}
class Foo {
public:
  virtual ~Foo() {}

  virtual Try<Nothing> initialize(Option<int> i) = 0;

  virtual Future<int> hello() = 0;

protected:
  Foo() {}
};
{code}

{{ParameterFoo}} will thus look as follows:

{code}
class ParameterFoo {
public:
  Try<Foo*> create() {
    return new ParameterFoo;
  }

  ParameterFoo() : i_(None()) {}

  virtual Try<Nothing> initialize(Option<int> i) {
    if (i.isNone()) {
      return Error(""Need value to initialize"");
    }

    i_ = i;

    return Nothing;
  }

  virtual Future<int> hello() {
    if (i_.isNone()) {
      return Future<int>::failure(""Not initialized"");
    }

    return i_.get();
  }

private:
  Option<int> i_;
};
{code}

Look that this {{initialize()}} method now has to be implemented by all descendants of {{Foo}}, even if there's a {{DatabaseFoo}} which takes is
return value for {{hello()}} from a DB, it will need to support {{int}} as an initialization parameter.

The problem is more severe the more specific the parameter to {{initialize()}} is. For example, if there is a very complex structure implementing ACLs, all implementations of an authorizer will need to import this structure even if they can completely ignore it.

In the {{Foo}} example if {{ParameterFoo}} were to become the default implementation of {{Foo}}, the tests would look as follows:

{code}
typedef ::testing::Types<ParameterFoo,
                         tests::Module<Foo, TestParameterFoo>>
  FooTestTypes;

TYPED_TEST_CASE(FooTest, FooTestTypes);

TYPED_TEST(FooTest, ATest)
{
  Try<Foo*> foo = TypeParam::create();
  ASSERT_SOME(foo);

  int fooValue = 1;
  foo.get()->initialize(fooValue);

  AWAIT_CHECK_EQUAL(foo.get()->hello(), fooValue);
}
{code}",3.0,"0.22.0,0.22.1,0.23.0",0.0,0.004524886877828055,0.0,0.0,0.0,0.0,0.2,0.12280701754385964,0.24285714285714285,0.10256410256410256,0.11515151515151516,0.11515151515151516,0.2049235474006116
Task,Implement a streaming response decoder for events stream,"We need a streaming response decoder to de-serialize chunks sent from the master on the events stream.

From the HTTP API design doc:
Master encodes each Event in RecordIO format, i.e. a string representation of length of the event in bytes followed by JSON or binary Protobuf  (possibly compressed) encoded event.

As of now for getting the basic features right , this is being done in the test-cases:

{code}
  auto reader = response.get().reader;
  ASSERT_SOME(reader);

  Future<std::string> eventFuture = reader.get().read();
  AWAIT_READY(eventFuture);

  Event event;
  event.ParseFromString(eventFuture.get());
{code}

Two things need to happen:
- We need master to emit events in RecordIO format i.e. event size followed by the serialized event instead of just the serialized events as is the case now.
- The decoder class should then abstract away the logic of reading the response and de-serializing events from the stream.

Ideally, the decoder should work with both ""json"" and ""protobuf"" responses.
",3.0,0,0.5,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.3846153846153846,0.4181818181818182,0.4121212121212121,0.0
Task,Replicated registry needs a representation of maintenance schedules,"In order to persist maintenance schedules across failovers of the master, the schedule information must be kept in the replicated registry.

This means adding an additional message in the Registry protobuf in src/master/registry.proto.  The status of each individual slave's maintenance will also be persisted in this way.
{code}
message Maintenance {
  message HostStatus {
    required string hostname = 1;

    // True if the slave is deactivated for maintenance.
    // False if the slave is draining in preparation for maintenance.
    required bool is_down = 2;  // Or an enum
  }

  message Schedule {
    // The set of affected slave(s).
    repeated HostStatus hosts = 1;

    // Interval in which this set of slaves is expected to be down for.
    optional Unavailability interval = 2;
  }

  message Schedules {
    repeated Schedule schedules;
  }

  optional Schedules schedules = 1;
}
{code}

Note: There can be multiple SlaveID's attached to a single hostname.",3.0,0,0.0,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.0
Task,Add 'principal' field to 'Resource.DiskInfo.Persistence',"In order to support authorization for persistent volumes, we should add the {{principal}} to {{Resource.DiskInfo}}, analogous to {{Resource.ReservationInfo.principal}}.",1.0,0,0.5,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.21794871794871795,0.26666666666666666,0.26666666666666666,0.0
Task,Add authorization for dynamic reservation,"Dynamic reservations should be authorized with the {{principal}} of the reserving entity (framework or master). The idea is to introduce {{Reserve}} and {{Unreserve}} into the ACL.

{code}
  message Reserve {
    // Subjects.
    required Entity principals = 1;

    // Objects.  MVP: Only possible values = ANY, NONE
    required Entity resources = 1;
  }

  message Unreserve {
    // Subjects.
    required Entity principals = 1;

    // Objects.
    required Entity reserver_principals = 2;
  }
{code}

When a framework/operator reserves resources, ""reserve"" ACLs are checked to see if the framework ({{FrameworkInfo.principal}}) or the operator ({{Credential.user}}) is authorized to reserve the specified resources. If not authorized, the reserve operation is rejected.

When a framework/operator unreserves resources, ""unreserve"" ACLs are checked to see if the framework ({{FrameworkInfo.principal}}) or the operator ({{Credential.user}}) is authorized to unreserve the resources reserved by a framework or operator ({{Resource.ReservationInfo.principal}}). If not authorized, the unreserve operation is rejected.",2.0,0,0.5,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.21794871794871795,0.26666666666666666,0.26666666666666666,0.0
Task,Expose docker container IP in Master's state.json,"We want to expose docker container IP to Mesos-DNS. One potential solution is to make it available via Master's state.json. We can set a label ""Docker.NetworkSettings.IPAddress"" in TaskStatus message (when it is sent the first time with TASK_RUNNING status).",2.0,0,0.0,0.3453996983408748,0.0,0.0,0.0,0.0,0.2,0.38596491228070173,0.35714285714285715,0.28205128205128205,0.29090909090909095,0.28484848484848485,0.0
Bug,FTP response code for success not recognized by fetcher.,"The response code for successful HTTP requests is 200, the response code for successful FTP file transfers is 226. The fetcher currently only checks for a response code of 200 even for FTP URIs. This results in failed fetching even though the resource gets downloaded successfully. This has been found by a dedicated external test using an FTP server.
",1.0,0.23.0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.2631578947368421,0.36428571428571427,0.28205128205128205,0.296969696969697,0.296969696969697,0.2110091743119266
Bug,Stout's UUID re-seeds a new random generator during each call to UUID::random.,"Per [~StephanErb] and [~kevints]'s observations on MESOS-2940, stout's UUID abstraction is re-seeding the random generator during each call to {{UUID::random()}}, which is really expensive.

This is confirmed in the perf graph from MESOS-2940.",3.0,0,0.5,0.03469079939668175,0.0,0.0,0.0,0.0,0.0,0.07017543859649122,0.04285714285714286,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.0
Task,Maintenance information is not populated in case of failover,"When a master starts up, or after a master has failed, it must re-populate maintenance information (i.e. from the registry to the local state).

Particularly, {{Master::recover}} in {{src/master/master.cpp}} should be changed to process maintenance information.",3.0,0,0.0,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.0
Task,Master does not handle InverseOffers in the Accept call (Event/Call API),"InverseOffers are similar to Offers in that they are Accepted or Declined based on their OfferID.  

Some additional logic may be neccesary in Master::accept (src/master/master.cpp) to gracefully handle the acceptance of InverseOffers.
* The InverseOffer needs to be removed from the set of pending InverseOffers.
* The InverseOffer should not result any errors/warnings.  

Note: accepted InverseOffers do not preclude further InverseOffers from being sent to the framework.  Instead, an accepted InverseOffer merely signifies that the framework is _currently_ fine with the expected downtime.",3.0,0,0.5,0.42533936651583715,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3142857142857143,0.5769230769230769,0.6181818181818182,0.6242424242424242,0.0
Improvement,Allow executors binding IP to be different than Slave binding IP.,"Currently, the Slave will bind either to the loopback IP (127.0.0.1) or to the IP passed via the '--ip' flag. When it launches a containerized executor (e.g, via Mesos Containerizer), the executor inherits the binding IP of the Slave. This is due to the fact that the '--ip' flags sets the environment variable `LIBPROCESS_IP` to the passed IP. The executor then inherits this environment variable and is forced to bind to the Slave IP.

If an executor is running in its own containerized environment, with a separate IP than that of the Slave, currently there is no way of forcing it to bind to its own IP. A potential solution is to use the executor environment decorator hooks to update LIBPROCESS_IP environment variable for the executor.",2.0,0,0.0,0.3453996983408748,0.0,0.0,0.0,0.0,0.2,0.38596491228070173,0.35714285714285715,0.28205128205128205,0.29090909090909095,0.28484848484848485,0.0
Improvement,Add a SUPPRESS call to the scheduler,"SUPPRESS call is the complement to the current REVIVE call i.e., it will inform Mesos to stop sending offers to the framework. 

For the scheduler driver to send only Call messages (MESOS-2913), DeactivateFrameworkMessage needs to be converted to Call(s). We can implement this by having the driver send a SUPPRESS call followed by a DECLINE call for outstanding offers.",3.0,0.25.0,0.5,0.3137254901960784,0.0,0.0,0.0,0.0,0.4,0.19298245614035087,0.14285714285714285,0.8846153846153846,0.9878787878787878,0.9878787878787878,0.2293577981651376
Documentation,Document containerizer launch ,"We currently dont have enough documentation for the containerizer component. This task adds documentation for containerizer launch sequence.
The mail goals are:
- Have diagrams (state, sequence, class etc) depicting the containerizer launch process.
- Make the documentation newbie friendly.
- Usable for future design discussions.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3357142857142857,0.14102564102564102,0.2424242424242424,0.2424242424242424,0.0
Bug,0.22.x scheduler driver drops 0.23.x reconciliation status updates due to missing StatusUpdate.uuid.,"In the process of fixing MESOS-2940, we accidentally introduced a non-backwards compatible change:

--> StatusUpdate.uuid was required in 0.22.x and was always set.
--> StatusUpdate.uuid is optional in 0.23.x and the master is not setting it for master-generated updates.

In 0.22.x, the scheduler driver ignores the 'uuid' for master/driver generated updates already. I'd suggest the following fix:

# In 0.23.x, rather than not setting StatusUpdate.uuid, set it to an empty string.
# In 0.23.x, ensure the scheduler driver also ignores empty StatusUpdate.uuids.
# In 0.24.x, stop setting StatusUpdate.uuid.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.0
Task,Factoring out the pattern for URL generation ,"fetcher_test.cpp uses the following code for generating URLs:

string url = ""http://"" + net::getHostname(process.self().address.ip).get() + "":"" + stringify(process.self().address.port) + ""/"" + process.self().id

it would be good to isolate that code in a function, and replace the code above with something like:

string url = ""http://"" + endpoint_url(process, ""uri_test"");
",1.0,0,0.5,0.03469079939668175,0.0,0.0,0.0,0.0,0.0,0.07017543859649122,0.04285714285714286,0.11538461538461538,0.11515151515151516,0.11515151515151516,0.0
Improvement,Implement Docker Image Provisioner Reference Store,"Create a comprehensive store to look up an image and tag's associated image layer ID. Implement add, remove, save, and update images and their associated tags.",3.0,0,0.5,0.277526395173454,0.0,0.0,0.0,0.0,0.4,0.08771929824561403,0.10714285714285714,0.0,0.024242424242424242,0.024242424242424242,0.0
Improvement,"Expose major, minor and patch components from stout Version  ","Stout version class does not expose version components, preventing computations manipulation of version information.  Solution is to make major, minor and patch public.",1.0,0,0.0,0.06334841628959276,0.0,0.0,0.0,0.0,0.0,0.2982456140350877,0.19999999999999998,0.21794871794871795,0.1515151515151515,0.1515151515151515,0.0
Task,Add task status update hooks for Master/Slave,The task termination hooks are needed for doing task-specific cleanup in Master/Slave.,3.0,0,0.0,0.3453996983408748,0.0,0.0,0.0,0.0,0.2,0.38596491228070173,0.35714285714285715,0.28205128205128205,0.29090909090909095,0.28484848484848485,0.0
Task,Add hooks for Slave exits,"The hook will be triggered on slave exits. A master hook module can use this to do Slave-specific cleanups.

In our particular use case, the hook would trigger cleanup of IPs assigned to the given Slave (see the [design doc | https://docs.google.com/document/d/17mXtAmdAXcNBwp_JfrxmZcQrs7EO6ancSbejrqjLQ0g/edit#]).",2.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.017543859649122806,0.049999999999999996,0.28205128205128205,0.29090909090909095,0.28484848484848485,0.0
Task,"Extend ContainerInfo to include ""NetworkInfo"" message","As per the [design doc|https://docs.google.com/document/d/17mXtAmdAXcNBwp_JfrxmZcQrs7EO6ancSbejrqjLQ0g], we need to enable frameworks to specify network requirements. The proposed message could be along the lines of:

{code}
/**
 * Collection of network request.
 * TODO(kapil): Add a high-level explanation/motivation.
 */
message NetworkInfo {
  // Specify IPAddress requirement.
  enum Protocol {
    IPv4 = 0,
    IPv6 = 1
  }

  // TODO: Document how to use this field to request an
  // 1) IPv4 address
  // 2) IPv6 address
  // 3) Any of the above
  optional Protocol protocol = 1;

  // Statically assigned IPs provided by the Framework.
  optional string ip_address = 2;

  // A group is the name given to a set of logically-related IPs that are
  // allowed to communicate within themselves. For example, one might want 
  // to create separate groups for dev, testing, qa and prod deployment 
  // environments.  
  repeated string groups = 3;

  // To tag certain metadata to be used by Isolator/IPAM. E.g., rack, pop, etc.
  optional Labels labels = 4;
};

message ContainerInfo {
 …
 repeated NetworkInfo network_infos;
…
};

message ContainerStatus {
   repeated NetworkInfo network_infos;
}

message TaskStatus {
 …
 // TODO: Comment on the fact that this is resolved during container setup.
 optional ContainerStatus container;
…
};
{code}",2.0,0,0.0,0.3453996983408748,0.0,0.0,0.0,0.0,0.2,0.38596491228070173,0.35714285714285715,0.28205128205128205,0.29090909090909095,0.28484848484848485,0.0
Task,Support existing message passing optimization with Event/Call.,"See the thread here:
http://markmail.org/thread/wvapc7vkbv7z6gbx

The scheduler driver currently sends framework messages directly to the slave, when possible:

{noformat}
                  (through master)
    Scheduler  —————> Master  —————>  Slave ————>  Executor
     Driver    ————————————————————>                Driver
                   (skip master)
{noformat}

The slave always sends messages directly to the scheduler driver:
{noformat}
    Scheduler         Master          Slave <————  Executor
     Driver    <————————————————————                Driver
                   (skip master)
{noformat}

In order for the scheduler driver to receive Events from the master, it needs enough information to continue directly sending messages to slaves. This was previously accomplished by sending the slave's pid inside the [offer message|https://github.com/apache/mesos/blob/0.23.0-rc1/src/messages/messages.proto#L168]:

{code}
message ResourceOffersMessage {
  repeated Offer offers = 1;
  repeated string pids = 2;
}
{code}

We could add an 'Address' to the Offer protobuf to provide the scheduler driver with the same information:

{code}
message Address {
  required string ip;
  required string hostname;
  required uint32_t port;

  // All HTTP requests to this address must begin with this prefix.
  required string path_prefix;
}

message Offer {
  required OfferID id = 1;
  required FrameworkID framework_id = 2;
  required SlaveID slave_id = 3;
  required string hostname = 4;   // Deprecated in favor of 'address'.
  optional Address address = 8;  // Obviates 'hostname'.
  ...
}
{code}

The path prefix is required for testing purposes, where we can have multiple slaves within a process (e.g. {{localhost:5051/slave(1)/state.json}} vs. {{localhost:5051/slave(2)/state.json}}).

This provides enough information to allow the scheduler driver to continue to directly send messages to the slaves, which unblocks MESOS-2910.",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.0
Bug,SSL tests can fail depending on hostname configuration,"Depending on how /etc/hosts is configured, the SSL tests can fail with a bad hostname match for the certificate.
We can avoid this by explicitly matching the hostname for the certificate to the IP that will be used during the test.",3.0,0,0.0,0.42533936651583715,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3142857142857143,0.24358974358974358,0.22424242424242424,0.22424242424242424,0.0
Bug,Rename Option<T>::get(const T& _t) to getOrElse() broke network isolator,"Change to Option from get() to getOrElse() breaks network isolator.  Building with '../configure --with-network-isolator' generates the following error:

../../src/slave/containerizer/isolators/network/port_mapping.cpp: In static member function 'static Try<mesos::slave::Isolator*> mesos::internal::slave::PortMappingIsolatorProcess::create(const mesos::internal::slave::Flags&)':
../../src/slave/containerizer/isolators/network/port_mapping.cpp:1103:29: error: no matching function for call to 'Option<std::basic_string<char> >::get(const char [1]) const'
       flags.resources.get(""""),
                             ^
../../src/slave/containerizer/isolators/network/port_mapping.cpp:1103:29: note: candidates are:
In file included from ../../3rdparty/libprocess/3rdparty/stout/include/stout/check.hpp:26:0,
                 from ../../3rdparty/libprocess/include/process/check.hpp:19,
                 from ../../3rdparty/libprocess/include/process/collect.hpp:7,
                 from ../../src/slave/containerizer/isolators/network/port_mapping.cpp:30:
../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp:130:12: note: const T& Option<T>::get() const [with T = std::basic_string<char>]
   const T& get() const { assert(isSome()); return t; }
            ^
../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp:130:12: note:   candidate expects 0 arguments, 1 provided
../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp:131:6: note: T& Option<T>::get() [with T = std::basic_string<char>]
   T& get() { assert(isSome()); return t; }
      ^
../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp:131:6: note:   candidate expects 0 arguments, 1 provided
make[2]: *** [slave/containerizer/isolators/network/libmesos_no_3rdparty_la-port_mapping.lo] Error 1
make[2]: Leaving directory `/home/pbrett/sandbox/mesos.master/build/src'
make[1]: *** [check] Error 2
make[1]: Leaving directory `/home/pbrett/sandbox/mesos.master/build/src'
make: *** [check-recursive] Error 1
",1.0,0.24.0,0.5,0.42533936651583715,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3142857142857143,0.21794871794871795,0.1515151515151515,0.1515151515151515,0.2201834862385321
Bug,SSL connection failure causes failed CHECK.,"{code}
[ RUN      ] SSLTest.BasicSameProcess
F0706 18:32:28.465451 238583808 libevent_ssl_socket.cpp:507] Check failed: 'self->bev' Must be non NULL
{code}",3.0,0,0.0,0.42533936651583715,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3142857142857143,0.24358974358974358,0.22424242424242424,0.22424242424242424,0.0
Bug,Document  per container unique egress flow and network queueing statistics,Document new network isolation capabilities in 0.23,3.0,0.23.0,0.0,0.06334841628959276,0.0,0.0,0.0,0.0,0.0,0.2982456140350877,0.19999999999999998,0.21794871794871795,0.1515151515151515,0.1515151515151515,0.2110091743119266
Bug,Compilation Error on Mac OS 10.10.4 with clang 3.5.0,"Compiling 0.23.0 (rc1) produces compilation errors on Mac OS 10.10.4 with {{g++}} based on LLVM 3.5. It looks like the issue was introduced in {{a5640ad813e6256b548fca068f04fd9fa3a03eda}}, https://reviews.apache.org/r/32838. In contrast to the commit message, compiling the rc with gcc4.4 on CentOS worked fine for me. 

According to 0.23 release notes and MESOS-2604, we should support clang 3.5. 

{code}
../../../../../3rdparty/libprocess/3rdparty/stout/tests/os_tests.cpp:543:25: error: conversion from 'void ()' to 'const Option<void (*)()>' is ambiguous
                   Fork(dosetsid,          // Great-great-granchild.
                        ^~~~~~~~
../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp:40:3: note: candidate constructor
  Option(const T& _t) : state(SOME), t(_t) {}
  ^
../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp:42:3: note: candidate constructor
  Option(T&& _t) : state(SOME), t(std::move(_t)) {}
  ^
../../../../../3rdparty/libprocess/3rdparty/stout/include/stout/option.hpp:45:3: note: candidate constructor [with U = void ()]
  Option(const U& u) : state(SOME), t(u) {}
  ^
{code}

Compiler version:
{code}
$ g++ --version
Configured with: --prefix=/Applications/Xcode.app/Contents/Developer/usr --with-gxx-include-dir=/usr/include/c++/4.2.1
Apple LLVM version 6.0 (clang-600.0.54) (based on LLVM 3.5svn)
Target: x86_64-apple-darwin14.4.0
Thread model: posix
{code}
",1.0,0.23.0,1.0,0.6726998491704375,0.0,0.0,0.0,0.1,0.6000000000000001,0.2807017543859649,0.29285714285714287,0.717948717948718,0.8242424242424242,0.09696969696969697,0.2110091743119266
Improvement,Deprecating '.json' extension in files endpoints url,Remove the '.json' extension on endpoints such as `/files/browse.json` so it become `/files/browse`,1.0,0,0.0,0.0784313725490196,0.3333333333333333,0.5,0.14285714285714285,0.0,0.0,0.05263157894736842,0.09285714285714286,0.038461538461538464,0.048484848484848485,0.048484848484848485,0.0
Improvement,Deprecating '.json' extension in slave endpoints url,Remove the '.json' extension on endpoints such as `/slave/state.json` so it become `/slave/state`,1.0,0,0.0,0.0784313725490196,0.3333333333333333,0.5,0.14285714285714285,0.0,0.0,0.05263157894736842,0.09285714285714286,0.038461538461538464,0.048484848484848485,0.048484848484848485,0.0
Bug,SSL tests don't work with --gtest_repeat,"commit bfa89f22e9d6a3f365113b32ee1cac5208a0456f
Author: Joris Van Remoortere <joris.van.remoortere@gmail.com>
Date:   Wed Jul 1 16:16:52 2015 -0700

    MESOS-2973: Allow SSL tests to run using gtest_repeat.
    
    The SSL ctx object carried some settings between reinitialize()
    calls. Re-construct the object to avoid this state transition.
    
    Review: https://reviews.apache.org/r/36074",3.0,0,0.0,0.42533936651583715,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3142857142857143,0.24358974358974358,0.22424242424242424,0.22424242424242424,0.0
Improvement,Implement shared copy based provisioner backend,"Currently Appc and Docker both implemented its own copy backend, but most of the logic is the same where the input is just a image name with its dependencies.
We can refactor both so that we just have one implementation that is shared between both provisioners, so appc and docker can reuse the shared copy backend.",3.0,0,0.0,0.277526395173454,0.0,0.0,0.0,0.0,0.4,0.08771929824561403,0.10714285714285714,0.02564102564102564,0.09090909090909091,0.09090909090909091,0.0
Improvement,Add implicit cast to string operator to Path.,"For example:

{code}inline Try<Nothing> rm(const std::string& path){code} does not have an overload for {code}inline Try<Nothing> rm(const Path& path){code}

The implementation should be something like: 
{code}
inline Try<Nothing> rm(const Path& path)
{
  rm(path.value);
}
{code}",2.0,0,0.5,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.11538461538461538,0.11515151515151516,0.11515151515151516,0.0
Improvement,libprocess io does not support peek(),"Finally, I so wish we could just do:

{code}
io::peek(request->socket, 6)
  .then([request](const string& data) {
    // Comment about the rules ...
    if (data.length() < 2) { // Rule 1
    
    } else if (...) { // Rule 2.
    
    } else if (...) { // Rule 3.
    
    }
    
    if (ssl) {
      accept_SSL_callback(request);
    } else {
      ...;
    }
  });
{code}

from:
https://reviews.apache.org/r/31207/",3.0,0,0.0,0.07088989441930618,0.0,0.0,0.0,0.0,0.2,0.14035087719298245,0.10714285714285714,0.11538461538461538,0.11515151515151516,0.11515151515151516,0.0
Bug,Slave fails with Abort stacktrace when DNS cannot resolve hostname,"If the DNS cannot resolve the hostname-to-IP for a slave node, we correctly return an {{Error}} object, but we then fail with a segfault.

This code adds a more user-friendly message and exits normally (with an {{EXIT_FAILURE}} code).

For example, forcing {{net::getIp()}} to always return an {{Error}}, now causes the slave to exit like this:

{noformat}
$ ./bin/mesos-slave.sh --master=10.10.1.121:5405
WARNING: Logging before InitGoogleLogging() is written to STDERR
E0630 11:31:45.777465 1944417024 process.cpp:899] Could not obtain the IP address for stratos.local; the DNS service may not be able to resolve it: >>> Marco was here!!!

$ echo $?
1
{noformat}",1.0,0.22.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15789473684210525,0.20714285714285713,0.1794871794871795,0.15757575757575756,0.17575757575757575,0.20192660550458713
Task,Add cpuacct subsystem utils to cgroups,"Current cgroups implementation does not have a cpuacct subsystem implementation. This subsystem reports important metrics like user and system CPU ticks spent by a process. ""cgroups"" namespace has subsystem specific utilities for ""cpu"", ""memory"" etc. It could use other subsystems specific utils (eg. cpuacct).

In the future, we could also view cgroups as a mesos-subsystem with  features like event notifications.

Although refactoring cgroups would be a different epic, listing the possible tasks:
  -  Have hierarchies, subsystems abstracted to represent the domain 
  - Create  ""cgroups service""
  -  ""cgroups service"" listen to update events from the OS on files like stats. This would be an interrupt based system(maybe use linux fsnotify)
  - ""cgroups service"" services events to mesos (containers for example).

",2.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3357142857142857,0.14102564102564102,0.2424242424242424,0.2424242424242424,0.0
Improvement,Add version to MasterInfo,This will help schedulers figure out the version of the master that they are interacting with. See MESOS-2736 for additional context.,1.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.15789473684210525,0.20714285714285713,0.8846153846153846,0.9878787878787878,0.9878787878787878,0.0
Bug,Stack trace in isolator tests on Linux VM,"PerfEventIsolatorTest fails with stack trace when run in Linux VM

[----------] 1 test from PerfEventIsolatorTest
[ RUN      ] PerfEventIsolatorTest.ROOT_CGROUPS_Sample
F0629 11:38:17.088412 14114 isolator_tests.cpp:837] CHECK_SOME(isolator): Failed to create PerfEvent isolator, invalid events: { cycles, task-clock } 
*** Check failure stack trace: ***
    @     0x2ab5e5aeeb1a  google::LogMessage::Fail()
    @     0x2ab5e5aeea66  google::LogMessage::SendToLog()
    @     0x2ab5e5aee468  google::LogMessage::Flush()
    @     0x2ab5e5af137c  google::LogMessageFatal::~LogMessageFatal()
    @           0x864b0c  _CheckFatal::~_CheckFatal()
    @           0xc458ed  mesos::internal::tests::PerfEventIsolatorTest_ROOT_CGROUPS_Sample_Test::TestBody()
    @          0x119fb17  testing::internal::HandleSehExceptionsInMethodIfSupported<>()
    @          0x119ac9e  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @          0x118305f  testing::Test::Run()
    @          0x1183782  testing::TestInfo::Run()
    @          0x1183d0a  testing::TestCase::Run()
    @          0x11889d4  testing::internal::UnitTestImpl::RunAllTests()
    @          0x11a09ae  testing::internal::HandleSehExceptionsInMethodIfSupported<>()
    @          0x119b9c3  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @          0x11878e0  testing::UnitTest::Run()
    @           0xcdc8c7  main
    @     0x2ab5e7fdbec5  (unknown)
    @           0x861a89  (unknown)
make[3]: *** [check-local] Aborted (core dumped)

[ RUN      ] UserCgroupIsolatorTest/2.ROOT_CGROUPS_UserCgroup
F0629 11:49:38.763434 18836 isolator_tests.cpp:1200] CHECK_SOME(isolator): Failed to create PerfEvent isolator, invalid events: { cpu-cycles } 
*** Check failure stack trace: ***
    @     0x2ba40eb2db1a  google::LogMessage::Fail()
    @     0x2ba40eb2da66  google::LogMessage::SendToLog()
    @     0x2ba40eb2d468  google::LogMessage::Flush()
    @     0x2ba40eb3037c  google::LogMessageFatal::~LogMessageFatal()
    @           0x864b0c  _CheckFatal::~_CheckFatal()
    @           0xc5ddb1  mesos::internal::tests::UserCgroupIsolatorTest_ROOT_CGROUPS_UserCgroup_Test<>::TestBody()
    @          0x119fc43  testing::internal::HandleSehExceptionsInMethodIfSupported<>()
    @          0x119adca  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @          0x118318b  testing::Test::Run()
    @          0x11838ae  testing::TestInfo::Run()
    @          0x1183e36  testing::TestCase::Run()
    @          0x1188b00  testing::internal::UnitTestImpl::RunAllTests()
    @          0x11a0ada  testing::internal::HandleSehExceptionsInMethodIfSupported<>()
    @          0x119baef  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @          0x1187a0c  testing::UnitTest::Run()
    @           0xcdc9f3  main
    @     0x2ba41101aec5  (unknown)
    @           0x861a89  (unknown)
make[3]: *** [check-local] Aborted (core dumped)

",1.0,0,0.0,0.06334841628959276,0.0,0.0,0.0,0.0,0.0,0.2982456140350877,0.19999999999999998,0.21794871794871795,0.1515151515151515,0.1515151515151515,0.0
Improvement,Inefficient container usage collection,"docker containerizer currently collects usage statistics by calling os's process statistics (eg ps ). There is scope for making this efficient, say by querying cgroups file system.

",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3357142857142857,0.14102564102564102,0.2424242424242424,0.2424242424242424,0.0
Task,Draft design for generalized Authorizer interface,"As mentioned in MESOS-2948 the current {{mesos::Authorizer}} interface is rather inflexible if new _Actions_ or _Objects_ need to be added.

A new API needs to be designed in a way that allows for arbitrary _Actions_ and _Objects_ to be added to the authorization mechanism without having to recompile mesos.",3.0,0,0.0,0.004524886877828055,0.0,0.0,0.0,0.0,0.2,0.12280701754385964,0.24285714285714285,0.10256410256410256,0.11515151515151516,0.11515151515151516,0.0
Improvement,Authorizer Module: Interface design,"h4.Motivation
Design an interface covering authorizer modules while staying minimally invasive in regards to changes to the existing {{LocalAuthorizer}} implementation.
",2.0,0,0.5,0.004524886877828055,0.0,0.0,0.0,0.0,0.2,0.12280701754385964,0.24285714285714285,0.3717948717948718,0.24848484848484845,0.24848484848484845,0.0
Bug,Use of EXPECT in test and relying on the checked condition afterwards.,"In docker_containerizer_test we have the following pattern.

{code}
    EXPECT_NE(0u, offers.get().size());

    const Offer& offer = offers.get()[0];
{code}

As we rely on the value afterwards we should use ASSERT_NE instead. In that case the test will fail immediately. ",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12280701754385964,0.32857142857142857,0.11538461538461538,0.23636363636363636,0.23636363636363636,0.0
Bug,mesos fails to compile under mac when libssl and libevent are enabled,"../configure --enable-debug --enable-libevent --enable-ssl && make

produces the following error:

poll.cpp' || echo '../../../3rdparty/libprocess/'`src/libevent_poll.cpp
libtool: compile:  g++ -DPACKAGE_NAME=\""libprocess\"" -DPACKAGE_TARNAME=\""libprocess\"" -DPACKAGE_VERSION=\""0.0.1\"" ""-DPACKAGE_STRING=\""libprocess 0.0.1\"""" -DPACKAGE_BUGREPORT=\""\"" -DPACKAGE_URL=\""\"" -DPACKAGE=\""libprocess\"" -DVERSION=\""0.0.1\"" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_DLFCN_H=1 -DLT_OBJDIR=\"".libs/\"" -DHAVE_APR_POOLS_H=1 -DHAVE_LIBAPR_1=1 -DHAVE_SVN_VERSION_H=1 -DHAVE_LIBSVN_SUBR_1=1 -DHAVE_SVN_DELTA_H=1 -DHAVE_LIBSVN_DELTA_1=1 -DHAVE_LIBCURL=1 -DHAVE_EVENT2_EVENT_H=1 -DHAVE_LIBEVENT=1 -DHAVE_EVENT2_THREAD_H=1 -DHAVE_LIBEVENT_PTHREADS=1 -DHAVE_OPENSSL_SSL_H=1 -DHAVE_LIBSSL=1 -DHAVE_LIBCRYPTO=1 -DHAVE_EVENT2_BUFFEREVENT_SSL_H=1 -DHAVE_LIBEVENT_OPENSSL=1 -DUSE_SSL_SOCKET=1 -DHAVE_PTHREAD_PRIO_INHERIT=1 -DHAVE_PTHREAD=1 -DHAVE_LIBZ=1 -DHAVE_LIBDL=1 -I. -I../../../3rdparty/libprocess -I../../../3rdparty/libprocess/include -I../../../3rdparty/libprocess/3rdparty/stout/include -I3rdparty/boost-1.53.0 -I3rdparty/libev-4.15 -I3rdparty/picojson-4f93734 -I3rdparty/glog-0.3.3/src -I3rdparty/ry-http-parser-1c3624a -I/usr/local/opt/openssl/include -I/usr/local/opt/libevent/include -I/usr/local/opt/subversion/include/subversion-1 -I/usr/include/apr-1 -I/usr/include/apr-1.0 -g1 -O0 -std=c++11 -stdlib=libc++ -DGTEST_USE_OWN_TR1_TUPLE=1 -MT libprocess_la-libevent_poll.lo -MD -MP -MF .deps/libprocess_la-libevent_poll.Tpo -c ../../../3rdparty/libprocess/src/libevent_poll.cpp  -fno-common -DPIC -o libprocess_la-libevent_poll.o
mv -f .deps/libprocess_la-socket.Tpo .deps/libprocess_la-socket.Plo
mv -f .deps/libprocess_la-subprocess.Tpo .deps/libprocess_la-subprocess.Plo
mv -f .deps/libprocess_la-libevent.Tpo .deps/libprocess_la-libevent.Plo
mv -f .deps/libprocess_la-metrics.Tpo .deps/libprocess_la-metrics.Plo
In file included from ../../../3rdparty/libprocess/src/libevent_ssl_socket.cpp:11:
In file included from ../../../3rdparty/libprocess/include/process/queue.hpp:9:
../../../3rdparty/libprocess/include/process/future.hpp:849:7: error: no viable conversion from 'const process::Future<const process::Future<process::network::Socket> >' to 'const process::network::Socket'
 set(u);
     ^
../../../3rdparty/libprocess/src/libevent_ssl_socket.cpp:769:10: note: in instantiation of function template specialization 'process::Future<process::network::Socket>::Future<process::Future<const process::Future<process::network::Socket> > >' requested here
 return accept_queue.get()
        ^
../../../3rdparty/libprocess/include/process/socket.hpp:21:7: note: candidate constructor (the implicit move constructor) not viable: no known conversion from 'const process::Future<const process::Future<process::network::Socket> >' to
     'process::network::Socket &&' for 1st argument
class Socket
     ^
../../../3rdparty/libprocess/include/process/socket.hpp:21:7: note: candidate constructor (the implicit copy constructor) not viable: no known conversion from 'const process::Future<const process::Future<process::network::Socket> >' to
     'const process::network::Socket &' for 1st argument
class Socket
     ^
../../../3rdparty/libprocess/include/process/future.hpp:411:21: note: passing argument to parameter '_t' here
 bool set(const T& _t);
                   ^
1 error generated.
make[4]: *** [libprocess_la-libevent_ssl_socket.lo] Error 1
make[4]: *** Waiting for unfinished jobs....
mv -f .deps/libprocess_la-libevent_poll.Tpo .deps/libprocess_la-libevent_poll.Plo
mv -f .deps/libprocess_la-openssl.Tpo .deps/libprocess_la-openssl.Plo
mv -f .deps/libprocess_la-process.Tpo .deps/libprocess_la-process.Plo
make[3]: *** [all-recursive] Error 1
make[2]: *** [all-recursive] Error 1
make[1]: *** [all] Error 2
make: *** [all-recursive] Error 1",2.0,0.23.0,0.5,0.42533936651583715,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3142857142857143,0.11538461538461538,0.11515151515151516,0.11515151515151516,0.2110091743119266
Improvement,Reconciliation is expensive for large numbers of tasks.,"We've observed that both implicit and explicit reconciliation are expensive for large numbers of tasks:

{noformat: title=Explicit O(100,000) tasks: 70secs}
I0625 20:55:23.716320 21937 master.cpp:3863] Performing explicit task state reconciliation for N tasks of framework F (NAME) at S@IP:PORT
I0625 20:56:34.812464 21937 master.cpp:5041] Removing task T with resources R of framework F on slave S at slave(1)@IP:PORT (HOST)
{noformat}

{noformat: title=Implicit with O(100,000) tasks: 60secs}
I0625 20:25:22.310601 21936 master.cpp:3802] Performing implicit task state reconciliation for framework F (NAME) at S@IP:PORT
I0625 20:26:23.874528 21921 master.cpp:218] Scheduling shutdown of slave S due to health check timeout
{noformat}

Let's add a benchmark to see if there are any bottlenecks here, and to guide improvements.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.0
Story,Testing the new workflow,"This is a simple test story to try out the new workflow.

Unfortunately, testing and getting it to work seems to be something that actually does take up time, so I'm tracking this here.",3.0,0.23.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15789473684210525,0.20714285714285713,0.1794871794871795,0.15757575757575756,0.17575757575757575,0.2110091743119266
Bug,Linux docker inspect crashes,"On linux,  when a simple task is being executed on docker container executor, the sandbox stderr shows a backtrace:

*** Aborted at 1435254156 (unix time) try ""date -d @1435254156"" if you are using GNU date ***
PC: @     0x7ffff2b1364d (unknown)
*** SIGSEGV (@0xfffffffffffffff8) received by PID 88424 (TID 0x7fffe88fb700) from PID 18446744073709551608; stack trace: ***
    @     0x7ffff25a4340 (unknown)
    @     0x7ffff2b1364d (unknown)
    @     0x7ffff2b724df (unknown)
    @           0x4a6466 Docker::Container::~Container()
    @     0x7ffff5bfa49a Option<>::~Option()
    @     0x7ffff5c15989 Option<>::operator=()
    @     0x7ffff5c09e9f Try<>::operator=()
    @     0x7ffff5c09ee3 Result<>::operator=()
    @     0x7ffff5c0a938 process::Future<>::set()
    @     0x7ffff5bff412 process::Promise<>::set()
    @     0x7ffff5be53e3 Docker::___inspect()
    @     0x7ffff5be3cf8 _ZZN6Docker9__inspectERKSsRKN7process5OwnedINS2_7PromiseINS_9ContainerEEEEERK6OptionI8DurationENS2_6FutureISsEERKNS2_10SubprocessEENKUlRKSG_E1_clESL_
    @     0x7ffff5be91e9 _ZZNK7process6FutureISsE5onAnyIZN6Docker9__inspectERKSsRKNS_5OwnedINS_7PromiseINS3_9ContainerEEEEERK6OptionI8DurationES1_RKNS_10SubprocessEEUlRKS1_E1_vEESM_OT_NS1_6PreferEENUlSM_E_clESM_
    @     0x7ffff5be9d9d _ZNSt17_Function_handlerIFvRKN7process6FutureISsEEEZNKS2_5onAnyIZN6Docker9__inspectERKSsRKNS0_5OwnedINS0_7PromiseINS7_9ContainerEEEEERK6OptionI8DurationES2_RKNS0_10SubprocessEEUlS4_E1_vEES4_OT_NS2_6PreferEEUlS4_E_E9_M_invokeERKSt9_Any_dataS4_
    @     0x7ffff5c1eadd std::function<>::operator()()
    @     0x7ffff5c15e07 process::Future<>::onAny()
    @     0x7ffff5be93a1 _ZNK7process6FutureISsE5onAnyIZN6Docker9__inspectERKSsRKNS_5OwnedINS_7PromiseINS3_9ContainerEEEEERK6OptionI8DurationES1_RKNS_10SubprocessEEUlRKS1_E1_vEESM_OT_NS1_6PreferE
    @     0x7ffff5be87f6 _ZNK7process6FutureISsE5onAnyIZN6Docker9__inspectERKSsRKNS_5OwnedINS_7PromiseINS3_9ContainerEEEEERK6OptionI8DurationES1_RKNS_10SubprocessEEUlRKS1_E1_EESM_OT_
    @     0x7ffff5be459c Docker::__inspect()
    @     0x7ffff5be337c _ZZN6Docker8_inspectERKSsRKN7process5OwnedINS2_7PromiseINS_9ContainerEEEEERK6OptionI8DurationEENKUlvE_clEv
    @     0x7ffff5be8c5a _ZZNK7process6FutureI6OptionIiEE5onAnyIZN6Docker8_inspectERKSsRKNS_5OwnedINS_7PromiseINS5_9ContainerEEEEERKS1_I8DurationEEUlvE_vEERKS3_OT_NS3_10LessPreferEENUlSL_E_clESL_
    @     0x7ffff5be9b36 _ZNSt17_Function_handlerIFvRKN7process6FutureI6OptionIiEEEEZNKS4_5onAnyIZN6Docker8_inspectERKSsRKNS0_5OwnedINS0_7PromiseINS9_9ContainerEEEEERKS2_I8DurationEEUlvE_vEES6_OT_NS4_10LessPreferEEUlS6_E_E9_M_invokeERKSt9_Any_dataS6_
    @     0x7ffff5c1e9b3 std::function<>::operator()()
    @     0x7ffff6184a1a _ZN7process8internal3runISt8functionIFvRKNS_6FutureI6OptionIiEEEEEJRS6_EEEvRKSt6vectorIT_SaISD_EEDpOT0_
    @     0x7ffff617e64d process::Future<>::set()
    @     0x7ffff6752e46 process::Promise<>::set()
    @     0x7ffff675faec process::internal::cleanup()
    @     0x7ffff6765293 _ZNSt5_BindIFPFvRKN7process6FutureI6OptionIiEEEPNS0_7PromiseIS3_EERKNS0_10SubprocessEESt12_PlaceholderILi1EES9_SA_EE6__callIvIS6_EILm0ELm1ELm2EEEET_OSt5tupleIIDpT0_EESt12_Index_tupleIIXspT1_EEE
    @     0x7ffff6764bcd _ZNSt5_BindIFPFvRKN7process6FutureI6OptionIiEEEPNS0_7PromiseIS3_EERKNS0_10SubprocessEESt12_PlaceholderILi1EES9_SA_EEclIJS6_EvEET0_DpOT_
    @     0x7ffff67642a5 _ZZNK7process6FutureI6OptionIiEE5onAnyISt5_BindIFPFvRKS3_PNS_7PromiseIS2_EERKNS_10SubprocessEESt12_PlaceholderILi1EESA_SB_EEvEES7_OT_NS3_6PreferEENUlS7_E_clES7_
    @     0x7ffff676531d _ZNSt17_Function_handlerIFvRKN7process6FutureI6OptionIiEEEEZNKS4_5onAnyISt5_BindIFPFvS6_PNS0_7PromiseIS3_EERKNS0_10SubprocessEESt12_PlaceholderILi1EESC_SD_EEvEES6_OT_NS4_6PreferEEUlS6_E_E9_M_invokeERKSt9_Any_dataS6_
    @     0x7ffff5c1e9b3 std::function<>::operator()()
(END)
 ",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3357142857142857,0.14102564102564102,0.2424242424242424,0.2424242424242424,0.0
Improvement,Update stout #include headers,Update stout to #include headers for symbols we rely on and reorder to comply with the style guide.,2.0,0,0.0,0.06334841628959276,0.0,0.0,0.0,0.0,0.0,0.2982456140350877,0.19999999999999998,0.21794871794871795,0.1515151515151515,0.1515151515151515,0.0
Bug,Invalid usage of ATOMIC_FLAG_INIT in member initialization,"The C++ specification states:

The macro ATOMIC_FLAG_INIT shall be defined in such a way that it can be used to initialize an object of type atomic_flag to the clear state. The macro can be used in the form: ""atomic_flag guard = ATOMIC_FLAG_INIT; ""It is unspecified whether the macro can be used in other initialization contexts."" 

Clang catches this (although reports it erroneously as a braced scaled init issue) and refuses to compile libprocess.",1.0,0.23.0,0.0,0.06334841628959276,0.0,0.0,0.0,0.0,0.0,0.2982456140350877,0.19999999999999998,0.21794871794871795,0.1515151515151515,0.1515151515151515,0.2110091743119266
Improvement,Add move constructors / assignment to Future.,"Now that we have C++11, let's add move constructors and move assignment operators for Future, similarly to what was done for Option. There is currently one move constructor for Future<T>, but not for T, U, and no assignment operator.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.0
Improvement,Add move constructors / assignment to Result.,"Now that we have C++11, let's add move constructors and move assignment operators for Result, similarly to what was done for Option.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.0
Improvement,Add move constructors / assignment to Try.,"Now that we have C++11, let's add move constructors and move assignment operators for Try, similarly to what was done for Option.",3.0,0,0.5,0.5701357466063348,0.0,0.0,0.0,0.0,0.0,0.6842105263157894,0.5642857142857143,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.0
Bug,Specify correct libnl version for configure check,"Currently configure.ac lists 3.2.24 as the required libnl version. However, https://reviews.apache.org/r/31503 caused the minimum required version to be bumped to 3.2.26. The configure check thus fails to error out during execution and the dependency is captured only during the build step.",1.0,0,0.0,0.3453996983408748,0.0,0.0,0.0,0.0,0.2,0.38596491228070173,0.35714285714285715,0.28205128205128205,0.29090909090909095,0.28484848484848485,0.0
Task,Add an Event message handler to scheduler library,"Adding this handler lets master send Event messages to the library.

See MESOS-2909 for additional context.

This ticket only tracks the installation of the handler and maybe handling of a single event for testing. Additional events handling will be captured in a different ticket(s).",3.0,0,0.5,0.5701357466063348,0.0,0.0,0.0,0.0,0.0,0.6842105263157894,0.5642857142857143,0.8846153846153846,0.9878787878787878,0.9878787878787878,0.0
Task,Add version field to RegisterFrameworkMessage and ReregisterFrameworkMessage,"In the same way we added 'version' field to RegisterSlaveMessage and ReregisterSlaveMessage, we should do it framework (re-)registration messages. This would help master determine which version of scheduler driver it is talking to.

We want this so that master can start sending Event messages to the scheduler driver (and scheduler library). In the long term, master will send a streaming response to the libraries, but in the meantime we can test the event protobufs by sending Event messages.",3.0,0,0.5,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.8846153846153846,0.9878787878787878,0.9878787878787878,0.0
Task,Slave : Synchronous Validation for Calls,"/call endpoint on the slave will return a 202 accepted code but has to do some basic validations before. In case of invalidation it will return a {{BadRequest}} back to the client.

- We need to create the required infrastructure to validate the request and then process it similar to {{src/master/validation.cpp}} in the {{namespace scheduler}} i.e. check if the protobuf is properly initialized, has the required attributes set pertaining to the call message etc.",3.0,0,0.5,0.0784313725490196,0.3333333333333333,0.5,0.14285714285714285,0.0,0.0,0.05263157894736842,0.09285714285714286,0.3846153846153846,0.4181818181818182,0.4121212121212121,0.0
Bug,Add slave metric to count container launch failures,"We have seen circumstances where a machine has been consistently unable to launch containers due to an inconsistent state (for example, unexpected network configuration).   Adding a metric to track container launch failures will allow us to detect and alert on slaves in such a state.",1.0,0,0.0,0.06334841628959276,0.0,0.0,0.0,0.0,0.0,0.2982456140350877,0.19999999999999998,0.21794871794871795,0.1515151515151515,0.1515151515151515,0.0
Bug,Network isolator should not fail when target state already exists,"Network isolator has multiple instances of the following pattern:

{noformat}
  Try<bool> something = ....::create();                                  
  if (something.isError()) {                                                   
    ++metrics.something_errors;                                      
    return Failure(""Failed to create something ..."")
  } else if (!icmpVethToEth0.get()) {                                               
    ++metrics.adding_veth_icmp_filters_already_exist;                               
    return Failure(""Something already exists"");
  }                                                                                 
{noformat}

These failures have occurred in operation due to the failure to recover or delete an orphan, causing the slave to remain on line but unable to create new resources.    We should convert the second failure message in this pattern to an information message since the final state of the system is the state that we requested.",3.0,0.23.0,0.0,0.06334841628959276,0.0,0.0,0.0,0.0,0.0,0.2982456140350877,0.19999999999999998,0.21794871794871795,0.1515151515151515,0.1515151515151515,0.2110091743119266
Task,Write tests for new JSON (ZooKeeper) functionality,"Follow up from MESOS-2340, need to ensure this does not break the ZooKeeper discovery functionality.",2.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15789473684210525,0.20714285714285713,0.1794871794871795,0.15757575757575756,0.17575757575757575,0.0
Task,Add queue size metrics for the allocator.,"In light of the performance regression in MESOS-2891, we'd like to have visibility into the queue size of the allocator. This will enable alerting on performance problems.

We currently have no metrics in the allocator.

I will also look into MESOS-1286 now that we have gcc 4.8, current queue size gauges require a trip through the Process' queue.",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.0
Bug,Sandbox URL doesn't work in web-ui when using SSL,"The links to the sandbox in the web ui don't work when ssl is enabled. 
This can happen if the certificate for the master and the slave do not match. This is a consequence of the redirection that happens when serving files.
The resolution to this is currently to set up your certificates to serve the hostnames of the master and slaves.",3.0,0,0.0,0.42533936651583715,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3142857142857143,0.24358974358974358,0.22424242424242424,0.22424242424242424,0.0
Bug,Add SSL switch to python configuration,The python egg requires explicit dependencies for SSL. Add these to the python configuration if ssl is enabled.,3.0,0,0.5,0.07088989441930618,0.0,0.0,0.0,0.0,0.2,0.14035087719298245,0.10714285714285714,0.24358974358974358,0.22424242424242424,0.22424242424242424,0.0
Documentation,Capture some testing patterns we use in a doc,In Mesos tests we use some tricks and patterns to express certain expectations. These are not always obvious and not documented. The intent of the ticket is to kick-start the document with the description of those tricks for posterity.,1.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.717948717948718,0.8242424242424242,0.09696969696969697,0.0
Improvement,Do not call hook manager if no hooks installed,"Hooks modules allow us to provide decorators during various aspects of a task lifecycle such as label decorator, environment decorator, etc.  Often the call into such a decorator hooks results in a new copy of labels, environment, etc., being returned to the call site. This is an unnecessary overhead if there are no hooks installed.

The proper way would be to call decorators via the hook manager only if there are some hooks installed. This would prevent unnecessary copying overhead if no hooks are available.",2.0,0,0.0,0.3453996983408748,0.0,0.0,0.0,0.0,0.2,0.38596491228070173,0.35714285714285715,0.28205128205128205,0.29090909090909095,0.28484848484848485,0.0
Bug,Random recursive_mutex errors in when running make check,"While running make check on OS X, from time to time {{recursive_mutex}} errors appear after running all the test successfully. Just one of the experience messages actually stops {{make check}} reporting an error.

The following error messages have been experienced:

{code}
libc++abi.dylib: libc++abi.dylib: libc++abi.dylib: libc++abi.dylib: libc++abi.dylib: libc++abi.dylib: terminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argumentterminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argumentterminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argumentterminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argumentterminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argumentterminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argument


*** Aborted at 1434553937 (unix time) try ""date -d @1434553937"" if you are using GNU date ***
{code}

{code}
libc++abi.dylib: terminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argument
*** Aborted at 1434557001 (unix time) try ""date -d @1434557001"" if you are using GNU date ***
libc++abi.dylib: PC: @     0x7fff93855286 __pthread_kill
libc++abi.dylib: *** SIGABRT (@0x7fff93855286) received by PID 88060 (TID 0x10fc40000) stack trace: ***
    @     0x7fff8e1d6f1a _sigtramp
libc++abi.dylib:     @        0x10fc3f1a8 (unknown)
libc++abi.dylib:     @     0x7fff979deb53 abort
libc++abi.dylib: libc++abi.dylib: libc++abi.dylib: terminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argumentterminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argumentterminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argumentterminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argumentterminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argumentterminating with uncaught exception of type std::__1::system_error: recursive_mutex lock failed: Invalid argumentMaking check in include
{code}

{code}
Assertion failed: (e == 0), function ~recursive_mutex, file /SourceCache/libcxx/libcxx-120/src/mutex.cpp, line 82.
*** Aborted at 1434555685 (unix time) try ""date -d @1434555685"" if you are using GNU date ***
PC: @     0x7fff93855286 __pthread_kill
*** SIGABRT (@0x7fff93855286) received by PID 60235 (TID 0x7fff7ebdc300) stack trace: ***
    @     0x7fff8e1d6f1a _sigtramp
    @        0x10b512350 google::CheckNotNull<>()
    @     0x7fff979deb53 abort
    @     0x7fff979a6c39 __assert_rtn
    @     0x7fff9bffdcc9 std::__1::recursive_mutex::~recursive_mutex()
    @        0x10b881928 process::ProcessManager::~ProcessManager()
    @        0x10b874445 process::ProcessManager::~ProcessManager()
    @        0x10b874418 process::finalize()
    @        0x10b2f7aec main
    @     0x7fff98edc5c9 start
make[5]: *** [check-local] Abort trap: 6
make[4]: *** [check-am] Error 2
make[3]: *** [check-recursive] Error 1
make[2]: *** [check-recursive] Error 1
make[1]: *** [check] Error 2
make: *** [check-recursive] Error 1
{code}",1.0,0,0.5,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.10256410256410256,0.11515151515151516,0.11515151515151516,0.0
Bug,Convert PortMappingStatistics to use automatic JSON encoding/decoding,"Simplify PortMappingStatistics by using JSON::Protocol and protobuf::parse to convert ResourceStatistics to/from line format.

This change will simplify the implementation of MESOS-2332.",2.0,0,0.0,0.06334841628959276,0.0,0.0,0.0,0.0,0.0,0.2982456140350877,0.19999999999999998,0.21794871794871795,0.1515151515151515,0.1515151515151515,0.0
Bug,style hook prevent's valid markdown files from getting committed,"According to the original [markdown specification|http://daringfireball.net/projects/markdown/syntax#p] and to the most [recent standarization|http://spec.commonmark.org/0.20/#hard-line-breaks] effort, two spaces at the end of a line create a hard line break (it breaks the line without starting a new paragraph), similar to the html code {{<br/>}}. 

However, there's a hook in mesos which prevent files with trailing whitespace to be committed.",1.0,0,0.0,0.004524886877828055,0.0,0.0,0.0,0.0,0.2,0.12280701754385964,0.24285714285714285,0.10256410256410256,0.11515151515151516,0.11515151515151516,0.0
Bug,Slave should send oversubscribed resource information after master failover.,"After a master failover, if the total amount of oversubscribed resources does not change, then the slave will not send the UpdateSlave message to the new master. The slave needs to send the information to the new master regardless of this.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.0
Bug,"mesos-fetcher won't fetch uris which begin with a "" ""","Discovered while running mesos with marathon on top. If I launch a marathon task with a URI which is "" http://apache.osuosl.org/mesos/0.22.1/mesos-0.22.1.tar.gz"" mesos will log to stderr:

{code}
I0611 22:39:22.815636 35673 logging.cpp:177] Logging to STDERR
I0611 22:39:25.643889 35673 fetcher.cpp:214] Fetching URI ' http://apache.osuosl.org/mesos/0.22.1/mesos-0.22.1.tar.gz'
I0611 22:39:25.648111 35673 fetcher.cpp:94] Hadoop Client not available, skipping fetch with Hadoop Client
Failed to fetch:  http://apache.osuosl.org/mesos/0.22.1/mesos-0.22.1.tar.gz
Failed to synchronize with slave (it's probably exited)
{code}

It would be nice if mesos trimmed leading whitespace before doing protocol detection so that simple mistakes are just fixed. ",2.0,0.22.1,0.5,0.07088989441930618,0.0,0.0,0.0,0.0,0.2,0.14035087719298245,0.10714285714285714,0.08974358974358974,0.0787878787878788,0.07272727272727272,0.20192660550458713
Story,Create the basic infrastructure to handle /scheduler endpoint,"This is the first basic step in ensuring the basic {{/call}} functionality: processing a 
{noformat}
POST /call
{noformat}
and returning:

- {{202}} if all goes well;
- {{401}} if not authorized; and
- {{403}} if the request is malformed.

We'll get more sophisticated as the work progressed (eg, supporting {{415}} if the content-type is not of the right kind).",3.0,0,0.5,0.0784313725490196,0.3333333333333333,0.5,0.14285714285714285,0.0,0.0,0.05263157894736842,0.09285714285714286,0.1794871794871795,0.15757575757575756,0.17575757575757575,0.0
Bug,Resources::parse(...) allows different resources of the same name to have different types.,"So code like this doesn't raise Error.
{code}
Resources::parse(""foo(role1):1;foo(role2):[0-1]"")
{code}

Doesn't look like allowing this adds value and this complicates resource maths/validation/reporting.

We should disallow this.",2.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15789473684210525,0.15,0.20512820512820512,0.13939393939393938,0.13939393939393938,0.0
Improvement,Implement Docker image provisioner,"Provisions a Docker image (provisions all its dependent layers), fetch an image from persistent store, and also destroy an image. 

Done when tested for local discovery and copy backend. ",3.0,0,0.0,0.277526395173454,0.0,0.0,0.0,0.0,0.4,0.08771929824561403,0.10714285714285714,0.02564102564102564,0.09090909090909091,0.09090909090909091,0.0
Improvement,Local filesystem docker image discovery,"Given a docker image name and the local directory where images can be found, creates a URI with a path to the corresponding image.

Done when system successfully checks for the image, untars the image if necessary, and returns the proper URI to the image.",2.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.007142857142857143,0.02564102564102564,0.09090909090909091,0.09090909090909091,0.0
Improvement,Add and document new labels field to framework info,"Add and document new labels field to framework info:

{code}
message FrameworkInfo {
  // Used to determine the Unix user that an executor or task should
  // be launched as. If the user field is set to an empty string Mesos
  // will automagically set it to the current user.
  required string user = 1;

  // Name of the framework that shows up in the Mesos Web UI.
  required string name = 2;

  // Note that 'id' is only available after a framework has
  // registered, however, it is included here in order to facilitate
  // scheduler failover (i.e., if it is set then the
  // MesosSchedulerDriver expects the scheduler is performing
  // failover).
  optional FrameworkID id = 3;

  ...

  // This field allows a framework to advertise its set of
  // capabilities (e.g., ability to receive offers for revocable
  // resources).
  repeated Capability capabilities = 10;

  optional Labels labels = 11;
}
{code}",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.01282051282051282,0.06666666666666667,0.06666666666666667,0.0
Bug,In Resources JSON model() resources of the same name overwrite each other.,"As shown here: https://github.com/apache/mesos/blob/8559d7b7356ec91795e564767588c6f4519653a5/src/common/http.cpp#L50

So if there are two ""cpus"" of different roles, whichever comes later will overwrite the previous.

We should instead aggregate different resources of the same name.

However, in the presence of revocable resources, in order to maintain backwards compatibility we should exclude revocable resources.",2.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15789473684210525,0.15,0.20512820512820512,0.13939393939393938,0.13939393939393938,0.0
Improvement,Decode network statistics from mesos-network-helper,Decode network statistics from mesos-network-helper and output to slave statistics.json,1.0,0,0.0,0.06334841628959276,0.0,0.0,0.0,0.0,0.0,0.2982456140350877,0.19999999999999998,0.21794871794871795,0.1515151515151515,0.1515151515151515,0.0
Improvement,Report per-container metrics for network bandwidth throttling to the slave,Report per-container metrics for network bandwidth throttling to the slave in the output of mesos-network-helper.,1.0,0,0.0,0.06334841628959276,0.0,0.0,0.0,0.0,0.0,0.2982456140350877,0.19999999999999998,0.21794871794871795,0.1515151515151515,0.1515151515151515,0.0
Improvement,Support different perf output formats,"The output format of perf changes in 3.14 (inserting an additional field) and in again in 4.1 (appending additional) fields. See kernel commits:
410136f5dd96b6013fe6d1011b523b1c247e1ccb
d73515c03c6a2706e088094ff6095a3abefd398b

Update the perf::parse() function to understand all these formats.",3.0,0,0.5,0.06334841628959276,0.0,0.0,0.0,0.0,0.0,0.2982456140350877,0.19999999999999998,0.10256410256410256,0.2121212121212121,0.2121212121212121,0.0
Task,Pass callback to the QoS Controller to retrieve ResourceUsage from Resource Monitor on demand.,"We need to allow QoS Controller to call 'ResourceMonitor::usages()'. We will pass it in a lambda.
",2.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.017543859649122806,0.03571428571428571,0.0,0.006060606060606061,0.006060606060606061,0.0
Improvement,Document and consolidate qdisc handles,The structure of traffic control qdiscs and filters in non-trivial with the knowledge of which handles are the parents of which filters or qdiscs are in the create and recovery functions and will be needed to collect statistics on the links.  Lets pull out the constants and document them.,1.0,0,0.0,0.06334841628959276,0.0,0.0,0.0,0.0,0.0,0.2982456140350877,0.19999999999999998,0.21794871794871795,0.1515151515151515,0.1515151515151515,0.0
Improvement,Support revocable/non-revocable CPU updates in Mesos containerizer,"MESOS-2652 provided preliminary support for revocable cpu resources only when specified in the initial resources for a container. Improve this to support updates to/from revocable cpu. Note, *any* revocable cpu will result in the entire container's cpu being treated as revocable at the cpu isolator level. Higher level logic is responsible for adding/removing based on some policy.",3.0,0.23.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.10526315789473684,0.16428571428571428,0.10256410256410256,0.2121212121212121,0.2121212121212121,0.2110091743119266
Bug,Flaky test: FetcherCacheHttpTest.HttpCachedSerialized,"FetcherCacheHttpTest.HttpCachedSerialized has been observed to fail (once  so far), but normally works fine. Here is the failure output:

[ RUN      ] FetcherCacheHttpTest.HttpCachedSerialized

GMOCK WARNING:
Uninteresting mock function call - returning directly.
    Function call: resourceOffers(0x3cca8e0, @0x2b1053422b20 { 128-byte object <D0-E1 59-49 10-2B 00-00 00-00 00-00 00-00 00-00 10-A1 00-68 10-2B 00-00 A0-DE 00-68 10-2B 00-00 20-DF 00-68 10-2B 00-00 40-9C 00-68 10-2B 00-00 90-2D 00-68 10-2B 00-00 04-00 00-00 04-00 00-00 04-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 00-00 10-2B 00-00 00-00 00-00 0F-00 00-00> })
Stack trace:
F0604 13:08:16.377907  6813 fetcher_cache_tests.cpp:354] CHECK_READY(offers): is PENDING Failed to wait for resource offers
*** Check failure stack trace: ***
    @     0x2b10488ff6c0  google::LogMessage::Fail()
    @     0x2b10488ff60c  google::LogMessage::SendToLog()
    @     0x2b10488ff00e  google::LogMessage::Flush()
    @     0x2b1048901f22  google::LogMessageFatal::~LogMessageFatal()
    @           0x9721e4  _CheckFatal::~_CheckFatal()
    @           0xb4da86  mesos::internal::tests::FetcherCacheTest::launchTask()
    @           0xb53f8d  mesos::internal::tests::FetcherCacheHttpTest_HttpCachedSerialized_Test::TestBody()
    @          0x116ac21  testing::internal::HandleSehExceptionsInMethodIfSupported<>()
    @          0x1165e1e  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @          0x114e1df  testing::Test::Run()
    @          0x114e902  testing::TestInfo::Run()
    @          0x114ee8a  testing::TestCase::Run()
    @          0x1153b54  testing::internal::UnitTestImpl::RunAllTests()
    @          0x116ba93  testing::internal::HandleSehExceptionsInMethodIfSupported<>()
    @          0x1166b0f  testing::internal::HandleExceptionsInMethodIfSupported<>()
    @          0x1152a60  testing::UnitTest::Run()
    @           0xcbc50f  main
    @     0x2b104af78ec5  (unknown)
    @           0x867559  (unknown)
make[4]: *** [check-local] Aborted
make[4]: Leaving directory `/home/jenkins/jenkins-slave/workspace/mesos-reviewbot/mesos-0.23.0/_build/src'
make[3]: *** [check-am] Error 2
make[3]: Leaving directory `/home/jenkins/jenkins-slave/workspace/mesos-reviewbot/mesos-0.23.0/_build/src'
make[2]: *** [check] Error 2
make[2]: Leaving directory `/home/jenkins/jenkins-slave/workspace/mesos-reviewbot/mesos-0.23.0/_build/src'
make[1]: *** [check-recursive] Error 1
make[1]: Leaving directory `/home/jenkins/jenkins-slave/workspace/mesos-reviewbot/mesos-0.23.0/_build'
make: *** [distcheck] Error 1
",2.0,0.23.0,0.0,0.06184012066365008,0.0,0.0,0.0,0.0,0.2,0.10526315789473684,0.12857142857142856,0.14102564102564102,0.16969696969696968,0.17575757575757575,0.2110091743119266
Task,As a developer I need an easy way to convert MasterInfo protobuf to/from JSON,"As a preliminary to MESOS-2340, this requires the implementation of a simple (de)serialization mechanism to JSON from/to {{MasterInfo}} protobuf.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15789473684210525,0.20714285714285713,0.1794871794871795,0.15757575757575756,0.17575757575757575,0.0
Task,Jira workflow appears inconsistent,"See attached screenshot - the story is in the {{Accepted}} state, so it should now have a {{Start Progress}} button, but it has a {{Stop Progress}} one instead.

Also, when in the {{In Progress}} it has an {{Accept}} button (I think) or something similar; also other states appear inconsistent.

This Story is about first looking at the workflow; ensuring the stories and their status(es) are consistent; that button in the UI are consistently applied and then correct any issues that may have been identified.

The assumption here is that the workflow is:
{noformat}
Open >> Accepted >> Progress >> Reviewable >> Resolved >> Closed
   Accept       Start      Ready         Resolve      Close
{noformat}
and, at each stage, it can be moved ""back by one"" ({{Unaccept}}, {{Stop Progress}}, {{Unresolve}}) and that, at any stage, it can be moved to {{Closed}} (for whatever reason).",2.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15789473684210525,0.20714285714285713,0.1794871794871795,0.15757575757575756,0.17575757575757575,0.0
Improvement,Log framework capabilities in the master.,"Now that {{Capabilities}} has been added to FrameworkInfo, we should log these in the master when a framework (re-)registers (i.e. which capabilities are enabled and disabled). This would make debugging easier for framework developers.

Ideally, folding in the old {{checkpoint}} capability and logging that as well. In the past, the fact that {{checkpoint}} defaults to false has tripped up a lot of developers.",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.0
Improvement,Remove dynamic allocation from Future<T>,Remove the dynamic allocation of `T*` inside `Future::Data`,3.0,0,0.0,0.42533936651583715,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3142857142857143,0.24358974358974358,0.22424242424242424,0.22424242424242424,0.0
Improvement,Rename Option<T>::get(const T& _t) to getOrElse() and refactor the original function,"As suggested, if we want to change the name then we should refactor the original function as opposed to having 2 copies. 
If we did have 2 versions of the same function, would it make more sense to delegate one of them to the other.

As of today, there is only one file need to be refactor: 3rdparty/libprocess/3rdparty/stout/include/stout/os/osx.hpp at line 151, 161",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.007142857142857143,0.0,0.0,0.0,0.0
Improvement,Add support for container rootfs to Mesos isolators,Mesos containers can have a different rootfs to the host. Update Isolator interface to pass rootfs during Isolator::prepare(). Update Isolators where  necessary.,1.0,0.22.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.10526315789473684,0.16428571428571428,0.10256410256410256,0.2121212121212121,0.2121212121212121,0.20192660550458713
Bug,Remove duplicate literals in ingress & fq_codel queueing disciplines,"fq_codel and ingress queueing disciplines include multiple uses of the string literals ""ingress"" and ""fq_codel"".  Any mismatch in these would cause runtime errors which can be prevented at compile time.",1.0,0,0.0,0.06334841628959276,0.0,0.0,0.0,0.0,0.0,0.2982456140350877,0.19999999999999998,0.21794871794871795,0.1515151515151515,0.1515151515151515,0.0
Improvement,Added constexpr to C++11 whitelist.,constexpr is currently used to eliminate initialization dependency issues for non-POD objects.  We should add it to the whitelist of acceptable c++11 features in the style guide.,1.0,0,0.0,0.06334841628959276,0.0,0.0,0.0,0.0,0.0,0.2982456140350877,0.19999999999999998,0.21794871794871795,0.1515151515151515,0.1515151515151515,0.0
Bug,getQdisc function in routing::queueing::internal.cpp returns incorrect qdisc,The getQdisc function ignores the passed link parameter and returns the first qdisc of the required type from any available interface.,1.0,0,0.0,0.06334841628959276,0.0,0.0,0.0,0.0,0.0,0.2982456140350877,0.19999999999999998,0.21794871794871795,0.1515151515151515,0.1515151515151515,0.0
Bug,Non-POD static variables used in fq_codel and ingress.,"We declare const non-POD static variables for the following:

fq_codel::HANDLE
ingress::ROOT
ingress::HANDLE

We can eliminate the risk of indeterminate initialization by converting to C++11 constexpr",1.0,0,0.0,0.06334841628959276,0.0,0.0,0.0,0.0,0.0,0.2982456140350877,0.19999999999999998,0.21794871794871795,0.1515151515151515,0.1515151515151515,0.0
Task,Slave should expose metrics about oversubscribed resources,metrics/snapshot should expose metrics on oversubscribed resources (allocated and available). ,2.0,0,0.5,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.8846153846153846,0.9878787878787878,0.9878787878787878,0.0
Task,Define protobuf for ResourceMonitor::Usage.,We need to expose ResourceMonitor::Usage so that module writers can access it. We will define a protobuf message for that.,1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.017543859649122806,0.03571428571428571,0.0,0.006060606060606061,0.006060606060606061,0.0
Bug,SIGSEGV received during ResourceMonitorProcess::usage(),"Observed in production.

{noformat:title=slave log}
I0523 17:03:59.830229 56587 port_mapping.cpp:2616] Freed ephemeral ports [33792,34816) for container with pid 47791
I0523 17:03:59.849773 56587 port_mapping.cpp:2764] Successfully performed cleanup for pid 47791
*** Aborted at 1432400641 (unix time) try ""date -d @1432400641"" if you are using GNU date ***
PC: @     0x7f100fcbfd85 _ZNSt17_Function_handlerIFvRKSsEZNK7process6FutureIN5mesos8internal5slave15ResourceMonitor5UsageEE8onFailedIZNS7_22ResourceMonitorProcess5usageENS5_11ContainerIDEEUlS1_E_vEERKSA_OT_NSA_6PreferEEUlS1_E_E9_M_invokeERKSt9_Any_dataS1_
I0523 17:03:59.898959 56587 slave.cpp:3246] Executor 'thermos-1432400210944-mesos-test-exhaust_diskspace-5-4744d0fb-e0a1-4e40-bb22-56bd5cbd9524' of framework 201103282247-0000000019-0000 terminated with signal Killed
I0523 17:04:03.419869 56587 slave.cpp:2547] Handling status update TASK_FAILED (UUID: 3be19404-f737-4a70-a330-d1d924a85dbb) for task 1432400210944-mesos-test-exhaust_diskspace-5-4744d0fb-e0a1-4e40-bb22-56bd5cbd9524 of framework 201103282247-0000000019-0000 from @0.0.0.0:0
I0523 17:04:03.773061 56587 slave.cpp:4077] Received a new estimation of the oversubscribable resources 
I0523 17:04:03.773907 56587 slave.cpp:4077] Received a new estimation of the oversubscribable resources 
I0523 17:04:03.774683 56587 slave.cpp:4077] Received a new estimation of the oversubscribable resources 
I0523 17:04:03.776345 56587 slave.cpp:4077] Received a new estimation of the oversubscribable resources 
*** SIGSEGV (@0x0) received by PID 56573 (TID 0x7f100a190940) from PID 0; stack trace: ***
    @     0x7f100f181ca0 (unknown)
    @     0x7f100fcbfd85 _ZNSt17_Function_handlerIFvRKSsEZNK7process6FutureIN5mesos8internal5slave15ResourceMonitor5UsageEE8onFailedIZNS7_22ResourceMonitorProcess5usageENS5_11ContainerIDEEUlS1_E_vEERKSA_OT_NSA_6PreferEEUlS1_E_E9_M_invokeERKSt9_Any_dataS1_
    @     0x7f100fb01506 process::internal::run<>()
    @     0x7f100fcc701b process::Future<>::fail()
    @     0x7f100fccfbde process::internal::thenf<>()
    @     0x7f100fd64bee _ZN7process8internal3runISt8functionIFvRKNS_6FutureIN5mesos18ResourceStatisticsEEEEEJRS6_EEEvRKSt6vectorIT_SaISD_EEDpOT0_
    @     0x7f100fd656dd process::Future<>::fail()
    @     0x7f100fd6c332 process::Promise<>::associate()
    @     0x7f100fe2777e _ZNSt17_Function_handlerIFvPN7process11ProcessBaseEEZNS0_8dispatchIN5mesos18ResourceStatisticsENS5_8internal5slave25MesosContainerizerProcessERKNS5_11ContainerIDESA_EENS0_6FutureIT_EERKNS0_3PIDIT0_EEMSH_FSF_T1_ET2_EUlS2_E_E9_M_invokeERKSt9_Any_dataS2_
    @     0x7f101015561a process::ProcessManager::resume()
    @     0x7f10101558dc process::schedule()
    @     0x7f100f17983d start_thread
    @     0x7f100e96bfcd clone
/usr/local/bin/mesos-slave.sh: line 102: 56573 Segmentation fault      (core dumped) $debug /usr/local/sbin/mesos-slave ""${MESOS_FLAGS[@]}""
Slave Exit Status: 139
{noformat}

{noformat:title=gdb core dump}
Thread 20 (Thread 0x7f100a190940 (LWP 56574)):
#0  _M_data (__functor=Unhandled dwarf expression opcode 0xf3
) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/basic_string.h:293
#1  _M_rep (__functor=Unhandled dwarf expression opcode 0xf3
) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/basic_string.h:301
#2  size (__functor=Unhandled dwarf expression opcode 0xf3
) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/basic_string.h:716
#3  operator<< <char, std::char_traits<char>, std::allocator<char> > (__functor=Unhandled dwarf expression opcode 0xf3
) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/bits/basic_string.h:2758
#4  operator<< (__functor=Unhandled dwarf expression opcode 0xf3
) at ../include/mesos/type_utils.hpp:267
#5  operator() (__functor=Unhandled dwarf expression opcode 0xf3
) at slave/monitor.cpp:129
#6  operator() (__functor=Unhandled dwarf expression opcode 0xf3
) at ../3rdparty/libprocess/include/process/future.hpp:220
#7  std::_Function_handler<void(const std::basic_string<char, std::char_traits<char>, std::allocator<char> >&), process::Future<T>::onFailed(F&&, process::Future<T>::Prefer) const [with F = mesos::internal::slave::ResourceMonitorProcess::usage(mesos::ContainerID)::__lambda180; <template-parameter-2-2> = void; T = mesos::internal::slave::ResourceMonitor::Usage]::__lambda2>::_M_invoke(const std::_Any_data &, const std::basic_string<char, std::char_traits<char>, std::allocator<char> > &) (__functor=Unhandled dwarf expression opcode 0xf3
) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/functional:2071
#8  0x00007f100fb01506 in process::internal::run<std::function<void(const std::basic_string<char>&)>, std::basic_string<char, std::char_traits<char>, std::allocator<char> >&>(const std::vector<std::function<void(const std::basic_string<char, std::char_traits<char>, std::allocator<char> >&)>, std::allocator<std::function<void(const std::basic_string<char, std::char_traits<char>, std::allocator<char> >&)> > > &) (callbacks=std::vector of length 1, capacity 1 = {...})
    at ../3rdparty/libprocess/include/process/future.hpp:420
#9  0x00007f100fcc701b in process::Future<mesos::internal::slave::ResourceMonitor::Usage>::fail (this=0x7f0ffc185ca8, _message=""Unknown container: c0ab6cd3-fe4f-49bd-8dd6-32b388fcfab2"")
    at ../3rdparty/libprocess/include/process/future.hpp:1406
#10 0x00007f100fccfbde in fail (f=Unhandled dwarf expression opcode 0xf3
) at ../3rdparty/libprocess/include/process/future.hpp:649
#11 process::internal::thenf<mesos::ResourceStatistics, mesos::internal::slave::ResourceMonitor::Usage>(const std::function<process::Future<mesos::internal::slave::ResourceMonitor::Usage>(const mesos::ResourceStatistics&)> &, const std::shared_ptr<process::Promise<mesos::internal::slave::ResourceMonitor::Usage> > &, const process::Future<mesos::ResourceStatistics> &) (f=Unhandled dwarf expression opcode 0xf3
) at ../3rdparty/libprocess/include/process/future.hpp:1193
#12 0x00007f100fd64bee in operator() (callbacks=std::vector of length 1, capacity 1 = {...}) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/functional:2464
#13 process::internal::run<std::function<void(const process::Future<mesos::ResourceStatistics>&)>, process::Future<mesos::ResourceStatistics>&>(const std::vector<std::function<void(const process::Future<mesos::ResourceStatistics>&)>, std::allocator<std::function<void(const process::Future<mesos::ResourceStatistics>&)> > > &) (callbacks=std::vector of length 1, capacity 1 = {...}) at ../3rdparty/libprocess/include/process/future.hpp:420
#14 0x00007f100fd656dd in process::Future<mesos::ResourceStatistics>::fail (this=0x7f0ff8046230, _message=""Unknown container: c0ab6cd3-fe4f-49bd-8dd6-32b388fcfab2"") at ../3rdparty/libprocess/include/process/future.hpp:1407
#15 0x00007f100fd6c332 in onFailed (this=Unhandled dwarf expression opcode 0xf3
) at ../3rdparty/libprocess/include/process/future.hpp:1121
#16 onFailed<std::_Bind<std::_Mem_fn<bool (process::Future<mesos::ResourceStatistics>::*)(const std::basic_string<char>&)>(process::Future<mesos::ResourceStatistics>, std::_Placeholder<1>)>, bool> (this=Unhandled dwarf expression opcode 0xf3
)
    at ../3rdparty/libprocess/include/process/future.hpp:221
#17 onFailed<std::_Bind<std::_Mem_fn<bool (process::Future<mesos::ResourceStatistics>::*)(const std::basic_string<char>&)>(process::Future<mesos::ResourceStatistics>, std::_Placeholder<1>)> > (this=Unhandled dwarf expression opcode 0xf3
)
    at ../3rdparty/libprocess/include/process/future.hpp:270
#18 process::Promise<mesos::ResourceStatistics>::associate (this=Unhandled dwarf expression opcode 0xf3
) at ../3rdparty/libprocess/include/process/future.hpp:635
#19 0x00007f100fe2777e in operator() (__functor=Unhandled dwarf expression opcode 0xf3
) at ../3rdparty/libprocess/include/process/dispatch.hpp:239
#20 std::_Function_handler<void(process::ProcessBase*), process::dispatch(const process::PID<T>&, process::Future<T> (T::*)(P0), A0) [with R = mesos::ResourceStatistics; T = mesos::internal::slave::MesosContainerizerProcess; P0 = const mesos::ContainerID&; A0 = mesos::ContainerID]::__lambda21>::_M_invoke(const std::_Any_data &, process::ProcessBase *) (__functor=Unhandled dwarf expression opcode 0xf3
) at /opt/rh/devtoolset-2/root/usr/include/c++/4.8.2/functional:2071
#21 0x00007f101015561a in process::ProcessManager::resume (this=0xc24d20, process=0x7f0ffc0169b0) at src/process.cpp:2172
#22 0x00007f10101558dc in process::schedule (arg=Unhandled dwarf expression opcode 0xf3
) at src/process.cpp:602
#23 0x00007f100f17983d in start_thread () from /lib64/libpthread.so.0
#24 0x00007f100e96bfcd in clone () from /lib64/libc.so.6
{noformat}",1.0,0.23.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.017543859649122806,0.049999999999999996,0.20512820512820512,0.13939393939393938,0.13939393939393938,0.2110091743119266
Task,Explicitly-defaulted functions are not allowed by styleguide,"As of right now the styleguide does not allow explicitly defaulted functions (being a c++ 11 feature).
They enhance readability, are supported by all relevant compiler (GCC 4.4+ and Clang 3.0+), and are introduced by some patches (e.g. https://reviews.apache.org/r/34277/). 
Therefore we should officially whitelist them in the styleguide.",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12280701754385964,0.32857142857142857,0.11538461538461538,0.23636363636363636,0.23636363636363636,0.0
Task,Delegating constructors are not allowed by styleguide,"As of right now the styleguide does not allow delegating constructors (being a c++ 11 feature).
They are already used in the code base (e.g. stout/option.hpp), are supported by all relevant compiler (GCC 4.7+ and Clang 3.0+), and enhance readability. 
Therefore we should officially whitelist them in the styleguide.",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12280701754385964,0.32857142857142857,0.11538461538461538,0.23636363636363636,0.23636363636363636,0.0
Task,Add correction message to inform slave about QoS Controller actions,"The QoS controller informs the slave about correcting actions (kill, resize, throttle best-effort containers, tasks, and so forth) through a protobuf message, called a QoSCorrection. This ticket tracks designing and creating this message.

For example:
{code}
message QoSCorrection {
  // NOTE: In future we can define more actions like
  // resize or freeze, but for now we have:
  // 1) kill - terminate the executor or task
  enum Type {
    KILL = 1;
  }
  //Kill action which will be performed on an executor
  message Kill {
    optional ExecutorID executor_id = 1;
  }

  required Type action = 1;
  optional string reason = 2;
  optional double timestamp = 3;
  optional Kill kill = 4;
}
{code}",1.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.017543859649122806,0.03571428571428571,0.0,0.0,0.0,0.0
Documentation,Reflect in documentation that isolator flags are only relevant for Mesos Containerizer,The isolator flags are only relevant when using the Mesos Containerizer. We should reflect this in the flag description to avoid confusion.,1.0,0,0.5,0.277526395173454,0.0,0.0,0.0,0.0,0.4,0.08771929824561403,0.10714285714285714,0.11538461538461538,0.23636363636363636,0.23636363636363636,0.0
Improvement,"Add -> operator for Option<T>, Try<T>, Result<T>, Future<T>.","Let's add operator overloads to Option<T> to allow access to the underlying T using the `->` operator.
",3.0,0,0.5,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.24358974358974358,0.22424242424242424,0.22424242424242424,0.0
Bug,Reduce multiple use of string literals,"We have several instances of string literals (e.g. ""mesos-containerizer"", ""net_tcp""rtt_microseconds_p50"") being used in multiple locations where mismatches would result in correctness issues.  We should replace these with a single definition to reduce the risk.",1.0,0,0.0,0.06334841628959276,0.0,0.0,0.0,0.0,0.0,0.2982456140350877,0.19999999999999998,0.21794871794871795,0.1515151515151515,0.1515151515151515,0.0
Improvement,Add HTB queueing discipline wrapper class,Network isolator uses a Hierarchical Token Bucket (HTB) traffic control discipline on the egress filter inside each container as the root for adding traffic filters.  A HTB wrapper is needed to access the network statistics for this interface.,3.0,0,0.0,0.06334841628959276,0.0,0.0,0.0,0.0,0.0,0.2982456140350877,0.19999999999999998,0.21794871794871795,0.1515151515151515,0.1515151515151515,0.0
Bug,Extend queueing discipline wrappers to expose network isolator statistics,Export Traffic Control statistics in queueing library to enable reporting out impact of network bandwidth statistics.,3.0,0,0.0,0.06334841628959276,0.0,0.0,0.0,0.0,0.0,0.2982456140350877,0.19999999999999998,0.21794871794871795,0.1515151515151515,0.1515151515151515,0.0
Story,As a Framework User I want to be able to discover my Task's IP,"The information exposed by the Framework via the {{WebUIUrl}} does not always resolves to a routable endpoint (eg, when the {{hostname}} is not publicly resolvable, or resolvable at all).

In order to facilitate service discovery (via, eg, Marathon UI) we want to add the information in {{FrameworksPid}} via the {{/state-summary}} endpoint.",3.0,0.22.1,0.5,0.42533936651583715,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3142857142857143,0.1794871794871795,0.15757575757575756,0.17575757575757575,0.20192660550458713
Documentation,Add documentation for maintainers.,"In order to scale the number of committers in the project, we proposed the concept of maintainers here:

http://markmail.org/thread/cjmdn3d7qfzbxhpm

To follow up on that proposal, we'll need some documentation to capture the concept of maintainers. Both how contributors can benefit from maintainer feedback and the expectations of ""maintainer-ship"".

In order to not enforce an excessive amount of process, maintainers will initially only serve as an encouraged means to help contributors find reviewers and get meaningful feedback.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.0
Improvement,Upgrade the design of MasterInfo,"Currently, the {{MasterInfo}} PB only supports an {{ip}} field as an {{int32}}.

Beyond making it harder (and opaque; open to subtle bugs) for languages other than C/C++ to decode into an IPv4 octets, this does not allow Mesos to support IPv6 Master nodes.

We should consider ways to upgrade it in ways that permit us to support both IPv4 / IPv6 nodes, and, possibly, in a way that makes it easy for languages such as Java/Python that already have PB support, so could easily deserialize this information.

See also MESOS-2709 for more info.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15789473684210525,0.20714285714285713,0.1794871794871795,0.15757575757575756,0.17575757575757575,0.0
Bug,The mesos-execute tool does not support zk:// master URLs,It appears that the {{mesos-execute}} command line tool does it's own PID validation of the {{--master}} param which prevents it from supporting clusters managed with ZooKeeper.,1.0,0.22.1,0.5,0.25037707390648567,0.3333333333333333,0.5,0.14285714285714285,0.5,0.0,0.2631578947368421,0.29285714285714287,0.01282051282051282,0.006060606060606061,0.006060606060606061,0.20192660550458713
Improvement,Deprecating '.json' extension in master endpoints urls,Add an endpoint for each master endpoint with a '.json' extension such as `/master/stats.json` so it becomes `/master/stats` after a deprecation cycle.,1.0,0,0.0,0.0784313725490196,0.3333333333333333,0.5,0.14285714285714285,0.0,0.0,0.05263157894736842,0.09285714285714286,0.038461538461538464,0.048484848484848485,0.048484848484848485,0.0
Improvement,Design Master discovery functionality for HTTP-only clients,"When building clients that do not bind to {{libmesos}} and only use the HTTP API (via ""pure"" language bindings - eg, Java-only) there is no simple way to discover the Master's IP address to connect to.

Rather than relying on 'out-of-band' configuration mechanisms, we would like to enable the ability of interrogating the ZooKeeper ensemble to discover the Master's IP address (and, possibly, other information) to which the HTTP API requests can be addressed to.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15789473684210525,0.20714285714285713,0.1794871794871795,0.15757575757575756,0.17575757575757575,0.0
Bug,Design doc for the Executor HTTP API,"This tracks the design of the Executor HTTP API.
",2.0,0,0.5,0.5701357466063348,0.0,0.0,0.0,0.0,0.0,0.6842105263157894,0.5642857142857143,0.10256410256410256,0.11515151515151516,0.11515151515151516,0.0
Bug,Incorrect zh:// URI scheme causes Slave to SegFault,"I have 4 slave nodes with the same hardware, operating system and mesos configuration. 

Few minutes ago, all 4 nodes were functioning well. I tried to change the config of *master* from _10.172.230.69:5050_ to _zh://10.172.230.69:2181/mesos_ and restarted them in turn. The other three had started normally but the last one got a segmentation fault as you can see below.

{code}
[root@iZ25to7d407Z ~]# mesos-slave --master=zh://10.172.230.69:2181/mesos --hostname=123.57.42.237 --containerizers=docker,mesos --quiet &
[1] 1216
[root@iZ25to7d407Z ~]# *** Aborted at 1431085131 (unix time) try ""date -d @1431085131"" if you are using GNU date ***
PC: @       0x3aede7b53c (unknown)
*** SIGSEGV (@0x0) received by PID 1216 (TID 0x7f12f984b820) from PID 0; stack trace: ***
    @       0x3aee20f710 (unknown)
    @       0x3aede7b53c (unknown)
    @       0x3aedecf630 (unknown)
    @     0x7f12fce1593f net::getIP()
    @     0x7f12fce507ae process::operator>>()
    @     0x7f12fce50107 process::UPID::UPID()
    @     0x7f12fc52af71 mesos::internal::MasterDetector::create()
    @           0x4b1290 main
    @       0x3aede1ed5d (unknown)
    @           0x4b00b9 (unknown)

[1]+  Segmentation fault      mesos-slave --master=zh://10.172.230.69:2181/mesos --hostname=123.57.42.237 --containerizers=docker,mesos --quiet
{code}",2.0,0.22.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.15789473684210525,0.20714285714285713,0.01282051282051282,0.0,0.0,0.20183486238532108
Documentation,Add correct format template declarations to the styleguide,"The general rule to format templates is to declare them as:

{code}
template <typename T> // notice the space between template and <
class Foo {
  …
};
{code}

However, the style is not documented anywhere nor it is inherited from the Google style guide.",1.0,0,0.0,0.004524886877828055,0.0,0.0,0.0,0.0,0.2,0.12280701754385964,0.24285714285714285,0.10256410256410256,0.11515151515151516,0.11515151515151516,0.0
Task,Modularize the QoS Controller,Modularize the QoS controller to enable custom correction policies,3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.017543859649122806,0.049999999999999996,0.01282051282051282,0.06666666666666667,0.06666666666666667,0.0
Task,Compare split/flattened cgroup hierarchy for CPU oversubscription,Investigate if a flat hierarchy is sufficient for oversubscription of CPU or if a two-way split is necessary/preferred.,3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.10256410256410256,0.2121212121212121,0.2121212121212121,0.0
Improvement,"Printing a resource should show information about reservation, disk etc","While new fields like DiskInfo and ReservationInfo have been added to Resource protobuf, the output stream operator hasn't been updated to show these. This is valuable information to have in the logs during debugging.",1.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.007142857142857143,0.8846153846153846,0.9878787878787878,0.9878787878787878,0.0
Task,Add a slave flag to enable oversubscription,Slave sends oversubscribable resources to master only when the flag is enabled.,2.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.8846153846153846,0.9878787878787878,0.9878787878787878,0.0
Improvement,Update modules doc with hook usage example,"Modules doc states the possibility of using hooks, but doesn't refer to necessary flags and usage example.",1.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.717948717948718,0.8242424242424242,0.09696969696969697,0.0
Bug,ContainerizerTest.ROOT_CGROUPS_BalloonFramework flaky,"{noformat}
I0429 00:58:35.267629  2086 slave.cpp:3210] Executor 'default' of framework 20150429-005830-16777343-5432-2023-0000 terminated with signal Aborted
I0429 00:58:35.270761  2086 slave.cpp:2512] Handling status update TASK_LOST (UUID: f969e350-6f91-4fa9-980e-1852554bd704) for task 1 of framework 201
50429-005830-16777343-5432-2023-0000 from @0.0.0.0:0
I0429 00:58:35.270983  2086 slave.cpp:4604] Terminating task 1
W0429 00:58:35.271574  2080 containerizer.cpp:903] Ignoring update for unknown container: 1298549a-a3d2-46ff-aad0-9dbc777affcc
I0429 00:58:35.272541  2074 status_update_manager.cpp:317] Received status update TASK_LOST (UUID: f969e350-6f91-4fa9-980e-1852554bd704) for task 1 o
f framework 20150429-005830-16777343-5432-2023-0000
I0429 00:58:35.272624  2074 status_update_manager.cpp:494] Creating StatusUpdate stream for task 1 of framework 20150429-005830-16777343-5432-2023-00
00
I0429 00:58:35.273217  2053 master.cpp:3493] Executor default of framework 20150429-005830-16777343-5432-2023-0000 on slave 20150429-005830-16777343-
5432-2023-S0 at slave(1)@10.35.12.124:5051 (smfd-aki-27-sr1.devel.twitter.com): terminated with signal Aborted

{noformat}

which is from

{code}
 60    // We use mlock and memset here to make sure that the memory                                                                                  
 61    // actually gets paged in and thus accounted for.                                                                                             
 62    if (mlock(buffer, chunk) != 0) {                                                                                                              
 63      perror(""Failed to lock memory, mlock"");                                                                                                     
 64      abort();                                                                                                                                    
 65    }                                                                                                                                             
 66                                                                                                                                                  
 67    if (memset(buffer, 1, chunk) != buffer) {                                                                                                     
 68      perror(""Failed to fill memory, memset"");                                                                                                    
 69      abort();                                                                                                                                    
 70    }  
{code}

This is the same as MESOS-2660: I've confirmed that swapping them fixed it.

",1.0,0,0.0,0.06787330316742082,0.0,0.0,0.0,0.0,0.0,0.08771929824561403,0.06428571428571428,0.02564102564102564,0.01212121212121212,0.01212121212121212,0.0
Bug,ROOT_CGROUPS_Listen and ROOT_IncreaseRSS tests are flaky,"[==========] Running 1 test from 1 test case.
[----------] Global test environment set-up.
[----------] 1 test from CgroupsAnyHierarchyWithCpuMemoryTest
[ RUN      ] CgroupsAnyHierarchyWithCpuMemoryTest.ROOT_CGROUPS_Listen
Failed to allocate RSS memory: Failed to lock memory, mlock: Resource temporarily unavailable../../../mesos/src/tests/cgroups_tests.cpp:571: Failure
Failed to wait 15secs for future
[  FAILED  ] CgroupsAnyHierarchyWithCpuMemoryTest.ROOT_CGROUPS_Listen (15121 ms)
[----------] 1 test from CgroupsAnyHierarchyWithCpuMemoryTest (15121 ms total)

[----------] Global test environment tear-down
[==========] 1 test from 1 test case ran. (15174 ms total)
[  PASSED  ] 0 tests.
[  FAILED  ] 1 test, listed below:
[  FAILED  ] CgroupsAnyHierarchyWithCpuMemoryTest.ROOT_CGROUPS_Listen",3.0,0,0.5,0.07088989441930618,0.0,0.0,0.0,0.0,0.2,0.14035087719298245,0.10714285714285714,0.7948717948717948,1.0,1.0,0.0
Story,Implement QoS controller,"This is a component of the slave that informs the slave about the possible ""corrections"" that need to be performed (e.g., shutdown container using recoverable resources).

This needs to be integrated with the resource monitor.

Need to figure out the metrics used for sending corrections (e.g., scheduling latency, usage, informed by executor/scheduler)

We also need to figure out the feedback loop between the QoS controller and the Resource Estimator.

{code}
class QoSController {
public:
  QoSController(ResourceMonitor* monitor);

  process::Queue<QoSCorrection> correction();
};
{code}

",3.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.017543859649122806,0.049999999999999996,0.8846153846153846,0.9878787878787878,0.9878787878787878,0.0
Task,Modularize the Resource Estimator,"Modularizing the resource estimator opens up the door for org specific implementations.

Test the estimator module.",3.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.017543859649122806,0.03571428571428571,0.8846153846153846,0.9878787878787878,0.9878787878787878,0.0
Improvement,Update Resource Monitor to return resource usage,Add usage() API call to return usage of all containers,3.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.017543859649122806,0.049999999999999996,0.8846153846153846,0.9878787878787878,0.9878787878787878,0.0
Bug,"Segfault in inline Try<IP> getIP(const std::string& hostname, int family)","We saw a segfault in production. Attaching the coredump, we see:

Core was generated by `/usr/local/sbin/mesos-slave --port=5051 --resources=cpus:23;mem:70298;ports:[31'.
Program terminated with signal 11, Segmentation fault.
#0  0x00007f639867c77e in free () from /lib64/libc.so.6
(gdb) bt
#0  0x00007f639867c77e in free () from /lib64/libc.so.6
#1  0x00007f63986c25d0 in freeaddrinfo () from /lib64/libc.so.6
#2  0x00007f6399deeafa in net::getIP (hostname=""<redacted>"", family=2) at ./3rdparty/stout/include/stout/net.hpp:201
#3  0x00007f6399e1f273 in process::initialize (delegate=Unhandled dwarf expression opcode 0xf3
) at src/process.cpp:837
#4  0x000000000042342f in main ()",1.0,0,0.0,0.06787330316742082,0.0,0.0,0.0,0.0,0.0,0.08771929824561403,0.06428571428571428,0.02564102564102564,0.01212121212121212,0.01212121212121212,0.0
Task,Move implementations of Framework struct functions out of master.hpp.,"To help reduce compile time and keep the header easy to read, let's move the implementations of the Framework struct functions out of master.hpp",1.0,0,0.5,0.0,0.3333333333333333,0.5,0.14285714285714285,0.0,0.2,0.5087719298245614,0.4357142857142857,0.24358974358974358,0.22424242424242424,0.22424242424242424,0.0
Task,Update style guide to disallow capture by reference of temporaries,"We modify the style guide to disallow constant references to temporaries as a whole. This means disallowing both (1) and (2) below.

h3. Background
1. Constant references to simple expression temporaries do extend the lifetime of the temporary till end of function scope:
* Temporary returned by function:
  {code}
  // See full example below.
  T f(const char* s) { return T(s); }

  {
    const T& good = f(""Ok"");
    // use of good is ok.
  }
  {code}
* Temporary constructed as simple expression:
  {code}
  // See full example below.
  {
    const T& good = T(""Ok"");
    // use of good is ok.
  }
  {code}

2. Constant references to expressions that result in a reference to a temporary do not extend the lifetime of the temporary:
  * Temporary returned by function:
  {code}
  // See full example below.
  T f(const char* s) { return T(s); }

  {
    const T& bad = f(""Bad!"").Member();
    // use of bad is invalid.
  }
  {code}
  * Temporary constructed as simple expression:
  {code}
  // See full example below.
  {
    const T& bad = T(""Bad!"").Member();
    // use of bad is invalid.
  }
  {code}

h3. Mesos Case
  - In Mesos we use Future<T> a lot. Many of our functions return Futures by value:
  {code}
  class Socket {
    Future<Socket> accept();
    Future<size_t> recv(char* data, size_t size);
    ...
  }
  {code}
  - Sometimes we capture these Futures:
  {code}
  {
    const Future<Socket>& accepted = socket.accept(); // Valid c++, propose we disallow.
  }
  {code}
  - Sometimes we chain these Futures:
  {code}
  {
    socket.accept().then(lambda::bind(_accepted)); // Temporary will be valid during 'then' expression evaluation.
  }
  {code}
  - Sometimes we do both:
  {code}
  {
    const Future<Socket>& accepted = socket.accept().then(lambda::bind(_accepted)); // Dangerous! 'accepted' lifetime will not be valid till end of scope. Disallow!
  }
  {code}

h3. Reasoning
- Although (1) is ok, and considered a [feature|http://herbsutter.com/2008/01/01/gotw-88-a-candidate-for-the-most-important-const/], (2) is extremely dangerous and leads to hard to track bugs.
- If we explicitly allow (1), but disallow (2), then my worry is that someone coming along to maintain the code later on may accidentally turn (1) into (2), without recognizing the severity of this mistake. For example:
{code}
// Original code:
const T& val = T();
std::cout << val << std::endl;
// New code:
const T& val = T().removeWhiteSpace();
std::cout << val << std::endl; // val could be corrupted since the destructor has been invoked and T's memory freed.
{code}
- If we disallow both cases: it will be easier to catch these mistakes early on in code reviews (and avoid these painful bugs), at the same cost of introducing a new style guide rule.

h3. Performance Implications
- BenH suggests c++ developers are commonly taught to capture by constant reference to hint to the compiler that the copy can be elided.
- Modern compilers use a Data Flow Graph to make optimizations such as
  - *In-place-construction*: leveraged by RVO and NRVO to construct the object in place on the stack. Similar to ""*Placement new*"": http://en.wikipedia.org/wiki/Placement_syntax
  - *RVO* (Return Value Optimization): http://en.wikipedia.org/wiki/Return_value_optimization
  - *NRVO* (Named Return Value Optimization): https://msdn.microsoft.com/en-us/library/ms364057%28v=vs.80%29.aspx
- Since modern compilers perform these optimizations, we no longer need to 'hint' to the compiler that the copies can be elided.

h3. Example program
{code}
#include <stdio.h>

class T {
public:
  T(const char* str) : Str(str) {
    printf(""+ T(%s)\n"", Str);
  }
  ~T() {
    printf(""- T(%s)\n"", Str);
  }
  const T& Member() const
  {
    return *this;
  }
private:
  const char* Str;
};

T f(const char* s) { return T(s); }

int main() {
  const T& good = T(""Ok"");
  const T& good_f = f(""Ok function"");

  const T& bad = T(""Bad!"").Member();
  const T& bad_f = T(""Bad function!"").Member();

  printf(""End of function scope...\n"");
}
{code}
Output:
{code}
+ T(Ok)
+ T(Ok function)
+ T(Bad!)
- T(Bad!)
+ T(Bad function!)
- T(Bad function!)
End of function scope...
- T(Ok function)
- T(Ok)
{code}",1.0,0,0.0,0.42533936651583715,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3142857142857143,0.24358974358974358,0.22424242424242424,0.22424242424242424,0.0
Documentation,Document the semantic change in decorator return values,"In order to enable decorator modules to _remove_ metadata (environment variables or labels), we changed the meaning of the return value for decorator hooks.

The Result<T> return values means:

||State||Before||After||
|Error|Error is propagated to the call-site|No change|
|None|The result of the decorator is not applied|No change|
|Some|The result of the decorator is *appended*|The result of the decorator *overwrites* the final labels/environment object|",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.017543859649122806,0.049999999999999996,0.01282051282051282,0.06666666666666667,0.06666666666666667,0.0
Task,Pipe 'updateFramework' path from master to Allocator to support framework re-registration,"Pipe the 'updateFramework' call from the master through the allocator, as described in the design doc in the epic: MESOS-703",1.0,0.22.0,0.0,0.42533936651583715,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3142857142857143,0.24358974358974358,0.22424242424242424,0.22424242424242424,0.20183486238532108
Improvement,Change docker rm command,"Right now it seems Mesos is using „docker rm –f ID“ to delete containers so bind mounts are not deleted. This means thousands of dirs in /var/lib/docker/vfs/dir   I would like to have the option to change it to „docker rm –f –v ID“ This deletes bind mounts but not persistant volumes.

Best,

Mike",2.0,0,0.5,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.0,0.0,0.0,0.0
Bug,Slave state.json frameworks.executors.queued_tasks wrong format?,"queued_tasks.executor_id is expected to be a string and not a complete json object. It should have the very same format as the tasks array on the same level.

Example, directly taken from slave

{noformat}
         ....
""queued_tasks"": [
{
  ""data"": """",
  ""executor_id"": {
    ""command"": {
      ""argv"": [],
      ""uris"": [
        {
          ""executable"": false,
          ""value"": ""http://downloads.foo.io/orchestra/storm-mesos/0.9.2-incubating-47-ovh.bb373df1c/storm-mesos-0.9.2-incubating.tgz""
        }
      ],
      ""value"": ""cd storm-mesos* && python bin/storm supervisor storm.mesos.MesosSupervisor""
    },
    ""data"": ""{\""assignmentid\"":\""srv4.hw.ca1.foo.com\"",\""supervisorid\"":\""srv4.hw.ca1.foo.com-stage-ingestion-stats-slave-111-1428421145\""}"",
    ""executor_id"": ""stage-ingestion-stats-slave-111-1428421145"",
    ""framework_id"": ""20150401-160104-251662508-5050-2197-0002"",
    ""name"": """",
    ""resources"": {
      ""cpus"": 0.5,
      ""disk"": 0,
      ""mem"": 1000
    }
  },
  ""id"": ""srv4.hw.ca1.foo.com-31708"",
  ""name"": ""worker srv4.hw.ca1.foo.com:31708"",
  ""resources"": {
    ""cpus"": 1,
    ""disk"": 0,
    ""mem"": 5120,
    ""ports"": ""[31708-31708]""
  },
  ""slave_id"": ""20150327-025553-218108076-5050-4122-S0""
},
...
]


{noformat}",3.0,0.22.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.017543859649122806,0.007142857142857143,0.02564102564102564,0.01212121212121212,0.01212121212121212,0.20183486238532108
Task,Update allocator docs,"Once Allocator interface changes, so does the way of writing new allocators. This should be reflected in Mesos docs. The modules doc should mention how to write and use allocator modules. Configuration doc should mention the new {{--allocator}} flag.",2.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.6140350877192982,0.5857142857142857,0.717948717948718,0.8242424242424242,0.09696969696969697,0.0
Improvement,Refactor launchHelper and statisticsHelper in port_mapping_tests to allow reuse,Refactor launchHelper and statisticsHelper in port_mapping_tests to allow reuse,2.0,0,0.0,0.06334841628959276,0.0,0.0,0.0,0.0,0.0,0.2982456140350877,0.19999999999999998,0.21794871794871795,0.1515151515151515,0.1515151515151515,0.0
Documentation,Create optional release step: update PyPi repositories,One of the build artifacts for a release is the python package `mesos.interface`. That needs to be uploaded to PyPi along with a release to allow for users of python frameworks to use that version of mesos.,2.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.014285714285714285,0.01282051282051282,0.06666666666666667,0.06666666666666667,0.0
Improvement,"Document tips, best practices, guidelines for doing code reviews.","We currently have a [""Committers Guide""|https://github.com/apache/mesos/blob/0.22.0/docs/committers-guide.md], however most of this information is relevant to all contributors looking to be participating in the code review process.

I'm proposing we extract much of this information into a more general ""Code Reviewing"" document, and include additional tips, best practices, lessons learned from members of the community.

This would be a great pre-requisite for on-boarding more committers and adding [MAINTAINERS|http://mail-archives.apache.org/mod_mbox/mesos-dev/201502.mbox/%3CCA+8RcoReugMVqoOpsnB8WGYBELa5fHwPA=J=YHJE22iwZvsbeQ@mail.gmail.com%3E].

The committers guide can be more specific to our expectations of committers, so we may want to make this into a ""committership"" document to help set expectations for contributors looking to become committers.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.0
Bug,Namespace handle symlinks in port_mapping isolator should not be under /var/run/netns,"Consider putting symlinks under /var/run/messo/netns. This is because 'ip' command assumes all files under /var/run/netns are valid namespaces without duplication and it has command like:

ip -all netns exec ip link

to list all links for each network namespace.",3.0,0,0.5,0.06787330316742082,0.0,0.0,0.0,0.0,0.0,0.08771929824561403,0.06428571428571428,0.7948717948717948,1.0,1.0,0.0
Bug,Do not use RunTaskMessage.framework_id.,"Assume that FrameworkInfo.id is always set and so need to read/set RunTaskMessage.framework_id.

This should land after https://issues.apache.org/jira/browse/MESOS-2558 has been shipped.",1.0,0,0.0,0.3453996983408748,0.0,0.0,0.0,0.0,0.2,0.38596491228070173,0.35714285714285715,0.28205128205128205,0.29090909090909095,0.28484848484848485,0.0
Documentation,Document issue with slave recovery when using systemd.,As the problem encountered in MESOS-2419 is a common problem with the default systemd configuration it would make sense to document this in the upgrade guide or somewhere else in the documentation.,1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12280701754385964,0.32857142857142857,0.11538461538461538,0.23636363636363636,0.23636363636363636,0.0
Bug,C++ Scheduler library should send HTTP Calls to master,"Once the scheduler library sends Call messages, we should update it to send Calls as HTTP requests to ""/call"" endpoint on master.",3.0,0,0.5,0.5701357466063348,0.0,0.0,0.0,0.0,0.0,0.6842105263157894,0.5642857142857143,0.8846153846153846,0.9878787878787878,0.9878787878787878,0.0
Documentation,Developer guide for libprocess,"Create a developer guide for libprocess that explains the philosophy behind it and explains the most important features as well as the prevalent use patterns in Mesos with examples. 

This could be similar to stout/README.md.
",2.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.12280701754385964,0.32857142857142857,0.14102564102564102,0.16969696969696968,0.17575757575757575,0.0
Bug,PerfTest.ROOT_SampleInit test fails.,"From MESOS-2300 as well, it looks like this test is not reliable:

{code}
[ RUN      ] PerfTest.ROOT_SampleInit
../../src/tests/perf_tests.cpp:147: Failure
Expected: (0u) < (statistics.get().cycles()), actual: 0 vs 0
../../src/tests/perf_tests.cpp:150: Failure
Expected: (0.0) < (statistics.get().task_clock()),
{code}

It looks like this test samples PID 1, which is either {{init}} or {{systemd}}. Per a chat with [~idownes] this should probably sample something that is guaranteed to be consuming cycles.",2.0,0.22.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.10526315789473684,0.16428571428571428,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.20183486238532108
Improvement,Log IP addresses from HTTP requests,"Querying /master/state.json is an expensive operation when a cluster is large, and it's possible to DOS the master via frequent and repeated queries (which is a separate problem). Querying the endpoint results in a log entry being written, but the entry lacks useful information, such as an IP address, response code and response size. These details are useful for tracking down who/what is querying the endpoint. Consider adding these details to the log entry, or even writing a separate [access|https://httpd.apache.org/docs/trunk/logs.html#accesslog] [log|https://httpd.apache.org/docs/trunk/logs.html#common]. Also consider writing log entries for _all_ HTTP requests (/metrics/snapshot produces no log entries).

{noformat:title=sample log entry}
I0319 18:06:18.824846 10521 http.cpp:478] HTTP request for '/master/state.json'
{noformat}",3.0,0,0.5,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.0,0.0,0.0,0.0
Bug,Change the default leaf qdisc to fq_codel inside containers,"When we enable bandwidth cap, htb is used on egress side inside containers, however, the default leaf qdisc for a htb class is still pfifo_fast, which is known to have buffer bloat. Change the default leaf qdisc to fq_codel too:

`tc qd add dev eth0 parent 1:1 fq_codel`

I can no longer see packet drops after this change.",1.0,0,0.0,0.03167420814479638,0.0,0.0,0.0,0.0,0.0,0.017543859649122806,0.02142857142857143,0.01282051282051282,0.01818181818181818,0.01818181818181818,0.0
Bug,Change the default leaf qdisc to fq_codel inside containers,"When we enable bandwidth cap, htb is used on egress side inside containers, however, the default leaf qdisc for a htb class is still pfifo_fast, which is known to have buffer bloat. Change the default leaf qdisc to fq_codel too:

`tc qd add dev eth0 parent 1:1 fq_codel`

I can no longer see packet drops after this change.",1.0,0,0.0,0.03167420814479638,0.0,0.0,0.0,0.0,0.0,0.017543859649122806,0.02142857142857143,0.01282051282051282,0.01818181818181818,0.01818181818181818,0.0
Bug,Change the default leaf qdisc to fq_codel inside containers,"When we enable bandwidth cap, htb is used on egress side inside containers, however, the default leaf qdisc for a htb class is still pfifo_fast, which is known to have buffer bloat. Change the default leaf qdisc to fq_codel too:

`tc qd add dev eth0 parent 1:1 fq_codel`

I can no longer see packet drops after this change.",1.0,0,0.0,0.03167420814479638,0.0,0.0,0.0,0.0,0.0,0.017543859649122806,0.02142857142857143,0.01282051282051282,0.01818181818181818,0.01818181818181818,0.0
Bug,Change the default leaf qdisc to fq_codel inside containers,"When we enable bandwidth cap, htb is used on egress side inside containers, however, the default leaf qdisc for a htb class is still pfifo_fast, which is known to have buffer bloat. Change the default leaf qdisc to fq_codel too:

`tc qd add dev eth0 parent 1:1 fq_codel`

I can no longer see packet drops after this change.",1.0,0,0.0,0.03167420814479638,0.0,0.0,0.0,0.0,0.0,0.017543859649122806,0.02142857142857143,0.01282051282051282,0.01818181818181818,0.01818181818181818,0.0
Bug,Change the default leaf qdisc to fq_codel inside containers,"When we enable bandwidth cap, htb is used on egress side inside containers, however, the default leaf qdisc for a htb class is still pfifo_fast, which is known to have buffer bloat. Change the default leaf qdisc to fq_codel too:

`tc qd add dev eth0 parent 1:1 fq_codel`

I can no longer see packet drops after this change.",1.0,0,0.0,0.010558069381598794,0.0,0.0,0.0,0.0,0.0,0.017543859649122806,0.02142857142857143,0.01282051282051282,0.01818181818181818,0.01818181818181818,0.0
Bug,Change the default leaf qdisc to fq_codel inside containers,"When we enable bandwidth cap, htb is used on egress side inside containers, however, the default leaf qdisc for a htb class is still pfifo_fast, which is known to have buffer bloat. Change the default leaf qdisc to fq_codel too:

`tc qd add dev eth0 parent 1:1 fq_codel`

I can no longer see packet drops after this change.",1.0,0,0.0,0.010558069381598794,0.0,0.0,0.0,0.0,0.0,0.017543859649122806,0.02142857142857143,0.01282051282051282,0.01818181818181818,0.01818181818181818,0.0
Bug,Change the default leaf qdisc to fq_codel inside containers,"When we enable bandwidth cap, htb is used on egress side inside containers, however, the default leaf qdisc for a htb class is still pfifo_fast, which is known to have buffer bloat. Change the default leaf qdisc to fq_codel too:

`tc qd add dev eth0 parent 1:1 fq_codel`

I can no longer see packet drops after this change.",1.0,0,0.0,0.010558069381598794,0.0,0.0,0.0,0.0,0.0,0.017543859649122806,0.02142857142857143,0.01282051282051282,0.01818181818181818,0.01818181818181818,0.0
Bug,Change the default leaf qdisc to fq_codel inside containers,"When we enable bandwidth cap, htb is used on egress side inside containers, however, the default leaf qdisc for a htb class is still pfifo_fast, which is known to have buffer bloat. Change the default leaf qdisc to fq_codel too:

`tc qd add dev eth0 parent 1:1 fq_codel`

I can no longer see packet drops after this change.",1.0,0,0.0,0.010558069381598794,0.0,0.0,0.0,0.0,0.0,0.017543859649122806,0.02142857142857143,0.01282051282051282,0.01818181818181818,0.01818181818181818,0.0
Bug,FetcherTest.ExtractNotExecutable is flaky,"Observed in our internal CI.

{code}
[ RUN      ] FetcherTest.ExtractNotExecutable
Using temporary directory '/tmp/FetcherTest_ExtractNotExecutable_R5R7Cn'
tar: Removing leading `/' from member names
I0316 18:55:48.509306 14678 fetcher.cpp:155] Starting to fetch URIs for container: de1e5165-82b4-434b-9149-8667cf652c64, directory: /tmp/FetcherTest_ExtractNotExecutable_R5R7Cn
I0316 18:55:48.509845 14678 fetcher.cpp:238] Fetching URIs using command '/var/jenkins/workspace/mesos-fedora-20-gcc/src/mesos-fetcher'
I0316 18:55:48.568611 15028 logging.cpp:177] Logging to STDERR
I0316 18:55:48.574928 15028 fetcher.cpp:214] Fetching URI '/tmp/DIjmjV.tar.gz'
I0316 18:55:48.575166 15028 fetcher.cpp:194] Copying resource from '/tmp/DIjmjV.tar.gz' to '/tmp/FetcherTest_ExtractNotExecutable_R5R7Cn'
tar: This does not look like a tar archive
tar: Exiting with failure status due to previous errors
Failed to extract /tmp/FetcherTest_ExtractNotExecutable_R5R7Cn/DIjmjV.tar.gz:Failed to extract: command tar -C '/tmp/FetcherTest_ExtractNotExecutable_R5R7Cn' -xf '/tmp/FetcherTest_ExtractNotExecutable_R5R7Cn/DIjmjV.tar.gz' exited with status: 512
tests/fetcher_tests.cpp:686: Failure
(fetch).failure(): Failed to fetch URIs for container 'de1e5165-82b4-434b-9149-8667cf652c64'with exit status: 256
[  FAILED  ] FetcherTest.ExtractNotExecutable (208 ms)
{code}",2.0,0.23.0,0.5,0.06184012066365008,0.0,0.0,0.0,0.0,0.2,0.10526315789473684,0.12857142857142856,0.8846153846153846,0.9878787878787878,0.9878787878787878,0.2110091743119266
Documentation,Doxygen style for libprocess,"Create a description of the Doxygen style to use for libprocess documentation. 

It is expected that this will later also become the Doxygen style for stout and Mesos, but we are working on libprocess only for now.

Possible outcome: a file named docs/doxygen-style.md

We hope for much input and expect a lot of discussion!
",1.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.12280701754385964,0.32857142857142857,0.14102564102564102,0.16969696969696968,0.17575757575757575,0.0
Documentation,Doxygen setup for libprocess,"Goals: 
- Initial doxygen setup. 
- Enable interested developers to generate already available doxygen content locally in their workspace and view it.
- Form the basis for future contributions of more doxygen content.

1. Devise a way to use Doxygen with Mesos source code. (For example, solve this by adding optional brew/apt-get installation to the ""Getting Started"" doc.)
2. Create a make target for libprocess documentation that can be manually triggered.
3. Create initial library top level documentation.
4. Enhance one header file with Doxygen. Make sure the generated output has all necessary links to navigate from the lib to the file and back, etc.
",2.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.12280701754385964,0.32857142857142857,0.14102564102564102,0.16969696969696968,0.17575757575757575,0.0
Task,Enable a framework to perform reservation operations.,"h3. Goal

This is the first step to supporting dynamic reservations. The goal of this task is to enable a framework to reply to a resource offer with *Reserve* and *Unreserve* offer operations as defined by {{Offer::Operation}} in {{mesos.proto}}.

h3. Overview

It's divided into a few subtasks so that it's clear what the small chunks to be addressed are. In summary, we need to introduce the {{Resource::ReservationInfo}} protobuf message to encapsulate the reservation information, enable the C++ {{Resources}} class to handle it then enable the master to handle reservation operations.

h3. Expected Outcome

* The framework will be able to send back reservation operations to (un)reserve resources.
* The reservations are kept only in the master since we don't send the {{CheckpointResources}} message to checkpoint the reservations on the slave yet.
* The reservations are considered to be reserved for the framework's role.",4.0,0,0.0,0.6726998491704375,0.0,0.0,0.0,0.1,0.6000000000000001,0.2807017543859649,0.29285714285714287,0.21794871794871795,0.26666666666666666,0.26666666666666666,0.0
Improvement,Add ability to distinguish slave removals metrics by reason.,"Currently we only expose a single removal metric ({{""master/slave_removals""}}) which makes it difficult to distinguish between removal reasons in the alerting.

Currently, a slave can be removed for the following reasons:

# Health checks failed.
# Slave unregistered.
# Slave was replaced by a new slave (on the same endpoint).

In the case of (2), we expect this to be due to maintenance and don't want to be notified as strongly as with health check failures.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.0
Task,Enable Resources::apply to handle reservation operations.,{{Resources::apply}} currently only handles {{Create}} and {{Destroy}} operations which exist for persistent volumes. We need to handle the {{Reserve}} and {{Unreserve}} operations for dynamic reservations as well.,3.0,0,0.0,0.6726998491704375,0.0,0.0,0.0,0.1,0.6000000000000001,0.2807017543859649,0.29285714285714287,0.21794871794871795,0.26666666666666666,0.26666666666666666,0.0
Task,Enable Resources to handle Resource::ReservationInfo,"After [MESOS-2475|https://issues.apache.org/jira/browse/MESOS-2475], our C++ {{Resources}} class needs to know how to handle {{Resource}} protobuf messages that have the {{reservation}} field set.",2.0,0,0.0,0.6726998491704375,0.0,0.0,0.0,0.1,0.6000000000000001,0.2807017543859649,0.29285714285714287,0.21794871794871795,0.26666666666666666,0.26666666666666666,0.0
Task,Add the Resource::ReservationInfo protobuf message,"The {{Resource::ReservationInfo}} protobuf message encapsulates information needed to keep track of reservations. It's named {{ReservationInfo}} rather than {{Reservation}} to keep consistency with {{Resource::DiskInfo}}.

Here's what it will look like:

{code}
message ReservationInfo {
  // Indicates the principal of the operator or framework that created the
  // reservation. This is used to determine whether this resource can be 
  // unreserved by an operator or a framework by checking the
  // ""unreserve"" ACL.
  required string principal;
}

// If this is set, this resource was dynamically reserved by an
// operator or a framework. Otherwise, this resource was
// statically configured by an operator via the --resources flag.
optional ReservationInfo reservation;
{code}",2.0,0,0.0,0.6726998491704375,0.0,0.0,0.0,0.1,0.6000000000000001,0.2807017543859649,0.29285714285714287,0.21794871794871795,0.26666666666666666,0.26666666666666666,0.0
Improvement,Allow --resources flag to take JSON.,"Currently, we used a customized format for --resources flag. As we introduce more and more stuffs (e.g., persistence, reservation) in Resource object, we need a more generic way to specify --resources.

For backward compatibility, we can scan the first character. If it is '[', then we invoke the JSON parser. Otherwise, we use the existing parser.",3.0,0,0.5,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.7948717948717948,1.0,1.0,0.0
Documentation,Write documentation for all the LIBPROCESS_* environment variables.,"libprocess uses a set of environment variables to modify its behaviour; however, these variables are not documented anywhere, nor it is defined where the documentation should be.

What would be needed is a decision whether the environment variables should be documented (a new doc file or reusing an existing one), and then add the documentation there.

After searching in the code, these are the variables which need to be documented:

# {{LIBPROCESS_IP}}
# {{LIBPROCESS_PORT}}
# {{LIBPROCESS_ADVERTISE_IP}}
# {{LIBPROCESS_ADVERTISE_PORT}}",2.0,0,0.5,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.10256410256410256,0.11515151515151516,0.11515151515151516,0.0
Bug,Authentication failure may lead to slave crash,"When slave authentication fails, the following attempt to transmit a {{UnregisterSlaveMessage}} may cause a crash within the slave.

{noformat}
E0309 01:08:34.819758 336699392 slave.cpp:740] Master master@192.168.178.20:5050 refused authentication
I0309 01:08:34.819787 336699392 slave.cpp:538] Master refused authentication; unregistering and shutting down

[libprotobuf FATAL google/protobuf/message_lite.cc:273] CHECK failed: IsInitialized(): Can't serialize message of type ""mesos.internal.UnregisterSlaveMessage"" because it is missing required fields: slave_id.value
libprocess: slave(1)@192.168.178.20:5051 terminating due to CHECK failed: IsInitialized(): Can't serialize message of type ""mesos.internal.UnregisterSlaveMessage"" because it is missing required fields: slave_id.value
{noformat}

The problem here is the following code:

{noformat}
      UnregisterSlaveMessage message_;
      message_.mutable_slave_id()->MergeFrom(info.id());
{noformat}

Authentication happens before registration. {{info.id}} is an optional member (of {{SlaveInfo}}) and not known yet. It is set later, while registering. So {{slave_id}} will remain unset.",1.0,0,0.0,0.3740573152337858,0.0,0.0,0.0,0.2,0.4,0.43859649122807015,0.2714285714285714,0.3717948717948718,0.24848484848484845,0.24848484848484845,0.0
Task,Add operator endpoints to create/destroy persistent volumes.,"Persistent volumes will not be released automatically.

So we probably need an endpoint for operators to forcefully release persistent volumes. We probably need to add principal to Persistence struct and use ACLs to control who can release what.

Additionally, it would be useful to have an endpoint for operators to create persistent volumes.",3.0,0,0.5,1.0,0.0,0.0,0.0,0.0,0.8,0.5438596491228069,0.39999999999999997,0.7948717948717948,1.0,1.0,0.0
Improvement,Add support for /proc/self/mountinfo on Linux,"/proc/self/mountinfo provides mount information specific to the calling process. This includes information on optional fields describing mount propagation, e.g., shared/slave mounts. 

Initially, add this to linux/fs then perhaps move existing users of MountTable to use the mountinfo, deprecating and removing the mostly (but not entirely) redundant code.",3.0,0.21.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.10526315789473684,0.16428571428571428,0.10256410256410256,0.2121212121212121,0.2121212121212121,0.19275229357798163
Improvement,Improve NsTest.ROOT_setns,"- Use symbol NAME directly to launch the subprocess instead of the hard-coded string.
 - Replaced the static string with char[].
",1.0,0,0.0,0.06787330316742082,0.0,0.0,0.0,0.0,0.0,0.08771929824561403,0.06428571428571428,0.02564102564102564,0.01212121212121212,0.01212121212121212,0.0
Documentation,Create styleguide for documentation,"As of right now different pages in our documentation use quite different styles. Consider for example the different emphasis for NOTE:
* {noformat}> NOTE: http://mesos.apache.org/documentation/latest/slave-recovery/{noformat}
*  {noformat}*NOTE*: http://mesos.apache.org/documentation/latest/upgrades/ {noformat} 

Would be great to establish a common style for the documentation!",2.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.12280701754385964,0.32857142857142857,0.11538461538461538,0.23636363636363636,0.23636363636363636,0.0
Improvement,Rate limit slaves removals during master recovery.,"Much like we rate limit slave removals in the common path (MESOS-1148), we need to rate limit slave removals that occur during master recovery. When a master recovers and is using a strict registry, slaves that do not re-register within a timeout will be removed.

Currently there is a safeguard in place to abort when too many slaves have not re-registered. However, in the case of a transient partition, we don't want to remove large sections of slaves without rate limiting.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.0
Bug,"replace unsafe ""find | xargs"" with ""find -exec""","The problem exists in
 1194:src/Makefile.am
 47:src/tests/balloon_framework_test.sh

The current ""find | xargs rm -rf"" in the Makefile could potentially destroy data if mesos source was in a folder with a space in the name. E.g. if you for some reason checkout mesos to ""/ mesos"" the command in src/Makefile.am would turn into a rm -rf /

""find | xargs"" should be NUL delimited with ""find -print0 | xargs -0"" for safer execution or can just be replaced with the find build-in option ""find -exec '{}' \+"" which behaves similar to xargs.

There was a second occurrence of this in a test script, though in that case it would only rmdir empty folders, so is less critical.

I submitted a PR here: https://github.com/apache/mesos/pull/36
",1.0,0.20.1,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05128205128205128,0.01818181818181818,0.01818181818181818,0.18357798165137615
Bug,DRFSorter needs to distinguish resources from different slaves.,"Currently the {{DRFSorter}} aggregates total and allocated resources across multiple slaves, which only works for scalar resources. We need to distinguish resources from different slaves.

Suppose we have 2 slaves and 1 framework. The framework is allocated all resources from both slaves.

{code}
Resources slaveResources =
  Resources::parse(""cpus:2;mem:512;ports:[31000-32000]"").get();

DRFSorter sorter;

sorter.add(slaveResources);  // Add slave1 resources
sorter.add(slaveResources);  // Add slave2 resources

// Total resources in sorter at this point is
// cpus(*):4; mem(*):1024; ports(*):[31000-32000].
// The scalar resources get aggregated correctly but ports do not.

sorter.add(""F"");

// The 2 calls to allocated only works because we simply do:
//   allocation[name] += resources;
// without checking that the 'resources' is available in the total.

sorter.allocated(""F"", slaveResources);
sorter.allocated(""F"", slaveResources);

// At this point, sorter.allocation(""F"") is:
// cpus(*):4; mem(*):1024; ports(*):[31000-32000].
{code}

To provide some context, this issue came up while trying to reserve all unreserved resources from every offer.

{code}
for (const Offer& offer : offers) { 
  Resources unreserved = offer.resources().unreserved();
  Resources reserved = unreserved.flatten(role, Resource::FRAMEWORK); 

  Offer::Operation reserve;
  reserve.set_type(Offer::Operation::RESERVE); 
  reserve.mutable_reserve()->mutable_resources()->CopyFrom(reserved); 
 
  driver->acceptOffers({offer.id()}, {reserve}); 
} 
{code}

Suppose the slave resources are the same as above:

{quote}
Slave1: {{cpus(\*):2; mem(\*):512; ports(\*):\[31000-32000\]}}
Slave2: {{cpus(\*):2; mem(\*):512; ports(\*):\[31000-32000\]}}
{quote}

Initial (incorrect) total resources in the DRFSorter is:

{quote}
{{cpus(\*):4; mem(\*):1024; ports(\*):\[31000-32000\]}}
{quote}

We receive 2 offers, 1 from each slave:

{quote}
Offer1: {{cpus(\*):2; mem(\*):512; ports(\*):\[31000-32000\]}}
Offer2: {{cpus(\*):2; mem(\*):512; ports(\*):\[31000-32000\]}}
{quote}

At this point, the resources allocated for the framework is:

{quote}
{{cpus(\*):4; mem(\*):1024; ports(\*):\[31000-32000\]}}
{quote}

After first {{RESERVE}} operation with Offer1:

The allocated resources for the framework becomes:

{quote}
{{cpus(\*):2; mem(\*):512; cpus(role):2; mem(role):512; ports(role):\[31000-32000\]}}
{quote}

During second {{RESERVE}} operation with Offer2:

{code:title=HierarchicalAllocatorProcess::updateAllocation}
  // ...

  FrameworkSorter* frameworkSorter =
    frameworkSorters[frameworks\[frameworkId\].role];

  Resources allocation = frameworkSorter->allocation(frameworkId.value());

  // Update the allocated resources.
  Try<Resources> updatedAllocation = allocation.apply(operations);
  CHECK_SOME(updatedAllocation);

  // ...
{code}

{{allocation}} in the above code is:

{quote}
{{cpus(\*):2; mem(\*):512; cpus(role):2; mem(role):512; ports(role):\[31000-32000\]}}
{quote}

We try to {{apply}} a {{RESERVE}} operation and we fail to find {{ports(\*):\[31000-32000\]}} which leads to the {{CHECK}} fail at {{CHECK_SOME(updatedAllocation);}}",2.0,0,0.0,0.6726998491704375,0.0,0.0,0.0,0.1,0.6000000000000001,0.2807017543859649,0.29285714285714287,0.21794871794871795,0.26666666666666666,0.26666666666666666,0.0
Improvement,Test script for verifying compatibility between Mesos components,"While our current unit/integration test suite catches functional bugs, it doesn't catch compatibility bugs (e.g, MESOS-2371). This is really crucial to provide operators the ability to do seamless upgrades on live clusters.

We should have a test suite / framework (ideally running on CI vetting each review on RB) that tests upgrade paths between master, slave, scheduler and executor.",2.0,0,0.5,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.8846153846153846,0.9878787878787878,0.9878787878787878,0.0
Bug,__init__.py not getting installed in $PREFIX/lib/pythonX.Y/site-packages/mesos,"When doing a {{make install}}, the src/python/native/src/mesos/__init__.py file is not getting installed in {{$PREFIX/lib/pythonX.Y/site-packages/mesos/}}.  

This makes it impossible to do the following import when {{PYTHONPATH}} is set to the {{site-packages}} directory.

{code}
import mesos.interface.mesos_pb2
{code}

The directories {{$PREFIX/lib/pythonX.Y/site-packages/mesos/interface, native}} do have their corresponding {{__init__.py}} files.

Reproducing the bug:
{code}
../configure --prefix=$HOME/test-install && make install
{code}",2.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.15789473684210525,0.20714285714285713,0.28205128205128205,0.29090909090909095,0.28484848484848485,0.0
Bug,MasterAllocatorTest/0.OutOfOrderDispatch is flaky,"
{noformat:title=}
[ RUN      ] MasterAllocatorTest/0.OutOfOrderDispatch
Using temporary directory '/tmp/MasterAllocatorTest_0_OutOfOrderDispatch_kjLb9b'
I0206 07:55:44.084333 15065 leveldb.cpp:175] Opened db in 25.006293ms
I0206 07:55:44.089635 15065 leveldb.cpp:182] Compacted db in 5.256332ms
I0206 07:55:44.089695 15065 leveldb.cpp:197] Created db iterator in 23534ns
I0206 07:55:44.089710 15065 leveldb.cpp:203] Seeked to beginning of db in 2175ns
I0206 07:55:44.089720 15065 leveldb.cpp:272] Iterated through 0 keys in the db in 417ns
I0206 07:55:44.089781 15065 replica.cpp:743] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0206 07:55:44.093750 15086 recover.cpp:448] Starting replica recovery
I0206 07:55:44.094044 15086 recover.cpp:474] Replica is in EMPTY status
I0206 07:55:44.095473 15086 replica.cpp:640] Replica in EMPTY status received a broadcasted recover request
I0206 07:55:44.095724 15086 recover.cpp:194] Received a recover response from a replica in EMPTY status
I0206 07:55:44.096097 15086 recover.cpp:565] Updating replica status to STARTING
I0206 07:55:44.106575 15086 leveldb.cpp:305] Persisting metadata (8 bytes) to leveldb took 10.289939ms
I0206 07:55:44.106613 15086 replica.cpp:322] Persisted replica status to STARTING
I0206 07:55:44.108144 15086 recover.cpp:474] Replica is in STARTING status
I0206 07:55:44.109122 15086 replica.cpp:640] Replica in STARTING status received a broadcasted recover request
I0206 07:55:44.110879 15091 recover.cpp:194] Received a recover response from a replica in STARTING status
I0206 07:55:44.117267 15087 recover.cpp:565] Updating replica status to VOTING
I0206 07:55:44.124771 15087 leveldb.cpp:305] Persisting metadata (8 bytes) to leveldb took 2.66794ms
I0206 07:55:44.124814 15087 replica.cpp:322] Persisted replica status to VOTING
I0206 07:55:44.124948 15087 recover.cpp:579] Successfully joined the Paxos group
I0206 07:55:44.125095 15087 recover.cpp:463] Recover process terminated
I0206 07:55:44.126204 15087 master.cpp:344] Master 20150206-075544-16842879-38895-15065 (utopic) started on 127.0.1.1:38895
I0206 07:55:44.126268 15087 master.cpp:390] Master only allowing authenticated frameworks to register
I0206 07:55:44.126281 15087 master.cpp:395] Master only allowing authenticated slaves to register
I0206 07:55:44.126307 15087 credentials.hpp:35] Loading credentials for authentication from '/tmp/MasterAllocatorTest_0_OutOfOrderDispatch_kjLb9b/credentials'
I0206 07:55:44.126683 15087 master.cpp:439] Authorization enabled
I0206 07:55:44.129329 15086 master.cpp:1350] The newly elected leader is master@127.0.1.1:38895 with id 20150206-075544-16842879-38895-15065
I0206 07:55:44.129361 15086 master.cpp:1363] Elected as the leading master!
I0206 07:55:44.129389 15086 master.cpp:1181] Recovering from registrar
I0206 07:55:44.129653 15088 registrar.cpp:312] Recovering registrar
I0206 07:55:44.130859 15088 log.cpp:659] Attempting to start the writer
I0206 07:55:44.132334 15088 replica.cpp:476] Replica received implicit promise request with proposal 1
I0206 07:55:44.135187 15088 leveldb.cpp:305] Persisting metadata (8 bytes) to leveldb took 2.825465ms
I0206 07:55:44.135390 15088 replica.cpp:344] Persisted promised to 1
I0206 07:55:44.138062 15091 coordinator.cpp:229] Coordinator attemping to fill missing position
I0206 07:55:44.139576 15091 replica.cpp:377] Replica received explicit promise request for position 0 with proposal 2
I0206 07:55:44.142156 15091 leveldb.cpp:342] Persisting action (8 bytes) to leveldb took 2.545543ms
I0206 07:55:44.142189 15091 replica.cpp:678] Persisted action at 0
I0206 07:55:44.143414 15091 replica.cpp:510] Replica received write request for position 0
I0206 07:55:44.143468 15091 leveldb.cpp:437] Reading position from leveldb took 28872ns
I0206 07:55:44.145982 15091 leveldb.cpp:342] Persisting action (14 bytes) to leveldb took 2.480277ms
I0206 07:55:44.146015 15091 replica.cpp:678] Persisted action at 0
I0206 07:55:44.147050 15089 replica.cpp:657] Replica received learned notice for position 0
I0206 07:55:44.154364 15089 leveldb.cpp:342] Persisting action (16 bytes) to leveldb took 7.281644ms
I0206 07:55:44.154400 15089 replica.cpp:678] Persisted action at 0
I0206 07:55:44.154422 15089 replica.cpp:663] Replica learned NOP action at position 0
I0206 07:55:44.155506 15091 log.cpp:675] Writer started with ending position 0
I0206 07:55:44.156746 15091 leveldb.cpp:437] Reading position from leveldb took 30248ns
I0206 07:55:44.173681 15091 registrar.cpp:345] Successfully fetched the registry (0B) in 43.977984ms
I0206 07:55:44.173821 15091 registrar.cpp:444] Applied 1 operations in 30768ns; attempting to update the 'registry'
I0206 07:55:44.176213 15086 log.cpp:683] Attempting to append 119 bytes to the log
I0206 07:55:44.176426 15086 coordinator.cpp:339] Coordinator attempting to write APPEND action at position 1
I0206 07:55:44.177608 15088 replica.cpp:510] Replica received write request for position 1
I0206 07:55:44.180059 15088 leveldb.cpp:342] Persisting action (136 bytes) to leveldb took 2.415145ms
I0206 07:55:44.180094 15088 replica.cpp:678] Persisted action at 1
I0206 07:55:44.181324 15084 replica.cpp:657] Replica received learned notice for position 1
I0206 07:55:44.183831 15084 leveldb.cpp:342] Persisting action (138 bytes) to leveldb took 2.473724ms
I0206 07:55:44.183866 15084 replica.cpp:678] Persisted action at 1
I0206 07:55:44.183887 15084 replica.cpp:663] Replica learned APPEND action at position 1
I0206 07:55:44.185510 15084 registrar.cpp:489] Successfully updated the 'registry' in 11.619072ms
I0206 07:55:44.185678 15086 log.cpp:702] Attempting to truncate the log to 1
I0206 07:55:44.186111 15086 coordinator.cpp:339] Coordinator attempting to write TRUNCATE action at position 2
I0206 07:55:44.186944 15086 replica.cpp:510] Replica received write request for position 2
I0206 07:55:44.187492 15084 registrar.cpp:375] Successfully recovered registrar
I0206 07:55:44.188016 15087 master.cpp:1208] Recovered 0 slaves from the Registry (83B) ; allowing 10mins for slaves to re-register
I0206 07:55:44.189678 15086 leveldb.cpp:342] Persisting action (16 bytes) to leveldb took 2.702559ms
I0206 07:55:44.189713 15086 replica.cpp:678] Persisted action at 2
I0206 07:55:44.190620 15086 replica.cpp:657] Replica received learned notice for position 2
I0206 07:55:44.193383 15086 leveldb.cpp:342] Persisting action (18 bytes) to leveldb took 2.737088ms
I0206 07:55:44.193455 15086 leveldb.cpp:400] Deleting ~1 keys from leveldb took 37762ns
I0206 07:55:44.193475 15086 replica.cpp:678] Persisted action at 2
I0206 07:55:44.193496 15086 replica.cpp:663] Replica learned TRUNCATE action at position 2
I0206 07:55:44.200028 15065 containerizer.cpp:102] Using isolation: posix/cpu,posix/mem
I0206 07:55:44.212924 15088 slave.cpp:172] Slave started on 46)@127.0.1.1:38895
I0206 07:55:44.213762 15088 credentials.hpp:83] Loading credential for authentication from '/tmp/MasterAllocatorTest_0_OutOfOrderDispatch_RuNyVQ/credential'
I0206 07:55:44.214251 15088 slave.cpp:281] Slave using credential for: test-principal
I0206 07:55:44.214653 15088 slave.cpp:299] Slave resources: cpus(*):2; mem(*):1024; disk(*):24988; ports(*):[31000-32000]
I0206 07:55:44.214918 15088 slave.cpp:328] Slave hostname: utopic
I0206 07:55:44.215116 15088 slave.cpp:329] Slave checkpoint: false
W0206 07:55:44.215332 15088 slave.cpp:331] Disabling checkpointing is deprecated and the --checkpoint flag will be removed in a future release. Please avoid using this flag
I0206 07:55:44.217061 15090 state.cpp:32] Recovering state from '/tmp/MasterAllocatorTest_0_OutOfOrderDispatch_RuNyVQ/meta'
I0206 07:55:44.235409 15088 status_update_manager.cpp:196] Recovering status update manager
I0206 07:55:44.235601 15088 containerizer.cpp:299] Recovering containerizer
I0206 07:55:44.236486 15088 slave.cpp:3526] Finished recovery
I0206 07:55:44.237709 15087 status_update_manager.cpp:170] Pausing sending status updates
I0206 07:55:44.237890 15088 slave.cpp:620] New master detected at master@127.0.1.1:38895
I0206 07:55:44.241575 15088 slave.cpp:683] Authenticating with master master@127.0.1.1:38895
I0206 07:55:44.247459 15088 slave.cpp:688] Using default CRAM-MD5 authenticatee
I0206 07:55:44.248617 15089 authenticatee.hpp:137] Creating new client SASL connection
I0206 07:55:44.249099 15089 master.cpp:3788] Authenticating slave(46)@127.0.1.1:38895
I0206 07:55:44.249137 15089 master.cpp:3799] Using default CRAM-MD5 authenticator
I0206 07:55:44.249728 15089 authenticator.hpp:169] Creating new server SASL connection
I0206 07:55:44.250285 15089 authenticatee.hpp:228] Received SASL authentication mechanisms: CRAM-MD5
I0206 07:55:44.250496 15089 authenticatee.hpp:254] Attempting to authenticate with mechanism 'CRAM-MD5'
I0206 07:55:44.250452 15088 slave.cpp:656] Detecting new master
I0206 07:55:44.251063 15091 authenticator.hpp:275] Received SASL authentication start
I0206 07:55:44.251124 15091 authenticator.hpp:397] Authentication requires more steps
I0206 07:55:44.251256 15089 authenticatee.hpp:274] Received SASL authentication step
I0206 07:55:44.251451 15090 authenticator.hpp:303] Received SASL authentication step
I0206 07:55:44.251575 15090 authenticator.hpp:389] Authentication success
I0206 07:55:44.251687 15090 master.cpp:3846] Successfully authenticated principal 'test-principal' at slave(46)@127.0.1.1:38895
I0206 07:55:44.253306 15089 authenticatee.hpp:314] Authentication success
I0206 07:55:44.258015 15089 slave.cpp:754] Successfully authenticated with master master@127.0.1.1:38895
I0206 07:55:44.258468 15089 master.cpp:2913] Registering slave at slave(46)@127.0.1.1:38895 (utopic) with id 20150206-075544-16842879-38895-15065-S0
I0206 07:55:44.259028 15089 registrar.cpp:444] Applied 1 operations in 88902ns; attempting to update the 'registry'
I0206 07:55:44.269492 15065 sched.cpp:149] Version: 0.22.0
I0206 07:55:44.270539 15090 sched.cpp:246] New master detected at master@127.0.1.1:38895
I0206 07:55:44.270614 15090 sched.cpp:302] Authenticating with master master@127.0.1.1:38895
I0206 07:55:44.270634 15090 sched.cpp:309] Using default CRAM-MD5 authenticatee
I0206 07:55:44.270900 15090 authenticatee.hpp:137] Creating new client SASL connection
I0206 07:55:44.272300 15089 log.cpp:683] Attempting to append 285 bytes to the log
I0206 07:55:44.272552 15089 coordinator.cpp:339] Coordinator attempting to write APPEND action at position 3
I0206 07:55:44.273609 15086 master.cpp:3788] Authenticating scheduler-d6cac0a1-d461-4a05-b19d-5cbdae239eb0@127.0.1.1:38895
I0206 07:55:44.273643 15086 master.cpp:3799] Using default CRAM-MD5 authenticator
I0206 07:55:44.273955 15086 authenticator.hpp:169] Creating new server SASL connection
I0206 07:55:44.274617 15090 authenticatee.hpp:228] Received SASL authentication mechanisms: CRAM-MD5
I0206 07:55:44.274813 15090 authenticatee.hpp:254] Attempting to authenticate with mechanism 'CRAM-MD5'
I0206 07:55:44.275171 15088 authenticator.hpp:275] Received SASL authentication start
I0206 07:55:44.275215 15088 authenticator.hpp:397] Authentication requires more steps
I0206 07:55:44.275408 15090 authenticatee.hpp:274] Received SASL authentication step
I0206 07:55:44.275696 15084 authenticator.hpp:303] Received SASL authentication step
I0206 07:55:44.275774 15084 authenticator.hpp:389] Authentication success
I0206 07:55:44.275876 15084 master.cpp:3846] Successfully authenticated principal 'test-principal' at scheduler-d6cac0a1-d461-4a05-b19d-5cbdae239eb0@127.0.1.1:38895
I0206 07:55:44.277593 15090 authenticatee.hpp:314] Authentication success
I0206 07:55:44.278201 15086 sched.cpp:390] Successfully authenticated with master master@127.0.1.1:38895
I0206 07:55:44.278548 15086 master.cpp:1568] Received registration request for framework 'framework1' at scheduler-d6cac0a1-d461-4a05-b19d-5cbdae239eb0@127.0.1.1:38895
I0206 07:55:44.278642 15086 master.cpp:1429] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0206 07:55:44.279157 15086 master.cpp:1632] Registering framework 20150206-075544-16842879-38895-15065-0000 (framework1) at scheduler-d6cac0a1-d461-4a05-b19d-5cbdae239eb0@127.0.1.1:38895
I0206 07:55:44.280081 15086 sched.cpp:440] Framework registered with 20150206-075544-16842879-38895-15065-0000
I0206 07:55:44.280320 15086 hierarchical_allocator_process.hpp:318] Added framework 20150206-075544-16842879-38895-15065-0000
I0206 07:55:44.281411 15089 replica.cpp:510] Replica received write request for position 3
I0206 07:55:44.282289 15085 master.cpp:2901] Ignoring register slave message from slave(46)@127.0.1.1:38895 (utopic) as admission is already in progress
I0206 07:55:44.284984 15089 leveldb.cpp:342] Persisting action (304 bytes) to leveldb took 3.368213ms
I0206 07:55:44.285020 15089 replica.cpp:678] Persisted action at 3
I0206 07:55:44.285893 15089 replica.cpp:657] Replica received learned notice for position 3
I0206 07:55:44.288350 15089 leveldb.cpp:342] Persisting action (306 bytes) to leveldb took 2.430449ms
I0206 07:55:44.288384 15089 replica.cpp:678] Persisted action at 3
I0206 07:55:44.288405 15089 replica.cpp:663] Replica learned APPEND action at position 3
I0206 07:55:44.290154 15089 registrar.cpp:489] Successfully updated the 'registry' in 31.046912ms
I0206 07:55:44.290307 15085 log.cpp:702] Attempting to truncate the log to 3
I0206 07:55:44.290671 15085 coordinator.cpp:339] Coordinator attempting to write TRUNCATE action at position 4
I0206 07:55:44.291482 15085 replica.cpp:510] Replica received write request for position 4
I0206 07:55:44.292559 15087 master.cpp:2970] Registered slave 20150206-075544-16842879-38895-15065-S0 at slave(46)@127.0.1.1:38895 (utopic) with cpus(*):2; mem(*):1024; disk(*):24988; ports(*):[31000-32000]
I0206 07:55:44.292940 15087 slave.cpp:788] Registered with master master@127.0.1.1:38895; given slave ID 20150206-075544-16842879-38895-15065-S0
I0206 07:55:44.293298 15087 hierarchical_allocator_process.hpp:450] Added slave 20150206-075544-16842879-38895-15065-S0 (utopic) with cpus(*):2; mem(*):1024; disk(*):24988; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):24988; ports(*):[31000-32000] available)
I0206 07:55:44.293684 15087 status_update_manager.cpp:177] Resuming sending status updates
I0206 07:55:44.294085 15087 master.cpp:3730] Sending 1 offers to framework 20150206-075544-16842879-38895-15065-0000 (framework1) at scheduler-d6cac0a1-d461-4a05-b19d-5cbdae239eb0@127.0.1.1:38895
I0206 07:55:44.299957 15085 leveldb.cpp:342] Persisting action (16 bytes) to leveldb took 8.442691ms
I0206 07:55:44.300165 15085 replica.cpp:678] Persisted action at 4
I0206 07:55:44.300698 15065 sched.cpp:1468] Asked to stop the driver
I0206 07:55:44.301127 15090 sched.cpp:806] Stopping framework '20150206-075544-16842879-38895-15065-0000'
I0206 07:55:44.301503 15090 master.cpp:1892] Asked to unregister framework 20150206-075544-16842879-38895-15065-0000
I0206 07:55:44.301535 15090 master.cpp:4158] Removing framework 20150206-075544-16842879-38895-15065-0000 (framework1) at scheduler-d6cac0a1-d461-4a05-b19d-5cbdae239eb0@127.0.1.1:38895
I0206 07:55:44.302376 15090 slave.cpp:1592] Asked to shut down framework 20150206-075544-16842879-38895-15065-0000 by master@127.0.1.1:38895
W0206 07:55:44.302407 15090 slave.cpp:1607] Cannot shut down unknown framework 20150206-075544-16842879-38895-15065-0000
I0206 07:55:44.302814 15090 hierarchical_allocator_process.hpp:397] Deactivated framework 20150206-075544-16842879-38895-15065-0000
I0206 07:55:44.302947 15090 hierarchical_allocator_process.hpp:351] Removed framework 20150206-075544-16842879-38895-15065-0000
I0206 07:55:44.309281 15086 hierarchical_allocator_process.hpp:642] Recovered cpus(*):2; mem(*):1024; disk(*):24988; ports(*):[31000-32000] (total allocatable: cpus(*):2; mem(*):1024; disk(*):24988; ports(*):[31000-32000]) on slave 20150206-075544-16842879-38895-15065-S0 from framework 20150206-075544-16842879-38895-15065-0000
I0206 07:55:44.310158 15084 replica.cpp:657] Replica received learned notice for position 4
I0206 07:55:44.313246 15084 leveldb.cpp:342] Persisting action (18 bytes) to leveldb took 3.055049ms
I0206 07:55:44.313328 15084 leveldb.cpp:400] Deleting ~2 keys from leveldb took 45270ns
I0206 07:55:44.313349 15084 replica.cpp:678] Persisted action at 4
I0206 07:55:44.313374 15084 replica.cpp:663] Replica learned TRUNCATE action at position 4
I0206 07:55:44.329591 15065 sched.cpp:149] Version: 0.22.0
I0206 07:55:44.330258 15088 sched.cpp:246] New master detected at master@127.0.1.1:38895
I0206 07:55:44.330346 15088 sched.cpp:302] Authenticating with master master@127.0.1.1:38895
I0206 07:55:44.330368 15088 sched.cpp:309] Using default CRAM-MD5 authenticatee
I0206 07:55:44.330652 15088 authenticatee.hpp:137] Creating new client SASL connection
I0206 07:55:44.331403 15088 master.cpp:3788] Authenticating scheduler-7bdaa90b-eb9f-4009-bd5a-d07fd3f24cec@127.0.1.1:38895
I0206 07:55:44.331717 15088 master.cpp:3799] Using default CRAM-MD5 authenticator
I0206 07:55:44.332293 15088 authenticator.hpp:169] Creating new server SASL connection
I0206 07:55:44.332655 15088 authenticatee.hpp:228] Received SASL authentication mechanisms: CRAM-MD5
I0206 07:55:44.332684 15088 authenticatee.hpp:254] Attempting to authenticate with mechanism 'CRAM-MD5'
I0206 07:55:44.332792 15088 authenticator.hpp:275] Received SASL authentication start
I0206 07:55:44.332835 15088 authenticator.hpp:397] Authentication requires more steps
I0206 07:55:44.332903 15088 authenticatee.hpp:274] Received SASL authentication step
I0206 07:55:44.332983 15088 authenticator.hpp:303] Received SASL authentication step
I0206 07:55:44.333056 15088 authenticator.hpp:389] Authentication success
I0206 07:55:44.333153 15088 authenticatee.hpp:314] Authentication success
I0206 07:55:44.333297 15091 master.cpp:3846] Successfully authenticated principal 'test-principal' at scheduler-7bdaa90b-eb9f-4009-bd5a-d07fd3f24cec@127.0.1.1:38895
I0206 07:55:44.334326 15087 sched.cpp:390] Successfully authenticated with master master@127.0.1.1:38895
I0206 07:55:44.334645 15087 master.cpp:1568] Received registration request for framework 'framework2' at scheduler-7bdaa90b-eb9f-4009-bd5a-d07fd3f24cec@127.0.1.1:38895
I0206 07:55:44.334722 15087 master.cpp:1429] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0206 07:55:44.335153 15087 master.cpp:1632] Registering framework 20150206-075544-16842879-38895-15065-0001 (framework2) at scheduler-7bdaa90b-eb9f-4009-bd5a-d07fd3f24cec@127.0.1.1:38895
I0206 07:55:44.336019 15087 sched.cpp:440] Framework registered with 20150206-075544-16842879-38895-15065-0001
I0206 07:55:44.336156 15087 hierarchical_allocator_process.hpp:318] Added framework 20150206-075544-16842879-38895-15065-0001
I0206 07:55:44.336796 15087 master.cpp:3730] Sending 1 offers to framework 20150206-075544-16842879-38895-15065-0001 (framework2) at scheduler-7bdaa90b-eb9f-4009-bd5a-d07fd3f24cec@127.0.1.1:38895
I0206 07:55:44.337725 15065 sched.cpp:1468] Asked to stop the driver
I0206 07:55:44.338002 15086 sched.cpp:806] Stopping framework '20150206-075544-16842879-38895-15065-0001'
I0206 07:55:44.338297 15090 master.cpp:1892] Asked to unregister framework 20150206-075544-16842879-38895-15065-0001
I0206 07:55:44.338353 15090 master.cpp:4158] Removing framework 20150206-075544-16842879-38895-15065-0001 (framework2) at scheduler-7bdaa90b-eb9f-4009-bd5a-d07fd3f24cec@127.0.1.1:38895
../../src/tests/master_allocator_tests.cpp:300: Failure
Mock function called more times than expected - taking default action specified at:
../../src/tests/mesos.hpp:713:
    Function call: deactivateFramework(@0x7fdb74008d70 20150206-075544-16842879-38895-15065-0001)
         Expected: to be called once
           Actual: called twice - over-saturated and active
../../src/tests/master_allocator_tests.cpp:312: Failure
Mock function called more times than expected - taking default action specified at:
../../src/tests/mesos.hpp:753:
    Function call: recoverResources(@0x7fdb74013040 20150206-075544-16842879-38895-15065-0001, @0x7fdb74013060 20150206-075544-16842879-38895-15065-S0, @0x7fdb74013080 { cpus(*):2, mem(*):1024, disk(*):24988, ports(*):[31000-32000] }, @0x7fdb74013098 16-byte object <01-00 00-00 DB-7F 00-00 00-00 00-00 00-00 00-00>)
         Expected: to be called once
           Actual: called twice - over-saturated and active
I0206 07:55:44.339527 15090 slave.cpp:1592] Asked to shut down framework 20150206-075544-16842879-38895-15065-0001 by master@127.0.1.1:38895
W0206 07:55:44.339558 15090 slave.cpp:1607] Cannot shut down unknown framework 20150206-075544-16842879-38895-15065-0001
I0206 07:55:44.339954 15090 hierarchical_allocator_process.hpp:397] Deactivated framework 20150206-075544-16842879-38895-15065-0001
I0206 07:55:44.340095 15090 hierarchical_allocator_process.hpp:642] Recovered cpus(*):2; mem(*):1024; disk(*):24988; ports(*):[31000-32000] (total allocatable: cpus(*):2; mem(*):1024; disk(*):24988; ports(*):[31000-32000]) on slave 20150206-075544-16842879-38895-15065-S0 from framework 20150206-075544-16842879-38895-15065-0001
I0206 07:55:44.340181 15090 hierarchical_allocator_process.hpp:351] Removed framework 20150206-075544-16842879-38895-15065-0001
I0206 07:55:44.340852 15085 master.cpp:781] Master terminating
I0206 07:55:44.345564 15086 slave.cpp:2680] master@127.0.1.1:38895 exited
W0206 07:55:44.345593 15086 slave.cpp:2683] Master disconnected! Waiting for a new master to be elected
I0206 07:55:44.393707 15065 slave.cpp:502] Slave terminating
[  FAILED  ] MasterAllocatorTest/0.OutOfOrderDispatch, where TypeParam = mesos::master::allocator::HierarchicalAllocatorProcess<mesos::master::allocator::DRFSorter, mesos::master::allocator::DRFSorter> (360 ms)
{noformat}",1.0,0.22.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15789473684210525,0.15,0.20512820512820512,0.13939393939393938,0.13939393939393938,0.20183486238532108
Task,Deprecate / Remove CommandInfo::ContainerInfo,IIUC this has been deprecated and all current code (except examples/docker_no_executor_framework.cpp) uses the top-level ContainerInfo?,2.0,0,0.5,0.0030165912518853697,0.3333333333333333,0.5,1.0,0.0,0.0,0.0,0.007142857142857143,0.10256410256410256,0.2121212121212121,0.2121212121212121,0.0
Improvement,remove unnecessary constants,"In {{src/slave/paths.cpp}} a number of string constants are defined to describe the formats of various paths. However, given there is a 1:1 mapping between the string constant and the functions that build the paths, the code would be more readable if the format strings were inline in the functions.

In the cases where one constant depends on another (see the {{EXECUTOR_INFO_PATH, EXECUTOR_PATH, FRAMEWORK_PATH, SLAVE_PATH, ROOT_PATH}} chain, for example) the function calls can just be chained together.

This will have the added benefit of removing some statically constructed string constants, which are dangerous.",2.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05263157894736842,0.12142857142857143,0.07692307692307693,0.13333333333333333,0.13333333333333333,0.0
Bug,MasterAuthorizationTest.FrameworkRemovedBeforeReregistration is flaky.,"Good run:

{noformat}
[ RUN      ] MasterAuthorizationTest.FrameworkRemovedBeforeReregistration
Using temporary directory '/tmp/MasterAuthorizationTest_FrameworkRemovedBeforeReregistration_ZU7oaD'
I0122 19:23:06.481690 17483 leveldb.cpp:176] Opened db in 21.058723ms
I0122 19:23:06.488590 17483 leveldb.cpp:183] Compacted db in 6.6715ms
I0122 19:23:06.488816 17483 leveldb.cpp:198] Created db iterator in 30034ns
I0122 19:23:06.489053 17483 leveldb.cpp:204] Seeked to beginning of db in 2908ns
I0122 19:23:06.489073 17483 leveldb.cpp:273] Iterated through 0 keys in the db in 492ns
I0122 19:23:06.489148 17483 replica.cpp:744] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0122 19:23:06.490272 17504 recover.cpp:449] Starting replica recovery
I0122 19:23:06.490900 17504 recover.cpp:475] Replica is in EMPTY status
I0122 19:23:06.492422 17504 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request
I0122 19:23:06.492694 17504 recover.cpp:195] Received a recover response from a replica in EMPTY status
I0122 19:23:06.493185 17504 recover.cpp:566] Updating replica status to STARTING
I0122 19:23:06.514881 17504 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 21.459963ms
I0122 19:23:06.514920 17504 replica.cpp:323] Persisted replica status to STARTING
I0122 19:23:06.515861 17501 master.cpp:262] Master 20150122-192306-16842879-46283-17483 (lucid) started on 127.0.1.1:46283
I0122 19:23:06.515910 17501 master.cpp:308] Master only allowing authenticated frameworks to register
I0122 19:23:06.515923 17501 master.cpp:313] Master only allowing authenticated slaves to register
I0122 19:23:06.515946 17501 credentials.hpp:36] Loading credentials for authentication from '/tmp/MasterAuthorizationTest_FrameworkRemovedBeforeReregistration_ZU7oaD/credentials'
I0122 19:23:06.516150 17501 master.cpp:357] Authorization enabled
I0122 19:23:06.517511 17501 hierarchical_allocator_process.hpp:285] Initialized hierarchical allocator process
I0122 19:23:06.517607 17501 whitelist_watcher.cpp:65] No whitelist given
I0122 19:23:06.518066 17498 master.cpp:1219] The newly elected leader is master@127.0.1.1:46283 with id 20150122-192306-16842879-46283-17483
I0122 19:23:06.518095 17498 master.cpp:1232] Elected as the leading master!
I0122 19:23:06.518121 17498 master.cpp:1050] Recovering from registrar
I0122 19:23:06.518333 17498 registrar.cpp:313] Recovering registrar
I0122 19:23:06.523987 17504 recover.cpp:475] Replica is in STARTING status
I0122 19:23:06.525090 17504 replica.cpp:641] Replica in STARTING status received a broadcasted recover request
I0122 19:23:06.525337 17504 recover.cpp:195] Received a recover response from a replica in STARTING status
I0122 19:23:06.525693 17504 recover.cpp:566] Updating replica status to VOTING
I0122 19:23:06.532680 17504 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 6.810884ms
I0122 19:23:06.532714 17504 replica.cpp:323] Persisted replica status to VOTING
I0122 19:23:06.532835 17504 recover.cpp:580] Successfully joined the Paxos group
I0122 19:23:06.533004 17504 recover.cpp:464] Recover process terminated
I0122 19:23:06.533833 17500 log.cpp:660] Attempting to start the writer
I0122 19:23:06.535225 17500 replica.cpp:477] Replica received implicit promise request with proposal 1
I0122 19:23:06.540340 17500 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 5.086139ms
I0122 19:23:06.540371 17500 replica.cpp:345] Persisted promised to 1
I0122 19:23:06.541502 17504 coordinator.cpp:230] Coordinator attemping to fill missing position
I0122 19:23:06.543021 17504 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2
I0122 19:23:06.548140 17504 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 5.083443ms
I0122 19:23:06.548171 17504 replica.cpp:679] Persisted action at 0
I0122 19:23:06.549746 17500 replica.cpp:511] Replica received write request for position 0
I0122 19:23:06.549926 17500 leveldb.cpp:438] Reading position from leveldb took 31962ns
I0122 19:23:06.555033 17500 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 5.065823ms
I0122 19:23:06.555064 17500 replica.cpp:679] Persisted action at 0
I0122 19:23:06.556094 17504 replica.cpp:658] Replica received learned notice for position 0
I0122 19:23:06.558815 17504 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 2.688382ms
I0122 19:23:06.558847 17504 replica.cpp:679] Persisted action at 0
I0122 19:23:06.558868 17504 replica.cpp:664] Replica learned NOP action at position 0
I0122 19:23:06.559917 17500 log.cpp:676] Writer started with ending position 0
I0122 19:23:06.560995 17500 leveldb.cpp:438] Reading position from leveldb took 27742ns
I0122 19:23:06.563467 17500 registrar.cpp:346] Successfully fetched the registry (0B) in 45.095936ms
I0122 19:23:06.563551 17500 registrar.cpp:445] Applied 1 operations in 19686ns; attempting to update the 'registry'
I0122 19:23:06.566107 17500 log.cpp:684] Attempting to append 118 bytes to the log
I0122 19:23:06.566267 17500 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I0122 19:23:06.567126 17500 replica.cpp:511] Replica received write request for position 1
I0122 19:23:06.582588 17500 leveldb.cpp:343] Persisting action (135 bytes) to leveldb took 15.425511ms
I0122 19:23:06.582631 17500 replica.cpp:679] Persisted action at 1
I0122 19:23:06.583425 17500 replica.cpp:658] Replica received learned notice for position 1
I0122 19:23:06.589001 17500 leveldb.cpp:343] Persisting action (137 bytes) to leveldb took 5.549486ms
I0122 19:23:06.589200 17500 replica.cpp:679] Persisted action at 1
I0122 19:23:06.589416 17500 replica.cpp:664] Replica learned APPEND action at position 1
I0122 19:23:06.596420 17500 registrar.cpp:490] Successfully updated the 'registry' in 32.815104ms
I0122 19:23:06.596551 17500 registrar.cpp:376] Successfully recovered registrar
I0122 19:23:06.596923 17500 master.cpp:1077] Recovered 0 slaves from the Registry (82B) ; allowing 10mins for slaves to re-register
I0122 19:23:06.597007 17500 log.cpp:703] Attempting to truncate the log to 1
I0122 19:23:06.597239 17500 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2
I0122 19:23:06.598464 17501 replica.cpp:511] Replica received write request for position 2
I0122 19:23:06.604038 17501 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 5.536264ms
I0122 19:23:06.604084 17501 replica.cpp:679] Persisted action at 2
I0122 19:23:06.608747 17503 replica.cpp:658] Replica received learned notice for position 2
I0122 19:23:06.614094 17503 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 5.315347ms
I0122 19:23:06.614171 17503 leveldb.cpp:401] Deleting ~1 keys from leveldb took 33021ns
I0122 19:23:06.614188 17503 replica.cpp:679] Persisted action at 2
I0122 19:23:06.614208 17503 replica.cpp:664] Replica learned TRUNCATE action at position 2
I0122 19:23:06.628820 17483 sched.cpp:151] Version: 0.22.0
I0122 19:23:06.629879 17505 sched.cpp:248] New master detected at master@127.0.1.1:46283
I0122 19:23:06.629973 17505 sched.cpp:304] Authenticating with master master@127.0.1.1:46283
I0122 19:23:06.629995 17505 sched.cpp:311] Using default CRAM-MD5 authenticatee
I0122 19:23:06.630314 17505 authenticatee.hpp:138] Creating new client SASL connection
I0122 19:23:06.630722 17505 master.cpp:4129] Authenticating scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283
I0122 19:23:06.630750 17505 master.cpp:4140] Using default CRAM-MD5 authenticator
I0122 19:23:06.631115 17505 authenticator.hpp:170] Creating new server SASL connection
I0122 19:23:06.631423 17505 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5
I0122 19:23:06.631459 17505 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5'
I0122 19:23:06.631563 17505 authenticator.hpp:276] Received SASL authentication start
I0122 19:23:06.631605 17505 authenticator.hpp:398] Authentication requires more steps
I0122 19:23:06.631671 17505 authenticatee.hpp:275] Received SASL authentication step
I0122 19:23:06.631748 17505 authenticator.hpp:304] Received SASL authentication step
I0122 19:23:06.631774 17505 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'lucid' server FQDN: 'lucid' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0122 19:23:06.631784 17505 auxprop.cpp:171] Looking up auxiliary property '*userPassword'
I0122 19:23:06.631822 17505 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0122 19:23:06.631856 17505 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'lucid' server FQDN: 'lucid' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0122 19:23:06.631870 17505 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0122 19:23:06.631877 17505 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0122 19:23:06.631892 17505 authenticator.hpp:390] Authentication success
I0122 19:23:06.631988 17505 authenticatee.hpp:315] Authentication success
I0122 19:23:06.632066 17505 master.cpp:4187] Successfully authenticated principal 'test-principal' at scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283
I0122 19:23:06.632359 17505 sched.cpp:392] Successfully authenticated with master master@127.0.1.1:46283
I0122 19:23:06.632382 17505 sched.cpp:515] Sending registration request to master@127.0.1.1:46283
I0122 19:23:06.632432 17505 sched.cpp:548] Will retry registration in 598.155756ms if necessary
I0122 19:23:06.632575 17505 master.cpp:1420] Received registration request for framework 'default' at scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283
I0122 19:23:06.632639 17505 master.cpp:1298] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0122 19:23:06.632912 17505 master.cpp:1484] Registering framework 20150122-192306-16842879-46283-17483-0000 (default) at scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283
I0122 19:23:06.633421 17505 hierarchical_allocator_process.hpp:319] Added framework 20150122-192306-16842879-46283-17483-0000
I0122 19:23:06.633448 17505 hierarchical_allocator_process.hpp:839] No resources available to allocate!
I0122 19:23:06.633458 17505 hierarchical_allocator_process.hpp:746] Performed allocation for 0 slaves in 17704ns
I0122 19:23:06.633919 17505 sched.cpp:442] Framework registered with 20150122-192306-16842879-46283-17483-0000
I0122 19:23:06.633980 17505 sched.cpp:456] Scheduler::registered took 37063ns
I0122 19:23:06.636554 17500 sched.cpp:242] Scheduler::disconnected took 14843ns
I0122 19:23:06.636579 17500 sched.cpp:248] New master detected at master@127.0.1.1:46283
I0122 19:23:06.636625 17500 sched.cpp:304] Authenticating with master master@127.0.1.1:46283
I0122 19:23:06.636641 17500 sched.cpp:311] Using default CRAM-MD5 authenticatee
I0122 19:23:06.636914 17500 authenticatee.hpp:138] Creating new client SASL connection
I0122 19:23:06.637313 17500 master.cpp:4129] Authenticating scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283
I0122 19:23:06.637341 17500 master.cpp:4140] Using default CRAM-MD5 authenticator
I0122 19:23:06.637675 17500 authenticator.hpp:170] Creating new server SASL connection
I0122 19:23:06.638056 17501 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5
I0122 19:23:06.638083 17501 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5'
I0122 19:23:06.638182 17501 authenticator.hpp:276] Received SASL authentication start
I0122 19:23:06.638221 17501 authenticator.hpp:398] Authentication requires more steps
I0122 19:23:06.638286 17501 authenticatee.hpp:275] Received SASL authentication step
I0122 19:23:06.638360 17501 authenticator.hpp:304] Received SASL authentication step
I0122 19:23:06.638383 17501 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'lucid' server FQDN: 'lucid' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0122 19:23:06.638393 17501 auxprop.cpp:171] Looking up auxiliary property '*userPassword'
I0122 19:23:06.638422 17501 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0122 19:23:06.638447 17501 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'lucid' server FQDN: 'lucid' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0122 19:23:06.638458 17501 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0122 19:23:06.638464 17501 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0122 19:23:06.638478 17501 authenticator.hpp:390] Authentication success
I0122 19:23:06.638566 17501 authenticatee.hpp:315] Authentication success
I0122 19:23:06.638643 17501 master.cpp:4187] Successfully authenticated principal 'test-principal' at scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283
I0122 19:23:06.638919 17501 sched.cpp:392] Successfully authenticated with master master@127.0.1.1:46283
I0122 19:23:06.638942 17501 sched.cpp:515] Sending registration request to master@127.0.1.1:46283
I0122 19:23:06.638994 17501 sched.cpp:548] Will retry registration in 489.304713ms if necessary
I0122 19:23:06.639169 17501 master.cpp:1557] Received re-registration request from framework 20150122-192306-16842879-46283-17483-0000 (default) at scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283
I0122 19:23:06.639242 17501 master.cpp:1298] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0122 19:23:06.639839 17483 sched.cpp:1471] Asked to stop the driver
I0122 19:23:06.640379 17499 sched.cpp:808] Stopping framework '20150122-192306-16842879-46283-17483-0000'
I0122 19:23:06.640697 17499 master.cpp:745] Framework 20150122-192306-16842879-46283-17483-0000 (default) at scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283 disconnected
I0122 19:23:06.640723 17499 master.cpp:1789] Disconnecting framework 20150122-192306-16842879-46283-17483-0000 (default) at scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283
I0122 19:23:06.640744 17499 master.cpp:1805] Deactivating framework 20150122-192306-16842879-46283-17483-0000 (default) at scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283
I0122 19:23:06.640806 17499 master.cpp:767] Giving framework 20150122-192306-16842879-46283-17483-0000 (default) at scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283 0ns to failover
I0122 19:23:06.640951 17499 hierarchical_allocator_process.hpp:398] Deactivated framework 20150122-192306-16842879-46283-17483-0000
I0122 19:23:06.646342 17498 master.cpp:1604] Dropping re-registration request of framework 20150122-192306-16842879-46283-17483-0000 (default)  at scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283 because it is not authenticated
I0122 19:23:06.648844 17498 master.cpp:3941] Framework failover timeout, removing framework 20150122-192306-16842879-46283-17483-0000 (default) at scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283
I0122 19:23:06.648871 17498 master.cpp:4499] Removing framework 20150122-192306-16842879-46283-17483-0000 (default) at scheduler-4156eae6-8d7f-423a-920a-02b11b7bd1ba@127.0.1.1:46283
I0122 19:23:06.649624 17498 hierarchical_allocator_process.hpp:352] Removed framework 20150122-192306-16842879-46283-17483-0000
I0122 19:23:06.656532 17483 master.cpp:654] Master terminating
[       OK ] MasterAuthorizationTest.FrameworkRemovedBeforeReregistration (216 ms)
{noformat}

Bad run:

{noformat}
[ RUN      ] MasterAuthorizationTest.FrameworkRemovedBeforeReregistration
Using temporary directory '/tmp/MasterAuthorizationTest_FrameworkRemovedBeforeReregistration_JDM2sm'
I0126 19:19:55.517570  2381 leveldb.cpp:176] Opened db in 34.341401ms
I0126 19:19:55.529630  2381 leveldb.cpp:183] Compacted db in 11.824435ms
I0126 19:19:55.529878  2381 leveldb.cpp:198] Created db iterator in 26176ns
I0126 19:19:55.530200  2381 leveldb.cpp:204] Seeked to beginning of db in 3457ns
I0126 19:19:55.530455  2381 leveldb.cpp:273] Iterated through 0 keys in the db in 902ns
I0126 19:19:55.530658  2381 replica.cpp:744] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0126 19:19:55.531492  2397 recover.cpp:449] Starting replica recovery
I0126 19:19:55.531793  2397 recover.cpp:475] Replica is in EMPTY status
I0126 19:19:55.533327  2397 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request
I0126 19:19:55.533608  2397 recover.cpp:195] Received a recover response from a replica in EMPTY status
I0126 19:19:55.534101  2397 recover.cpp:566] Updating replica status to STARTING
I0126 19:19:55.550417  2397 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 16.106821ms
I0126 19:19:55.550472  2397 replica.cpp:323] Persisted replica status to STARTING
I0126 19:19:55.551434  2397 recover.cpp:475] Replica is in STARTING status
I0126 19:19:55.552846  2397 replica.cpp:641] Replica in STARTING status received a broadcasted recover request
I0126 19:19:55.553099  2397 recover.cpp:195] Received a recover response from a replica in STARTING status
I0126 19:19:55.553565  2397 recover.cpp:566] Updating replica status to VOTING
I0126 19:19:55.564590  2397 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 10.719218ms
I0126 19:19:55.564919  2397 replica.cpp:323] Persisted replica status to VOTING
I0126 19:19:55.565982  2397 recover.cpp:580] Successfully joined the Paxos group
I0126 19:19:55.566231  2397 recover.cpp:464] Recover process terminated
I0126 19:19:55.567878  2401 master.cpp:262] Master 20150126-191955-16842879-51862-2381 (lucid) started on 127.0.1.1:51862
I0126 19:19:55.567927  2401 master.cpp:308] Master only allowing authenticated frameworks to register
I0126 19:19:55.567950  2401 master.cpp:313] Master only allowing authenticated slaves to register
I0126 19:19:55.567978  2401 credentials.hpp:36] Loading credentials for authentication from '/tmp/MasterAuthorizationTest_FrameworkRemovedBeforeReregistration_JDM2sm/credentials'
I0126 19:19:55.568220  2401 master.cpp:357] Authorization enabled
I0126 19:19:55.569890  2401 hierarchical_allocator_process.hpp:285] Initialized hierarchical allocator process
I0126 19:19:55.569999  2401 whitelist_watcher.cpp:65] No whitelist given
I0126 19:19:55.570694  2401 master.cpp:1219] The newly elected leader is master@127.0.1.1:51862 with id 20150126-191955-16842879-51862-2381
I0126 19:19:55.570721  2401 master.cpp:1232] Elected as the leading master!
I0126 19:19:55.570742  2401 master.cpp:1050] Recovering from registrar
I0126 19:19:55.570977  2401 registrar.cpp:313] Recovering registrar
I0126 19:19:55.571959  2401 log.cpp:660] Attempting to start the writer
I0126 19:19:55.573441  2401 replica.cpp:477] Replica received implicit promise request with proposal 1
I0126 19:19:55.590724  2401 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 17.243964ms
I0126 19:19:55.590785  2401 replica.cpp:345] Persisted promised to 1
I0126 19:19:55.592140  2396 coordinator.cpp:230] Coordinator attemping to fill missing position
I0126 19:19:55.593834  2396 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2
I0126 19:19:55.603837  2396 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 9.955824ms
I0126 19:19:55.603902  2396 replica.cpp:679] Persisted action at 0
I0126 19:19:55.606082  2401 replica.cpp:511] Replica received write request for position 0
I0126 19:19:55.606331  2401 leveldb.cpp:438] Reading position from leveldb took 44524ns
I0126 19:19:55.612546  2401 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 5.870411ms
I0126 19:19:55.612597  2401 replica.cpp:679] Persisted action at 0
I0126 19:19:55.613416  2401 replica.cpp:658] Replica received learned notice for position 0
I0126 19:19:55.616269  2401 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 2.82145ms
I0126 19:19:55.616305  2401 replica.cpp:679] Persisted action at 0
I0126 19:19:55.616328  2401 replica.cpp:664] Replica learned NOP action at position 0
I0126 19:19:55.628062  2399 log.cpp:676] Writer started with ending position 0
I0126 19:19:55.629328  2399 leveldb.cpp:438] Reading position from leveldb took 57003ns
I0126 19:19:55.631995  2399 registrar.cpp:346] Successfully fetched the registry (0B) in 60.973824ms
I0126 19:19:55.632109  2399 registrar.cpp:445] Applied 1 operations in 35531ns; attempting to update the 'registry'
I0126 19:19:55.634799  2399 log.cpp:684] Attempting to append 117 bytes to the log
I0126 19:19:55.634996  2399 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I0126 19:19:55.636651  2397 replica.cpp:511] Replica received write request for position 1
I0126 19:19:55.642165  2397 leveldb.cpp:343] Persisting action (134 bytes) to leveldb took 5.474306ms
I0126 19:19:55.642215  2397 replica.cpp:679] Persisted action at 1
I0126 19:19:55.643226  2397 replica.cpp:658] Replica received learned notice for position 1
I0126 19:19:55.648574  2397 leveldb.cpp:343] Persisting action (136 bytes) to leveldb took 5.317891ms
I0126 19:19:55.648808  2397 replica.cpp:679] Persisted action at 1
I0126 19:19:55.649158  2397 replica.cpp:664] Replica learned APPEND action at position 1
I0126 19:19:55.663101  2397 registrar.cpp:490] Successfully updated the 'registry' in 30.918144ms
I0126 19:19:55.663267  2397 registrar.cpp:376] Successfully recovered registrar
I0126 19:19:55.663699  2397 master.cpp:1077] Recovered 0 slaves from the Registry (81B) ; allowing 10mins for slaves to re-register
I0126 19:19:55.663795  2397 log.cpp:703] Attempting to truncate the log to 1
I0126 19:19:55.664083  2397 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2
I0126 19:19:55.665573  2403 replica.cpp:511] Replica received write request for position 2
I0126 19:19:55.671500  2403 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 5.883759ms
I0126 19:19:55.671547  2403 replica.cpp:679] Persisted action at 2
I0126 19:19:55.672780  2403 replica.cpp:658] Replica received learned notice for position 2
I0126 19:19:55.685999  2403 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 12.808643ms
I0126 19:19:55.686099  2403 leveldb.cpp:401] Deleting ~1 keys from leveldb took 49867ns
I0126 19:19:55.686121  2403 replica.cpp:679] Persisted action at 2
I0126 19:19:55.686149  2403 replica.cpp:664] Replica learned TRUNCATE action at position 2
I0126 19:19:55.722545  2381 sched.cpp:151] Version: 0.22.0
I0126 19:19:55.723795  2401 sched.cpp:248] New master detected at master@127.0.1.1:51862
I0126 19:19:55.723891  2401 sched.cpp:304] Authenticating with master master@127.0.1.1:51862
I0126 19:19:55.723914  2401 sched.cpp:311] Using default CRAM-MD5 authenticatee
I0126 19:19:55.724244  2401 authenticatee.hpp:138] Creating new client SASL connection
I0126 19:19:55.724694  2401 master.cpp:4129] Authenticating scheduler-80465e3f-73a3-4bd0-ba66-4dca62e9cdee@127.0.1.1:51862
I0126 19:19:55.724725  2401 master.cpp:4140] Using default CRAM-MD5 authenticator
I0126 19:19:55.725108  2401 authenticator.hpp:170] Creating new server SASL connection
I0126 19:19:55.725390  2401 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5
I0126 19:19:55.725415  2401 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5'
I0126 19:19:55.725515  2401 authenticator.hpp:276] Received SASL authentication start
I0126 19:19:55.725566  2401 authenticator.hpp:398] Authentication requires more steps
I0126 19:19:55.725632  2401 authenticatee.hpp:275] Received SASL authentication step
I0126 19:19:55.725710  2401 authenticator.hpp:304] Received SASL authentication step
I0126 19:19:55.725744  2401 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'lucid' server FQDN: 'lucid' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0126 19:19:55.725757  2401 auxprop.cpp:171] Looking up auxiliary property '*userPassword'
I0126 19:19:55.725808  2401 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0126 19:19:55.725834  2401 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'lucid' server FQDN: 'lucid' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0126 19:19:55.725847  2401 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0126 19:19:55.725853  2401 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0126 19:19:55.725867  2401 authenticator.hpp:390] Authentication success
I0126 19:19:55.728629  2399 authenticatee.hpp:315] Authentication success
I0126 19:19:55.729228  2399 sched.cpp:392] Successfully authenticated with master master@127.0.1.1:51862
I0126 19:19:55.729277  2399 sched.cpp:515] Sending registration request to master@127.0.1.1:51862
I0126 19:19:55.729365  2399 sched.cpp:548] Will retry registration in 3.855403ms if necessary
I0126 19:19:55.729671  2399 master.cpp:1411] Queuing up registration request for framework 'default' at scheduler-80465e3f-73a3-4bd0-ba66-4dca62e9cdee@127.0.1.1:51862 because authentication is still in progress
I0126 19:19:55.733487  2400 master.cpp:4187] Successfully authenticated principal 'test-principal' at scheduler-80465e3f-73a3-4bd0-ba66-4dca62e9cdee@127.0.1.1:51862
I0126 19:19:55.734094  2400 master.cpp:1420] Received registration request for framework 'default' at scheduler-80465e3f-73a3-4bd0-ba66-4dca62e9cdee@127.0.1.1:51862
I0126 19:19:55.734177  2400 master.cpp:1298] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0126 19:19:55.734724  2400 master.cpp:1484] Registering framework 20150126-191955-16842879-51862-2381-0000 (default) at scheduler-80465e3f-73a3-4bd0-ba66-4dca62e9cdee@127.0.1.1:51862
I0126 19:19:55.735335  2402 hierarchical_allocator_process.hpp:319] Added framework 20150126-191955-16842879-51862-2381-0000
I0126 19:19:55.735376  2402 hierarchical_allocator_process.hpp:831] No resources available to allocate!
I0126 19:19:55.735389  2402 hierarchical_allocator_process.hpp:738] Performed allocation for 0 slaves in 22978ns
I0126 19:19:55.741891  2398 sched.cpp:515] Sending registration request to master@127.0.1.1:51862
I0126 19:19:55.744575  2398 sched.cpp:548] Will retry registration in 3.86742709secs if necessary
I0126 19:19:55.744742  2398 sched.cpp:442] Framework registered with 20150126-191955-16842879-51862-2381-0000
I0126 19:19:55.744827  2398 sched.cpp:456] Scheduler::registered took 60111ns
I0126 19:19:55.744956  2398 master.cpp:1420] Received registration request for framework 'default' at scheduler-80465e3f-73a3-4bd0-ba66-4dca62e9cdee@127.0.1.1:51862
I0126 19:19:55.745020  2398 master.cpp:1298] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0126 19:19:55.749315  2401 sched.cpp:242] Scheduler::disconnected took 19450ns
I0126 19:19:55.749343  2401 sched.cpp:248] New master detected at master@127.0.1.1:51862
I0126 19:19:55.749394  2401 sched.cpp:304] Authenticating with master master@127.0.1.1:51862
I0126 19:19:55.749411  2401 sched.cpp:311] Using default CRAM-MD5 authenticatee
I0126 19:19:55.749743  2401 authenticatee.hpp:138] Creating new client SASL connection
I0126 19:19:55.750208  2401 master.cpp:4129] Authenticating scheduler-80465e3f-73a3-4bd0-ba66-4dca62e9cdee@127.0.1.1:51862
I0126 19:19:55.750238  2401 master.cpp:4140] Using default CRAM-MD5 authenticator
I0126 19:19:55.750629  2401 authenticator.hpp:170] Creating new server SASL connection
I0126 19:19:55.750938  2401 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5
I0126 19:19:55.750963  2401 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5'
I0126 19:19:55.751063  2401 authenticator.hpp:276] Received SASL authentication start
I0126 19:19:55.751109  2401 authenticator.hpp:398] Authentication requires more steps
I0126 19:19:55.751175  2401 authenticatee.hpp:275] Received SASL authentication step
I0126 19:19:55.751269  2401 authenticator.hpp:304] Received SASL authentication step
I0126 19:19:55.751296  2401 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'lucid' server FQDN: 'lucid' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0126 19:19:55.751307  2401 auxprop.cpp:171] Looking up auxiliary property '*userPassword'
I0126 19:19:55.751358  2401 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0126 19:19:55.751392  2401 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'lucid' server FQDN: 'lucid' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0126 19:19:55.751405  2401 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0126 19:19:55.751413  2401 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0126 19:19:55.751427  2401 authenticator.hpp:390] Authentication success
I0126 19:19:55.751524  2401 authenticatee.hpp:315] Authentication success
I0126 19:19:55.751605  2401 master.cpp:4187] Successfully authenticated principal 'test-principal' at scheduler-80465e3f-73a3-4bd0-ba66-4dca62e9cdee@127.0.1.1:51862
I0126 19:19:55.751898  2401 sched.cpp:392] Successfully authenticated with master master@127.0.1.1:51862
I0126 19:19:55.751922  2401 sched.cpp:515] Sending registration request to master@127.0.1.1:51862
I0126 19:19:55.751996  2401 sched.cpp:548] Will retry registration in 1.511226315secs if necessary
I0126 19:19:55.752174  2401 master.cpp:1557] Received re-registration request from framework 20150126-191955-16842879-51862-2381-0000 (default) at scheduler-80465e3f-73a3-4bd0-ba66-4dca62e9cdee@127.0.1.1:51862
I0126 19:19:55.752256  2401 master.cpp:1298] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0126 19:19:55.752485  2401 master.cpp:1610] Re-registering framework 20150126-191955-16842879-51862-2381-0000 (default)  at scheduler-80465e3f-73a3-4bd0-ba66-4dca62e9cdee@127.0.1.1:51862
I0126 19:19:55.752527  2401 master.cpp:1650] Allowing framework 20150126-191955-16842879-51862-2381-0000 (default) at scheduler-80465e3f-73a3-4bd0-ba66-4dca62e9cdee@127.0.1.1:51862 to re-register with an already used id
I0126 19:19:55.752689  2401 sched.cpp:484] Framework re-registered with 20150126-191955-16842879-51862-2381-0000
tests/master_authorization_tests.cpp:980: Failure
Mock function called more times than expected - returning directly.
    Function call: reregistered(0x7fff5cef57e0, @0x56077d0 id: ""20150126-191955-16842879-51862-2381""
ip: 16842879
port: 51862
pid: ""master@127.0.1.1:51862""
hostname: ""lucid""
)
         Expected: to be never called
           Actual: called once - over-saturated and active
I0126 19:19:55.753191  2401 sched.cpp:498] Scheduler::reregistered took 478798ns
I0126 19:19:55.753600  2381 sched.cpp:1471] Asked to stop the driver
I0126 19:19:55.754518  2402 sched.cpp:808] Stopping framework '20150126-191955-16842879-51862-2381-0000'
I0126 19:19:55.755089  2402 master.cpp:1744] Asked to unregister framework 20150126-191955-16842879-51862-2381-0000
I0126 19:19:55.755302  2402 master.cpp:4499] Removing framework 20150126-191955-16842879-51862-2381-0000 (default) at scheduler-80465e3f-73a3-4bd0-ba66-4dca62e9cdee@127.0.1.1:51862
I0126 19:19:55.759419  2402 hierarchical_allocator_process.hpp:398] Deactivated framework 20150126-191955-16842879-51862-2381-0000
I0126 19:19:55.759850  2402 hierarchical_allocator_process.hpp:352] Removed framework 20150126-191955-16842879-51862-2381-0000
I0126 19:19:55.761160  2400 master.cpp:1462] Dropping registration request for framework 'default' at scheduler-80465e3f-73a3-4bd0-ba66-4dca62e9cdee@127.0.1.1:51862 because it is not authenticated
I0126 19:19:55.771309  2381 master.cpp:654] Master terminating
[  FAILED  ] MasterAuthorizationTest.FrameworkRemovedBeforeReregistration	 (312 ms)
{noformat}",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.0
Bug,FaultToleranceTest.SchedulerFailoverFrameworkMessage is flaky.,"Bad Run:
{noformat}
[ RUN      ] FaultToleranceTest.SchedulerFailoverFrameworkMessage
Using temporary directory '/tmp/FaultToleranceTest_SchedulerFailoverFrameworkMessage_f3jYkr'
I0123 18:50:11.669674 15688 leveldb.cpp:176] Opened db in 31.920683ms
I0123 18:50:11.678328 15688 leveldb.cpp:183] Compacted db in 8.580569ms
I0123 18:50:11.678455 15688 leveldb.cpp:198] Created db iterator in 38478ns
I0123 18:50:11.678478 15688 leveldb.cpp:204] Seeked to beginning of db in 3057ns
I0123 18:50:11.678489 15688 leveldb.cpp:273] Iterated through 0 keys in the db in 427ns
I0123 18:50:11.678539 15688 replica.cpp:744] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0123 18:50:11.682271 15705 recover.cpp:449] Starting replica recovery
I0123 18:50:11.682634 15705 recover.cpp:475] Replica is in EMPTY status
I0123 18:50:11.684389 15708 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request
I0123 18:50:11.685132 15708 recover.cpp:195] Received a recover response from a replica in EMPTY status
I0123 18:50:11.689842 15708 recover.cpp:566] Updating replica status to STARTING
I0123 18:50:11.702548 15708 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 12.484558ms
I0123 18:50:11.702615 15708 replica.cpp:323] Persisted replica status to STARTING
I0123 18:50:11.703531 15708 recover.cpp:475] Replica is in STARTING status
I0123 18:50:11.705080 15704 replica.cpp:641] Replica in STARTING status received a broadcasted recover request
I0123 18:50:11.712587 15708 recover.cpp:195] Received a recover response from a replica in STARTING status
I0123 18:50:11.722898 15708 recover.cpp:566] Updating replica status to VOTING
I0123 18:50:11.725427 15703 master.cpp:262] Master 20150123-185011-16777343-37526-15688 (localhost.localdomain) started on 127.0.0.1:37526
W0123 18:50:11.725464 15703 master.cpp:266] 
**************************************************
Master bound to loopback interface! Cannot communicate with remote schedulers or slaves. You might want to set '--ip' flag to a routable IP address.
**************************************************
I0123 18:50:11.725502 15703 master.cpp:308] Master only allowing authenticated frameworks to register
I0123 18:50:11.725513 15703 master.cpp:313] Master only allowing authenticated slaves to register
I0123 18:50:11.725543 15703 credentials.hpp:36] Loading credentials for authentication from '/tmp/FaultToleranceTest_SchedulerFailoverFrameworkMessage_f3jYkr/credentials'
I0123 18:50:11.725774 15703 master.cpp:357] Authorization enabled
I0123 18:50:11.728428 15707 whitelist_watcher.cpp:65] No whitelist given
I0123 18:50:11.729169 15707 master.cpp:1219] The newly elected leader is master@127.0.0.1:37526 with id 20150123-185011-16777343-37526-15688
I0123 18:50:11.729200 15707 master.cpp:1232] Elected as the leading master!
I0123 18:50:11.729223 15707 master.cpp:1050] Recovering from registrar
I0123 18:50:11.729595 15706 registrar.cpp:313] Recovering registrar
I0123 18:50:11.730715 15703 hierarchical_allocator_process.hpp:285] Initialized hierarchical allocator process
I0123 18:50:11.737431 15708 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 14.259597ms
I0123 18:50:11.737511 15708 replica.cpp:323] Persisted replica status to VOTING
I0123 18:50:11.737768 15708 recover.cpp:580] Successfully joined the Paxos group
I0123 18:50:11.737977 15708 recover.cpp:464] Recover process terminated
I0123 18:50:11.739083 15706 log.cpp:660] Attempting to start the writer
I0123 18:50:11.741236 15706 replica.cpp:477] Replica received implicit promise request with proposal 1
I0123 18:50:11.750435 15706 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 8.813783ms
I0123 18:50:11.750514 15706 replica.cpp:345] Persisted promised to 1
I0123 18:50:11.752239 15708 coordinator.cpp:230] Coordinator attemping to fill missing position
I0123 18:50:11.754176 15706 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2
I0123 18:50:11.763464 15706 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 8.799822ms
I0123 18:50:11.763535 15706 replica.cpp:679] Persisted action at 0
I0123 18:50:11.765697 15709 replica.cpp:511] Replica received write request for position 0
I0123 18:50:11.766293 15709 leveldb.cpp:438] Reading position from leveldb took 54028ns
I0123 18:50:11.776468 15709 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 9.789169ms
I0123 18:50:11.776561 15709 replica.cpp:679] Persisted action at 0
I0123 18:50:11.777515 15709 replica.cpp:658] Replica received learned notice for position 0
I0123 18:50:11.785459 15709 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 7.897242ms
I0123 18:50:11.785531 15709 replica.cpp:679] Persisted action at 0
I0123 18:50:11.785565 15709 replica.cpp:664] Replica learned NOP action at position 0
I0123 18:50:11.786633 15709 log.cpp:676] Writer started with ending position 0
I0123 18:50:11.788460 15709 leveldb.cpp:438] Reading position from leveldb took 266087ns
I0123 18:50:11.801141 15709 registrar.cpp:346] Successfully fetched the registry (0B) in 71.491072ms
I0123 18:50:11.801300 15709 registrar.cpp:445] Applied 1 operations in 41795ns; attempting to update the 'registry'
I0123 18:50:11.805186 15707 log.cpp:684] Attempting to append 136 bytes to the log
I0123 18:50:11.805454 15707 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I0123 18:50:11.806677 15703 replica.cpp:511] Replica received write request for position 1
I0123 18:50:11.815621 15703 leveldb.cpp:343] Persisting action (155 bytes) to leveldb took 8.89177ms
I0123 18:50:11.815692 15703 replica.cpp:679] Persisted action at 1
I0123 18:50:11.817358 15704 replica.cpp:658] Replica received learned notice for position 1
I0123 18:50:11.825014 15704 leveldb.cpp:343] Persisting action (157 bytes) to leveldb took 7.578558ms
I0123 18:50:11.825088 15704 replica.cpp:679] Persisted action at 1
I0123 18:50:11.825124 15704 replica.cpp:664] Replica learned APPEND action at position 1
I0123 18:50:11.827008 15705 registrar.cpp:490] Successfully updated the 'registry' in 25.629952ms
I0123 18:50:11.827143 15705 registrar.cpp:376] Successfully recovered registrar
I0123 18:50:11.827517 15705 master.cpp:1077] Recovered 0 slaves from the Registry (98B) ; allowing 10mins for slaves to re-register
I0123 18:50:11.828515 15704 log.cpp:703] Attempting to truncate the log to 1
I0123 18:50:11.829074 15704 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2
I0123 18:50:11.830546 15709 replica.cpp:511] Replica received write request for position 2
I0123 18:50:11.837752 15709 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 7.142431ms
I0123 18:50:11.837826 15709 replica.cpp:679] Persisted action at 2
I0123 18:50:11.839334 15709 replica.cpp:658] Replica received learned notice for position 2
I0123 18:50:11.847069 15709 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 7.116607ms
I0123 18:50:11.847214 15709 leveldb.cpp:401] Deleting ~1 keys from leveldb took 74008ns
I0123 18:50:11.847241 15709 replica.cpp:679] Persisted action at 2
I0123 18:50:11.847295 15709 replica.cpp:664] Replica learned TRUNCATE action at position 2
I0123 18:50:11.870337 15710 slave.cpp:173] Slave started on 94)@127.0.0.1:37526
W0123 18:50:11.870980 15710 slave.cpp:176] 
**************************************************
Slave bound to loopback interface! Cannot communicate with remote master(s). You might want to set '--ip' flag to a routable IP address.
**************************************************
I0123 18:50:11.871412 15710 credentials.hpp:84] Loading credential for authentication from '/tmp/FaultToleranceTest_SchedulerFailoverFrameworkMessage_TB8Rh3/credential'
I0123 18:50:11.871819 15710 slave.cpp:282] Slave using credential for: test-principal
I0123 18:50:11.873178 15710 slave.cpp:300] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0123 18:50:11.873620 15710 slave.cpp:329] Slave hostname: localhost.localdomain
I0123 18:50:11.873837 15710 slave.cpp:330] Slave checkpoint: false
W0123 18:50:11.874068 15710 slave.cpp:332] Disabling checkpointing is deprecated and the --checkpoint flag will be removed in a future release. Please avoid using this flag
I0123 18:50:11.879103 15705 state.cpp:33] Recovering state from '/tmp/FaultToleranceTest_SchedulerFailoverFrameworkMessage_TB8Rh3/meta'
W0123 18:50:11.882972 15688 sched.cpp:1246] 
**************************************************
Scheduler driver bound to loopback interface! Cannot communicate with remote master(s). You might want to set 'LIBPROCESS_IP' environment variable to use a routable IP address.
**************************************************
I0123 18:50:11.884106 15709 status_update_manager.cpp:197] Recovering status update manager
I0123 18:50:11.884703 15710 slave.cpp:3519] Finished recovery
I0123 18:50:11.892076 15704 status_update_manager.cpp:171] Pausing sending status updates
I0123 18:50:11.892590 15710 slave.cpp:613] New master detected at master@127.0.0.1:37526
I0123 18:50:11.892937 15710 slave.cpp:676] Authenticating with master master@127.0.0.1:37526
I0123 18:50:11.893165 15710 slave.cpp:681] Using default CRAM-MD5 authenticatee
I0123 18:50:11.893754 15708 authenticatee.hpp:138] Creating new client SASL connection
I0123 18:50:11.894120 15708 master.cpp:4129] Authenticating slave(94)@127.0.0.1:37526
I0123 18:50:11.894153 15708 master.cpp:4140] Using default CRAM-MD5 authenticator
I0123 18:50:11.894628 15708 authenticator.hpp:170] Creating new server SASL connection
I0123 18:50:11.894913 15708 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5
I0123 18:50:11.894942 15708 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5'
I0123 18:50:11.895043 15708 authenticator.hpp:276] Received SASL authentication start
I0123 18:50:11.895095 15708 authenticator.hpp:398] Authentication requires more steps
I0123 18:50:11.895165 15708 authenticatee.hpp:275] Received SASL authentication step
I0123 18:50:11.895261 15708 authenticator.hpp:304] Received SASL authentication step
I0123 18:50:11.895292 15708 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'localhost.localdomain' server FQDN: 'localhost.localdomain' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0123 18:50:11.895305 15708 auxprop.cpp:171] Looking up auxiliary property '*userPassword'
I0123 18:50:11.895354 15708 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0123 18:50:11.895881 15710 slave.cpp:649] Detecting new master
I0123 18:50:11.898449 15708 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'localhost.localdomain' server FQDN: 'localhost.localdomain' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0123 18:50:11.899024 15708 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0123 18:50:11.899106 15708 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0123 18:50:11.899190 15708 authenticator.hpp:390] Authentication success
I0123 18:50:11.899569 15706 authenticatee.hpp:315] Authentication success
I0123 18:50:11.902299 15706 slave.cpp:747] Successfully authenticated with master master@127.0.0.1:37526
I0123 18:50:11.902847 15706 slave.cpp:1075] Will retry registration in 19.809649ms if necessary
I0123 18:50:11.903264 15705 master.cpp:3214] Queuing up registration request from slave(94)@127.0.0.1:37526 because authentication is still in progress
I0123 18:50:11.903497 15705 master.cpp:4187] Successfully authenticated principal 'test-principal' at slave(94)@127.0.0.1:37526
I0123 18:50:11.903940 15705 master.cpp:3275] Registering slave at slave(94)@127.0.0.1:37526 (localhost.localdomain) with id 20150123-185011-16777343-37526-15688-S0
I0123 18:50:11.904398 15705 registrar.cpp:445] Applied 1 operations in 63679ns; attempting to update the 'registry'
I0123 18:50:11.917883 15688 sched.cpp:151] Version: 0.22.0
I0123 18:50:11.919347 15703 log.cpp:684] Attempting to append 315 bytes to the log
I0123 18:50:11.921039 15703 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3
I0123 18:50:11.919992 15706 sched.cpp:248] New master detected at master@127.0.0.1:37526
I0123 18:50:11.921352 15706 sched.cpp:304] Authenticating with master master@127.0.0.1:37526
I0123 18:50:11.921408 15706 sched.cpp:311] Using default CRAM-MD5 authenticatee
I0123 18:50:11.921773 15706 authenticatee.hpp:138] Creating new client SASL connection
I0123 18:50:11.922266 15706 master.cpp:4129] Authenticating scheduler-2cecb105-ca23-4048-9707-12b1e4422e11@127.0.0.1:37526
I0123 18:50:11.922301 15706 master.cpp:4140] Using default CRAM-MD5 authenticator
I0123 18:50:11.923928 15703 replica.cpp:511] Replica received write request for position 3
I0123 18:50:11.924285 15707 authenticator.hpp:170] Creating new server SASL connection
I0123 18:50:11.925091 15707 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5
I0123 18:50:11.925122 15707 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5'
I0123 18:50:11.925194 15707 authenticator.hpp:276] Received SASL authentication start
I0123 18:50:11.925257 15707 authenticator.hpp:398] Authentication requires more steps
I0123 18:50:11.925325 15707 authenticatee.hpp:275] Received SASL authentication step
I0123 18:50:11.925442 15707 authenticator.hpp:304] Received SASL authentication step
I0123 18:50:11.925473 15707 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'localhost.localdomain' server FQDN: 'localhost.localdomain' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0123 18:50:11.925487 15707 auxprop.cpp:171] Looking up auxiliary property '*userPassword'
I0123 18:50:11.925532 15707 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0123 18:50:11.925559 15707 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'localhost.localdomain' server FQDN: 'localhost.localdomain' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0123 18:50:11.925571 15707 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0123 18:50:11.925580 15707 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0123 18:50:11.925595 15707 authenticator.hpp:390] Authentication success
I0123 18:50:11.925695 15707 authenticatee.hpp:315] Authentication success
I0123 18:50:11.925792 15707 master.cpp:4187] Successfully authenticated principal 'test-principal' at scheduler-2cecb105-ca23-4048-9707-12b1e4422e11@127.0.0.1:37526
I0123 18:50:11.926127 15707 sched.cpp:392] Successfully authenticated with master master@127.0.0.1:37526
I0123 18:50:11.926154 15707 sched.cpp:515] Sending registration request to master@127.0.0.1:37526
I0123 18:50:11.926215 15707 sched.cpp:548] Will retry registration in 866.81063ms if necessary
I0123 18:50:11.926640 15707 master.cpp:1420] Received registration request for framework 'default' at scheduler-2cecb105-ca23-4048-9707-12b1e4422e11@127.0.0.1:37526
I0123 18:50:11.926960 15707 master.cpp:1298] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0123 18:50:11.927691 15707 master.cpp:1484] Registering framework 20150123-185011-16777343-37526-15688-0000 (default) at scheduler-2cecb105-ca23-4048-9707-12b1e4422e11@127.0.0.1:37526
I0123 18:50:11.928292 15708 hierarchical_allocator_process.hpp:319] Added framework 20150123-185011-16777343-37526-15688-0000
I0123 18:50:11.928326 15708 hierarchical_allocator_process.hpp:839] No resources available to allocate!
I0123 18:50:11.928340 15708 hierarchical_allocator_process.hpp:746] Performed allocation for 0 slaves in 21080ns
I0123 18:50:11.934458 15707 sched.cpp:442] Framework registered with 20150123-185011-16777343-37526-15688-0000
I0123 18:50:11.934927 15707 sched.cpp:456] Scheduler::registered took 112885ns
I0123 18:50:11.935747 15709 slave.cpp:1075] Will retry registration in 19.609252ms if necessary
I0123 18:50:11.935981 15709 master.cpp:3263] Ignoring register slave message from slave(94)@127.0.0.1:37526 (localhost.localdomain) as admission is already in progress
I0123 18:50:11.938997 15703 leveldb.cpp:343] Persisting action (334 bytes) to leveldb took 10.171709ms
I0123 18:50:11.939049 15703 replica.cpp:679] Persisted action at 3
I0123 18:50:11.940630 15709 replica.cpp:658] Replica received learned notice for position 3
I0123 18:50:11.945473 15709 leveldb.cpp:343] Persisting action (336 bytes) to leveldb took 4.804742ms
I0123 18:50:11.945521 15709 replica.cpp:679] Persisted action at 3
I0123 18:50:11.945550 15709 replica.cpp:664] Replica learned APPEND action at position 3
I0123 18:50:11.947105 15709 registrar.cpp:490] Successfully updated the 'registry' in 42.637056ms
I0123 18:50:11.948020 15703 master.cpp:3329] Registered slave 20150123-185011-16777343-37526-15688-S0 at slave(94)@127.0.0.1:37526 (localhost.localdomain) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0123 18:50:11.948318 15703 hierarchical_allocator_process.hpp:453] Added slave 20150123-185011-16777343-37526-15688-S0 (localhost.localdomain) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] available)
I0123 18:50:11.948719 15703 hierarchical_allocator_process.hpp:764] Performed allocation for slave 20150123-185011-16777343-37526-15688-S0 in 355831ns
I0123 18:50:11.948813 15703 slave.cpp:781] Registered with master master@127.0.0.1:37526; given slave ID 20150123-185011-16777343-37526-15688-S0
I0123 18:50:11.948969 15703 slave.cpp:2588] Received ping from slave-observer(92)@127.0.0.1:37526
I0123 18:50:11.949324 15703 master.cpp:4071] Sending 1 offers to framework 20150123-185011-16777343-37526-15688-0000 (default) at scheduler-2cecb105-ca23-4048-9707-12b1e4422e11@127.0.0.1:37526
I0123 18:50:11.949571 15706 status_update_manager.cpp:178] Resuming sending status updates
I0123 18:50:11.950023 15709 log.cpp:703] Attempting to truncate the log to 3
I0123 18:50:11.950810 15705 sched.cpp:605] Scheduler::resourceOffers took 135580ns
I0123 18:50:11.952793 15708 master.cpp:2677] Processing ACCEPT call for offers: [ 20150123-185011-16777343-37526-15688-O0 ] on slave 20150123-185011-16777343-37526-15688-S0 at slave(94)@127.0.0.1:37526 (localhost.localdomain) for framework 20150123-185011-16777343-37526-15688-0000 (default) at scheduler-2cecb105-ca23-4048-9707-12b1e4422e11@127.0.0.1:37526
I0123 18:50:11.952852 15708 master.cpp:2513] Authorizing framework principal 'test-principal' to launch task 1 as user 'jenkins'
W0123 18:50:11.954649 15708 master.cpp:2130] Executor default for task 1 uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases.
W0123 18:50:11.954988 15708 master.cpp:2142] Executor default for task 1 uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases.
I0123 18:50:11.955579 15708 master.hpp:782] Adding task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20150123-185011-16777343-37526-15688-S0 (localhost.localdomain)
I0123 18:50:11.956035 15703 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4
I0123 18:50:11.957592 15704 replica.cpp:511] Replica received write request for position 4
I0123 18:50:11.958485 15708 master.cpp:2885] Launching task 1 of framework 20150123-185011-16777343-37526-15688-0000 (default) at scheduler-2cecb105-ca23-4048-9707-12b1e4422e11@127.0.0.1:37526 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20150123-185011-16777343-37526-15688-S0 at slave(94)@127.0.0.1:37526 (localhost.localdomain)
I0123 18:50:11.960578 15706 slave.cpp:1130] Got assigned task 1 for framework 20150123-185011-16777343-37526-15688-0000
I0123 18:50:11.961293 15706 slave.cpp:1245] Launching task 1 for framework 20150123-185011-16777343-37526-15688-0000
I0123 18:50:11.964450 15704 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 6.81421ms
I0123 18:50:11.964496 15704 replica.cpp:679] Persisted action at 4
I0123 18:50:11.966328 15705 replica.cpp:658] Replica received learned notice for position 4
I0123 18:50:11.969648 15706 slave.cpp:3921] Launching executor default of framework 20150123-185011-16777343-37526-15688-0000 in work directory '/tmp/FaultToleranceTest_SchedulerFailoverFrameworkMessage_TB8Rh3/slaves/20150123-185011-16777343-37526-15688-S0/frameworks/20150123-185011-16777343-37526-15688-0000/executors/default/runs/02536e4f-fb59-4b75-99aa-611fd7fffcb1'
I0123 18:50:11.976954 15705 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 10.584003ms
I0123 18:50:11.977078 15705 leveldb.cpp:401] Deleting ~2 keys from leveldb took 72466ns
I0123 18:50:11.977104 15705 replica.cpp:679] Persisted action at 4
I0123 18:50:11.977138 15705 replica.cpp:664] Replica learned TRUNCATE action at position 4
I0123 18:50:11.978016 15706 exec.cpp:147] Version: 0.22.0
I0123 18:50:11.978646 15710 exec.cpp:197] Executor started at: executor(50)@127.0.0.1:37526 with pid 15688
I0123 18:50:11.982480 15706 slave.cpp:1368] Queuing task '1' for executor default of framework '20150123-185011-16777343-37526-15688-0000
I0123 18:50:11.982676 15706 slave.cpp:566] Successfully attached file '/tmp/FaultToleranceTest_SchedulerFailoverFrameworkMessage_TB8Rh3/slaves/20150123-185011-16777343-37526-15688-S0/frameworks/20150123-185011-16777343-37526-15688-0000/executors/default/runs/02536e4f-fb59-4b75-99aa-611fd7fffcb1'
I0123 18:50:11.982770 15706 slave.cpp:1912] Got registration for executor 'default' of framework 20150123-185011-16777343-37526-15688-0000 from executor(50)@127.0.0.1:37526
I0123 18:50:11.983203 15706 slave.cpp:2031] Flushing queued task 1 for executor 'default' of framework 20150123-185011-16777343-37526-15688-0000
I0123 18:50:11.983505 15706 slave.cpp:2890] Monitoring executor 'default' of framework '20150123-185011-16777343-37526-15688-0000' in container '02536e4f-fb59-4b75-99aa-611fd7fffcb1'
I0123 18:50:11.983749 15706 exec.cpp:221] Executor registered on slave 20150123-185011-16777343-37526-15688-S0
I0123 18:50:11.986131 15706 exec.cpp:233] Executor::registered took 30292ns
I0123 18:50:11.989857 15706 exec.cpp:308] Executor asked to run task '1'
I0123 18:50:11.990216 15706 exec.cpp:317] Executor::launchTask took 83992ns
I0123 18:50:11.992413 15706 exec.cpp:540] Executor sending status update TASK_RUNNING (UUID: 3887f5e3-3349-4d1d-8e18-299be6c3c294) for task 1 of framework 20150123-185011-16777343-37526-15688-0000
I0123 18:50:11.996598 15703 slave.cpp:2265] Handling status update TASK_RUNNING (UUID: 3887f5e3-3349-4d1d-8e18-299be6c3c294) for task 1 of framework 20150123-185011-16777343-37526-15688-0000 from executor(50)@127.0.0.1:37526
I0123 18:50:11.996922 15703 status_update_manager.cpp:317] Received status update TASK_RUNNING (UUID: 3887f5e3-3349-4d1d-8e18-299be6c3c294) for task 1 of framework 20150123-185011-16777343-37526-15688-0000
I0123 18:50:11.996960 15703 status_update_manager.cpp:494] Creating StatusUpdate stream for task 1 of framework 20150123-185011-16777343-37526-15688-0000
I0123 18:50:11.997187 15703 status_update_manager.cpp:371] Forwarding update TASK_RUNNING (UUID: 3887f5e3-3349-4d1d-8e18-299be6c3c294) for task 1 of framework 20150123-185011-16777343-37526-15688-0000 to the slave
I0123 18:50:11.997541 15703 slave.cpp:2508] Forwarding the update TASK_RUNNING (UUID: 3887f5e3-3349-4d1d-8e18-299be6c3c294) for task 1 of framework 20150123-185011-16777343-37526-15688-0000 to master@127.0.0.1:37526
I0123 18:50:11.997678 15703 slave.cpp:2435] Status update manager successfully handled status update TASK_RUNNING (UUID: 3887f5e3-3349-4d1d-8e18-299be6c3c294) for task 1 of framework 20150123-185011-16777343-37526-15688-0000
I0123 18:50:11.997707 15703 slave.cpp:2441] Sending acknowledgement for status update TASK_RUNNING (UUID: 3887f5e3-3349-4d1d-8e18-299be6c3c294) for task 1 of framework 20150123-185011-16777343-37526-15688-0000 to executor(50)@127.0.0.1:37526
I0123 18:50:11.997936 15703 master.cpp:3652] Forwarding status update TASK_RUNNING (UUID: 3887f5e3-3349-4d1d-8e18-299be6c3c294) for task 1 of framework 20150123-185011-16777343-37526-15688-0000
I0123 18:50:11.998054 15703 master.cpp:3624] Status update TASK_RUNNING (UUID: 3887f5e3-3349-4d1d-8e18-299be6c3c294) for task 1 of framework 20150123-185011-16777343-37526-15688-0000 from slave 20150123-185011-16777343-37526-15688-S0 at slave(94)@127.0.0.1:37526 (localhost.localdomain)
I0123 18:50:11.998106 15703 master.cpp:4934] Updating the latest state of task 1 of framework 20150123-185011-16777343-37526-15688-0000 to TASK_RUNNING
I0123 18:50:11.998301 15703 sched.cpp:696] Scheduler::statusUpdate took 54363ns
I0123 18:50:11.998615 15707 master.cpp:3125] Forwarding status update acknowledgement 3887f5e3-3349-4d1d-8e18-299be6c3c294 for task 1 of framework 20150123-185011-16777343-37526-15688-0000 (default) at scheduler-2cecb105-ca23-4048-9707-12b1e4422e11@127.0.0.1:37526 to slave 20150123-185011-16777343-37526-15688-S0 at slave(94)@127.0.0.1:37526 (localhost.localdomain)
I0123 18:50:11.998867 15707 status_update_manager.cpp:389] Received status update acknowledgement (UUID: 3887f5e3-3349-4d1d-8e18-299be6c3c294) for task 1 of framework 20150123-185011-16777343-37526-15688-0000
I0123 18:50:11.999047 15707 slave.cpp:1852] Status update manager successfully handled status update acknowledgement (UUID: 3887f5e3-3349-4d1d-8e18-299be6c3c294) for task 1 of framework 20150123-185011-16777343-37526-15688-0000
W0123 18:50:12.001930 15688 sched.cpp:1246] 
**************************************************
Scheduler driver bound to loopback interface! Cannot communicate with remote master(s). You might want to set 'LIBPROCESS_IP' environment variable to use a routable IP address.
**************************************************
I0123 18:50:12.006674 15706 exec.cpp:354] Executor received status update acknowledgement 3887f5e3-3349-4d1d-8e18-299be6c3c294 for task 1 of framework 20150123-185011-16777343-37526-15688-0000
I0123 18:50:12.015889 15688 sched.cpp:151] Version: 0.22.0
I0123 18:50:12.017143 15706 sched.cpp:248] New master detected at master@127.0.0.1:37526
I0123 18:50:12.017241 15706 sched.cpp:304] Authenticating with master master@127.0.0.1:37526
I0123 18:50:12.017264 15706 sched.cpp:311] Using default CRAM-MD5 authenticatee
I0123 18:50:12.017680 15710 authenticatee.hpp:138] Creating new client SASL connection
I0123 18:50:12.018093 15710 master.cpp:4129] Authenticating scheduler-9b22c538-3b80-4309-80bd-e4c06956dd3e@127.0.0.1:37526
I0123 18:50:12.018129 15710 master.cpp:4140] Using default CRAM-MD5 authenticator
I0123 18:50:12.018590 15710 authenticator.hpp:170] Creating new server SASL connection
I0123 18:50:12.018904 15710 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5
I0123 18:50:12.018934 15710 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5'
I0123 18:50:12.019039 15710 authenticator.hpp:276] Received SASL authentication start
I0123 18:50:12.019101 15710 authenticator.hpp:398] Authentication requires more steps
I0123 18:50:12.019172 15710 authenticatee.hpp:275] Received SASL authentication step
I0123 18:50:12.019273 15710 authenticator.hpp:304] Received SASL authentication step
I0123 18:50:12.019304 15710 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'localhost.localdomain' server FQDN: 'localhost.localdomain' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0123 18:50:12.019316 15710 auxprop.cpp:171] Looking up auxiliary property '*userPassword'
I0123 18:50:12.019364 15710 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0123 18:50:12.020604 15710 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'localhost.localdomain' server FQDN: 'localhost.localdomain' SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0123 18:50:12.020859 15710 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0123 18:50:12.021114 15710 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0123 18:50:12.021402 15710 authenticator.hpp:390] Authentication success
I0123 18:50:12.021790 15705 authenticatee.hpp:315] Authentication success
I0123 18:50:12.029628 15705 sched.cpp:392] Successfully authenticated with master master@127.0.0.1:37526
I0123 18:50:12.029682 15705 sched.cpp:515] Sending registration request to master@127.0.0.1:37526
I0123 18:50:12.029784 15705 sched.cpp:548] Will retry registration in 371.903559ms if necessary
I0123 18:50:12.030015 15705 master.cpp:1525] Queuing up re-registration request for framework 20150123-185011-16777343-37526-15688-0000 (default) at scheduler-9b22c538-3b80-4309-80bd-e4c06956dd3e@127.0.0.1:37526 because authentication is still in progress
I0123 18:50:12.030215 15710 master.cpp:4187] Successfully authenticated principal 'test-principal' at scheduler-9b22c538-3b80-4309-80bd-e4c06956dd3e@127.0.0.1:37526
I0123 18:50:12.030539 15710 master.cpp:1557] Received re-registration request from framework 20150123-185011-16777343-37526-15688-0000 (default) at scheduler-9b22c538-3b80-4309-80bd-e4c06956dd3e@127.0.0.1:37526
I0123 18:50:12.030618 15710 master.cpp:1298] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0123 18:50:12.031014 15710 master.cpp:1610] Re-registering framework 20150123-185011-16777343-37526-15688-0000 (default)  at scheduler-9b22c538-3b80-4309-80bd-e4c06956dd3e@127.0.0.1:37526
I0123 18:50:12.031060 15710 master.cpp:1639] Framework 20150123-185011-16777343-37526-15688-0000 (default) at scheduler-2cecb105-ca23-4048-9707-12b1e4422e11@127.0.0.1:37526 failed over
I0123 18:50:12.031723 15703 sched.cpp:442] Framework registered with 20150123-185011-16777343-37526-15688-0000
I0123 18:50:12.031841 15703 sched.cpp:456] Scheduler::registered took 54566ns
I0123 18:50:12.032662 15709 slave.cpp:1762] Updating framework 20150123-185011-16777343-37526-15688-0000 pid to scheduler-9b22c538-3b80-4309-80bd-e4c06956dd3e@127.0.0.1:37526
I0123 18:50:12.032924 15709 status_update_manager.cpp:178] Resuming sending status updates
I0123 18:50:12.034113 15703 slave.cpp:2571] Sending message for framework 20150123-185011-16777343-37526-15688-0000 to scheduler-9b22c538-3b80-4309-80bd-e4c06956dd3e@127.0.0.1:37526
I0123 18:50:12.034302 15703 sched.cpp:782] Scheduler::frameworkMessage took 53684ns
I0123 18:50:12.034771 15688 sched.cpp:1471] Asked to stop the driver
I0123 18:50:12.034864 15688 sched.cpp:1471] Asked to stop the driver
I0123 18:50:12.034942 15688 master.cpp:654] Master terminating
W0123 18:50:12.035094 15688 master.cpp:4979] Removing task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] of framework 20150123-185011-16777343-37526-15688-0000 on slave 20150123-185011-16777343-37526-15688-S0 at slave(94)@127.0.0.1:37526 (localhost.localdomain) in non-terminal state TASK_RUNNING
I0123 18:50:12.035724 15688 master.cpp:5022] Removing executor 'default' with resources  of framework 20150123-185011-16777343-37526-15688-0000 on slave 20150123-185011-16777343-37526-15688-S0 at slave(94)@127.0.0.1:37526 (localhost.localdomain)
I0123 18:50:12.036705 15709 sched.cpp:808] Stopping framework '20150123-185011-16777343-37526-15688-0000'
I0123 18:50:12.036960 15709 hierarchical_allocator_process.hpp:653] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total allocatable: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]) on slave 20150123-185011-16777343-37526-15688-S0 from framework 20150123-185011-16777343-37526-15688-0000
I0123 18:50:12.037048 15709 slave.cpp:2673] master@127.0.0.1:37526 exited
W0123 18:50:12.037071 15709 slave.cpp:2676] Master disconnected! Waiting for a new master to be elected
I0123 18:50:12.037359 15710 sched.cpp:788] Ignoring error message because the driver is not running!
I0123 18:50:12.037513 15710 sched.cpp:808] Stopping framework '20150123-185011-16777343-37526-15688-0000'
I0123 18:50:12.076481 15688 slave.cpp:495] Slave terminating
I0123 18:50:12.080759 15688 slave.cpp:1585] Asked to shut down framework 20150123-185011-16777343-37526-15688-0000 by @0.0.0.0:0
I0123 18:50:12.081023 15688 slave.cpp:1610] Shutting down framework 20150123-185011-16777343-37526-15688-0000
I0123 18:50:12.081351 15688 slave.cpp:3198] Shutting down executor 'default' of framework 201501",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.0
Epic,Add authentication support for HTTP API,"Since most of the communication between mesos components will happen through HTTP with the arrival of the [HTTP API|https://issues.apache.org/jira/browse/MESOS-2288], it makes sense to use HTTP standard mechanisms to authenticate this communication.",1.0,0,0.5,0.004524886877828055,0.0,0.0,0.0,0.0,0.2,0.12280701754385964,0.24285714285714285,0.8846153846153846,0.9878787878787878,0.9878787878787878,0.0
Task,Move all scheduler driver validations to master,"With HTTP API, the scheduler driver will no longer exist and hence all the validations should move to the master.",3.0,0,0.5,0.0784313725490196,0.3333333333333333,0.5,0.14285714285714285,0.0,0.0,0.05263157894736842,0.09285714285714286,0.8846153846153846,0.9878787878787878,0.9878787878787878,0.0
Bug,SlaveRecoveryTest.ReconcileKillTask is flaky.,"Saw this on an internal CI:

{noformat}
[ RUN      ] SlaveRecoveryTest/0.ReconcileKillTask
Using temporary directory '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_D5wSwg'
I0126 19:10:52.005317 13291 leveldb.cpp:176] Opened db in 978670ns
I0126 19:10:52.006155 13291 leveldb.cpp:183] Compacted db in 541346ns
I0126 19:10:52.006494 13291 leveldb.cpp:198] Created db iterator in 24562ns
I0126 19:10:52.006798 13291 leveldb.cpp:204] Seeked to beginning of db in 3254ns
I0126 19:10:52.007036 13291 leveldb.cpp:273] Iterated through 0 keys in the db in 949ns
I0126 19:10:52.007369 13291 replica.cpp:744] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0126 19:10:52.008362 13308 recover.cpp:449] Starting replica recovery
I0126 19:10:52.009141 13308 recover.cpp:475] Replica is in EMPTY status
I0126 19:10:52.016494 13308 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request
I0126 19:10:52.017333 13309 recover.cpp:195] Received a recover response from a replica in EMPTY status
I0126 19:10:52.018244 13309 recover.cpp:566] Updating replica status to STARTING
I0126 19:10:52.019064 13305 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 113577ns
I0126 19:10:52.019487 13305 replica.cpp:323] Persisted replica status to STARTING
I0126 19:10:52.019937 13309 recover.cpp:475] Replica is in STARTING status
I0126 19:10:52.021492 13307 replica.cpp:641] Replica in STARTING status received a broadcasted recover request
I0126 19:10:52.022665 13309 recover.cpp:195] Received a recover response from a replica in STARTING status
I0126 19:10:52.027971 13312 recover.cpp:566] Updating replica status to VOTING
I0126 19:10:52.028590 13312 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 78452ns
I0126 19:10:52.028869 13312 replica.cpp:323] Persisted replica status to VOTING
I0126 19:10:52.029252 13312 recover.cpp:580] Successfully joined the Paxos group
I0126 19:10:52.030828 13307 recover.cpp:464] Recover process terminated
I0126 19:10:52.049947 13306 master.cpp:262] Master 20150126-191052-2272962752-35545-13291 (fedora-19) started on 192.168.122.135:35545
I0126 19:10:52.050499 13306 master.cpp:308] Master only allowing authenticated frameworks to register
I0126 19:10:52.050765 13306 master.cpp:313] Master only allowing authenticated slaves to register
I0126 19:10:52.051048 13306 credentials.hpp:36] Loading credentials for authentication from '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_D5wSwg/credentials'
I0126 19:10:52.051589 13306 master.cpp:357] Authorization enabled
I0126 19:10:52.052531 13305 hierarchical_allocator_process.hpp:285] Initialized hierarchical allocator process
I0126 19:10:52.052881 13311 whitelist_watcher.cpp:65] No whitelist given
I0126 19:10:52.055524 13306 master.cpp:1219] The newly elected leader is master@192.168.122.135:35545 with id 20150126-191052-2272962752-35545-13291
I0126 19:10:52.056226 13306 master.cpp:1232] Elected as the leading master!
I0126 19:10:52.056639 13306 master.cpp:1050] Recovering from registrar
I0126 19:10:52.057045 13307 registrar.cpp:313] Recovering registrar
I0126 19:10:52.058554 13312 log.cpp:660] Attempting to start the writer
I0126 19:10:52.060868 13309 replica.cpp:477] Replica received implicit promise request with proposal 1
I0126 19:10:52.061691 13309 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 91680ns
I0126 19:10:52.062261 13309 replica.cpp:345] Persisted promised to 1
I0126 19:10:52.064559 13310 coordinator.cpp:230] Coordinator attemping to fill missing position
I0126 19:10:52.069105 13311 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2
I0126 19:10:52.069860 13311 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 94858ns
I0126 19:10:52.070350 13311 replica.cpp:679] Persisted action at 0
I0126 19:10:52.080348 13305 replica.cpp:511] Replica received write request for position 0
I0126 19:10:52.081153 13305 leveldb.cpp:438] Reading position from leveldb took 62247ns
I0126 19:10:52.081676 13305 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 81487ns
I0126 19:10:52.082053 13305 replica.cpp:679] Persisted action at 0
I0126 19:10:52.083566 13309 replica.cpp:658] Replica received learned notice for position 0
I0126 19:10:52.085734 13309 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 283144ns
I0126 19:10:52.086067 13309 replica.cpp:679] Persisted action at 0
I0126 19:10:52.086448 13309 replica.cpp:664] Replica learned NOP action at position 0
I0126 19:10:52.089784 13306 log.cpp:676] Writer started with ending position 0
I0126 19:10:52.093415 13309 leveldb.cpp:438] Reading position from leveldb took 66744ns
I0126 19:10:52.104814 13306 registrar.cpp:346] Successfully fetched the registry (0B) in 47.451136ms
I0126 19:10:52.105731 13306 registrar.cpp:445] Applied 1 operations in 42124ns; attempting to update the 'registry'
I0126 19:10:52.111935 13305 log.cpp:684] Attempting to append 131 bytes to the log
I0126 19:10:52.112754 13305 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I0126 19:10:52.114297 13308 replica.cpp:511] Replica received write request for position 1
I0126 19:10:52.114908 13308 leveldb.cpp:343] Persisting action (150 bytes) to leveldb took 98332ns
I0126 19:10:52.115387 13308 replica.cpp:679] Persisted action at 1
I0126 19:10:52.117277 13305 replica.cpp:658] Replica received learned notice for position 1
I0126 19:10:52.118142 13305 leveldb.cpp:343] Persisting action (152 bytes) to leveldb took 227799ns
I0126 19:10:52.118621 13305 replica.cpp:679] Persisted action at 1
I0126 19:10:52.118979 13305 replica.cpp:664] Replica learned APPEND action at position 1
I0126 19:10:52.121311 13305 registrar.cpp:490] Successfully updated the 'registry' in 15.161088ms
I0126 19:10:52.121548 13311 log.cpp:703] Attempting to truncate the log to 1
I0126 19:10:52.122697 13311 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2
I0126 19:10:52.124316 13307 replica.cpp:511] Replica received write request for position 2
I0126 19:10:52.124913 13307 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 87281ns
I0126 19:10:52.125334 13307 replica.cpp:679] Persisted action at 2
I0126 19:10:52.127018 13311 replica.cpp:658] Replica received learned notice for position 2
I0126 19:10:52.127835 13311 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 201050ns
I0126 19:10:52.128232 13311 leveldb.cpp:401] Deleting ~1 keys from leveldb took 78012ns
I0126 19:10:52.128835 13305 registrar.cpp:376] Successfully recovered registrar
I0126 19:10:52.128551 13311 replica.cpp:679] Persisted action at 2
I0126 19:10:52.130105 13311 replica.cpp:664] Replica learned TRUNCATE action at position 2
I0126 19:10:52.131479 13312 master.cpp:1077] Recovered 0 slaves from the Registry (95B) ; allowing 10mins for slaves to re-register
I0126 19:10:52.143465 13291 containerizer.cpp:103] Using isolation: posix/cpu,posix/mem
I0126 19:10:52.170471 13309 slave.cpp:173] Slave started on 101)@192.168.122.135:35545
I0126 19:10:52.171723 13309 credentials.hpp:84] Loading credential for authentication from '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/credential'
I0126 19:10:52.172286 13309 slave.cpp:282] Slave using credential for: test-principal
I0126 19:10:52.172821 13309 slave.cpp:300] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0126 19:10:52.173982 13309 slave.cpp:329] Slave hostname: fedora-19
I0126 19:10:52.174505 13309 slave.cpp:330] Slave checkpoint: true
I0126 19:10:52.179308 13309 state.cpp:33] Recovering state from '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/meta'
I0126 19:10:52.180075 13308 status_update_manager.cpp:197] Recovering status update manager
I0126 19:10:52.180611 13308 containerizer.cpp:300] Recovering containerizer
I0126 19:10:52.182473 13309 slave.cpp:3519] Finished recovery
I0126 19:10:52.184403 13312 slave.cpp:613] New master detected at master@192.168.122.135:35545
I0126 19:10:52.184916 13312 slave.cpp:676] Authenticating with master master@192.168.122.135:35545
I0126 19:10:52.185230 13312 slave.cpp:681] Using default CRAM-MD5 authenticatee
I0126 19:10:52.185715 13312 slave.cpp:649] Detecting new master
I0126 19:10:52.186420 13312 authenticatee.hpp:138] Creating new client SASL connection
I0126 19:10:52.186002 13311 status_update_manager.cpp:171] Pausing sending status updates
I0126 19:10:52.188293 13312 master.cpp:4129] Authenticating slave(101)@192.168.122.135:35545
I0126 19:10:52.188748 13312 master.cpp:4140] Using default CRAM-MD5 authenticator
I0126 19:10:52.189525 13312 authenticator.hpp:170] Creating new server SASL connection
I0126 19:10:52.191082 13305 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5
I0126 19:10:52.191550 13305 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5'
I0126 19:10:52.191990 13312 authenticator.hpp:276] Received SASL authentication start
I0126 19:10:52.192365 13312 authenticator.hpp:398] Authentication requires more steps
I0126 19:10:52.192800 13311 authenticatee.hpp:275] Received SASL authentication step
I0126 19:10:52.193244 13312 authenticator.hpp:304] Received SASL authentication step
I0126 19:10:52.193565 13312 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0126 19:10:52.193902 13312 auxprop.cpp:171] Looking up auxiliary property '*userPassword'
I0126 19:10:52.194301 13312 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0126 19:10:52.195669 13312 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0126 19:10:52.196048 13312 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0126 19:10:52.196395 13312 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0126 19:10:52.196723 13312 authenticator.hpp:390] Authentication success
I0126 19:10:52.197206 13305 authenticatee.hpp:315] Authentication success
I0126 19:10:52.204121 13305 slave.cpp:747] Successfully authenticated with master master@192.168.122.135:35545
I0126 19:10:52.204676 13310 master.cpp:4187] Successfully authenticated principal 'test-principal' at slave(101)@192.168.122.135:35545
I0126 19:10:52.205729 13305 slave.cpp:1075] Will retry registration in 5.608661ms if necessary
I0126 19:10:52.206451 13310 master.cpp:3275] Registering slave at slave(101)@192.168.122.135:35545 (fedora-19) with id 20150126-191052-2272962752-35545-13291-S0
I0126 19:10:52.210019 13310 registrar.cpp:445] Applied 1 operations in 235087ns; attempting to update the 'registry'
I0126 19:10:52.220736 13308 slave.cpp:1075] Will retry registration in 9.28397ms if necessary
I0126 19:10:52.221309 13311 master.cpp:3263] Ignoring register slave message from slave(101)@192.168.122.135:35545 (fedora-19) as admission is already in progress
I0126 19:10:52.224818 13307 log.cpp:684] Attempting to append 302 bytes to the log
I0126 19:10:52.225554 13307 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3
I0126 19:10:52.227422 13305 replica.cpp:511] Replica received write request for position 3
I0126 19:10:52.227969 13305 leveldb.cpp:343] Persisting action (321 bytes) to leveldb took 100350ns
I0126 19:10:52.228276 13305 replica.cpp:679] Persisted action at 3
I0126 19:10:52.232475 13312 replica.cpp:658] Replica received learned notice for position 3
I0126 19:10:52.233280 13312 leveldb.cpp:343] Persisting action (323 bytes) to leveldb took 546567ns
I0126 19:10:52.233726 13312 replica.cpp:679] Persisted action at 3
I0126 19:10:52.234035 13312 replica.cpp:664] Replica learned APPEND action at position 3
I0126 19:10:52.236556 13310 registrar.cpp:490] Successfully updated the 'registry' in 26.040064ms
I0126 19:10:52.237330 13305 log.cpp:703] Attempting to truncate the log to 3
I0126 19:10:52.238056 13311 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4
I0126 19:10:52.239594 13311 replica.cpp:511] Replica received write request for position 4
I0126 19:10:52.240129 13311 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 92868ns
I0126 19:10:52.240458 13311 replica.cpp:679] Persisted action at 4
I0126 19:10:52.241976 13308 replica.cpp:658] Replica received learned notice for position 4
I0126 19:10:52.242645 13308 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 95635ns
I0126 19:10:52.242990 13308 leveldb.cpp:401] Deleting ~2 keys from leveldb took 58066ns
I0126 19:10:52.243337 13308 replica.cpp:679] Persisted action at 4
I0126 19:10:52.243695 13308 replica.cpp:664] Replica learned TRUNCATE action at position 4
I0126 19:10:52.245657 13291 sched.cpp:151] Version: 0.22.0
I0126 19:10:52.247625 13305 master.cpp:3329] Registered slave 20150126-191052-2272962752-35545-13291-S0 at slave(101)@192.168.122.135:35545 (fedora-19) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0126 19:10:52.248942 13307 slave.cpp:781] Registered with master master@192.168.122.135:35545; given slave ID 20150126-191052-2272962752-35545-13291-S0
I0126 19:10:52.250396 13307 slave.cpp:797] Checkpointing SlaveInfo to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/meta/slaves/20150126-191052-2272962752-35545-13291-S0/slave.info'
I0126 19:10:52.250731 13309 status_update_manager.cpp:178] Resuming sending status updates
I0126 19:10:52.251765 13307 slave.cpp:2588] Received ping from slave-observer(99)@192.168.122.135:35545
I0126 19:10:52.247951 13310 hierarchical_allocator_process.hpp:453] Added slave 20150126-191052-2272962752-35545-13291-S0 (fedora-19) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] available)
I0126 19:10:52.252810 13310 hierarchical_allocator_process.hpp:831] No resources available to allocate!
I0126 19:10:52.254365 13310 hierarchical_allocator_process.hpp:756] Performed allocation for slave 20150126-191052-2272962752-35545-13291-S0 in 1.732701ms
I0126 19:10:52.254137 13307 sched.cpp:248] New master detected at master@192.168.122.135:35545
I0126 19:10:52.257863 13307 sched.cpp:304] Authenticating with master master@192.168.122.135:35545
I0126 19:10:52.258249 13307 sched.cpp:311] Using default CRAM-MD5 authenticatee
I0126 19:10:52.258908 13306 authenticatee.hpp:138] Creating new client SASL connection
I0126 19:10:52.261397 13309 master.cpp:4129] Authenticating scheduler-6da85b48-b57f-4202-b630-c45f8f652321@192.168.122.135:35545
I0126 19:10:52.261776 13309 master.cpp:4140] Using default CRAM-MD5 authenticator
I0126 19:10:52.264528 13309 authenticator.hpp:170] Creating new server SASL connection
I0126 19:10:52.266248 13312 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5
I0126 19:10:52.266749 13312 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5'
I0126 19:10:52.267143 13312 authenticator.hpp:276] Received SASL authentication start
I0126 19:10:52.267525 13312 authenticator.hpp:398] Authentication requires more steps
I0126 19:10:52.267917 13312 authenticatee.hpp:275] Received SASL authentication step
I0126 19:10:52.268404 13312 authenticator.hpp:304] Received SASL authentication step
I0126 19:10:52.268725 13312 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0126 19:10:52.269078 13312 auxprop.cpp:171] Looking up auxiliary property '*userPassword'
I0126 19:10:52.269498 13312 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0126 19:10:52.269881 13312 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0126 19:10:52.270385 13312 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0126 19:10:52.271015 13312 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0126 19:10:52.271599 13312 authenticator.hpp:390] Authentication success
I0126 19:10:52.272126 13312 authenticatee.hpp:315] Authentication success
I0126 19:10:52.272415 13305 master.cpp:4187] Successfully authenticated principal 'test-principal' at scheduler-6da85b48-b57f-4202-b630-c45f8f652321@192.168.122.135:35545
I0126 19:10:52.273998 13307 sched.cpp:392] Successfully authenticated with master master@192.168.122.135:35545
I0126 19:10:52.274415 13307 sched.cpp:515] Sending registration request to master@192.168.122.135:35545
I0126 19:10:52.274842 13307 sched.cpp:548] Will retry registration in 674.656506ms if necessary
I0126 19:10:52.275235 13305 master.cpp:1420] Received registration request for framework 'default' at scheduler-6da85b48-b57f-4202-b630-c45f8f652321@192.168.122.135:35545
I0126 19:10:52.276017 13305 master.cpp:1298] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0126 19:10:52.277027 13305 master.cpp:1484] Registering framework 20150126-191052-2272962752-35545-13291-0000 (default) at scheduler-6da85b48-b57f-4202-b630-c45f8f652321@192.168.122.135:35545
I0126 19:10:52.278285 13308 hierarchical_allocator_process.hpp:319] Added framework 20150126-191052-2272962752-35545-13291-0000
I0126 19:10:52.279575 13308 hierarchical_allocator_process.hpp:738] Performed allocation for 1 slaves in 697902ns
I0126 19:10:52.287966 13305 master.cpp:4071] Sending 1 offers to framework 20150126-191052-2272962752-35545-13291-0000 (default) at scheduler-6da85b48-b57f-4202-b630-c45f8f652321@192.168.122.135:35545
I0126 19:10:52.288776 13307 sched.cpp:442] Framework registered with 20150126-191052-2272962752-35545-13291-0000
I0126 19:10:52.289373 13307 sched.cpp:456] Scheduler::registered took 21674ns
I0126 19:10:52.289932 13307 sched.cpp:605] Scheduler::resourceOffers took 76147ns
I0126 19:10:52.293220 13311 master.cpp:2677] Processing ACCEPT call for offers: [ 20150126-191052-2272962752-35545-13291-O0 ] on slave 20150126-191052-2272962752-35545-13291-S0 at slave(101)@192.168.122.135:35545 (fedora-19) for framework 20150126-191052-2272962752-35545-13291-0000 (default) at scheduler-6da85b48-b57f-4202-b630-c45f8f652321@192.168.122.135:35545
I0126 19:10:52.293586 13311 master.cpp:2513] Authorizing framework principal 'test-principal' to launch task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e as user 'jenkins'
I0126 19:10:52.295825 13311 master.hpp:782] Adding task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20150126-191052-2272962752-35545-13291-S0 (fedora-19)
I0126 19:10:52.296272 13311 master.cpp:2885] Launching task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 (default) at scheduler-6da85b48-b57f-4202-b630-c45f8f652321@192.168.122.135:35545 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20150126-191052-2272962752-35545-13291-S0 at slave(101)@192.168.122.135:35545 (fedora-19)
I0126 19:10:52.296886 13309 slave.cpp:1130] Got assigned task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e for framework 20150126-191052-2272962752-35545-13291-0000
I0126 19:10:52.297324 13309 slave.cpp:3846] Checkpointing FrameworkInfo to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/meta/slaves/20150126-191052-2272962752-35545-13291-S0/frameworks/20150126-191052-2272962752-35545-13291-0000/framework.info'
I0126 19:10:52.297919 13309 slave.cpp:3853] Checkpointing framework pid 'scheduler-6da85b48-b57f-4202-b630-c45f8f652321@192.168.122.135:35545' to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/meta/slaves/20150126-191052-2272962752-35545-13291-S0/frameworks/20150126-191052-2272962752-35545-13291-0000/framework.pid'
I0126 19:10:52.299072 13309 slave.cpp:1245] Launching task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e for framework 20150126-191052-2272962752-35545-13291-0000
I0126 19:10:52.308050 13309 slave.cpp:4289] Checkpointing ExecutorInfo to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/meta/slaves/20150126-191052-2272962752-35545-13291-S0/frameworks/20150126-191052-2272962752-35545-13291-0000/executors/61eaeec3-e8ca-4e15-82d6-284c05c3bb6e/executor.info'
I0126 19:10:52.310894 13309 slave.cpp:3921] Launching executor 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 in work directory '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/slaves/20150126-191052-2272962752-35545-13291-S0/frameworks/20150126-191052-2272962752-35545-13291-0000/executors/61eaeec3-e8ca-4e15-82d6-284c05c3bb6e/runs/960eca2c-9e2c-415a-b6a5-159efca1f1b0'
I0126 19:10:52.311957 13308 containerizer.cpp:445] Starting container '960eca2c-9e2c-415a-b6a5-159efca1f1b0' for executor '61eaeec3-e8ca-4e15-82d6-284c05c3bb6e' of framework '20150126-191052-2272962752-35545-13291-0000'
W0126 19:10:52.313951 13307 containerizer.cpp:296] CommandInfo.grace_period flag is not set, using default value: 3secs
I0126 19:10:52.330166 13309 slave.cpp:4312] Checkpointing TaskInfo to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/meta/slaves/20150126-191052-2272962752-35545-13291-S0/frameworks/20150126-191052-2272962752-35545-13291-0000/executors/61eaeec3-e8ca-4e15-82d6-284c05c3bb6e/runs/960eca2c-9e2c-415a-b6a5-159efca1f1b0/tasks/61eaeec3-e8ca-4e15-82d6-284c05c3bb6e/task.info'
I0126 19:10:52.333307 13309 slave.cpp:1368] Queuing task '61eaeec3-e8ca-4e15-82d6-284c05c3bb6e' for executor 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework '20150126-191052-2272962752-35545-13291-0000
I0126 19:10:52.332506 13307 launcher.cpp:137] Forked child with pid '15795' for container '960eca2c-9e2c-415a-b6a5-159efca1f1b0'
I0126 19:10:52.334852 13307 containerizer.cpp:655] Checkpointing executor's forked pid 15795 to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/meta/slaves/20150126-191052-2272962752-35545-13291-S0/frameworks/20150126-191052-2272962752-35545-13291-0000/executors/61eaeec3-e8ca-4e15-82d6-284c05c3bb6e/runs/960eca2c-9e2c-415a-b6a5-159efca1f1b0/pids/forked.pid'
I0126 19:10:52.339607 13309 slave.cpp:566] Successfully attached file '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/slaves/20150126-191052-2272962752-35545-13291-S0/frameworks/20150126-191052-2272962752-35545-13291-0000/executors/61eaeec3-e8ca-4e15-82d6-284c05c3bb6e/runs/960eca2c-9e2c-415a-b6a5-159efca1f1b0'
I0126 19:10:52.341423 13309 slave.cpp:2890] Monitoring executor '61eaeec3-e8ca-4e15-82d6-284c05c3bb6e' of framework '20150126-191052-2272962752-35545-13291-0000' in container '960eca2c-9e2c-415a-b6a5-159efca1f1b0'
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0126 19:10:52.584766 15795 process.cpp:958] libprocess is initialized on 192.168.122.135:41245 for 8 cpus
I0126 19:10:52.597306 15795 logging.cpp:177] Logging to STDERR
I0126 19:10:52.606741 15795 exec.cpp:147] Version: 0.22.0
I0126 19:10:52.617653 15825 exec.cpp:197] Executor started at: executor(1)@192.168.122.135:41245 with pid 15795
I0126 19:10:52.643771 13309 slave.cpp:1912] Got registration for executor '61eaeec3-e8ca-4e15-82d6-284c05c3bb6e' of framework 20150126-191052-2272962752-35545-13291-0000 from executor(1)@192.168.122.135:41245
I0126 19:10:52.644484 13309 slave.cpp:1998] Checkpointing executor pid 'executor(1)@192.168.122.135:41245' to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/meta/slaves/20150126-191052-2272962752-35545-13291-S0/frameworks/20150126-191052-2272962752-35545-13291-0000/executors/61eaeec3-e8ca-4e15-82d6-284c05c3bb6e/runs/960eca2c-9e2c-415a-b6a5-159efca1f1b0/pids/libprocess.pid'
I0126 19:10:52.648509 13309 slave.cpp:2031] Flushing queued task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e for executor '61eaeec3-e8ca-4e15-82d6-284c05c3bb6e' of framework 20150126-191052-2272962752-35545-13291-0000
I0126 19:10:52.701879 15830 exec.cpp:221] Executor registered on slave 20150126-191052-2272962752-35545-13291-S0
Shutdown timeout is set to 3secsRegistered executor on fedora-19
I0126 19:10:52.706497 15830 exec.cpp:233] Executor::registered took 2.369798ms
I0126 19:10:52.710708 15830 exec.cpp:308] Executor asked to run task '61eaeec3-e8ca-4e15-82d6-284c05c3bb6e'
Starting task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e
I0126 19:10:52.713075 15830 exec.cpp:317] Executor::launchTask took 1.248631ms
sh -c 'sleep 1000'
Forked command at 15832
I0126 19:10:52.720675 15824 exec.cpp:540] Executor sending status update TASK_RUNNING (UUID: 9577ef79-7e59-4be6-a892-5a20baa13647) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000
I0126 19:10:52.722925 13308 slave.cpp:2265] Handling status update TASK_RUNNING (UUID: 9577ef79-7e59-4be6-a892-5a20baa13647) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 from executor(1)@192.168.122.135:41245
I0126 19:10:52.723328 13308 status_update_manager.cpp:317] Received status update TASK_RUNNING (UUID: 9577ef79-7e59-4be6-a892-5a20baa13647) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000
I0126 19:10:52.723371 13308 status_update_manager.cpp:494] Creating StatusUpdate stream for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000
I0126 19:10:52.723803 13308 status_update_manager.hpp:346] Checkpointing UPDATE for status update TASK_RUNNING (UUID: 9577ef79-7e59-4be6-a892-5a20baa13647) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000
I0126 19:10:52.723963 13308 status_update_manager.cpp:371] Forwarding update TASK_RUNNING (UUID: 9577ef79-7e59-4be6-a892-5a20baa13647) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 to the slave
I0126 19:10:52.724717 13312 slave.cpp:2508] Forwarding the update TASK_RUNNING (UUID: 9577ef79-7e59-4be6-a892-5a20baa13647) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 to master@192.168.122.135:35545
I0126 19:10:52.725385 13305 master.cpp:3652] Forwarding status update TASK_RUNNING (UUID: 9577ef79-7e59-4be6-a892-5a20baa13647) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000
I0126 19:10:52.725857 13305 master.cpp:3624] Status update TASK_RUNNING (UUID: 9577ef79-7e59-4be6-a892-5a20baa13647) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 from slave 20150126-191052-2272962752-35545-13291-S0 at slave(101)@192.168.122.135:35545 (fedora-19)
I0126 19:10:52.726471 13305 master.cpp:4934] Updating the latest state of task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 to TASK_RUNNING
I0126 19:10:52.726269 13311 sched.cpp:696] Scheduler::statusUpdate took 22534ns
I0126 19:10:52.727679 13311 master.cpp:3125] Forwarding status update acknowledgement 9577ef79-7e59-4be6-a892-5a20baa13647 for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 (default) at scheduler-6da85b48-b57f-4202-b630-c45f8f652321@192.168.122.135:35545 to slave 20150126-191052-2272962752-35545-13291-S0 at slave(101)@192.168.122.135:35545 (fedora-19)
I0126 19:10:52.728380 13308 status_update_manager.cpp:389] Received status update acknowledgement (UUID: 9577ef79-7e59-4be6-a892-5a20baa13647) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000
I0126 19:10:52.728579 13311 slave.cpp:2435] Status update manager successfully handled status update TASK_RUNNING (UUID: 9577ef79-7e59-4be6-a892-5a20baa13647) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000
I0126 19:10:52.729403 13311 slave.cpp:2441] Sending acknowledgement for status update TASK_RUNNING (UUID: 9577ef79-7e59-4be6-a892-5a20baa13647) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 to executor(1)@192.168.122.135:41245
I0126 19:10:52.728869 13308 status_update_manager.hpp:346] Checkpointing ACK for status update TASK_RUNNING (UUID: 9577ef79-7e59-4be6-a892-5a20baa13647) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000
I0126 19:10:52.731828 13307 slave.cpp:1852] Status update manager successfully handled status update acknowledgement (UUID: 9577ef79-7e59-4be6-a892-5a20baa13647) for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000
I0126 19:10:52.732923 13307 slave.cpp:495] Slave terminating
I0126 19:10:52.739572 15827 exec.cpp:354] Executor received status update acknowledgement 9577ef79-7e59-4be6-a892-5a20baa13647 for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000
I0126 19:10:52.743466 13306 master.cpp:795] Slave 20150126-191052-2272962752-35545-13291-S0 at slave(101)@192.168.122.135:35545 (fedora-19) disconnected
I0126 19:10:52.743948 13306 master.cpp:1826] Disconnecting slave 20150126-191052-2272962752-35545-13291-S0 at slave(101)@192.168.122.135:35545 (fedora-19)
I0126 19:10:52.744940 13306 master.cpp:1845] Deactivating slave 20150126-191052-2272962752-35545-13291-S0 at slave(101)@192.168.122.135:35545 (fedora-19)
I0126 19:10:52.752821 13306 hierarchical_allocator_process.hpp:512] Slave 20150126-191052-2272962752-35545-13291-S0 deactivated
I0126 19:10:52.765900 13291 containerizer.cpp:103] Using isolation: posix/cpu,posix/mem
I0126 19:10:52.766723 13309 master.cpp:2961] Asked to kill task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000
W0126 19:10:52.767549 13309 master.cpp:3030] Cannot kill task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000 (default) at scheduler-6da85b48-b57f-4202-b630-c45f8f652321@192.168.122.135:35545 because the slave 20150126-191052-2272962752-35545-13291-S0 at slave(101)@192.168.122.135:35545 (fedora-19) is disconnected. Kill will be retried if the slave re-registers
I0126 19:10:52.789048 13307 slave.cpp:173] Slave started on 102)@192.168.122.135:35545
I0126 19:10:52.790671 13307 credentials.hpp:84] Loading credential for authentication from '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/credential'
I0126 19:10:52.791497 13307 slave.cpp:282] Slave using credential for: test-principal
I0126 19:10:52.792064 13307 slave.cpp:300] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0126 19:10:52.793090 13307 slave.cpp:329] Slave hostname: fedora-19
I0126 19:10:52.793556 13307 slave.cpp:330] Slave checkpoint: true
I0126 19:10:52.795727 13311 state.cpp:33] Recovering state from '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/meta'
I0126 19:10:52.796282 13311 state.cpp:668] Failed to find resources file '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/meta/resources/resources.info'
I0126 19:10:52.804524 13309 slave.cpp:3601] Recovering framework 20150126-191052-2272962752-35545-13291-0000
I0126 19:10:52.805106 13309 slave.cpp:4040] Recovering executor '61eaeec3-e8ca-4e15-82d6-284c05c3bb6e' of framework 20150126-191052-2272962752-35545-13291-0000
I0126 19:10:52.807494 13309 slave.cpp:566] Successfully attached file '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_qbguuM/slaves/20150126-191052-2272962752-35545-13291-S0/frameworks/20150126-191052-2272962752-35545-13291-0000/executors/61eaeec3-e8ca-4e15-82d6-284c05c3bb6e/runs/960eca2c-9e2c-415a-b6a5-159efca1f1b0'
I0126 19:10:52.807888 13310 status_update_manager.cpp:197] Recovering status update manager
I0126 19:10:52.808390 13310 status_update_manager.cpp:205] Recovering executor '61eaeec3-e8ca-4e15-82d6-284c05c3bb6e' of framework 20150126-191052-2272962752-35545-13291-0000
I0126 19:10:52.808830 13310 status_update_manager.cpp:494] Creating StatusUpdate stream for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e of framework 20150126-191052-2272962752-35545-13291-0000
I0126 19:10:52.809484 13310 status_update_manager.hpp:310] Replaying status update stream for task 61eaeec3-e8ca-4e15-82d6-284c05c3bb6e
I0126 19:10:52.810966 13308 containerizer.cpp:300] Recovering containerizer
I0126 19:10:52.811550 13308 containerizer.cpp:342] Recovering container '960eca2c-9e2c-415a-b6a5-159efca1f1b0' for executor '61eaeec3-e8ca-4e15-82d",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.0
Improvement,Deprecate plain text Credential format.,"Currently two formats of credentials are supported: JSON

{code}
  ""credentials"": [
    {
      ""principal"": ""sherman"",
      ""secret"": ""kitesurf""
    }
{code}

And a new line file:
{code}
principal1 secret1
pricipal2 secret2
{code}

We should deprecate the new line format and remove support for the old format.",3.0,0.21.1,0.5,0.0,0.0,0.0,0.0,0.0,0.2,0.2631578947368421,0.36428571428571427,0.08974358974358974,0.0787878787878788,0.07272727272727272,0.19275229357798163
Improvement,Document header include rules in style guide,"We have several ways of sorting, grouping and ordering headers includes in Mesos. We should agree on a rule set and do a style scan.",3.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.2,0.2631578947368421,0.36428571428571427,0.01282051282051282,0.06666666666666667,0.06666666666666667,0.0
Improvement,"Add ""tests"" target to Makefile for building-but-not-running tests.","'make check' allows one to build and run the test suite. However, often we just want to build the tests.  Currently, this is done by setting GTEST_FILTER to an empty string.

It will be nice to have a dedicated target such as 'make tests' that allows one to build the test suite without running it.",1.0,0,0.0,0.3453996983408748,0.0,0.0,0.0,0.0,0.2,0.38596491228070173,0.35714285714285715,0.28205128205128205,0.29090909090909095,0.28484848484848485,0.0
Bug,Suppress MockAllocator::transformAllocation() warnings.,"After transforming allocated resources feature was added to allocator, a number of warnings are popping out for allocator tests. Commits leading to this behaviour:
{{dacc88292cc13d4b08fe8cda4df71110a96cb12a}}
{{5a02d5bdc75d3b1149dcda519016374be06ec6bd}}
corresponding reviews:
https://reviews.apache.org/r/29083
https://reviews.apache.org/r/29084

Here is an example:
{code}
[ RUN ] MasterAllocatorTest/0.FrameworkReregistersFirst GMOCK WARNING: Uninteresting mock function call - taking default action specified at: ../../../src/tests/mesos.hpp:719: Function call: transformAllocation(@0x7fd3bb5274d8 20150115-185632-1677764800-59671-44186-0000, @0x7fd3bb5274f8 20150115-185632-1677764800-59671-44186-S0, @0x1119140e0 16-byte object <F0-5E 52-BB D3-7F 00-00 C0-5F 52-BB D3-7F 00-00>) Stack trace: [ OK ] MasterAllocatorTest/0.FrameworkReregistersFirst (204 ms)
{code}",3.0,0,1.0,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.717948717948718,0.8242424242424242,0.09696969696969697,0.0
Bug,SlaveTest.MesosExecutorGracefulShutdown is flaky,"Observed this on internal CI

{noformat}
[ RUN      ] SlaveTest.MesosExecutorGracefulShutdown
Using temporary directory '/tmp/SlaveTest_MesosExecutorGracefulShutdown_AWdtVJ'
I0124 08:14:04.399211  7926 leveldb.cpp:176] Opened db in 27.364056ms
I0124 08:14:04.402632  7926 leveldb.cpp:183] Compacted db in 3.357646ms
I0124 08:14:04.402691  7926 leveldb.cpp:198] Created db iterator in 23822ns
I0124 08:14:04.402708  7926 leveldb.cpp:204] Seeked to beginning of db in 1913ns
I0124 08:14:04.402716  7926 leveldb.cpp:273] Iterated through 0 keys in the db in 458ns
I0124 08:14:04.402767  7926 replica.cpp:744] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0124 08:14:04.403728  7951 recover.cpp:449] Starting replica recovery
I0124 08:14:04.404011  7951 recover.cpp:475] Replica is in EMPTY status
I0124 08:14:04.407765  7950 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request
I0124 08:14:04.408710  7951 recover.cpp:195] Received a recover response from a replica in EMPTY status
I0124 08:14:04.419666  7951 recover.cpp:566] Updating replica status to STARTING
I0124 08:14:04.429719  7953 master.cpp:262] Master 20150124-081404-16842879-47787-7926 (utopic) started on 127.0.1.1:47787
I0124 08:14:04.429790  7953 master.cpp:308] Master only allowing authenticated frameworks to register
I0124 08:14:04.429802  7953 master.cpp:313] Master only allowing authenticated slaves to register
I0124 08:14:04.429826  7953 credentials.hpp:36] Loading credentials for authentication from '/tmp/SlaveTest_MesosExecutorGracefulShutdown_AWdtVJ/credentials'
I0124 08:14:04.430277  7953 master.cpp:357] Authorization enabled
I0124 08:14:04.432682  7953 master.cpp:1219] The newly elected leader is master@127.0.1.1:47787 with id 20150124-081404-16842879-47787-7926
I0124 08:14:04.432816  7953 master.cpp:1232] Elected as the leading master!
I0124 08:14:04.432894  7953 master.cpp:1050] Recovering from registrar
I0124 08:14:04.433212  7950 registrar.cpp:313] Recovering registrar
I0124 08:14:04.434226  7951 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 14.323302ms
I0124 08:14:04.434270  7951 replica.cpp:323] Persisted replica status to STARTING
I0124 08:14:04.434489  7951 recover.cpp:475] Replica is in STARTING status
I0124 08:14:04.436164  7951 replica.cpp:641] Replica in STARTING status received a broadcasted recover request
I0124 08:14:04.439368  7947 recover.cpp:195] Received a recover response from a replica in STARTING status
I0124 08:14:04.440626  7947 recover.cpp:566] Updating replica status to VOTING
I0124 08:14:04.443667  7947 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 2.698664ms
I0124 08:14:04.443759  7947 replica.cpp:323] Persisted replica status to VOTING
I0124 08:14:04.443925  7947 recover.cpp:580] Successfully joined the Paxos group
I0124 08:14:04.444160  7947 recover.cpp:464] Recover process terminated
I0124 08:14:04.444543  7949 log.cpp:660] Attempting to start the writer
I0124 08:14:04.446331  7949 replica.cpp:477] Replica received implicit promise request with proposal 1
I0124 08:14:04.449329  7949 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 2.690453ms
I0124 08:14:04.449388  7949 replica.cpp:345] Persisted promised to 1
I0124 08:14:04.450637  7947 coordinator.cpp:230] Coordinator attemping to fill missing position
I0124 08:14:04.452271  7949 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2
I0124 08:14:04.455124  7949 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 2.593522ms
I0124 08:14:04.455157  7949 replica.cpp:679] Persisted action at 0
I0124 08:14:04.456594  7951 replica.cpp:511] Replica received write request for position 0
I0124 08:14:04.456657  7951 leveldb.cpp:438] Reading position from leveldb took 30358ns
I0124 08:14:04.464860  7951 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 8.164646ms
I0124 08:14:04.464903  7951 replica.cpp:679] Persisted action at 0
I0124 08:14:04.465947  7949 replica.cpp:658] Replica received learned notice for position 0
I0124 08:14:04.471567  7949 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 5.587838ms
I0124 08:14:04.471601  7949 replica.cpp:679] Persisted action at 0
I0124 08:14:04.471622  7949 replica.cpp:664] Replica learned NOP action at position 0
I0124 08:14:04.472682  7951 log.cpp:676] Writer started with ending position 0
I0124 08:14:04.473919  7951 leveldb.cpp:438] Reading position from leveldb took 28676ns
I0124 08:14:04.491591  7951 registrar.cpp:346] Successfully fetched the registry (0B) in 58.337024ms
I0124 08:14:04.491704  7951 registrar.cpp:445] Applied 1 operations in 28163ns; attempting to update the 'registry'
I0124 08:14:04.493938  7953 log.cpp:684] Attempting to append 118 bytes to the log
I0124 08:14:04.494122  7953 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I0124 08:14:04.495069  7953 replica.cpp:511] Replica received write request for position 1
I0124 08:14:04.500089  7953 leveldb.cpp:343] Persisting action (135 bytes) to leveldb took 4.989356ms
I0124 08:14:04.500123  7953 replica.cpp:679] Persisted action at 1
I0124 08:14:04.501271  7950 replica.cpp:658] Replica received learned notice for position 1
I0124 08:14:04.505698  7950 leveldb.cpp:343] Persisting action (137 bytes) to leveldb took 4.396221ms
I0124 08:14:04.505734  7950 replica.cpp:679] Persisted action at 1
I0124 08:14:04.505755  7950 replica.cpp:664] Replica learned APPEND action at position 1
I0124 08:14:04.507313  7950 registrar.cpp:490] Successfully updated the 'registry' in 15.52896ms
I0124 08:14:04.507478  7953 log.cpp:703] Attempting to truncate the log to 1
I0124 08:14:04.507848  7953 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2
I0124 08:14:04.508743  7953 replica.cpp:511] Replica received write request for position 2
I0124 08:14:04.509214  7950 registrar.cpp:376] Successfully recovered registrar
I0124 08:14:04.509682  7946 master.cpp:1077] Recovered 0 slaves from the Registry (82B) ; allowing 10mins for slaves to re-register
I0124 08:14:04.514654  7953 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 5.880031ms
I0124 08:14:04.514689  7953 replica.cpp:679] Persisted action at 2
I0124 08:14:04.515736  7953 replica.cpp:658] Replica received learned notice for position 2
I0124 08:14:04.522014  7953 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 6.245138ms
I0124 08:14:04.522086  7953 leveldb.cpp:401] Deleting ~1 keys from leveldb took 37803ns
I0124 08:14:04.522107  7953 replica.cpp:679] Persisted action at 2
I0124 08:14:04.522128  7953 replica.cpp:664] Replica learned TRUNCATE action at position 2
I0124 08:14:04.531460  7926 containerizer.cpp:103] Using isolation: posix/cpu,posix/mem
I0124 08:14:04.547194  7951 slave.cpp:173] Slave started on 208)@127.0.1.1:47787
I0124 08:14:04.555682  7951 credentials.hpp:84] Loading credential for authentication from '/tmp/SlaveTest_MesosExecutorGracefulShutdown_kB74xo/credential'
I0124 08:14:04.556622  7951 slave.cpp:282] Slave using credential for: test-principal
I0124 08:14:04.557052  7951 slave.cpp:300] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0124 08:14:04.557842  7951 slave.cpp:329] Slave hostname: utopic
I0124 08:14:04.558091  7951 slave.cpp:330] Slave checkpoint: false
W0124 08:14:04.558352  7951 slave.cpp:332] Disabling checkpointing is deprecated and the --checkpoint flag will be removed in a future release. Please avoid using this flag
I0124 08:14:04.566864  7948 state.cpp:33] Recovering state from '/tmp/SlaveTest_MesosExecutorGracefulShutdown_kB74xo/meta'
I0124 08:14:04.575711  7951 status_update_manager.cpp:197] Recovering status update manager
I0124 08:14:04.575904  7951 containerizer.cpp:300] Recovering containerizer
I0124 08:14:04.577112  7951 slave.cpp:3519] Finished recovery
I0124 08:14:04.577374  7926 sched.cpp:151] Version: 0.22.0
I0124 08:14:04.578663  7950 sched.cpp:248] New master detected at master@127.0.1.1:47787
I0124 08:14:04.578759  7950 sched.cpp:304] Authenticating with master master@127.0.1.1:47787
I0124 08:14:04.578781  7950 sched.cpp:311] Using default CRAM-MD5 authenticatee
I0124 08:14:04.579071  7950 authenticatee.hpp:138] Creating new client SASL connection
I0124 08:14:04.579550  7947 master.cpp:4129] Authenticating scheduler-4a6c5cde-c54a-455a-aaad-6fc4e8ee99ef@127.0.1.1:47787
I0124 08:14:04.579582  7947 master.cpp:4140] Using default CRAM-MD5 authenticator
I0124 08:14:04.580031  7947 authenticator.hpp:170] Creating new server SASL connection
I0124 08:14:04.580402  7947 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5
I0124 08:14:04.580430  7947 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5'
I0124 08:14:04.580538  7947 authenticator.hpp:276] Received SASL authentication start
I0124 08:14:04.580581  7947 authenticator.hpp:398] Authentication requires more steps
I0124 08:14:04.580651  7947 authenticatee.hpp:275] Received SASL authentication step
I0124 08:14:04.580746  7947 authenticator.hpp:304] Received SASL authentication step
I0124 08:14:04.580837  7947 authenticator.hpp:390] Authentication success
I0124 08:14:04.580940  7947 authenticatee.hpp:315] Authentication success
I0124 08:14:04.581009  7947 master.cpp:4187] Successfully authenticated principal 'test-principal' at scheduler-4a6c5cde-c54a-455a-aaad-6fc4e8ee99ef@127.0.1.1:47787
I0124 08:14:04.581328  7947 sched.cpp:392] Successfully authenticated with master master@127.0.1.1:47787
I0124 08:14:04.581509  7947 master.cpp:1420] Received registration request for framework 'default' at scheduler-4a6c5cde-c54a-455a-aaad-6fc4e8ee99ef@127.0.1.1:47787
I0124 08:14:04.581585  7947 master.cpp:1298] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0124 08:14:04.582033  7947 master.cpp:1484] Registering framework 20150124-081404-16842879-47787-7926-0000 (default) at scheduler-4a6c5cde-c54a-455a-aaad-6fc4e8ee99ef@127.0.1.1:47787
I0124 08:14:04.582595  7947 hierarchical_allocator_process.hpp:319] Added framework 20150124-081404-16842879-47787-7926-0000
I0124 08:14:04.583051  7947 sched.cpp:442] Framework registered with 20150124-081404-16842879-47787-7926-0000
I0124 08:14:04.584087  7951 slave.cpp:613] New master detected at master@127.0.1.1:47787
I0124 08:14:04.584388  7951 slave.cpp:676] Authenticating with master master@127.0.1.1:47787
I0124 08:14:04.584564  7951 slave.cpp:681] Using default CRAM-MD5 authenticatee
I0124 08:14:04.584951  7951 slave.cpp:649] Detecting new master
I0124 08:14:04.585219  7951 status_update_manager.cpp:171] Pausing sending status updates
I0124 08:14:04.585604  7951 authenticatee.hpp:138] Creating new client SASL connection
I0124 08:14:04.587666  7953 master.cpp:4129] Authenticating slave(208)@127.0.1.1:47787
I0124 08:14:04.587702  7953 master.cpp:4140] Using default CRAM-MD5 authenticator
I0124 08:14:04.588434  7953 authenticator.hpp:170] Creating new server SASL connection
I0124 08:14:04.588764  7953 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5
I0124 08:14:04.588790  7953 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5'
I0124 08:14:04.588896  7953 authenticator.hpp:276] Received SASL authentication start
I0124 08:14:04.588935  7953 authenticator.hpp:398] Authentication requires more steps
I0124 08:14:04.589005  7953 authenticatee.hpp:275] Received SASL authentication step
I0124 08:14:04.589082  7953 authenticator.hpp:304] Received SASL authentication step
I0124 08:14:04.589140  7953 authenticator.hpp:390] Authentication success
I0124 08:14:04.589232  7953 authenticatee.hpp:315] Authentication success
I0124 08:14:04.589300  7953 master.cpp:4187] Successfully authenticated principal 'test-principal' at slave(208)@127.0.1.1:47787
I0124 08:14:04.589587  7953 slave.cpp:747] Successfully authenticated with master master@127.0.1.1:47787
I0124 08:14:04.589913  7953 master.cpp:3275] Registering slave at slave(208)@127.0.1.1:47787 (utopic) with id 20150124-081404-16842879-47787-7926-S0
I0124 08:14:04.590322  7953 registrar.cpp:445] Applied 1 operations in 60404ns; attempting to update the 'registry'
I0124 08:14:04.595336  7948 log.cpp:684] Attempting to append 283 bytes to the log
I0124 08:14:04.595552  7948 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3
I0124 08:14:04.596535  7948 replica.cpp:511] Replica received write request for position 3
I0124 08:14:04.597846  7951 master.cpp:3263] Ignoring register slave message from slave(208)@127.0.1.1:47787 (utopic) as admission is already in progress
I0124 08:14:04.602326  7948 leveldb.cpp:343] Persisting action (302 bytes) to leveldb took 5.758211ms
I0124 08:14:04.602363  7948 replica.cpp:679] Persisted action at 3
I0124 08:14:04.603492  7951 replica.cpp:658] Replica received learned notice for position 3
I0124 08:14:04.608952  7951 leveldb.cpp:343] Persisting action (304 bytes) to leveldb took 5.427195ms
I0124 08:14:04.608985  7951 replica.cpp:679] Persisted action at 3
I0124 08:14:04.609007  7951 replica.cpp:664] Replica learned APPEND action at position 3
I0124 08:14:04.610643  7951 registrar.cpp:490] Successfully updated the 'registry' in 20.258048ms
I0124 08:14:04.610800  7948 log.cpp:703] Attempting to truncate the log to 3
I0124 08:14:04.611184  7948 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4
I0124 08:14:04.612076  7948 replica.cpp:511] Replica received write request for position 4
I0124 08:14:04.613061  7946 master.cpp:3329] Registered slave 20150124-081404-16842879-47787-7926-S0 at slave(208)@127.0.1.1:47787 (utopic) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0124 08:14:04.613299  7946 hierarchical_allocator_process.hpp:453] Added slave 20150124-081404-16842879-47787-7926-S0 (utopic) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] available)
I0124 08:14:04.613688  7946 slave.cpp:781] Registered with master master@127.0.1.1:47787; given slave ID 20150124-081404-16842879-47787-7926-S0
I0124 08:14:04.614112  7946 master.cpp:4071] Sending 1 offers to framework 20150124-081404-16842879-47787-7926-0000 (default) at scheduler-4a6c5cde-c54a-455a-aaad-6fc4e8ee99ef@127.0.1.1:47787
I0124 08:14:04.614228  7946 status_update_manager.cpp:178] Resuming sending status updates
I0124 08:14:04.617481  7947 master.cpp:2677] Processing ACCEPT call for offers: [ 20150124-081404-16842879-47787-7926-O0 ] on slave 20150124-081404-16842879-47787-7926-S0 at slave(208)@127.0.1.1:47787 (utopic) for framework 20150124-081404-16842879-47787-7926-0000 (default) at scheduler-4a6c5cde-c54a-455a-aaad-6fc4e8ee99ef@127.0.1.1:47787
I0124 08:14:04.617535  7947 master.cpp:2513] Authorizing framework principal 'test-principal' to launch task 7c16772d-4aed-4719-81c4-658a2cc22543 as user 'jenkins'
I0124 08:14:04.618736  7947 master.hpp:782] Adding task 7c16772d-4aed-4719-81c4-658a2cc22543 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20150124-081404-16842879-47787-7926-S0 (utopic)
I0124 08:14:04.618854  7947 master.cpp:2885] Launching task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000 (default) at scheduler-4a6c5cde-c54a-455a-aaad-6fc4e8ee99ef@127.0.1.1:47787 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20150124-081404-16842879-47787-7926-S0 at slave(208)@127.0.1.1:47787 (utopic)
I0124 08:14:04.619209  7947 slave.cpp:1130] Got assigned task 7c16772d-4aed-4719-81c4-658a2cc22543 for framework 20150124-081404-16842879-47787-7926-0000
I0124 08:14:04.619472  7948 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 7.364828ms
I0124 08:14:04.619941  7948 replica.cpp:679] Persisted action at 4
I0124 08:14:04.624851  7953 replica.cpp:658] Replica received learned notice for position 4
I0124 08:14:04.625757  7947 slave.cpp:1245] Launching task 7c16772d-4aed-4719-81c4-658a2cc22543 for framework 20150124-081404-16842879-47787-7926-0000
I0124 08:14:04.630590  7953 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 5.705336ms
I0124 08:14:04.630805  7953 leveldb.cpp:401] Deleting ~2 keys from leveldb took 51263ns
I0124 08:14:04.630828  7953 replica.cpp:679] Persisted action at 4
I0124 08:14:04.630851  7953 replica.cpp:664] Replica learned TRUNCATE action at position 4
I0124 08:14:04.633968  7947 slave.cpp:3921] Launching executor 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000 in work directory '/tmp/SlaveTest_MesosExecutorGracefulShutdown_kB74xo/slaves/20150124-081404-16842879-47787-7926-S0/frameworks/20150124-081404-16842879-47787-7926-0000/executors/7c16772d-4aed-4719-81c4-658a2cc22543/runs/53887a08-f11d-4a2f-a659-a715d9fcf3d2'
I0124 08:14:04.634963  7951 containerizer.cpp:445] Starting container '53887a08-f11d-4a2f-a659-a715d9fcf3d2' for executor '7c16772d-4aed-4719-81c4-658a2cc22543' of framework '20150124-081404-16842879-47787-7926-0000'
W0124 08:14:04.636931  7951 containerizer.cpp:296] CommandInfo.grace_period flag is not set, using default value: 3secs
I0124 08:14:04.655591  7947 slave.cpp:1368] Queuing task '7c16772d-4aed-4719-81c4-658a2cc22543' for executor 7c16772d-4aed-4719-81c4-658a2cc22543 of framework '20150124-081404-16842879-47787-7926-0000
I0124 08:14:04.656992  7951 launcher.cpp:137] Forked child with pid '11030' for container '53887a08-f11d-4a2f-a659-a715d9fcf3d2'
I0124 08:14:04.673646  7951 slave.cpp:2890] Monitoring executor '7c16772d-4aed-4719-81c4-658a2cc22543' of framework '20150124-081404-16842879-47787-7926-0000' in container '53887a08-f11d-4a2f-a659-a715d9fcf3d2'
I0124 08:14:04.964946 11044 exec.cpp:147] Version: 0.22.0
I0124 08:14:05.113059  7948 slave.cpp:1912] Got registration for executor '7c16772d-4aed-4719-81c4-658a2cc22543' of framework 20150124-081404-16842879-47787-7926-0000 from executor(1)@127.0.1.1:49174
I0124 08:14:05.121086  7948 slave.cpp:2031] Flushing queued task 7c16772d-4aed-4719-81c4-658a2cc22543 for executor '7c16772d-4aed-4719-81c4-658a2cc22543' of framework 20150124-081404-16842879-47787-7926-0000
I0124 08:14:05.266849 11062 exec.cpp:221] Executor registered on slave 20150124-081404-16842879-47787-7926-S0
Shutdown timeout is set to 3secsRegistered executor on utopic
Starting task 7c16772d-4aed-4719-81c4-658a2cc22543
Forked command at 11067
sh -c 'sleep 1000'
I0124 08:14:05.492084  7953 slave.cpp:2265] Handling status update TASK_RUNNING (UUID: 54742a87-ef02-4e72-a19b-83b0eeb62568) for task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000 from executor(1)@127.0.1.1:49174
I0124 08:14:05.492805  7953 status_update_manager.cpp:317] Received status update TASK_RUNNING (UUID: 54742a87-ef02-4e72-a19b-83b0eeb62568) for task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000
I0124 08:14:05.493762  7953 slave.cpp:2508] Forwarding the update TASK_RUNNING (UUID: 54742a87-ef02-4e72-a19b-83b0eeb62568) for task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000 to master@127.0.1.1:47787
I0124 08:14:05.493948  7953 slave.cpp:2441] Sending acknowledgement for status update TASK_RUNNING (UUID: 54742a87-ef02-4e72-a19b-83b0eeb62568) for task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000 to executor(1)@127.0.1.1:49174
I0124 08:14:05.495378  7949 master.cpp:3652] Forwarding status update TASK_RUNNING (UUID: 54742a87-ef02-4e72-a19b-83b0eeb62568) for task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000
I0124 08:14:05.495584  7949 master.cpp:3624] Status update TASK_RUNNING (UUID: 54742a87-ef02-4e72-a19b-83b0eeb62568) for task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000 from slave 20150124-081404-16842879-47787-7926-S0 at slave(208)@127.0.1.1:47787 (utopic)
I0124 08:14:05.495678  7949 master.cpp:4934] Updating the latest state of task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000 to TASK_RUNNING
I0124 08:14:05.496422  7949 master.cpp:3125] Forwarding status update acknowledgement 54742a87-ef02-4e72-a19b-83b0eeb62568 for task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000 (default) at scheduler-4a6c5cde-c54a-455a-aaad-6fc4e8ee99ef@127.0.1.1:47787 to slave 20150124-081404-16842879-47787-7926-S0 at slave(208)@127.0.1.1:47787 (utopic)
I0124 08:14:05.497735  7946 master.cpp:2961] Asked to kill task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000
I0124 08:14:05.497859  7946 master.cpp:3021] Telling slave 20150124-081404-16842879-47787-7926-S0 at slave(208)@127.0.1.1:47787 (utopic) to kill task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000 (default) at scheduler-4a6c5cde-c54a-455a-aaad-6fc4e8ee99ef@127.0.1.1:47787
I0124 08:14:05.498589  7947 status_update_manager.cpp:389] Received status update acknowledgement (UUID: 54742a87-ef02-4e72-a19b-83b0eeb62568) for task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000
I0124 08:14:05.499006  7953 slave.cpp:1424] Asked to kill task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000
Shutting down
Sending SIGTERM to process tree at pid 11067
Killing the following process trees:
[ 
-+- 11067 sh -c sleep 1000 
 \--- 11068 sleep 1000 
]
2015-01-24 08:14:07,295:7926(0x7f30b1b34700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:57753] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
Process 11067 did not terminate after 3secs, sending SIGKILL to process tree at 11067
Killed the following process trees:
[ 
-+- 11067 sh -c sleep 1000 
 \--- 11068 sleep 1000 
]
Command terminated with signal Killed (pid: 11067)
I0124 08:14:09.063453  7953 slave.cpp:2265] Handling status update TASK_KILLED (UUID: 4bd05372-2705-46e5-8182-5cb6907fbab3) for task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000 from executor(1)@127.0.1.1:49174
I0124 08:14:09.069545  7953 status_update_manager.cpp:317] Received status update TASK_KILLED (UUID: 4bd05372-2705-46e5-8182-5cb6907fbab3) for task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000
I0124 08:14:09.070265  7953 slave.cpp:2508] Forwarding the update TASK_KILLED (UUID: 4bd05372-2705-46e5-8182-5cb6907fbab3) for task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000 to master@127.0.1.1:47787
I0124 08:14:09.070996  7947 master.cpp:3652] Forwarding status update TASK_KILLED (UUID: 4bd05372-2705-46e5-8182-5cb6907fbab3) for task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000
I0124 08:14:09.071182  7947 master.cpp:3624] Status update TASK_KILLED (UUID: 4bd05372-2705-46e5-8182-5cb6907fbab3) for task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000 from slave 20150124-081404-16842879-47787-7926-S0 at slave(208)@127.0.1.1:47787 (utopic)
I0124 08:14:09.071260  7947 master.cpp:4934] Updating the latest state of task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000 to TASK_KILLED
I0124 08:14:09.072052  7947 hierarchical_allocator_process.hpp:653] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total allocatable: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]) on slave 20150124-081404-16842879-47787-7926-S0 from framework 20150124-081404-16842879-47787-7926-0000
I0124 08:14:09.072449  7947 master.cpp:4993] Removing task 7c16772d-4aed-4719-81c4-658a2cc22543 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] of framework 20150124-081404-16842879-47787-7926-0000 on slave 20150124-081404-16842879-47787-7926-S0 at slave(208)@127.0.1.1:47787 (utopic)
I0124 08:14:09.072700  7947 master.cpp:3125] Forwarding status update acknowledgement 4bd05372-2705-46e5-8182-5cb6907fbab3 for task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000 (default) at scheduler-4a6c5cde-c54a-455a-aaad-6fc4e8ee99ef@127.0.1.1:47787 to slave 20150124-081404-16842879-47787-7926-S0 at slave(208)@127.0.1.1:47787 (utopic)
../../src/tests/slave_tests.cpp:1736: Failure
Expected: (std::string::npos) != (statusKilled.get().message().find(""Terminated"")), actual: 18446744073709551615 vs 18446744073709551615
I0124 08:14:09.073422  7926 sched.cpp:1471] Asked to stop the driver
I0124 08:14:09.073629  7926 master.cpp:654] Master terminating
I0124 08:14:09.075768  7950 sched.cpp:808] Stopping framework '20150124-081404-16842879-47787-7926-0000'
I0124 08:14:09.079352  7953 slave.cpp:2441] Sending acknowledgement for status update TASK_KILLED (UUID: 4bd05372-2705-46e5-8182-5cb6907fbab3) for task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000 to executor(1)@127.0.1.1:49174
I0124 08:14:09.085199  7953 slave.cpp:2673] master@127.0.1.1:47787 exited
W0124 08:14:09.085232  7953 slave.cpp:2676] Master disconnected! Waiting for a new master to be elected
I0124 08:14:09.085263  7953 status_update_manager.cpp:389] Received status update acknowledgement (UUID: 4bd05372-2705-46e5-8182-5cb6907fbab3) for task 7c16772d-4aed-4719-81c4-658a2cc22543 of framework 20150124-081404-16842879-47787-7926-0000
I0124 08:14:09.120879  7946 containerizer.cpp:879] Destroying container '53887a08-f11d-4a2f-a659-a715d9fcf3d2'
I0124 08:14:09.216553  7952 containerizer.cpp:1084] Executor for container '53887a08-f11d-4a2f-a659-a715d9fcf3d2' has exited
I0124 08:14:09.218641  7952 slave.cpp:2948] Executor '7c16772d-4aed-4719-81c4-658a2cc22543' of framework 20150124-081404-16842879-47787-7926-0000 terminated with signal Killed
I0124 08:14:09.218855  7952 slave.cpp:3057] Cleaning up executor '7c16772d-4aed-4719-81c4-658a2cc22543' of framework 20150124-081404-16842879-47787-7926-0000
I0124 08:14:09.223268  7947 gc.cpp:56] Scheduling '/tmp/SlaveTest_MesosExecutorGracefulShutdown_kB74xo/slaves/20150124-081404-16842879-47787-7926-S0/frameworks/20150124-081404-16842879-47787-7926-0000/executors/7c16772d-4aed-4719-81c4-658a2cc22543/runs/53887a08-f11d-4a2f-a659-a715d9fcf3d2' for gc 6.99999746482667days in the future
I0124 08:14:09.224205  7947 gc.cpp:56] Scheduling '/tmp/SlaveTest_MesosExecutorGracefulShutdown_kB74xo/slaves/20150124-081404-16842879-47787-7926-S0/frameworks/20150124-081404-16842879-47787-7926-0000/executors/7c16772d-4aed-4719-81c4-658a2cc22543' for gc 6.99999746293926days in the future
I0124 08:14:09.227552  7952 slave.cpp:3136] Cleaning up framework 20150124-081404-16842879-47787-7926-0000
I0124 08:14:09.229786  7949 status_update_manager.cpp:279] Closing status update streams for framework 20150124-081404-16842879-47787-7926-0000
I0124 08:14:09.230849  7952 slave.cpp:495] Slave terminating
I0124 08:14:09.230989  7952 gc.cpp:56] Scheduling '/tmp/SlaveTest_MesosExecutorGracefulShutdown_kB74xo/slaves/20150124-081404-16842879-47787-7926-S0/frameworks/20150124-081404-16842879-47787-7926-0000' for gc 6.99999732935407days in the future
[  FAILED  ] SlaveTest.MesosExecutorGracefulShutdown (4881 ms)
{noformat}",3.0,0.22.0,0.5,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.8846153846153846,0.9878787878787878,0.9878787878787878,0.20183486238532108
Bug,HookTest.VerifySlaveLaunchExecutorHook is flaky,"Observed this on internal CI

{code}
[ RUN      ] HookTest.VerifySlaveLaunchExecutorHook
Using temporary directory '/tmp/HookTest_VerifySlaveLaunchExecutorHook_GjBgME'
I0114 18:51:34.659353  4720 leveldb.cpp:176] Opened db in 1.255951ms
I0114 18:51:34.662112  4720 leveldb.cpp:183] Compacted db in 596090ns
I0114 18:51:34.662364  4720 leveldb.cpp:198] Created db iterator in 177877ns
I0114 18:51:34.662719  4720 leveldb.cpp:204] Seeked to beginning of db in 19709ns
I0114 18:51:34.663010  4720 leveldb.cpp:273] Iterated through 0 keys in the db in 18208ns
I0114 18:51:34.663312  4720 replica.cpp:744] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0114 18:51:34.664266  4735 recover.cpp:449] Starting replica recovery
I0114 18:51:34.664908  4735 recover.cpp:475] Replica is in EMPTY status
I0114 18:51:34.667842  4734 replica.cpp:641] Replica in EMPTY status received a broadcasted recover request
I0114 18:51:34.669117  4735 recover.cpp:195] Received a recover response from a replica in EMPTY status
I0114 18:51:34.677913  4735 recover.cpp:566] Updating replica status to STARTING
I0114 18:51:34.683157  4735 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 137939ns
I0114 18:51:34.683507  4735 replica.cpp:323] Persisted replica status to STARTING
I0114 18:51:34.684013  4735 recover.cpp:475] Replica is in STARTING status
I0114 18:51:34.685554  4738 replica.cpp:641] Replica in STARTING status received a broadcasted recover request
I0114 18:51:34.696512  4736 recover.cpp:195] Received a recover response from a replica in STARTING status
I0114 18:51:34.700552  4735 recover.cpp:566] Updating replica status to VOTING
I0114 18:51:34.701128  4735 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 115624ns
I0114 18:51:34.701478  4735 replica.cpp:323] Persisted replica status to VOTING
I0114 18:51:34.701817  4735 recover.cpp:580] Successfully joined the Paxos group
I0114 18:51:34.702569  4735 recover.cpp:464] Recover process terminated
I0114 18:51:34.716439  4736 master.cpp:262] Master 20150114-185134-2272962752-57018-4720 (fedora-19) started on 192.168.122.135:57018
I0114 18:51:34.716913  4736 master.cpp:308] Master only allowing authenticated frameworks to register
I0114 18:51:34.717136  4736 master.cpp:313] Master only allowing authenticated slaves to register
I0114 18:51:34.717488  4736 credentials.hpp:36] Loading credentials for authentication from '/tmp/HookTest_VerifySlaveLaunchExecutorHook_GjBgME/credentials'
I0114 18:51:34.718077  4736 master.cpp:357] Authorization enabled
I0114 18:51:34.719238  4738 whitelist_watcher.cpp:65] No whitelist given
I0114 18:51:34.719755  4737 hierarchical_allocator_process.hpp:285] Initialized hierarchical allocator process
I0114 18:51:34.722584  4736 master.cpp:1219] The newly elected leader is master@192.168.122.135:57018 with id 20150114-185134-2272962752-57018-4720
I0114 18:51:34.722865  4736 master.cpp:1232] Elected as the leading master!
I0114 18:51:34.723310  4736 master.cpp:1050] Recovering from registrar
I0114 18:51:34.723760  4734 registrar.cpp:313] Recovering registrar
I0114 18:51:34.725229  4740 log.cpp:660] Attempting to start the writer
I0114 18:51:34.727893  4739 replica.cpp:477] Replica received implicit promise request with proposal 1
I0114 18:51:34.728425  4739 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 114781ns
I0114 18:51:34.728662  4739 replica.cpp:345] Persisted promised to 1
I0114 18:51:34.731271  4741 coordinator.cpp:230] Coordinator attemping to fill missing position
I0114 18:51:34.733223  4734 replica.cpp:378] Replica received explicit promise request for position 0 with proposal 2
I0114 18:51:34.734076  4734 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 87441ns
I0114 18:51:34.734441  4734 replica.cpp:679] Persisted action at 0
I0114 18:51:34.740272  4739 replica.cpp:511] Replica received write request for position 0
I0114 18:51:34.740910  4739 leveldb.cpp:438] Reading position from leveldb took 59846ns
I0114 18:51:34.741672  4739 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 189259ns
I0114 18:51:34.741919  4739 replica.cpp:679] Persisted action at 0
I0114 18:51:34.743000  4739 replica.cpp:658] Replica received learned notice for position 0
I0114 18:51:34.746844  4739 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 328487ns
I0114 18:51:34.747118  4739 replica.cpp:679] Persisted action at 0
I0114 18:51:34.747553  4739 replica.cpp:664] Replica learned NOP action at position 0
I0114 18:51:34.751344  4737 log.cpp:676] Writer started with ending position 0
I0114 18:51:34.753504  4734 leveldb.cpp:438] Reading position from leveldb took 61183ns
I0114 18:51:34.762962  4737 registrar.cpp:346] Successfully fetched the registry (0B) in 38.907904ms
I0114 18:51:34.763610  4737 registrar.cpp:445] Applied 1 operations in 67206ns; attempting to update the 'registry'
I0114 18:51:34.766079  4736 log.cpp:684] Attempting to append 130 bytes to the log
I0114 18:51:34.766769  4736 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I0114 18:51:34.768215  4741 replica.cpp:511] Replica received write request for position 1
I0114 18:51:34.768759  4741 leveldb.cpp:343] Persisting action (149 bytes) to leveldb took 87970ns
I0114 18:51:34.768995  4741 replica.cpp:679] Persisted action at 1
I0114 18:51:34.770691  4736 replica.cpp:658] Replica received learned notice for position 1
I0114 18:51:34.771273  4736 leveldb.cpp:343] Persisting action (151 bytes) to leveldb took 83590ns
I0114 18:51:34.771579  4736 replica.cpp:679] Persisted action at 1
I0114 18:51:34.771917  4736 replica.cpp:664] Replica learned APPEND action at position 1
I0114 18:51:34.773252  4738 log.cpp:703] Attempting to truncate the log to 1
I0114 18:51:34.773756  4735 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2
I0114 18:51:34.775552  4736 replica.cpp:511] Replica received write request for position 2
I0114 18:51:34.775846  4736 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 71503ns
I0114 18:51:34.776695  4736 replica.cpp:679] Persisted action at 2
I0114 18:51:34.785259  4739 replica.cpp:658] Replica received learned notice for position 2
I0114 18:51:34.786252  4737 registrar.cpp:490] Successfully updated the 'registry' in 22.340864ms
I0114 18:51:34.787094  4737 registrar.cpp:376] Successfully recovered registrar
I0114 18:51:34.787749  4737 master.cpp:1077] Recovered 0 slaves from the Registry (94B) ; allowing 10mins for slaves to re-register
I0114 18:51:34.787282  4739 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 707150ns
I0114 18:51:34.788692  4739 leveldb.cpp:401] Deleting ~1 keys from leveldb took 60262ns
I0114 18:51:34.789048  4739 replica.cpp:679] Persisted action at 2
I0114 18:51:34.789329  4739 replica.cpp:664] Replica learned TRUNCATE action at position 2
I0114 18:51:34.819548  4738 slave.cpp:173] Slave started on 171)@192.168.122.135:57018
I0114 18:51:34.820530  4738 credentials.hpp:84] Loading credential for authentication from '/tmp/HookTest_VerifySlaveLaunchExecutorHook_AYxNqe/credential'
I0114 18:51:34.820952  4738 slave.cpp:282] Slave using credential for: test-principal
I0114 18:51:34.821516  4738 slave.cpp:300] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0114 18:51:34.822217  4738 slave.cpp:329] Slave hostname: fedora-19
I0114 18:51:34.822502  4738 slave.cpp:330] Slave checkpoint: false
W0114 18:51:34.822857  4738 slave.cpp:332] Disabling checkpointing is deprecated and the --checkpoint flag will be removed in a future release. Please avoid using this flag
I0114 18:51:34.824998  4737 state.cpp:33] Recovering state from '/tmp/HookTest_VerifySlaveLaunchExecutorHook_AYxNqe/meta'
I0114 18:51:34.834015  4738 status_update_manager.cpp:197] Recovering status update manager
I0114 18:51:34.834810  4738 slave.cpp:3519] Finished recovery
I0114 18:51:34.835906  4734 status_update_manager.cpp:171] Pausing sending status updates
I0114 18:51:34.836423  4738 slave.cpp:613] New master detected at master@192.168.122.135:57018
I0114 18:51:34.836908  4738 slave.cpp:676] Authenticating with master master@192.168.122.135:57018
I0114 18:51:34.837190  4738 slave.cpp:681] Using default CRAM-MD5 authenticatee
I0114 18:51:34.837820  4737 authenticatee.hpp:138] Creating new client SASL connection
I0114 18:51:34.838784  4738 slave.cpp:649] Detecting new master
I0114 18:51:34.839306  4740 master.cpp:4130] Authenticating slave(171)@192.168.122.135:57018
I0114 18:51:34.839957  4740 master.cpp:4141] Using default CRAM-MD5 authenticator
I0114 18:51:34.841236  4740 authenticator.hpp:170] Creating new server SASL connection
I0114 18:51:34.842681  4741 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5
I0114 18:51:34.843118  4741 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5'
I0114 18:51:34.843581  4740 authenticator.hpp:276] Received SASL authentication start
I0114 18:51:34.843962  4740 authenticator.hpp:398] Authentication requires more steps
I0114 18:51:34.844357  4740 authenticatee.hpp:275] Received SASL authentication step
I0114 18:51:34.844780  4740 authenticator.hpp:304] Received SASL authentication step
I0114 18:51:34.845113  4740 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0114 18:51:34.845507  4740 auxprop.cpp:171] Looking up auxiliary property '*userPassword'
I0114 18:51:34.845835  4740 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0114 18:51:34.846238  4740 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0114 18:51:34.846542  4740 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0114 18:51:34.846806  4740 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0114 18:51:34.847110  4740 authenticator.hpp:390] Authentication success
I0114 18:51:34.847808  4734 authenticatee.hpp:315] Authentication success
I0114 18:51:34.851029  4734 slave.cpp:747] Successfully authenticated with master master@192.168.122.135:57018
I0114 18:51:34.851608  4737 master.cpp:4188] Successfully authenticated principal 'test-principal' at slave(171)@192.168.122.135:57018
I0114 18:51:34.854962  4720 sched.cpp:151] Version: 0.22.0
I0114 18:51:34.856674  4734 slave.cpp:1075] Will retry registration in 3.085482ms if necessary
I0114 18:51:34.857434  4739 sched.cpp:248] New master detected at master@192.168.122.135:57018
I0114 18:51:34.861433  4739 sched.cpp:304] Authenticating with master master@192.168.122.135:57018
I0114 18:51:34.861693  4739 sched.cpp:311] Using default CRAM-MD5 authenticatee
I0114 18:51:34.857795  4737 master.cpp:3276] Registering slave at slave(171)@192.168.122.135:57018 (fedora-19) with id 20150114-185134-2272962752-57018-4720-S0
I0114 18:51:34.862951  4737 authenticatee.hpp:138] Creating new client SASL connection
I0114 18:51:34.863919  4735 registrar.cpp:445] Applied 1 operations in 120272ns; attempting to update the 'registry'
I0114 18:51:34.864645  4738 master.cpp:4130] Authenticating scheduler-c45273e4-6eb5-44ee-bf45-71b353db648f@192.168.122.135:57018
I0114 18:51:34.865033  4738 master.cpp:4141] Using default CRAM-MD5 authenticator
I0114 18:51:34.866904  4738 authenticator.hpp:170] Creating new server SASL connection
I0114 18:51:34.868840  4737 authenticatee.hpp:229] Received SASL authentication mechanisms: CRAM-MD5
I0114 18:51:34.869125  4737 authenticatee.hpp:255] Attempting to authenticate with mechanism 'CRAM-MD5'
I0114 18:51:34.869523  4737 authenticator.hpp:276] Received SASL authentication start
I0114 18:51:34.869835  4737 authenticator.hpp:398] Authentication requires more steps
I0114 18:51:34.870213  4737 authenticatee.hpp:275] Received SASL authentication step
I0114 18:51:34.870622  4737 authenticator.hpp:304] Received SASL authentication step
I0114 18:51:34.870946  4737 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0114 18:51:34.871219  4737 auxprop.cpp:171] Looking up auxiliary property '*userPassword'
I0114 18:51:34.871554  4737 auxprop.cpp:171] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0114 18:51:34.871968  4737 auxprop.cpp:99] Request to lookup properties for user: 'test-principal' realm: 'fedora-19' server FQDN: 'fedora-19' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0114 18:51:34.872297  4737 auxprop.cpp:121] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0114 18:51:34.872655  4737 auxprop.cpp:121] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0114 18:51:34.873024  4737 authenticator.hpp:390] Authentication success
I0114 18:51:34.873428  4737 authenticatee.hpp:315] Authentication success
I0114 18:51:34.873632  4739 master.cpp:4188] Successfully authenticated principal 'test-principal' at scheduler-c45273e4-6eb5-44ee-bf45-71b353db648f@192.168.122.135:57018
I0114 18:51:34.875006  4740 sched.cpp:392] Successfully authenticated with master master@192.168.122.135:57018
I0114 18:51:34.875319  4740 sched.cpp:515] Sending registration request to master@192.168.122.135:57018
I0114 18:51:34.876200  4740 sched.cpp:548] Will retry registration in 1.952991346secs if necessary
I0114 18:51:34.876729  4738 master.cpp:1417] Received registration request for framework 'default' at scheduler-c45273e4-6eb5-44ee-bf45-71b353db648f@192.168.122.135:57018
I0114 18:51:34.877040  4738 master.cpp:1298] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0114 18:51:34.878059  4738 master.cpp:1481] Registering framework 20150114-185134-2272962752-57018-4720-0000 (default) at scheduler-c45273e4-6eb5-44ee-bf45-71b353db648f@192.168.122.135:57018
I0114 18:51:34.878473  4739 log.cpp:684] Attempting to append 300 bytes to the log
I0114 18:51:34.879464  4737 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3
I0114 18:51:34.880116  4734 hierarchical_allocator_process.hpp:319] Added framework 20150114-185134-2272962752-57018-4720-0000
I0114 18:51:34.880470  4734 hierarchical_allocator_process.hpp:839] No resources available to allocate!
I0114 18:51:34.882331  4734 hierarchical_allocator_process.hpp:746] Performed allocation for 0 slaves in 1.901284ms
I0114 18:51:34.884024  4741 sched.cpp:442] Framework registered with 20150114-185134-2272962752-57018-4720-0000
I0114 18:51:34.884454  4741 sched.cpp:456] Scheduler::registered took 44320ns
I0114 18:51:34.881965  4737 replica.cpp:511] Replica received write request for position 3
I0114 18:51:34.885218  4737 leveldb.cpp:343] Persisting action (319 bytes) to leveldb took 134480ns
I0114 18:51:34.885716  4737 replica.cpp:679] Persisted action at 3
I0114 18:51:34.886034  4739 slave.cpp:1075] Will retry registration in 22.947772ms if necessary
I0114 18:51:34.886291  4740 master.cpp:3264] Ignoring register slave message from slave(171)@192.168.122.135:57018 (fedora-19) as admission is already in progress
I0114 18:51:34.894690  4736 replica.cpp:658] Replica received learned notice for position 3
I0114 18:51:34.898638  4736 leveldb.cpp:343] Persisting action (321 bytes) to leveldb took 215501ns
I0114 18:51:34.899055  4736 replica.cpp:679] Persisted action at 3
I0114 18:51:34.899416  4736 replica.cpp:664] Replica learned APPEND action at position 3
I0114 18:51:34.911782  4736 registrar.cpp:490] Successfully updated the 'registry' in 46.176768ms
I0114 18:51:34.912286  4740 log.cpp:703] Attempting to truncate the log to 3
I0114 18:51:34.913108  4740 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4
I0114 18:51:34.915027  4736 master.cpp:3330] Registered slave 20150114-185134-2272962752-57018-4720-S0 at slave(171)@192.168.122.135:57018 (fedora-19) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0114 18:51:34.915642  4735 hierarchical_allocator_process.hpp:453] Added slave 20150114-185134-2272962752-57018-4720-S0 (fedora-19) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] available)
I0114 18:51:34.917809  4735 hierarchical_allocator_process.hpp:764] Performed allocation for slave 20150114-185134-2272962752-57018-4720-S0 in 514027ns
I0114 18:51:34.916689  4738 replica.cpp:511] Replica received write request for position 4
I0114 18:51:34.915784  4741 slave.cpp:781] Registered with master master@192.168.122.135:57018; given slave ID 20150114-185134-2272962752-57018-4720-S0
I0114 18:51:34.919293  4741 slave.cpp:2588] Received ping from slave-observer(156)@192.168.122.135:57018
I0114 18:51:34.919775  4740 status_update_manager.cpp:178] Resuming sending status updates
I0114 18:51:34.920374  4736 master.cpp:4072] Sending 1 offers to framework 20150114-185134-2272962752-57018-4720-0000 (default) at scheduler-c45273e4-6eb5-44ee-bf45-71b353db648f@192.168.122.135:57018
I0114 18:51:34.920569  4738 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 1.540136ms
I0114 18:51:34.921092  4738 replica.cpp:679] Persisted action at 4
I0114 18:51:34.927111  4735 replica.cpp:658] Replica received learned notice for position 4
I0114 18:51:34.927299  4734 sched.cpp:605] Scheduler::resourceOffers took 1.335524ms
I0114 18:51:34.930418  4735 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 1.596377ms
I0114 18:51:34.930882  4735 leveldb.cpp:401] Deleting ~2 keys from leveldb took 67578ns
I0114 18:51:34.931115  4735 replica.cpp:679] Persisted action at 4
I0114 18:51:34.931529  4735 replica.cpp:664] Replica learned TRUNCATE action at position 4
I0114 18:51:34.930356  4734 master.cpp:2541] Processing reply for offers: [ 20150114-185134-2272962752-57018-4720-O0 ] on slave 20150114-185134-2272962752-57018-4720-S0 at slave(171)@192.168.122.135:57018 (fedora-19) for framework 20150114-185134-2272962752-57018-4720-0000 (default) at scheduler-c45273e4-6eb5-44ee-bf45-71b353db648f@192.168.122.135:57018
I0114 18:51:34.932834  4734 master.cpp:2647] Authorizing framework principal 'test-principal' to launch task 1 as user 'jenkins'
W0114 18:51:34.934442  4736 master.cpp:2124] Executor default for task 1 uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases.
W0114 18:51:34.934960  4736 master.cpp:2136] Executor default for task 1 uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases.
I0114 18:51:34.935878  4736 master.hpp:766] Adding task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20150114-185134-2272962752-57018-4720-S0 (fedora-19)
I0114 18:51:34.939453  4738 hierarchical_allocator_process.hpp:610] Updated allocation of framework 20150114-185134-2272962752-57018-4720-0000 on slave 20150114-185134-2272962752-57018-4720-S0 from cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] to cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0114 18:51:34.939950  4736 master.cpp:2897] Launching task 1 of framework 20150114-185134-2272962752-57018-4720-0000 (default) at scheduler-c45273e4-6eb5-44ee-bf45-71b353db648f@192.168.122.135:57018 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20150114-185134-2272962752-57018-4720-S0 at slave(171)@192.168.122.135:57018 (fedora-19)
I0114 18:51:34.940467  4736 test_hook_module.cpp:52] Executing 'masterLaunchTaskLabelDecorator' hook
I0114 18:51:34.941490  4740 slave.cpp:1130] Got assigned task 1 for framework 20150114-185134-2272962752-57018-4720-0000
I0114 18:51:34.942873  4740 slave.cpp:1245] Launching task 1 for framework 20150114-185134-2272962752-57018-4720-0000
I0114 18:51:34.943469  4740 test_hook_module.cpp:71] Executing 'slaveLaunchExecutorEnvironmentDecorator' hook
I0114 18:51:34.946705  4740 slave.cpp:3921] Launching executor default of framework 20150114-185134-2272962752-57018-4720-0000 in work directory '/tmp/HookTest_VerifySlaveLaunchExecutorHook_AYxNqe/slaves/20150114-185134-2272962752-57018-4720-S0/frameworks/20150114-185134-2272962752-57018-4720-0000/executors/default/runs/d73da0e7-3d52-4a0e-91d0-eaef735fd65d'
I0114 18:51:34.956496  4740 exec.cpp:147] Version: 0.22.0
I0114 18:51:34.960752  4737 exec.cpp:197] Executor started at: executor(56)@192.168.122.135:57018 with pid 4720
I0114 18:51:34.964501  4740 slave.cpp:1368] Queuing task '1' for executor default of framework '20150114-185134-2272962752-57018-4720-0000
I0114 18:51:34.965133  4740 slave.cpp:566] Successfully attached file '/tmp/HookTest_VerifySlaveLaunchExecutorHook_AYxNqe/slaves/20150114-185134-2272962752-57018-4720-S0/frameworks/20150114-185134-2272962752-57018-4720-0000/executors/default/runs/d73da0e7-3d52-4a0e-91d0-eaef735fd65d'
I0114 18:51:34.965605  4740 slave.cpp:1912] Got registration for executor 'default' of framework 20150114-185134-2272962752-57018-4720-0000 from executor(56)@192.168.122.135:57018
I0114 18:51:34.966933  4734 exec.cpp:221] Executor registered on slave 20150114-185134-2272962752-57018-4720-S0
I0114 18:51:34.968889  4740 slave.cpp:2031] Flushing queued task 1 for executor 'default' of framework 20150114-185134-2272962752-57018-4720-0000
I0114 18:51:34.969743  4740 slave.cpp:2890] Monitoring executor 'default' of framework '20150114-185134-2272962752-57018-4720-0000' in container 'd73da0e7-3d52-4a0e-91d0-eaef735fd65d'
I0114 18:51:34.973484  4734 exec.cpp:233] Executor::registered took 4.814445ms
I0114 18:51:34.974081  4734 exec.cpp:308] Executor asked to run task '1'
I0114 18:51:34.974431  4734 exec.cpp:317] Executor::launchTask took 184910ns
I0114 18:51:34.975292  4720 sched.cpp:1471] Asked to stop the driver
I0114 18:51:34.975817  4738 sched.cpp:808] Stopping framework '20150114-185134-2272962752-57018-4720-0000'
I0114 18:51:34.975697  4720 master.cpp:654] Master terminating
W0114 18:51:34.976610  4720 master.cpp:4980] Removing task 1 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] of framework 20150114-185134-2272962752-57018-4720-0000 on slave 20150114-185134-2272962752-57018-4720-S0 at slave(171)@192.168.122.135:57018 (fedora-19) in non-terminal state TASK_STAGING
I0114 18:51:34.977880  4720 master.cpp:5023] Removing executor 'default' with resources  of framework 20150114-185134-2272962752-57018-4720-0000 on slave 20150114-185134-2272962752-57018-4720-S0 at slave(171)@192.168.122.135:57018 (fedora-19)
I0114 18:51:34.978196  4741 hierarchical_allocator_process.hpp:653] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total allocatable: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]) on slave 20150114-185134-2272962752-57018-4720-S0 from framework 20150114-185134-2272962752-57018-4720-0000
I0114 18:51:34.982658  4735 slave.cpp:2673] master@192.168.122.135:57018 exited
W0114 18:51:34.983065  4735 slave.cpp:2676] Master disconnected! Waiting for a new master to be elected
I0114 18:51:35.029485  4720 slave.cpp:495] Slave terminating
I0114 18:51:35.034024  4720 slave.cpp:1585] Asked to shut down framework 20150114-185134-2272962752-57018-4720-0000 by @0.0.0.0:0
I0114 18:51:35.034335  4720 slave.cpp:1610] Shutting down framework 20150114-185134-2272962752-57018-4720-0000
I0114 18:51:35.034857  4720 slave.cpp:3198] Shutting down executor 'default' of framework 20150114-185134-2272962752-57018-4720-0000
tests/hook_tests.cpp:271: Failure
Value of: os::isfile(path.get())
  Actual: true
Expected: false
[  FAILED  ] HookTest.VerifySlaveLaunchExecutorHook (412 ms)

{code}",3.0,0.22.0,0.5,0.3453996983408748,0.0,0.0,0.0,0.0,0.2,0.38596491228070173,0.35714285714285715,0.8846153846153846,0.9878787878787878,0.9878787878787878,0.20183486238532108
Documentation,Add user documentation for reservations,"Add a user guide for reservations which describes basic usage of them, how ACLs are used to specify who can unreserve whose resources, and few advanced usage cases.",2.0,0,0.0,0.6726998491704375,0.0,0.0,0.0,0.1,0.6000000000000001,0.2807017543859649,0.29285714285714287,0.21794871794871795,0.26666666666666666,0.26666666666666666,0.0
Wish,Add ContainerId to the TaskStatus message,"{{TaskStatus}} provides the frameworks with certain information ({{executorId}}, {{slaveId}}, etc.) which is useful when collecting statistics about cluster performance; however, it is difficult to associate tasks to the container it is executed since this information stays always within mesos itself. Therefore it would be good to provide the framework scheduler with this information, adding a new field in the {{TaskStatus}} message.

See comments for a use case.",3.0,0,0.5,0.277526395173454,0.0,0.0,0.0,0.0,0.4,0.08771929824561403,0.10714285714285714,0.0,0.0,0.0,0.0
Task,deprecate unused flag 'cgroups_subsystems',cgroups_subsystems is a slave flag that is no longer used and should be deprecated.,1.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.10526315789473684,0.16428571428571428,0.07692307692307693,0.13333333333333333,0.13333333333333333,0.0
Bug,Performance issue in libprocess SocketManager.,"Noticed an issue in production under which the master is slow to respond after failover for ~15 minutes.

After looking at some perf data, the top offender is:

{noformat}
    12.02%  mesos-master  libmesos-0.21.0-rc3.so  [.] std::_Rb_tree<process::ProcessBase*, process::ProcessBase*, std::_Identity<process::ProcessBase*>, std::less<process::ProcessBase*>, std::allocator<process::ProcessBase*> >::erase(process::ProcessBase* const&)
...
     3.29%  mesos-master  libmesos-0.21.0-rc3.so  [.] process::SocketManager::exited(process::ProcessBase*)
{noformat}

It appears that in the SocketManager, whenever an internal Process exits, we loop over all the links unnecessarily:

{code}
void SocketManager::exited(ProcessBase* process)
{
  // An exited event is enough to cause the process to get deleted
  // (e.g., by the garbage collector), which means we can't
  // dereference process (or even use the address) after we enqueue at
  // least one exited event. Thus, we save the process pid.
  const UPID pid = process->pid;

  // Likewise, we need to save the current time of the process so we
  // can update the clocks of linked processes as appropriate.
  const Time time = Clock::now(process);

  synchronized (this) {
    // Iterate through the links, removing any links the process might
    // have had and creating exited events for any linked processes.
    foreachpair (const UPID& linkee, set<ProcessBase*>& processes, links) {
      processes.erase(process);

      if (linkee == pid) {
        foreach (ProcessBase* linker, processes) {
          CHECK(linker != process) << ""Process linked with itself"";
          synchronized (timeouts) {
            if (Clock::paused()) {
              Clock::update(linker, time);
            }
          }
          linker->enqueue(new ExitedEvent(linkee));
        }
      }
    }

    links.erase(pid);
  }
}
{code}

On clusters with 10,000s of slaves, this means we hold the socket manager lock for a very expensive loop erasing nothing from a set! This is because, the master contains links from the Master Process to each slave. However, when a random ephemeral Process terminates, we don't need to loop over each slave link.

While we hold this lock, the following calls will block:

{code}
class SocketManager
{
public:
  Socket accepted(int s);
  void link(ProcessBase* process, const UPID& to);
  PID<HttpProxy> proxy(const Socket& socket);
  void send(Encoder* encoder, bool persist);
  void send(const Response& response,
            const Request& request,
            const Socket& socket);
  void send(Message* message);
  Encoder* next(int s);
  void close(int s);
  void exited(const Node& node);
  void exited(ProcessBase* process);
...
{code}

As a result, the slave observers and the master can block calling send()!

Short term, we will try to fix this issue by removing the unnecessary looping. Longer term, it would be nice to avoid all this locking when sending on independent sockets.",3.0,0.21.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.19266055045871558
Bug, PerfEventIsolatorTest.ROOT_CGROUPS_Sample requires 'perf' to be installed,The perf::valid() relies on the 'perf' command being installed. This isn't always the case. Configure should probably check for the perf command exists.,1.0,0,0.5,0.0784313725490196,0.3333333333333333,0.5,0.14285714285714285,0.0,0.0,0.05263157894736842,0.09285714285714286,0.08974358974358974,0.0787878787878788,0.07272727272727272,0.0
Bug,Turning on cgroups_limit_swap effectively disables memory isolation,"Our test runs show that enabling cgroups_limit_swap effectively disables memory isolation altogether.

Per: https://access.redhat.com/documentation/en-US/Red_Hat_Enterprise_Linux/6/html/Resource_Management_Guide/sec-memory.html

""It is important to set the memory.limit_in_bytes parameter before setting the memory.memsw.limit_in_bytes parameter: attempting to do so in the reverse order results in an error. This is because memory.memsw.limit_in_bytes becomes available only after all memory limitations (previously set in memory.limit_in_bytes) are exhausted.""

Looks like the flag sets ""memory.memsw.limit_in_bytes"" if true and ""memory.limit_in_bytes"" if false, but should always set ""memory.limit_in_bytes"" and in addition set ""memory.memsw.limit_in_bytes"" if true. Otherwise the limits won't be set and enforced.

See: https://github.com/apache/mesos/blob/c8598f7f5a24a01b6a68e0f060b79662ee97af89/src/slave/containerizer/isolators/cgroups/mem.cpp#L365
",2.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.10526315789473684,0.16428571428571428,0.01282051282051282,0.0,0.0,0.0
Improvement,killTask() should perform reconciliation for unknown tasks.,"Currently, {{killTask}} uses its own reconciliation logic, which has diverged from the {{reconcileTasks}} logic. Specifically, when the task is unknown and a non-strict registry is in use, {{killTask}} will not send TASK_LOST whereas {{reconcileTask}} will.

We should make these consistent.
",3.0,0,0.5,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.8846153846153846,0.9878787878787878,0.9878787878787878,0.0
Improvement,Correct naming of cgroup memory statistics,"mem_rss_bytes is *not* RSS but is the total memory usage (memory.usage_in_bytes) of the cgroup, including file cache etc. Actual RSS is reported as mem_anon_bytes. These, and others, should be consistently named.",3.0,0.23.0,0.5,0.06787330316742082,0.0,0.0,0.0,0.0,0.0,0.08771929824561403,0.06428571428571428,0.10256410256410256,0.2121212121212121,0.2121212121212121,0.2110091743119266
Improvement,Expose number of processes and threads in a container,"The CFS cpu statistics (cpus_nr_throttled, cpus_nr_periods, cpus_throttled_time) are difficult to interpret.
1) nr_throttled is the number of intervals where *any* throttling occurred
2) throttled_time is the aggregate time *across all runnable tasks* (tasks in the Linux sense).

For example, in a typical 60 second sampling interval: nr_periods = 600, nr_throttled could be 60, i.e., 10% of intervals, but throttled_time could be much higher than (60/600) * 60 = 6 seconds if there is more than one task that is runnable but throttled. *Each* throttled task contributes to the total throttled time.

Small test to demonstrate throttled_time > nr_periods * quota_interval:

5 x {{'openssl speed'}} running with quota=100ms:
{noformat}
cat cpu.stat && sleep 1 && cat cpu.stat
nr_periods 3228
nr_throttled 1276
throttled_time 528843772540
nr_periods 3238
nr_throttled 1286
throttled_time 531668964667
{noformat}
All 10 intervals throttled (100%) for total time of 2.8 seconds in 1 second (""more than 100%"" of the time interval)


It would be helpful to expose the number of processes and tasks in the container cgroup. This would be at a very coarse granularity but would give some guidance.",2.0,0.20.0,0.5,0.06787330316742082,0.0,0.0,0.0,0.0,0.0,0.08771929824561403,0.06428571428571428,0.10256410256410256,0.2121212121212121,0.2121212121212121,0.1834862385321101
Improvement,Implement simple slave recovery behavior for fetcher cache,"Clean the fetcher cache completely upon slave restart/recovery. This implements correct, albeit not ideal behavior. More efficient schemes that restore knowledge about cached files or even resume downloads can be added later. ",2.0,0,0.0,0.06184012066365008,0.0,0.0,0.0,0.0,0.2,0.10526315789473684,0.12857142857142856,0.14102564102564102,0.16969696969696968,0.17575757575757575,0.0
Task,Add optional 'Unavailability' to resource offers to provide maintenance awareness.,"In order to inform frameworks about upcoming maintenance on offered resources, per MESOS-1474, we'd like to add an optional 'Unavailability' information to offers:

{code}
message Interval {
  optional double start = 1; // Time, in seconds since the Epoch.
  optional double duration = 2; // Time, in seconds.
}

message Offer {
  // Existing fields
  ...
 
  // Signifies that the resources in this Offer are part of a planned
  // maintenance schedule in the specified window.  Any tasks launched
  // using these resources may be killed when the window arrives.
  // This field gives additional information about the maintenance.
  // The maintenance may not necessarily start at exactly at this interval,
  // nor last for exactly the duration of this interval.
  optional Interval unavailability = 9;
}
{code}",3.0,0,0.5,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.0
Task,Add InverseOffer to Event/Call API.,"The initial use case for InverseOffer in the framework API will be the maintenance primitives in mesos: MESOS-1474.

One way to add this is to tack it on to the OFFERS Event:

{code}
message Offers {
  repeated Offer offers = 1;
  repeated InverseOffer inverse_offers = 2;
}
{code}",3.0,0,0.5,0.42533936651583715,0.0,0.0,0.0,0.0,0.2,0.22807017543859648,0.3142857142857143,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.0
Task,Add InverseOffer protobuf message.,"InverseOffer was defined as part of the maintenance work in MESOS-1474, design doc here: https://docs.google.com/document/d/16k0lVwpSGVOyxPSyXKmGC-gbNmRlisNEe4p-fAUSojk/edit?usp=sharing

{code}
/**
 * A request to return some resources occupied by a framework.
 */
message InverseOffer {
  required OfferID id = 1;
  required FrameworkID framework_id = 2;

  // A list of resources being requested back from the framework.
  repeated Resource resources = 3;

  // Specified if the resources need to be released from a particular slave.
  optional SlaveID slave_id = 4;

  // The resources in this InverseOffer are part of a planned maintenance
  // schedule in the specified window.  Any tasks running using these
  // resources may be killed when the window arrives.
  optional Interval unavailability = 5;
}
{code}

This ticket is to capture the addition of the InverseOffer protobuf to mesos.proto, the necessary API changes for Event/Call and the language bindings will be tracked separately.",3.0,0,0.5,0.3499245852187029,0.0,0.0,0.0,0.6000000000000001,0.4,1.0,0.8071428571428572,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.0
Task,Deprecate stats.json endpoints for Master and Slave,"With the introduction of the libprocess {{/metrics/snapshot}} endpoint, metrics are now duplicated in the Master and Slave between this and {{stats.json}}. We should deprecate the {{stats.json}} endpoints.

Manual inspection of {{stats.json}} shows that all metrics are now covered by the new endpoint for Master and Slave.",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05263157894736842,0.12142857142857143,0.07692307692307693,0.13333333333333333,0.13333333333333333,0.0
Improvement,Refactor fetcher code in preparation for fetcher cache,"Refactor/rearrange fetcher-related code so that cache functionality can be dropped in. One could do both together in one go. This is splitting up reviews into smaller chunks. It will not immediately be obvious how this change will be used later, but it will look better-factored and still do the exact same thing as before. In particular, a download routine to be reused several times in launcher/fetcher will be factored out and the remainder of fetcher-related code can be moved from the containerizer realm into fetcher.cpp.",1.0,0,0.0,0.06184012066365008,0.0,0.0,0.0,0.0,0.2,0.10526315789473684,0.12857142857142856,0.14102564102564102,0.16969696969696968,0.17575757575757575,0.0
Bug,MesosContainerizerExecuteTest.IoRedirection test is flaky,"Observed this on ASF CI:

{code}
[ RUN      ] MesosContainerizerExecuteTest.IoRedirection
Using temporary directory '/tmp/MesosContainerizerExecuteTest_IoRedirection_PbBn8a'
I1108 00:34:25.820514 30391 containerizer.cpp:100] Using isolation: posix/cpu,posix/mem
I1108 00:34:25.821048 30411 containerizer.cpp:424] Starting container 'test_container' for executor 'executor' of framework ''
I1108 00:34:25.824015 30411 launcher.cpp:137] Forked child with pid '4221' for container 'test_container'
I1108 00:34:25.825438 30408 containerizer.cpp:571] Fetching URIs for container 'test_container' using command '/home/jenkins/jenkins-slave/workspace/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Disable-Java-Disable-Python-Disable-Webui/build/src/mesos-fetcher'
I1108 00:34:25.984254 30419 containerizer.cpp:1117] Executor for container 'test_container' has exited
I1108 00:34:25.984341 30419 containerizer.cpp:946] Destroying container 'test_container'
../../src/tests/containerizer_tests.cpp:487: Failure
Value of: (os::read(path::join(directory, ""stderr""))).get()
  Actual: ""I1108 00:34:25.872990  4224 logging.cpp:177] Logging to STDERR\nthis is stderr\n""
Expected: errMsg + ""\n""
Which is: ""this is stderr\n""
[  FAILED  ] MesosContainerizerExecuteTest.IoRedirection (185 ms)
[----------] 1 test from MesosContainerizerExecuteTest (185 ms total)
{code}",1.0,0.22.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.8846153846153846,0.9878787878787878,0.9878787878787878,0.20183486238532108
Bug,RunState::recover should always recover 'completed',"RunState::recover() will return partial state if it cannot find or open the libprocess pid file. Specifically, it does not recover the 'completed' flag.

However, if the slave has removed the executor (because launch failed or the executor failed to register) the sentinel flag will be set and this fact should be recovered. This ensures that container recovery is not attempted later.

This was discovered when the LinuxLauncher failed to recover because it was asked to recover two containers with the same forkedPid. Investigation showed the executors both OOM'ed before registering, i.e., no libprocess pid file was present. However, the containerizer had detected the OOM, destroyed the container, and notified the slave which cleaned everything up: failing the task and calling removeExecutor (which writes the completed sentinel file.)",1.0,0.20.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.10526315789473684,0.16428571428571428,0.10256410256410256,0.2121212121212121,0.2121212121212121,0.1834862385321101
Bug,AllocatorTest/0.SlaveReregistersFirst is flaky,"{noformat:title=}
[ RUN      ] AllocatorTest/0.SlaveReregistersFirst
Using temporary directory '/tmp/AllocatorTest_0_SlaveReregistersFirst_YPe61d'
I1028 23:48:22.360447 31190 leveldb.cpp:176] Opened db in 2.192575ms
I1028 23:48:22.361253 31190 leveldb.cpp:183] Compacted db in 760753ns
I1028 23:48:22.361320 31190 leveldb.cpp:198] Created db iterator in 22188ns
I1028 23:48:22.361340 31190 leveldb.cpp:204] Seeked to beginning of db in 1950ns
I1028 23:48:22.361351 31190 leveldb.cpp:273] Iterated through 0 keys in the db in 345ns
I1028 23:48:22.361403 31190 replica.cpp:741] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I1028 23:48:22.362185 31217 recover.cpp:437] Starting replica recovery
I1028 23:48:22.362764 31219 recover.cpp:463] Replica is in EMPTY status
I1028 23:48:22.363955 31210 replica.cpp:638] Replica in EMPTY status received a broadcasted recover request
I1028 23:48:22.364320 31217 recover.cpp:188] Received a recover response from a replica in EMPTY status
I1028 23:48:22.364820 31211 recover.cpp:554] Updating replica status to STARTING
I1028 23:48:22.365365 31215 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 418991ns
I1028 23:48:22.365391 31215 replica.cpp:320] Persisted replica status to STARTING
I1028 23:48:22.365617 31217 recover.cpp:463] Replica is in STARTING status
I1028 23:48:22.366328 31206 master.cpp:312] Master 20141028-234822-3193029443-50043-31190 (pietas.apache.org) started on 67.195.81.190:50043
I1028 23:48:22.366377 31206 master.cpp:358] Master only allowing authenticated frameworks to register
I1028 23:48:22.366391 31206 master.cpp:363] Master only allowing authenticated slaves to register
I1028 23:48:22.366402 31206 credentials.hpp:36] Loading credentials for authentication from '/tmp/AllocatorTest_0_SlaveReregistersFirst_YPe61d/credentials'
I1028 23:48:22.366708 31206 master.cpp:392] Authorization enabled
I1028 23:48:22.366886 31209 replica.cpp:638] Replica in STARTING status received a broadcasted recover request
I1028 23:48:22.367311 31208 master.cpp:120] No whitelist given. Advertising offers for all slaves
I1028 23:48:22.367312 31207 recover.cpp:188] Received a recover response from a replica in STARTING status
I1028 23:48:22.367686 31211 hierarchical_allocator_process.hpp:299] Initializing hierarchical allocator process with master : master@67.195.81.190:50043
I1028 23:48:22.367863 31212 recover.cpp:554] Updating replica status to VOTING
I1028 23:48:22.368477 31218 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 375527ns
I1028 23:48:22.368505 31218 replica.cpp:320] Persisted replica status to VOTING
I1028 23:48:22.368517 31204 master.cpp:1242] The newly elected leader is master@67.195.81.190:50043 with id 20141028-234822-3193029443-50043-31190
I1028 23:48:22.368549 31204 master.cpp:1255] Elected as the leading master!
I1028 23:48:22.368567 31204 master.cpp:1073] Recovering from registrar
I1028 23:48:22.368621 31215 recover.cpp:568] Successfully joined the Paxos group
I1028 23:48:22.368716 31219 registrar.cpp:313] Recovering registrar
I1028 23:48:22.369000 31215 recover.cpp:452] Recover process terminated
I1028 23:48:22.369523 31208 log.cpp:656] Attempting to start the writer
I1028 23:48:22.370909 31205 replica.cpp:474] Replica received implicit promise request with proposal 1
I1028 23:48:22.371266 31205 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 325016ns
I1028 23:48:22.371290 31205 replica.cpp:342] Persisted promised to 1
I1028 23:48:22.371979 31218 coordinator.cpp:230] Coordinator attemping to fill missing position
I1028 23:48:22.373378 31210 replica.cpp:375] Replica received explicit promise request for position 0 with proposal 2
I1028 23:48:22.373746 31210 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 329018ns
I1028 23:48:22.373772 31210 replica.cpp:676] Persisted action at 0
I1028 23:48:22.374897 31214 replica.cpp:508] Replica received write request for position 0
I1028 23:48:22.374951 31214 leveldb.cpp:438] Reading position from leveldb took 26002ns
I1028 23:48:22.375272 31214 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 289094ns
I1028 23:48:22.375298 31214 replica.cpp:676] Persisted action at 0
I1028 23:48:22.375886 31204 replica.cpp:655] Replica received learned notice for position 0
I1028 23:48:22.376258 31204 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 346650ns
I1028 23:48:22.376277 31204 replica.cpp:676] Persisted action at 0
I1028 23:48:22.376298 31204 replica.cpp:661] Replica learned NOP action at position 0
I1028 23:48:22.376843 31215 log.cpp:672] Writer started with ending position 0
I1028 23:48:22.378056 31205 leveldb.cpp:438] Reading position from leveldb took 28265ns
I1028 23:48:22.380323 31217 registrar.cpp:346] Successfully fetched the registry (0B) in 11.55584ms
I1028 23:48:22.380466 31217 registrar.cpp:445] Applied 1 operations in 50632ns; attempting to update the 'registry'
I1028 23:48:22.382472 31217 log.cpp:680] Attempting to append 139 bytes to the log
I1028 23:48:22.382715 31210 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I1028 23:48:22.383463 31210 replica.cpp:508] Replica received write request for position 1
I1028 23:48:22.383857 31210 leveldb.cpp:343] Persisting action (158 bytes) to leveldb took 363758ns
I1028 23:48:22.383875 31210 replica.cpp:676] Persisted action at 1
I1028 23:48:22.384397 31218 replica.cpp:655] Replica received learned notice for position 1
I1028 23:48:22.384840 31218 leveldb.cpp:343] Persisting action (160 bytes) to leveldb took 420161ns
I1028 23:48:22.384862 31218 replica.cpp:676] Persisted action at 1
I1028 23:48:22.384882 31218 replica.cpp:661] Replica learned APPEND action at position 1
I1028 23:48:22.385684 31211 registrar.cpp:490] Successfully updated the 'registry' in 5.158144ms
I1028 23:48:22.385818 31211 registrar.cpp:376] Successfully recovered registrar
I1028 23:48:22.385912 31214 log.cpp:699] Attempting to truncate the log to 1
I1028 23:48:22.386101 31218 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2
I1028 23:48:22.386124 31211 master.cpp:1100] Recovered 0 slaves from the Registry (101B) ; allowing 10mins for slaves to re-register
I1028 23:48:22.387398 31209 replica.cpp:508] Replica received write request for position 2
I1028 23:48:22.387758 31209 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 334969ns
I1028 23:48:22.387776 31209 replica.cpp:676] Persisted action at 2
I1028 23:48:22.388272 31204 replica.cpp:655] Replica received learned notice for position 2
I1028 23:48:22.388453 31204 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 159390ns
I1028 23:48:22.388501 31204 leveldb.cpp:401] Deleting ~1 keys from leveldb took 30409ns
I1028 23:48:22.388516 31204 replica.cpp:676] Persisted action at 2
I1028 23:48:22.388531 31204 replica.cpp:661] Replica learned TRUNCATE action at position 2
I1028 23:48:22.400737 31207 slave.cpp:169] Slave started on 34)@67.195.81.190:50043
I1028 23:48:22.400786 31207 credentials.hpp:84] Loading credential for authentication from '/tmp/AllocatorTest_0_SlaveReregistersFirst_QPPV21/credential'
I1028 23:48:22.400996 31207 slave.cpp:276] Slave using credential for: test-principal
I1028 23:48:22.401304 31207 slave.cpp:289] Slave resources: cpus(*):2; mem(*):1024; disk(*):3.70122e+06; ports(*):[31000-32000]
I1028 23:48:22.401413 31207 slave.cpp:318] Slave hostname: pietas.apache.org
I1028 23:48:22.401520 31207 slave.cpp:319] Slave checkpoint: false
W1028 23:48:22.401535 31207 slave.cpp:321] Disabling checkpointing is deprecated and the --checkpoint flag will be removed in a future release. Please avoid using this flag
I1028 23:48:22.402349 31207 state.cpp:33] Recovering state from '/tmp/AllocatorTest_0_SlaveReregistersFirst_QPPV21/meta'
I1028 23:48:22.402678 31207 status_update_manager.cpp:197] Recovering status update manager
I1028 23:48:22.403048 31211 slave.cpp:3456] Finished recovery
I1028 23:48:22.403815 31215 slave.cpp:602] New master detected at master@67.195.81.190:50043
I1028 23:48:22.403852 31215 slave.cpp:665] Authenticating with master master@67.195.81.190:50043
I1028 23:48:22.403875 31206 status_update_manager.cpp:171] Pausing sending status updates
I1028 23:48:22.403961 31215 slave.cpp:638] Detecting new master
I1028 23:48:22.404016 31211 authenticatee.hpp:133] Creating new client SASL connection
I1028 23:48:22.404230 31204 master.cpp:3853] Authenticating slave(34)@67.195.81.190:50043
I1028 23:48:22.404464 31205 authenticator.hpp:161] Creating new server SASL connection
I1028 23:48:22.404613 31211 authenticatee.hpp:224] Received SASL authentication mechanisms: CRAM-MD5
I1028 23:48:22.404649 31211 authenticatee.hpp:250] Attempting to authenticate with mechanism 'CRAM-MD5'
I1028 23:48:22.404734 31211 authenticator.hpp:267] Received SASL authentication start
I1028 23:48:22.404783 31211 authenticator.hpp:389] Authentication requires more steps
I1028 23:48:22.404898 31215 authenticatee.hpp:270] Received SASL authentication step
I1028 23:48:22.404999 31215 authenticator.hpp:295] Received SASL authentication step
I1028 23:48:22.405030 31215 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pietas.apache.org' server FQDN: 'pietas.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I1028 23:48:22.405047 31215 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I1028 23:48:22.405086 31215 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I1028 23:48:22.405109 31215 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pietas.apache.org' server FQDN: 'pietas.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I1028 23:48:22.405122 31215 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I1028 23:48:22.405129 31215 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I1028 23:48:22.405146 31215 authenticator.hpp:381] Authentication success
I1028 23:48:22.405243 31213 authenticatee.hpp:310] Authentication success
I1028 23:48:22.405253 31214 master.cpp:3893] Successfully authenticated principal 'test-principal' at slave(34)@67.195.81.190:50043
I1028 23:48:22.405505 31213 slave.cpp:722] Successfully authenticated with master master@67.195.81.190:50043
I1028 23:48:22.405619 31213 slave.cpp:1050] Will retry registration in 17.050994ms if necessary
I1028 23:48:22.405819 31215 master.cpp:3032] Registering slave at slave(34)@67.195.81.190:50043 (pietas.apache.org) with id 20141028-234822-3193029443-50043-31190-S0
I1028 23:48:22.406262 31216 registrar.cpp:445] Applied 1 operations in 52647ns; attempting to update the 'registry'
I1028 23:48:22.406697 31190 sched.cpp:137] Version: 0.21.0
I1028 23:48:22.407083 31211 sched.cpp:233] New master detected at master@67.195.81.190:50043
I1028 23:48:22.407114 31211 sched.cpp:283] Authenticating with master master@67.195.81.190:50043
I1028 23:48:22.407290 31214 authenticatee.hpp:133] Creating new client SASL connection
I1028 23:48:22.407424 31214 master.cpp:3853] Authenticating scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043
I1028 23:48:22.407659 31207 authenticator.hpp:161] Creating new server SASL connection
I1028 23:48:22.407757 31207 authenticatee.hpp:224] Received SASL authentication mechanisms: CRAM-MD5
I1028 23:48:22.407774 31207 authenticatee.hpp:250] Attempting to authenticate with mechanism 'CRAM-MD5'
I1028 23:48:22.407830 31207 authenticator.hpp:267] Received SASL authentication start
I1028 23:48:22.407868 31207 authenticator.hpp:389] Authentication requires more steps
I1028 23:48:22.407927 31207 authenticatee.hpp:270] Received SASL authentication step
I1028 23:48:22.408015 31212 authenticator.hpp:295] Received SASL authentication step
I1028 23:48:22.408037 31212 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pietas.apache.org' server FQDN: 'pietas.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I1028 23:48:22.408046 31212 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I1028 23:48:22.408072 31212 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I1028 23:48:22.408092 31212 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pietas.apache.org' server FQDN: 'pietas.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I1028 23:48:22.408100 31212 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I1028 23:48:22.408105 31212 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I1028 23:48:22.408116 31212 authenticator.hpp:381] Authentication success
I1028 23:48:22.408192 31210 authenticatee.hpp:310] Authentication success
I1028 23:48:22.408210 31217 master.cpp:3893] Successfully authenticated principal 'test-principal' at scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043
I1028 23:48:22.408419 31210 sched.cpp:357] Successfully authenticated with master master@67.195.81.190:50043
I1028 23:48:22.408460 31210 sched.cpp:476] Sending registration request to master@67.195.81.190:50043
I1028 23:48:22.408568 31217 master.cpp:1362] Received registration request for framework 'default' at scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043
I1028 23:48:22.408617 31217 master.cpp:1321] Authorizing framework principal 'test-principal' to receive offers for role '*'
I1028 23:48:22.408937 31214 master.cpp:1426] Registering framework 20141028-234822-3193029443-50043-31190-0000 (default) at scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043
I1028 23:48:22.409265 31213 sched.cpp:407] Framework registered with 20141028-234822-3193029443-50043-31190-0000
I1028 23:48:22.409267 31212 hierarchical_allocator_process.hpp:329] Added framework 20141028-234822-3193029443-50043-31190-0000
I1028 23:48:22.409312 31212 hierarchical_allocator_process.hpp:697] No resources available to allocate!
I1028 23:48:22.409324 31215 log.cpp:680] Attempting to append 316 bytes to the log
I1028 23:48:22.409333 31213 sched.cpp:421] Scheduler::registered took 38591ns
I1028 23:48:22.409327 31212 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 24107ns
I1028 23:48:22.409518 31205 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3
I1028 23:48:22.410127 31206 replica.cpp:508] Replica received write request for position 3
I1028 23:48:22.410706 31206 leveldb.cpp:343] Persisting action (335 bytes) to leveldb took 554098ns
I1028 23:48:22.410725 31206 replica.cpp:676] Persisted action at 3
I1028 23:48:22.411151 31217 replica.cpp:655] Replica received learned notice for position 3
I1028 23:48:22.411499 31217 leveldb.cpp:343] Persisting action (337 bytes) to leveldb took 326572ns
I1028 23:48:22.411519 31217 replica.cpp:676] Persisted action at 3
I1028 23:48:22.411533 31217 replica.cpp:661] Replica learned APPEND action at position 3
I1028 23:48:22.412292 31219 registrar.cpp:490] Successfully updated the 'registry' in 5.972992ms
I1028 23:48:22.412518 31218 log.cpp:699] Attempting to truncate the log to 3
I1028 23:48:22.412621 31213 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4
I1028 23:48:22.412734 31219 slave.cpp:2522] Received ping from slave-observer(38)@67.195.81.190:50043
I1028 23:48:22.412787 31206 master.cpp:3086] Registered slave 20141028-234822-3193029443-50043-31190-S0 at slave(34)@67.195.81.190:50043 (pietas.apache.org) with cpus(*):2; mem(*):1024; disk(*):3.70122e+06; ports(*):[31000-32000]
I1028 23:48:22.412858 31219 slave.cpp:756] Registered with master master@67.195.81.190:50043; given slave ID 20141028-234822-3193029443-50043-31190-S0
I1028 23:48:22.412994 31210 status_update_manager.cpp:178] Resuming sending status updates
I1028 23:48:22.413014 31211 hierarchical_allocator_process.hpp:442] Added slave 20141028-234822-3193029443-50043-31190-S0 (pietas.apache.org) with cpus(*):2; mem(*):1024; disk(*):3.70122e+06; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):3.70122e+06; ports(*):[31000-32000] available)
I1028 23:48:22.413159 31211 hierarchical_allocator_process.hpp:734] Offering cpus(*):2; mem(*):1024; disk(*):3.70122e+06; ports(*):[31000-32000] on slave 20141028-234822-3193029443-50043-31190-S0 to framework 20141028-234822-3193029443-50043-31190-0000
I1028 23:48:22.413290 31208 replica.cpp:508] Replica received write request for position 4
I1028 23:48:22.413421 31211 hierarchical_allocator_process.hpp:679] Performed allocation for slave 20141028-234822-3193029443-50043-31190-S0 in 346658ns
I1028 23:48:22.413650 31208 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 336067ns
I1028 23:48:22.413668 31208 replica.cpp:676] Persisted action at 4
I1028 23:48:22.413797 31216 master.cpp:3795] Sending 1 offers to framework 20141028-234822-3193029443-50043-31190-0000 (default) at scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043
I1028 23:48:22.414077 31212 replica.cpp:655] Replica received learned notice for position 4
I1028 23:48:22.414356 31212 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 260401ns
I1028 23:48:22.414403 31212 leveldb.cpp:401] Deleting ~2 keys from leveldb took 28541ns
I1028 23:48:22.414417 31212 replica.cpp:676] Persisted action at 4
I1028 23:48:22.414446 31212 replica.cpp:661] Replica learned TRUNCATE action at position 4
I1028 23:48:22.414422 31207 sched.cpp:544] Scheduler::resourceOffers took 310278ns
I1028 23:48:22.415086 31214 master.cpp:2321] Processing reply for offers: [ 20141028-234822-3193029443-50043-31190-O0 ] on slave 20141028-234822-3193029443-50043-31190-S0 at slave(34)@67.195.81.190:50043 (pietas.apache.org) for framework 20141028-234822-3193029443-50043-31190-0000 (default) at scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043
W1028 23:48:22.415163 31214 master.cpp:1969] Executor default for task 0 uses less CPUs (None) than the minimum required (0.01). Please update your executor, as this will be mandatory in future releases.
W1028 23:48:22.415186 31214 master.cpp:1980] Executor default for task 0 uses less memory (None) than the minimum required (32MB). Please update your executor, as this will be mandatory in future releases.
I1028 23:48:22.415256 31214 master.cpp:2417] Authorizing framework principal 'test-principal' to launch task 0 as user 'jenkins'
I1028 23:48:22.416033 31219 master.hpp:877] Adding task 0 with resources cpus(*):1; mem(*):500 on slave 20141028-234822-3193029443-50043-31190-S0 (pietas.apache.org)
I1028 23:48:22.416084 31219 master.cpp:2480] Launching task 0 of framework 20141028-234822-3193029443-50043-31190-0000 (default) at scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043 with resources cpus(*):1; mem(*):500 on slave 20141028-234822-3193029443-50043-31190-S0 at slave(34)@67.195.81.190:50043 (pietas.apache.org)
I1028 23:48:22.416317 31214 slave.cpp:1081] Got assigned task 0 for framework 20141028-234822-3193029443-50043-31190-0000
I1028 23:48:22.416679 31215 hierarchical_allocator_process.hpp:563] Recovered cpus(*):1; mem(*):524; disk(*):3.70122e+06; ports(*):[31000-32000] (total allocatable: cpus(*):1; mem(*):524; disk(*):3.70122e+06; ports(*):[31000-32000]) on slave 20141028-234822-3193029443-50043-31190-S0 from framework 20141028-234822-3193029443-50043-31190-0000
I1028 23:48:22.416721 31215 hierarchical_allocator_process.hpp:599] Framework 20141028-234822-3193029443-50043-31190-0000 filtered slave 20141028-234822-3193029443-50043-31190-S0 for 5secs
I1028 23:48:22.416724 31214 slave.cpp:1191] Launching task 0 for framework 20141028-234822-3193029443-50043-31190-0000
I1028 23:48:22.418534 31214 slave.cpp:3871] Launching executor default of framework 20141028-234822-3193029443-50043-31190-0000 in work directory '/tmp/AllocatorTest_0_SlaveReregistersFirst_QPPV21/slaves/20141028-234822-3193029443-50043-31190-S0/frameworks/20141028-234822-3193029443-50043-31190-0000/executors/default/runs/d593f433-3c16-4678-8f76-4038fe2841c4'
I1028 23:48:22.420557 31214 exec.cpp:132] Version: 0.21.0
I1028 23:48:22.420755 31213 exec.cpp:182] Executor started at: executor(22)@67.195.81.190:50043 with pid 31190
I1028 23:48:22.420903 31214 slave.cpp:1317] Queuing task '0' for executor default of framework '20141028-234822-3193029443-50043-31190-0000
I1028 23:48:22.420997 31214 slave.cpp:555] Successfully attached file '/tmp/AllocatorTest_0_SlaveReregistersFirst_QPPV21/slaves/20141028-234822-3193029443-50043-31190-S0/frameworks/20141028-234822-3193029443-50043-31190-0000/executors/default/runs/d593f433-3c16-4678-8f76-4038fe2841c4'
I1028 23:48:22.421058 31214 slave.cpp:1849] Got registration for executor 'default' of framework 20141028-234822-3193029443-50043-31190-0000 from executor(22)@67.195.81.190:50043
I1028 23:48:22.421295 31214 slave.cpp:1968] Flushing queued task 0 for executor 'default' of framework 20141028-234822-3193029443-50043-31190-0000
I1028 23:48:22.421391 31205 exec.cpp:206] Executor registered on slave 20141028-234822-3193029443-50043-31190-S0
I1028 23:48:22.421495 31214 slave.cpp:2802] Monitoring executor 'default' of framework '20141028-234822-3193029443-50043-31190-0000' in container 'd593f433-3c16-4678-8f76-4038fe2841c4'
I1028 23:48:22.422873 31205 exec.cpp:218] Executor::registered took 19148ns
I1028 23:48:22.422991 31205 exec.cpp:293] Executor asked to run task '0'
I1028 23:48:22.423085 31205 exec.cpp:302] Executor::launchTask took 76519ns
I1028 23:48:22.424541 31205 exec.cpp:525] Executor sending status update TASK_RUNNING (UUID: 10174aa0-0e5a-4f9d-a530-dee64e93f222) for task 0 of framework 20141028-234822-3193029443-50043-31190-0000
I1028 23:48:22.424724 31205 slave.cpp:2202] Handling status update TASK_RUNNING (UUID: 10174aa0-0e5a-4f9d-a530-dee64e93f222) for task 0 of framework 20141028-234822-3193029443-50043-31190-0000 from executor(22)@67.195.81.190:50043
I1028 23:48:22.424932 31213 status_update_manager.cpp:317] Received status update TASK_RUNNING (UUID: 10174aa0-0e5a-4f9d-a530-dee64e93f222) for task 0 of framework 20141028-234822-3193029443-50043-31190-0000
I1028 23:48:22.424963 31213 status_update_manager.cpp:494] Creating StatusUpdate stream for task 0 of framework 20141028-234822-3193029443-50043-31190-0000
I1028 23:48:22.425122 31213 status_update_manager.cpp:371] Forwarding update TASK_RUNNING (UUID: 10174aa0-0e5a-4f9d-a530-dee64e93f222) for task 0 of framework 20141028-234822-3193029443-50043-31190-0000 to the slave
I1028 23:48:22.425257 31205 slave.cpp:2442] Forwarding the update TASK_RUNNING (UUID: 10174aa0-0e5a-4f9d-a530-dee64e93f222) for task 0 of framework 20141028-234822-3193029443-50043-31190-0000 to master@67.195.81.190:50043
I1028 23:48:22.425398 31205 slave.cpp:2369] Status update manager successfully handled status update TASK_RUNNING (UUID: 10174aa0-0e5a-4f9d-a530-dee64e93f222) for task 0 of framework 20141028-234822-3193029443-50043-31190-0000
I1028 23:48:22.425420 31205 slave.cpp:2375] Sending acknowledgement for status update TASK_RUNNING (UUID: 10174aa0-0e5a-4f9d-a530-dee64e93f222) for task 0 of framework 20141028-234822-3193029443-50043-31190-0000 to executor(22)@67.195.81.190:50043
I1028 23:48:22.425583 31212 master.cpp:3410] Forwarding status update TASK_RUNNING (UUID: 10174aa0-0e5a-4f9d-a530-dee64e93f222) for task 0 of framework 20141028-234822-3193029443-50043-31190-0000
I1028 23:48:22.425621 31206 exec.cpp:339] Executor received status update acknowledgement 10174aa0-0e5a-4f9d-a530-dee64e93f222 for task 0 of framework 20141028-234822-3193029443-50043-31190-0000
I1028 23:48:22.425786 31212 master.cpp:3382] Status update TASK_RUNNING (UUID: 10174aa0-0e5a-4f9d-a530-dee64e93f222) for task 0 of framework 20141028-234822-3193029443-50043-31190-0000 from slave 20141028-234822-3193029443-50043-31190-S0 at slave(34)@67.195.81.190:50043 (pietas.apache.org)
I1028 23:48:22.425832 31212 master.cpp:4617] Updating the latest state of task 0 of framework 20141028-234822-3193029443-50043-31190-0000 to TASK_RUNNING
I1028 23:48:22.425885 31208 sched.cpp:635] Scheduler::statusUpdate took 49727ns
I1028 23:48:22.426082 31208 master.cpp:2882] Forwarding status update acknowledgement 10174aa0-0e5a-4f9d-a530-dee64e93f222 for task 0 of framework 20141028-234822-3193029443-50043-31190-0000 (default) at scheduler-0aa33fc7-0d29-487c-80eb-f933681f9c95@67.195.81.190:50043 to slave 20141028-234822-3193029443-50043-31190-S0 at slave(34)@67.195.81.190:50043 (pietas.apache.org)
I1028 23:48:22.426360 31206 status_update_manager.cpp:389] Received status update acknowledgement (UUID: 10174aa0-0e5a-4f9d-a530-dee64e93f222) for task 0 of framework 20141028-234822-3193029443-50043-31190-0000
I1028 23:48:22.426623 31206 slave.cpp:1789] Status update manager successfully handled status update acknowledgement (UUID: 10174aa0-0e5a-4f9d-a530-dee64e93f222) for task 0 of framework 20141028-234822-3193029443-50043-31190-0000
I1028 23:48:22.426893 31210 master.cpp:677] Master terminating
W1028 23:48:22.427028 31210 master.cpp:4662] Removing task 0 with resources cpus(*):1; mem(*):500 of framework 20141028-234822-3193029443-50043-31190-0000 on slave 20141028-234822-3193029443-50043-31190-S0 at slave(34)@67.195.81.190:50043 (pietas.apache.org) in non-terminal state TASK_RUNNING
I1028 23:48:22.427397 31209 hierarchical_allocator_process.hpp:563] Recovered cpus(*):1; mem(*):500 (total allocatable: cpus(*):2; mem(*):1024; disk(*):3.70122e+06; ports(*):[31000-32000]) on slave 20141028-234822-3193029443-50043-31190-S0 from framework 20141028-234822-3193029443-50043-31190-0000
I1028 23:48:22.427512 31210 master.cpp:4705] Removing executor 'default' with resources  of framework 20141028-234822-3193029443-50043-31190-0000 on slave 20141028-234822-3193029443-50043-31190-S0 at slave(34)@67.195.81.190:50043 (pietas.apache.org)
I1028 23:48:22.428129 31206 slave.cpp:2607] master@67.195.81.190:50043 exited
W1028 23:48:22.428153 31206 slave.cpp:2610] Master disconnected! Waiting for a new master to be elected
I1028 23:48:22.434645 31190 leveldb.cpp:176] Opened db in 2.551453ms
I1028 23:48:22.437157 31190 leveldb.cpp:183] Compacted db in 2.484612ms
I1028 23:48:22.437203 31190 leveldb.cpp:198] Created db iterator in 19171ns
I1028 23:48:22.437235 31190 leveldb.cpp:204] Seeked to beginning of db in 18300ns
I1028 23:48:22.437306 31190 leveldb.cpp:273] Iterated through 3 keys in the db in 59465ns
I1028 23:48:22.437347 31190 replica.cpp:741] Replica recovered with log positions 3 -> 4 with 0 holes and 0 unlearned
I1028 23:48:22.437827 31216 recover.cpp:437] Starting replica recovery
I1028 23:48:22.438127 31216 recover.cpp:463] Replica is in VOTING status
I1028 23:48:22.438443 31216 recover.cpp:452] Recover process terminated
I1028 23:48:22.439877 31212 master.cpp:312] Master 20141028-234822-3193029443-50043-31190 (pietas.apache.org) started on 67.195.81.190:50043
I1028 23:48:22.439916 31212 master.cpp:358] Master only allowing authenticated frameworks to register
I1028 23:48:22.439931 31212 master.cpp:363] Master only allowing authenticated slaves to register
I1028 23:48:22.439946 31212 credentials.hpp:36] Loading credentials for authentication from '/tmp/AllocatorTest_0_SlaveReregistersFirst_YPe61d/credentials'
I1028 23:48:22.440142 31212 master.cpp:392] Authorization enabled
I1028 23:48:22.440439 31218 master.cpp:120] No whitelist given. Advertising offers for all slaves
I1028 23:48:22.440901 31213 hierarchical_allocator_process.hpp:299] Initializing hierarchical allocator process with master : master@67.195.81.190:50043
I1028 23:48:22.441395 31206 master.cpp:1242] The newly elected leader is master@67.195.81.190:50043 with id 20141028-234822-3193029443-50043-31190
I1028 23:48:22.441421 31206 master.cpp:1255] Elected as the leading master!
I1028 23:48:22.441457 31206 master.cpp:1073] Recovering from registrar
I1028 23:48:22.441623 31205 registrar.cpp:313] Recovering registrar
I1028 23:48:22.442172 31219 log.cpp:656] Attempting to start the writer
I1028 23:48:22.443235 31219 replica.cpp:474] Replica received implicit promise request with proposal 2
I1028 23:48:22.443685 31219 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 427888ns
I1028 23:48:22.443703 31219 replica.cpp:342] Persisted promised to 2
I1028 23:48:22.444371 31213 coordinator.cpp:230] Coordinator attemping to fill missing position
I1028 23:48:22.444687 31209 log.cpp:672] Writer started with ending position 4
I1028 23:48:22.445754 31215 leveldb.cpp:438] Reading position from leveldb took 47909ns
I1028 23:48:22.445826 31215 leveldb.cpp:438] Reading position from leveldb took 30611ns
I1028 23:48:22.446941 31218 registrar.cpp:346] Successfully fetched the registry (277B) in 5.213184ms
I1028 23:48:22.447118 31218 registrar.cpp:445] Applied 1 operations in 42362ns; attempting to update the 'registry'
I1028 23:48:22.449329 31204 log.cpp:680] Attempting to append 316 bytes to the log
I1028 23:48:22.449477 31218 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 5
I1028 23:48:22.450187 31215 replica.cpp:508] Replica received write request for position 5
I1028 23:48:22.450767 31215 leveldb.cpp:343] Persisting action (335 bytes) to leveldb took 554400ns
I1028 23:48:22.450788 31215 replica.cpp:676] Persisted action at 5
I1028 23:48:22.451561 31215 replica.cpp:655] Replica received learned notice for position 5
I1028 23:48:22.451979 31215 leveldb.cpp:343] Persisting action (337 bytes) to leveldb took 397219ns
I1028 23:48:22.452000 31215 replica.cpp:676] Persisted action at 5
I1028 23:48:22.452020 31215 replica.cpp:661] Replica learned APPEND action at position 5
I1028 23:48:22.452993 31213 registrar.cpp:490] Successfully updated the 'registry' in 5.816832ms
I1028 23:48:22.453136 31213 registrar.cpp:376] Successfully recovered registrar
I1028 23:48:22.453238 31208 log.cpp:699] Attempting to truncate the log to 5
I1028 23:48:22.453384 31214 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 6
I1028 23:48:22.453518 31215 master.cpp:1100] Recovered 1 slaves from the Registry (277B) ; allowing 10mins for slaves to re-register
I1028 23:48:22.454116 31207 replica.cpp:508] Replica received write request for position 6
I1028 23:48:22.454570 31207 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 427424ns
I1028 23:48:22.454589 31207 replica.cpp:676] Persisted action at 6
I1028 23:48:22.455095 31219 replica.cpp:655] Replica received learned notice for position 6
I1028 23:48:22.455399 31219 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 282466ns
I1028 23:48:22.455462 31219 leveldb.cpp:401] Deleting ~2 keys from leveldb took 43939ns
I1028 23:48:22.455478 31219 replica.cpp:676] Persisted action at 6
I1028 23:48:22.455494 31219 replica.cpp:661] Replica learned TRUNCATE action at position 6
I1028 23:48:22.465553 31213 status_update_manager.cpp:171] Pausing sending status updates
I1028 23:48:22.465566 31216 slave.cpp:602] New master detected at master@67.195.81.190:50043
I1028 23:48:22.465612 31216 slave.cpp:665] Authenticating with master master@67.195.81.190:50043
I1028 23:48:23.441506 31206 hierarchical_allocator_process.hpp:697] No resources available to allocate!
I1028 23:48:27.441004 31214 master.cpp:120] No whitelist given. Advertising offers for all slaves
I1028 23:48:30.101379 31206 hierarchical_allocator_process.hpp:659] Performed allocation for 0 slaves in 6.659877806secs
I1028 23:48:30.101568 31216 slave.cpp:638] Detecting new master
I1028 23:48:30.101632 31214 authenticatee.hpp:133] Creating new client SASL connection
I1028 23:48:30.102021 31218 master.cpp:3853] Authenticating slave(34)@67.195.81.190:50043
I1028 23:48:30.102329 31212 authenticator.hpp:161] Creating new server SASL connection
I1028 23:48:30.102505 31216 authenticatee.hpp:224] Received SASL authentication mechanisms: CRAM-MD5
I1028 23:48:30.102545 31216 authenticatee.hpp:250] Attempting to authenticate with mechanism 'CRAM-MD5'
I1028 23:48:30.102638 31216 authenticator.hpp:267] Received SASL authentication start
I1028 23:48:30.102709 31216 authenticator.hpp:389] Authentication requires more steps
I1028 23:48:30.102812 31216 authenticatee.hpp:270] Received SASL authentication step
I1028 23:48:30.102957 31204 authenticator.hpp:295] Received SASL authentication step
I1028 23:48:30.102982 31204 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'pietas.apache.org' server FQDN: 'pietas.apache.org' SASL_",2.0,0.21.0,0.5,0.3695324283559578,0.0,0.0,0.0,0.0,0.0,0.47368421052631576,0.6785714285714286,0.20512820512820512,0.13939393939393938,0.13939393939393938,0.19266055045871558
Bug,Container network stats reported by the port mapping isolator is the reverse of the actual network stats.,"Looks like the TX/RX network stats reported is the reverse of the actual network stats. The reason is because we simply get TX/RX data from veth on the host.

Since veth pair is a tunnel, the ingress of veth on host is the egress of eth0 in container (and vice versa). Therefore, we need to flip the data we got from veth.

{noformat}
[jyu@... ~]$ sudo ip netns exec 24926 /sbin/ip -s link show dev eth0
2: eth0: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP mode DEFAULT qlen 1000
    link/ether f0:4d:a2:75:74:05 brd ff:ff:ff:ff:ff:ff
    RX: bytes  packets  errors  dropped overrun mcast   
    46030857691178 12561038581 0       0       0       0      
    TX: bytes  packets  errors  dropped carrier collsns 
    29792886058561 15036798198 0       0       0       0      
[jyu@... ~]$ ip -s link show dev mesos24926
7412: mesos24926: <BROADCAST,MULTICAST,UP,LOWER_UP> mtu 1500 qdisc pfifo_fast state UP mode DEFAULT qlen 1000
    link/ether f0:4d:a2:75:74:05 brd ff:ff:ff:ff:ff:ff
    RX: bytes  packets  errors  dropped overrun mcast   
    29793066979551 15036894749 0       0       0       0      
    TX: bytes  packets  errors  dropped carrier collsns 
    46031126366116 12561113732 0       0       0       0
{noformat}",1.0,0.20.1,0.5,0.06787330316742082,0.0,0.0,0.0,0.0,0.0,0.08771929824561403,0.06428571428571428,0.7948717948717948,1.0,1.0,0.18357798165137615
Bug,slave and offer ids are indistinguishable in the logs,It is currently impossible to tell slave ids and offer ids apart when looking at logs. Adding some differentiator will make log reading a little simpler.,1.0,0.20.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05263157894736842,0.12142857142857143,0.07692307692307693,0.13333333333333333,0.13333333333333333,0.1834862385321101
Bug,RBT only takes revision ranges as args for versions >= 0.6,the {{support/post-reviews.py}} script doesn't differentiate between RBT versions although the calling conventions for passing revision ranges are different. ,1.0,0.20.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05263157894736842,0.12142857142857143,0.07692307692307693,0.13333333333333333,0.13333333333333333,0.1834862385321101
Bug,Test RoutingTest.INETSockets fails on some machine,"{noformat}
[ RUN      ] RoutingTest.INETSockets
../../../mesos/src/tests/routing_tests.cpp:238: Failure
infos: Input data out of range
ABORT: (../../../mesos/3rdparty/libprocess/3rdparty/stout/include/stout/try.hpp:92): Try::get() but state == ERROR: Input data out of range*** Aborted at 1414000937 (unix time) try ""date -d @1414000937"" if you are using GNU date ***
PC: @     0x7f2c2d509fc5 __GI_raise
*** SIGABRT (@0x1b49000040b1) received by PID 16561 (TID 0x7f2c31031720) from PID 16561; stack trace: ***
    @     0x7f2c2f0d4ca0 (unknown)
    @     0x7f2c2d509fc5 __GI_raise
    @     0x7f2c2d50ba70 __GI_abort
    @           0x4cf782 _Abort()
    @           0x4cf7bc _Abort()
    @           0x99459e RoutingTest_INETSockets_Test::TestBody()
    @           0xa1c363 testing::internal::HandleExceptionsInMethodIfSupported<>()
    @           0xa13617 testing::Test::Run()
    @           0xa136be testing::TestInfo::Run()
    @           0xa137c5 testing::TestCase::Run()
    @           0xa13a68 testing::internal::UnitTestImpl::RunAllTests()
    @           0xa13cf7 testing::UnitTest::Run()
    @           0x49bc4b main
    @     0x7f2c2d4f79f4 __libc_start_main
    @           0x4aad79 (unknown)
make[3]: *** [check-local] Aborted
{noformat}",2.0,0,0.5,0.06787330316742082,0.0,0.0,0.0,0.0,0.0,0.08771929824561403,0.06428571428571428,0.7948717948717948,1.0,1.0,0.0
Task,Add event queue size metrics to scheduler driver,"In the master process, we expose metrics for event queue sizes for various event types. We should do the same for the scheduler driver process.",2.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05263157894736842,0.12142857142857143,0.07692307692307693,0.13333333333333333,0.13333333333333333,0.0
Improvement,Make executor's user owner of executor's cgroup directory,"Currently, when cgroups are enabled, and executor is spawned, it's mounted under, for ex: /sys/fs/cgroup/cpu/mesos/<mesos-id>. This directory in current implementation is only writable by root user. This prevents process launched by executor to mount its child processes under this cgroup, because the cgroup directory is only writable by root.

To enable a executor spawned process to mount it's child processes under it's cgroup directory, the cgroup directory should be made writable by the user which spawns the executor.",3.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.10526315789473684,0.16428571428571428,0.0,0.0,0.0,0.0
Bug,os::killtree() incorrectly returns early if pid has terminated,"If groups == true and/or sessions == true then os::killtree() should continue to signal all processes in the process group and/or session, even if the leading pid has terminated.",2.0,"0.18.0,0.18.1,0.18.2,0.19.0,0.19.1,0.20.0,0.20.1",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.10526315789473684,0.16428571428571428,0.10256410256410256,0.2121212121212121,0.2121212121212121,0.17306684141546522
Bug,UpdateFramework message might reach the slave before Reregistered message and get dropped,"In reregisterSlave() we send 'SlaveReregisteredMessage' before we link the slave pid, which means a temporary socket will be created and used.

Subsequently, after linking, we send the UpdateFrameworkMessage, which creates and uses a persistent socket.

This might lead to out-of-order delivery, resulting in UpdateFrameworkMessage reaching the slave before the SlaveReregisteredMessage and getting dropped because the slave is not yet (re-)registered.",1.0,0,0.5,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.8846153846153846,0.9878787878787878,0.9878787878787878,0.0
Bug,Performance regression in the Master's http metrics.,"As part of the change to hold on to terminal unacknowledged tasks in the master, we introduced a performance regression during the following patch:

https://github.com/apache/mesos/commit/0760b007ad65bc91e8cea377339978c78d36d247
{noformat}
commit 0760b007ad65bc91e8cea377339978c78d36d247
Author: Benjamin Mahler <bmahler@twitter.com>
Date:   Thu Sep 11 10:48:20 2014 -0700

    Minor cleanups to the Master code.

    Review: https://reviews.apache.org/r/25566
{noformat}

Rather than keeping a running count of allocated resources, we now compute resources on-demand. This was done in order to ignore terminal task's resources.

As a result of this change, the /stats.json and /metrics/snapshot endpoints on the master have slowed down substantially on large clusters.

{noformat}
$ time curl localhost:5050/health
real	0m0.004s
user	0m0.001s
sys	0m0.002s

$ time curl localhost:5050/stats.json > /dev/null
real	0m15.402s
user	0m0.001s
sys	0m0.003s

$ time curl localhost:5050/metrics/snapshot > /dev/null
real	0m6.059s
user	0m0.002s
sys	0m0.002s
{noformat}

{{perf top}} reveals some of the resource computation during a request to stats.json:
{noformat: perf top}
Events: 36K cycles
 10.53%  libc-2.5.so             [.] _int_free
  9.90%  libc-2.5.so             [.] malloc
  8.56%  libmesos-0.21.0.so  [.] std::_Rb_tree<process::ProcessBase*, process::ProcessBase*, std::_Identity<process::ProcessBase*>, std::less<process::ProcessBase*>, std::allocator<process::ProcessBase*> >::
  8.23%  libc-2.5.so             [.] _int_malloc
  5.80%  libstdc++.so.6.0.8      [.] std::_Rb_tree_increment(std::_Rb_tree_node_base*)
  5.33%  [kernel]                [k] _raw_spin_lock
  3.13%  libstdc++.so.6.0.8      [.] std::string::assign(std::string const&)
  2.95%  libmesos-0.21.0.so  [.] process::SocketManager::exited(process::ProcessBase*)
  2.43%  libmesos-0.21.0.so  [.] mesos::Resource::MergeFrom(mesos::Resource const&)
  1.88%  libmesos-0.21.0.so  [.] mesos::internal::master::Slave::used() const
  1.48%  libstdc++.so.6.0.8      [.] __gnu_cxx::__atomic_add(int volatile*, int)
  1.45%  [kernel]                [k] find_busiest_group
  1.41%  libc-2.5.so             [.] free
  1.38%  libmesos-0.21.0.so  [.] mesos::Value_Range::MergeFrom(mesos::Value_Range const&)
  1.13%  libmesos-0.21.0.so  [.] mesos::Value_Scalar::MergeFrom(mesos::Value_Scalar const&)
  1.12%  libmesos-0.21.0.so  [.] mesos::Resource::SharedDtor()
  1.07%  libstdc++.so.6.0.8      [.] __gnu_cxx::__exchange_and_add(int volatile*, int)
  0.94%  libmesos-0.21.0.so  [.] google::protobuf::UnknownFieldSet::MergeFrom(google::protobuf::UnknownFieldSet const&)
  0.92%  libstdc++.so.6.0.8      [.] operator new(unsigned long)
  0.88%  libmesos-0.21.0.so  [.] mesos::Value_Ranges::MergeFrom(mesos::Value_Ranges const&)
  0.75%  libmesos-0.21.0.so  [.] mesos::matches(mesos::Resource const&, mesos::Resource const&)
{noformat}",3.0,0.21.0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.19266055045871558
Task,Support specifying libnl3 install location.,"LIBNL_CFLAGS uses a hard-coded path in the configure script, instead of detecting the location.",2.0,"0.22.0,0.22.1",0.5,0.3453996983408748,0.0,0.0,0.0,0.0,0.2,0.38596491228070173,0.35714285714285715,0.7948717948717948,1.0,1.0,0.20188073394495412
Bug,Remove /proc and /sys remounts from port_mapping isolator,"/proc/net reflects a new network namespace regardless and remount doesn't actually do what we expected anyway, i.e., it's not sufficient for a new pid namespace and a new mount is required.",3.0,"0.20.0,0.20.1",0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.10526315789473684,0.16428571428571428,0.10256410256410256,0.2121212121212121,0.2121212121212121,0.1835321100917431
Documentation,Create a guide to becoming a committer,"We have a committer's guide, but the process by which one becomes a committer is unclear. We should set some guidelines and a process by which we can grow contributors into committers.",3.0,0,0.5,0.06184012066365008,0.0,0.0,0.0,0.0,0.2,0.10526315789473684,0.12857142857142856,0.07692307692307693,0.13333333333333333,0.13333333333333333,0.0
Task,Expose RTT in container stats,"As we expose the bandwidth, so we should expose the RTT as a measure of latency each container is experiencing.

We can use {{ss}} to get the per-socket statistics and filter and aggregate accordingly to get a measure of RTT.",3.0,0,0.5,0.06787330316742082,0.0,0.0,0.0,0.0,0.0,0.08771929824561403,0.06428571428571428,0.07692307692307693,0.13333333333333333,0.13333333333333333,0.0
Improvement,Provide an option to validate flag value in stout/flags. ,"Currently we can provide the default value for a flag, but cannot check if the flag is set to a reasonable value and, e.g., issue a warning. Passing an optional lambda checker to {{FlagBase::add()}} can be a possible solution.",3.0,0,1.0,0.0784313725490196,0.3333333333333333,0.5,0.14285714285714285,0.0,0.0,0.05263157894736842,0.09285714285714286,0.717948717948718,0.8242424242424242,0.09696969696969697,0.0
Improvement,introduce unique_ptr,"* add unique_ptr to the configure check
* document use of unique_ptr in style guide
** use when possible, use std::move when necessary
* move raw pointers to Owned to establish ownership
* deprecate Owned in favour of unique_ptr
",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05263157894736842,0.12142857142857143,0.07692307692307693,0.13333333333333333,0.13333333333333333,0.0
Improvement,Allow variadic templates,"Add variadic templates to the C++11 configure check. Once there, we can start using them in the code-base.",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05263157894736842,0.12142857142857143,0.07692307692307693,0.13333333333333333,0.13333333333333333,0.0
Bug,MasterZooKeeperTest.LostZooKeeperCluster is flaky,"{noformat:title=}
tests/master_tests.cpp:1795: Failure
Failed to wait 10secs for slaveRegisteredMessage
{noformat}

Should have placed the FUTURE_MESSAGE that attempts to capture this messages before the slave starts...",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.20512820512820512,0.13939393939393938,0.13939393939393938,0.0
Improvement,Libprocess: report bind parameters on failure,"When you attempt to start slave or master and there's another one already running there, it is nice to report what are the actual parameters to {{bind}} call that failed.",1.0,0,0.0,0.006033182503770739,0.0,0.0,0.0,0.0,0.0,0.0,0.007142857142857143,0.01282051282051282,0.006060606060606061,0.006060606060606061,0.0
Bug,The slave does not send pending tasks during re-registration.,"In what looks like an oversight, the pending tasks and executors in the slave (Framework::pending) are not sent in the re-registration message.

For tasks, this can lead to spurious TASK_LOST notifications being generated by the master when it falsely thinks the tasks are not present on the slave.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.0
Bug,Automate disallowing of commits mixing mesos/libprocess/stout,"For various reasons, we don't want to mix mesos/libprocess/stout changes into a single commit. Typically, it is up to the reviewee/reviewer to catch this. 

It wold be nice to automate this via the pre-commit hook .",2.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.017543859649122806,0.007142857142857143,0.8846153846153846,0.9878787878787878,0.9878787878787878,0.0
Bug,Improve reconciliation between master and slave.,"As we update the Master to keep tasks in memory until they are both terminal and acknowledged (MESOS-1410), the lifetime of tasks in Mesos will look as follows:

{code}
Master           Slave
 {}               {}
{Tn}              {}  // Master receives Task T, non-terminal. Forwards to slave.
{Tn}             {Tn} // Slave receives Task T, non-terminal.
{Tn}             {Tt} // Task becomes terminal on slave. Update forwarded.
{Tt}             {Tt} // Master receives update, forwards to framework.
 {}              {Tt} // Master receives ack, forwards to slave.
 {}               {}  // Slave receives ack.
{code}

In the current form of reconciliation, the slave sends to the master all tasks that are not both terminal and acknowledged. At any point in the above lifecycle, the slave's re-registration message can reach the master.

Note the following properties:

*(1)* The master may have a non-terminal task, not present in the slave's re-registration message.
*(2)* The master may have a non-terminal task, present in the slave's re-registration message but in a different state.
*(3)* The slave's re-registration message may contain a terminal unacknowledged task unknown to the master.

In the current master / slave [reconciliation|https://github.com/apache/mesos/blob/0.19.1/src/master/master.cpp#L3146] code, the master assumes that case (1) is because a launch task message was dropped, and it sends TASK_LOST. We've seen above that (1) can happen even when the task reaches the slave correctly, so this can lead to inconsistency!

After chatting with [~vinodkone], we're considering updating the reconciliation to occur as follows:


→ Slave sends all tasks that are not both terminal and acknowledged, during re-registration. This is the same as before.

→ If the master sees tasks that are missing in the slave, the master sends the tasks that need to be reconciled to the slave for the tasks. This can be piggy-backed on the re-registration message.

→ The slave will send TASK_LOST if the task is not known to it. Preferably in a retried manner, unless we update socket closure on the slave to force a re-registration.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.0
Task,Create user doc for framework rate limiting feature,Create a Markdown doc under /docs,2.0,0.20.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15789473684210525,0.15,0.20512820512820512,0.13939393939393938,0.13939393939393938,0.1834862385321101
Bug,ZooKeeperMasterContenderDetectorTest.MasterDetectorTimedoutSession is flaky,"{noformat:title=}
[ RUN      ] ZooKeeperMasterContenderDetectorTest.MasterDetectorTimedoutSession
I0806 01:18:37.648684 17458 zookeeper_test_server.cpp:158] Started ZooKeeperTestServer on port 42069
2014-08-06 01:18:37,650:17458(0x2b4679ca5700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5
2014-08-06 01:18:37,650:17458(0x2b4679ca5700):ZOO_INFO@log_env@716: Client environment:host.name=lucid
2014-08-06 01:18:37,650:17458(0x2b4679ca5700):ZOO_INFO@log_env@723: Client environment:os.name=Linux
2014-08-06 01:18:37,650:17458(0x2b4679ca5700):ZOO_INFO@log_env@724: Client environment:os.arch=2.6.32-64-generic
2014-08-06 01:18:37,650:17458(0x2b4679ca5700):ZOO_INFO@log_env@725: Client environment:os.version=#128-Ubuntu SMP Tue Jul 15 08:32:40 UTC 2014
2014-08-06 01:18:37,651:17458(0x2b4679ca5700):ZOO_INFO@log_env@733: Client environment:user.name=(null)
2014-08-06 01:18:37,651:17458(0x2b4679ca5700):ZOO_INFO@log_env@741: Client environment:user.home=/home/jenkins
2014-08-06 01:18:37,651:17458(0x2b4679ca5700):ZOO_INFO@log_env@753: Client environment:user.dir=/var/jenkins/workspace/mesos-ubuntu-10.04-gcc/src
2014-08-06 01:18:37,651:17458(0x2b4679ca5700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=127.0.0.1:42069 sessionTimeout=5000 watcher=0x2b467450bc00 sessionId=0 sessionPasswd=<null> context=0x1682db0 flags=0
2014-08-06 01:18:37,656:17458(0x2b468638b700):ZOO_INFO@check_events@1703: initiated connection to server [127.0.0.1:42069]
2014-08-06 01:18:37,669:17458(0x2b468638b700):ZOO_INFO@check_events@1750: session establishment complete on server [127.0.0.1:42069], sessionId=0x147aa6601cf0000, negotiated timeout=6000
I0806 01:18:37.671725 17486 group.cpp:313] Group process (group(37)@127.0.1.1:55561) connected to ZooKeeper
I0806 01:18:37.671758 17486 group.cpp:787] Syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0)
I0806 01:18:37.671771 17486 group.cpp:385] Trying to create path '/mesos' in ZooKeeper
2014-08-06 01:18:39,101:17458(0x2b4687394700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:36197] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-08-06 01:18:42,441:17458(0x2b4687394700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:36197] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
I0806 01:18:42.656673 17481 contender.cpp:131] Joining the ZK group
I0806 01:18:42.662484 17484 contender.cpp:247] New candidate (id='0') has entered the contest for leadership
I0806 01:18:42.663754 17481 detector.cpp:138] Detected a new leader: (id='0')
I0806 01:18:42.663884 17481 group.cpp:658] Trying to get '/mesos/info_0000000000' in ZooKeeper
I0806 01:18:42.664788 17483 detector.cpp:426] A new leading master (UPID=@128.150.152.0:10000) is detected
2014-08-06 01:18:42,666:17458(0x2b4679ea6700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5
2014-08-06 01:18:42,666:17458(0x2b4679ea6700):ZOO_INFO@log_env@716: Client environment:host.name=lucid
2014-08-06 01:18:42,666:17458(0x2b4679ea6700):ZOO_INFO@log_env@723: Client environment:os.name=Linux
2014-08-06 01:18:42,666:17458(0x2b4679ea6700):ZOO_INFO@log_env@724: Client environment:os.arch=2.6.32-64-generic
2014-08-06 01:18:42,666:17458(0x2b4679ea6700):ZOO_INFO@log_env@725: Client environment:os.version=#128-Ubuntu SMP Tue Jul 15 08:32:40 UTC 2014
2014-08-06 01:18:42,666:17458(0x2b4679ea6700):ZOO_INFO@log_env@733: Client environment:user.name=(null)
2014-08-06 01:18:42,666:17458(0x2b4679ea6700):ZOO_INFO@log_env@741: Client environment:user.home=/home/jenkins
2014-08-06 01:18:42,666:17458(0x2b4679ea6700):ZOO_INFO@log_env@753: Client environment:user.dir=/var/jenkins/workspace/mesos-ubuntu-10.04-gcc/src
2014-08-06 01:18:42,666:17458(0x2b4679ea6700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=127.0.0.1:42069 sessionTimeout=5000 watcher=0x2b467450bc00 sessionId=0 sessionPasswd=<null> context=0x15c00f0 flags=0
2014-08-06 01:18:42,668:17458(0x2b4686d91700):ZOO_INFO@check_events@1703: initiated connection to server [127.0.0.1:42069]
2014-08-06 01:18:42,672:17458(0x2b4686d91700):ZOO_INFO@check_events@1750: session establishment complete on server [127.0.0.1:42069], sessionId=0x147aa6601cf0001, negotiated timeout=6000
I0806 01:18:42.673542 17485 group.cpp:313] Group process (group(38)@127.0.1.1:55561) connected to ZooKeeper
I0806 01:18:42.673570 17485 group.cpp:787] Syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0)
I0806 01:18:42.673580 17485 group.cpp:385] Trying to create path '/mesos' in ZooKeeper
2014-08-06 01:18:46,796:17458(0x2b468638b700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 2131ms
2014-08-06 01:18:46,796:17458(0x2b468638b700):ZOO_ERROR@handle_socket_error_msg@1643: Socket [127.0.0.1:42069] zk retcode=-7, errno=110(Connection timed out): connection to 127.0.0.1:42069 timed out (exceeded timeout by 131ms)
2014-08-06 01:18:46,796:17458(0x2b468638b700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 2131ms
2014-08-06 01:18:46,796:17458(0x2b4686d91700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 2115ms
2014-08-06 01:18:46,796:17458(0x2b4686d91700):ZOO_ERROR@handle_socket_error_msg@1643: Socket [127.0.0.1:42069] zk retcode=-7, errno=110(Connection timed out): connection to 127.0.0.1:42069 timed out (exceeded timeout by 115ms)
2014-08-06 01:18:46,796:17458(0x2b4686d91700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 2115ms
2014-08-06 01:18:46,799:17458(0x2b4687394700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 1025ms
2014-08-06 01:18:46,800:17458(0x2b4687394700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:36197] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
I0806 01:18:46.806895 17486 group.cpp:418] Lost connection to ZooKeeper, attempting to reconnect ...
I0806 01:18:46.807857 17479 group.cpp:418] Lost connection to ZooKeeper, attempting to reconnect ...
I0806 01:18:47.669064 17482 contender.cpp:131] Joining the ZK group
2014-08-06 01:18:47,669:17458(0x2b4686d91700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 2989ms
2014-08-06 01:18:47,669:17458(0x2b4686d91700):ZOO_INFO@check_events@1703: initiated connection to server [127.0.0.1:42069]
2014-08-06 01:18:47,671:17458(0x2b4686d91700):ZOO_INFO@check_events@1750: session establishment complete on server [127.0.0.1:42069], sessionId=0x147aa6601cf0001, negotiated timeout=6000
I0806 01:18:47.682868 17485 contender.cpp:247] New candidate (id='1') has entered the contest for leadership
I0806 01:18:47.683404 17482 group.cpp:313] Group process (group(38)@127.0.1.1:55561) reconnected to ZooKeeper
I0806 01:18:47.683445 17482 group.cpp:787] Syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0)
I0806 01:18:47.685998 17482 detector.cpp:138] Detected a new leader: (id='0')
I0806 01:18:47.686142 17482 group.cpp:658] Trying to get '/mesos/info_0000000000' in ZooKeeper
I0806 01:18:47.687289 17479 detector.cpp:426] A new leading master (UPID=@128.150.152.0:10000) is detected
2014-08-06 01:18:47,687:17458(0x2b467a2a8700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5
2014-08-06 01:18:47,687:17458(0x2b467a2a8700):ZOO_INFO@log_env@716: Client environment:host.name=lucid
2014-08-06 01:18:47,687:17458(0x2b467a2a8700):ZOO_INFO@log_env@723: Client environment:os.name=Linux
2014-08-06 01:18:47,687:17458(0x2b467a2a8700):ZOO_INFO@log_env@724: Client environment:os.arch=2.6.32-64-generic
2014-08-06 01:18:47,687:17458(0x2b467a2a8700):ZOO_INFO@log_env@725: Client environment:os.version=#128-Ubuntu SMP Tue Jul 15 08:32:40 UTC 2014
2014-08-06 01:18:47,687:17458(0x2b467a2a8700):ZOO_INFO@log_env@733: Client environment:user.name=(null)
2014-08-06 01:18:47,687:17458(0x2b467a2a8700):ZOO_INFO@log_env@741: Client environment:user.home=/home/jenkins
2014-08-06 01:18:47,687:17458(0x2b467a2a8700):ZOO_INFO@log_env@753: Client environment:user.dir=/var/jenkins/workspace/mesos-ubuntu-10.04-gcc/src
2014-08-06 01:18:47,687:17458(0x2b467a2a8700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=127.0.0.1:42069 sessionTimeout=5000 watcher=0x2b467450bc00 sessionId=0 sessionPasswd=<null> context=0x2b467c0421c0 flags=0
2014-08-06 01:18:47,699:17458(0x2b4687de6700):ZOO_INFO@check_events@1703: initiated connection to server [127.0.0.1:42069]
2014-08-06 01:18:47,712:17458(0x2b4687de6700):ZOO_INFO@check_events@1750: session establishment complete on server [127.0.0.1:42069], sessionId=0x147aa6601cf0002, negotiated timeout=6000
I0806 01:18:47.712846 17479 group.cpp:313] Group process (group(39)@127.0.1.1:55561) connected to ZooKeeper
I0806 01:18:47.712873 17479 group.cpp:787] Syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0)
I0806 01:18:47.712882 17479 group.cpp:385] Trying to create path '/mesos' in ZooKeeper
I0806 01:18:47.714648 17479 detector.cpp:138] Detected a new leader: (id='0')
I0806 01:18:47.714759 17479 group.cpp:658] Trying to get '/mesos/info_0000000000' in ZooKeeper
I0806 01:18:47.716130 17479 detector.cpp:426] A new leading master (UPID=@128.150.152.0:10000) is detected
2014-08-06 01:18:47,718:17458(0x2b4686d91700):ZOO_ERROR@handle_socket_error_msg@1721: Socket [127.0.0.1:42069] zk retcode=-4, errno=112(Host is down): failed while receiving a server response
I0806 01:18:47.718889 17479 group.cpp:418] Lost connection to ZooKeeper, attempting to reconnect ...
2014-08-06 01:18:47,720:17458(0x2b4687de6700):ZOO_ERROR@handle_socket_error_msg@1721: Socket [127.0.0.1:42069] zk retcode=-4, errno=112(Host is down): failed while receiving a server response
I0806 01:18:47.720788 17484 group.cpp:418] Lost connection to ZooKeeper, attempting to reconnect ...
I0806 01:18:47.724663 17458 zookeeper_test_server.cpp:122] Shutdown ZooKeeperTestServer on port 42069
2014-08-06 01:18:48,798:17458(0x2b468638b700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 4133ms
2014-08-06 01:18:48,798:17458(0x2b468638b700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-08-06 01:18:49,720:17458(0x2b4686d91700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 33ms
2014-08-06 01:18:49,721:17458(0x2b4686d91700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-08-06 01:18:49,722:17458(0x2b4687de6700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-08-06 01:18:50,136:17458(0x2b4687394700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:36197] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-08-06 01:18:50,800:17458(0x2b468638b700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-08-06 01:18:51,723:17458(0x2b4686d91700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-08-06 01:18:51,723:17458(0x2b4687de6700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-08-06 01:18:52,801:17458(0x2b468638b700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
W0806 01:18:52.842553 17481 group.cpp:456] Timed out waiting to reconnect to ZooKeeper. Forcing ZooKeeper session (sessionId=147aa6601cf0000) expiration
I0806 01:18:52.842911 17481 group.cpp:472] ZooKeeper session expired
I0806 01:18:52.843468 17485 detector.cpp:126] The current leader (id=0) is lost
I0806 01:18:52.843483 17485 detector.cpp:138] Detected a new leader: None
I0806 01:18:52.843618 17485 contender.cpp:196] Membership cancelled: 0
2014-08-06 01:18:52,843:17458(0x2b4679aa4700):ZOO_INFO@zookeeper_close@2522: Freeing zookeeper resources for sessionId=0x147aa6601cf0000

2014-08-06 01:18:52,844:17458(0x2b4679aa4700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5
2014-08-06 01:18:52,844:17458(0x2b4679aa4700):ZOO_INFO@log_env@716: Client environment:host.name=lucid
2014-08-06 01:18:52,844:17458(0x2b4679aa4700):ZOO_INFO@log_env@723: Client environment:os.name=Linux
2014-08-06 01:18:52,844:17458(0x2b4679aa4700):ZOO_INFO@log_env@724: Client environment:os.arch=2.6.32-64-generic
2014-08-06 01:18:52,844:17458(0x2b4679aa4700):ZOO_INFO@log_env@725: Client environment:os.version=#128-Ubuntu SMP Tue Jul 15 08:32:40 UTC 2014
2014-08-06 01:18:52,844:17458(0x2b4679aa4700):ZOO_INFO@log_env@733: Client environment:user.name=(null)
2014-08-06 01:18:52,844:17458(0x2b4679aa4700):ZOO_INFO@log_env@741: Client environment:user.home=/home/jenkins
2014-08-06 01:18:52,844:17458(0x2b4679aa4700):ZOO_INFO@log_env@753: Client environment:user.dir=/var/jenkins/workspace/mesos-ubuntu-10.04-gcc/src
2014-08-06 01:18:52,844:17458(0x2b4679aa4700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=127.0.0.1:42069 sessionTimeout=5000 watcher=0x2b467450bc00 sessionId=0 sessionPasswd=<null> context=0x1349ad0 flags=0
2014-08-06 01:18:52,844:17458(0x2b468698f700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-08-06 01:18:53,473:17458(0x2b4687394700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:36197] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
W0806 01:18:53.720684 17480 group.cpp:456] Timed out waiting to reconnect to ZooKeeper. Forcing ZooKeeper session (sessionId=147aa6601cf0001) expiration
I0806 01:18:53.721132 17480 group.cpp:472] ZooKeeper session expired
I0806 01:18:53.721516 17479 detector.cpp:126] The current leader (id=0) is lost
I0806 01:18:53.721534 17479 detector.cpp:138] Detected a new leader: None
I0806 01:18:53.721696 17479 contender.cpp:196] Membership cancelled: 1
2014-08-06 01:18:53,721:17458(0x2b46798a3700):ZOO_INFO@zookeeper_close@2522: Freeing zookeeper resources for sessionId=0x147aa6601cf0001

2014-08-06 01:18:53,722:17458(0x2b46798a3700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5
2014-08-06 01:18:53,722:17458(0x2b46798a3700):ZOO_INFO@log_env@716: Client environment:host.name=lucid
2014-08-06 01:18:53,722:17458(0x2b46798a3700):ZOO_INFO@log_env@723: Client environment:os.name=Linux
2014-08-06 01:18:53,722:17458(0x2b46798a3700):ZOO_INFO@log_env@724: Client environment:os.arch=2.6.32-64-generic
2014-08-06 01:18:53,722:17458(0x2b46798a3700):ZOO_INFO@log_env@725: Client environment:os.version=#128-Ubuntu SMP Tue Jul 15 08:32:40 UTC 2014
2014-08-06 01:18:53,722:17458(0x2b46798a3700):ZOO_INFO@log_env@733: Client environment:user.name=(null)
2014-08-06 01:18:53,722:17458(0x2b46798a3700):ZOO_INFO@log_env@741: Client environment:user.home=/home/jenkins
2014-08-06 01:18:53,722:17458(0x2b46798a3700):ZOO_INFO@log_env@753: Client environment:user.dir=/var/jenkins/workspace/mesos-ubuntu-10.04-gcc/src
2014-08-06 01:18:53,722:17458(0x2b46798a3700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=127.0.0.1:42069 sessionTimeout=5000 watcher=0x2b467450bc00 sessionId=0 sessionPasswd=<null> context=0x16a0550 flags=0
2014-08-06 01:18:53,723:17458(0x2b4686f92700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-08-06 01:18:53,726:17458(0x2b4687de6700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
W0806 01:18:53.730258 17479 group.cpp:456] Timed out waiting to reconnect to ZooKeeper. Forcing ZooKeeper session (sessionId=147aa6601cf0002) expiration
I0806 01:18:53.730736 17479 group.cpp:472] ZooKeeper session expired
I0806 01:18:53.731081 17481 detector.cpp:126] The current leader (id=0) is lost
I0806 01:18:53.731132 17481 detector.cpp:138] Detected a new leader: None
2014-08-06 01:18:53,731:17458(0x2b46796a2700):ZOO_INFO@zookeeper_close@2522: Freeing zookeeper resources for sessionId=0x147aa6601cf0002

2014-08-06 01:18:53,731:17458(0x2b46796a2700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5
2014-08-06 01:18:53,731:17458(0x2b46796a2700):ZOO_INFO@log_env@716: Client environment:host.name=lucid
2014-08-06 01:18:53,731:17458(0x2b46796a2700):ZOO_INFO@log_env@723: Client environment:os.name=Linux
2014-08-06 01:18:53,731:17458(0x2b46796a2700):ZOO_INFO@log_env@724: Client environment:os.arch=2.6.32-64-generic
2014-08-06 01:18:53,731:17458(0x2b46796a2700):ZOO_INFO@log_env@725: Client environment:os.version=#128-Ubuntu SMP Tue Jul 15 08:32:40 UTC 2014
2014-08-06 01:18:53,731:17458(0x2b46796a2700):ZOO_INFO@log_env@733: Client environment:user.name=(null)
2014-08-06 01:18:53,731:17458(0x2b46796a2700):ZOO_INFO@log_env@741: Client environment:user.home=/home/jenkins
2014-08-06 01:18:53,732:17458(0x2b46796a2700):ZOO_INFO@log_env@753: Client environment:user.dir=/var/jenkins/workspace/mesos-ubuntu-10.04-gcc/src
2014-08-06 01:18:53,732:17458(0x2b46796a2700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=127.0.0.1:42069 sessionTimeout=5000 watcher=0x2b467450bc00 sessionId=0 sessionPasswd=<null> context=0x2b467c035f30 flags=0
2014-08-06 01:18:53,733:17458(0x2b4687be5700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-08-06 01:18:54,512:17458(0x2b468698f700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-08-06 01:18:55,393:17458(0x2b4686f92700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-08-06 01:18:55,403:17458(0x2b4687be5700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-08-06 01:18:56,301:17458(0x2b468698f700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 122ms
2014-08-06 01:18:56,302:17458(0x2b468698f700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-08-06 01:18:56,809:17458(0x2b4687394700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:36197] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-08-06 01:18:57,939:17458(0x2b4686f92700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 879ms
2014-08-06 01:18:57,940:17458(0x2b4686f92700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-08-06 01:18:57,940:17458(0x2b4687be5700):ZOO_WARN@zookeeper_interest@1557: Exceeded deadline by 870ms
2014-08-06 01:18:57,940:17458(0x2b4687be5700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:42069] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
tests/master_contender_detector_tests.cpp:574: Failure
Failed to wait 10secs for leaderReconnecting
2014-08-06 01:18:57,941:17458(0x2b46794a0120):ZOO_INFO@zookeeper_close@2522: Freeing zookeeper resources for sessionId=0

I0806 01:18:57.949972 17458 contender.cpp:186] Now cancelling the membership: 1
2014-08-06 01:18:57,950:17458(0x2b46794a0120):ZOO_INFO@zookeeper_close@2522: Freeing zookeeper resources for sessionId=0

I0806 01:18:57.950731 17458 contender.cpp:186] Now cancelling the membership: 0
2014-08-06 01:18:57,951:17458(0x2b46794a0120):ZOO_INFO@zookeeper_close@2522: Freeing zookeeper resources for sessionId=0

../3rdparty/libprocess/include/process/gmock.hpp:298: Failure
Actual function call count doesn't match EXPECT_CALL(filter->mock, filter(testing::A<const DispatchEvent&>()))...
    Expected args: dispatch matcher (1, 16-byte object <50-20 4A-00 00-00 00-00 00-00 00-00 00-00 00-00>)
         Expected: to be called once
           Actual: never called - unsatisfied and active
[  FAILED  ] ZooKeeperMasterContenderDetectorTest.MasterDetectorTimedoutSession (20308 ms)
{noformat}",1.0,0.20.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15789473684210525,0.15,0.20512820512820512,0.13939393939393938,0.13939393939393938,0.1834862385321101
Task,Add filter to allocator resourcesRecovered method,The allocator already allows filters to be added when resources are unused. It is useful to also allow the same behaviour in {{resourcesRecovered}}.,2.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05263157894736842,0.12142857142857143,0.07692307692307693,0.13333333333333333,0.13333333333333333,0.0
Task,Set maximum executors per slave to avoid overcommit of ephemeral ports,"With network isolation, we statically assign ephemeral port ranges. As such there is a upper bound on the number of containers each slave can support.

We should avoid sending offers for slaves that have hit that limit as any tasks will fail to launch and will be LOST. ",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05263157894736842,0.12142857142857143,0.07692307692307693,0.13333333333333333,0.13333333333333333,0.0
Bug,Apache Jenkins build fails due to -lsnappy is set when building leveldb,"The failed build: https://builds.apache.org/job/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Set-JAVA_HOME/2261/consoleFull
{noformat:title=the log where -lsnappy is used when compiling leveldb}
gzip -d -c ../../3rdparty/leveldb.tar.gz | tar xf -
test ! -e ../../3rdparty/leveldb.patch || patch -d leveldb -p1 <../../3rdparty/leveldb.patch
touch leveldb-stamp
cd leveldb && \
          make  CC=""gcc"" CXX=""g++"" OPT=""-g -g2 -O2 -Wno-unused-local-typedefs -std=c++11 -fPIC""
make[5]: Entering directory `/home/jenkins/jenkins-slave/workspace/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Set-JAVA_HOME/build/mesos-0.20.0/_build/3rdparty/leveldb'
g++ -pthread -lsnappy -shared -Wl,-soname -Wl,/home/jenkins/jenkins-slave/workspace/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Set-JAVA_HOME/build/mesos-0.20.0/_build/3rdparty/leveldb/libleveldb.so.1 -I. -I./include -fno-builtin-memcmp -pthread -DOS_LINUX -DLEVELDB_PLATFORM_POSIX -DSNAPPY -g -g2 -O2 -Wno-unused-local-typedefs -std=c++11 -fPIC -fPIC db/builder.cc db/c.cc db/db_impl.cc db/db_iter.cc db/dbformat.cc db/filename.cc db/log_reader.cc db/log_writer.cc db/memtable.cc db/repair.cc db/table_cache.cc db/version_edit.cc db/version_set.cc db/write_batch.cc table/block.cc table/block_builder.cc table/filter_block.cc table/format.cc table/iterator.cc table/merger.cc table/table.cc table/table_builder.cc table/two_level_iterator.cc util/arena.cc util/bloom.cc util/cache.cc util/coding.cc util/comparator.cc util/crc32c.cc util/env.cc util/env_posix.cc util/filter_policy.cc util/hash.cc util/histogram.cc util/logging.cc util/options.cc util/status.cc  port/port_posix.cc -o libleveldb.so.1.4
ln -fs libleveldb.so.1.4 libleveldb.so
ln -fs libleveldb.so.1.4 libleveldb.so.1
g++ -I. -I./include -fno-builtin-memcmp -pthread -DOS_LINUX -DLEVELDB_PLATFORM_POSIX -DSNAPPY -g -g2 -O2 -Wno-unused-local-typedefs -std=c++11 -fPIC -c db/builder.cc -o db/builder.o
{noformat}

{noformat:title=the error}
/bin/bash ../libtool  --tag=CXX   --mode=link g++ -pthread -g -g2 -O2 -Wno-unused-local-typedefs -std=c++11   -o mesos-local local/mesos_local-main.o libmesos.la -lsasl2 -lcurl -lz  -lrt
libtool: link: g++ -pthread -g -g2 -O2 -Wno-unused-local-typedefs -std=c++11 -o .libs/mesos-local local/mesos_local-main.o  ./.libs/libmesos.so -lsasl2 /usr/lib/x86_64-linux-gnu/libcurl.so -lz -lrt -pthread -Wl,-rpath -Wl,/home/jenkins/jenkins-slave/workspace/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Set-JAVA_HOME/build/mesos-0.20.0/_inst/lib
./.libs/libmesos.so: undefined reference to `snappy::RawCompress(char const*, unsigned long, char*, unsigned long*)'
./.libs/libmesos.so: undefined reference to `snappy::RawUncompress(char const*, unsigned long, char*)'
./.libs/libmesos.so: undefined reference to `snappy::GetUncompressedLength(char const*, unsigned long, unsigned long*)'
./.libs/libmesos.so: undefined reference to `snappy::MaxCompressedLength(unsigned long)'
{noformat}",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15789473684210525,0.15,0.20512820512820512,0.13939393939393938,0.13939393939393938,0.0
Improvement,Reconciliation does not send back tasks pending validation / authorization.,"Per Vinod's feedback on https://reviews.apache.org/r/23542/, we do not send back TASK_STAGING for those tasks that are pending in the Master (validation / authorization still in progress).

For both implicit and explicit task reconciliation, the master could reply with TASK_STAGING for these tasks, as this provides additional information to the framework.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,1.0,0.9122807017543859,0.75,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.0
Bug,SlaveRecoveryTest/0.ReconcileKillTask is flaky,"Observed this on Jenkins.

{code}
[ RUN      ] SlaveRecoveryTest/0.ReconcileKillTask
Using temporary directory '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_3zJ6DG'
I0714 15:08:43.915114 27216 leveldb.cpp:176] Opened db in 474.695188ms
I0714 15:08:43.933645 27216 leveldb.cpp:183] Compacted db in 18.068942ms
I0714 15:08:43.934129 27216 leveldb.cpp:198] Created db iterator in 7860ns
I0714 15:08:43.934439 27216 leveldb.cpp:204] Seeked to beginning of db in 2560ns
I0714 15:08:43.934779 27216 leveldb.cpp:273] Iterated through 0 keys in the db in 1400ns
I0714 15:08:43.935098 27216 replica.cpp:741] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0714 15:08:43.936027 27238 recover.cpp:425] Starting replica recovery
I0714 15:08:43.936225 27238 recover.cpp:451] Replica is in EMPTY status
I0714 15:08:43.936867 27238 replica.cpp:638] Replica in EMPTY status received a broadcasted recover request
I0714 15:08:43.937049 27238 recover.cpp:188] Received a recover response from a replica in EMPTY status
I0714 15:08:43.937232 27238 recover.cpp:542] Updating replica status to STARTING
I0714 15:08:43.945600 27235 master.cpp:288] Master 20140714-150843-16842879-55850-27216 (quantal) started on 127.0.1.1:55850
I0714 15:08:43.945643 27235 master.cpp:325] Master only allowing authenticated frameworks to register
I0714 15:08:43.945651 27235 master.cpp:330] Master only allowing authenticated slaves to register
I0714 15:08:43.945658 27235 credentials.hpp:36] Loading credentials for authentication from '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_3zJ6DG/credentials'
I0714 15:08:43.945808 27235 master.cpp:359] Authorization enabled
I0714 15:08:43.946369 27235 hierarchical_allocator_process.hpp:301] Initializing hierarchical allocator process with master : master@127.0.1.1:55850
I0714 15:08:43.946419 27235 master.cpp:122] No whitelist given. Advertising offers for all slaves
I0714 15:08:43.946614 27235 master.cpp:1128] The newly elected leader is master@127.0.1.1:55850 with id 20140714-150843-16842879-55850-27216
I0714 15:08:43.946630 27235 master.cpp:1141] Elected as the leading master!
I0714 15:08:43.946637 27235 master.cpp:959] Recovering from registrar
I0714 15:08:43.946707 27235 registrar.cpp:313] Recovering registrar
I0714 15:08:43.957895 27238 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 20.529301ms
I0714 15:08:43.957978 27238 replica.cpp:320] Persisted replica status to STARTING
I0714 15:08:43.958142 27238 recover.cpp:451] Replica is in STARTING status
I0714 15:08:43.958664 27238 replica.cpp:638] Replica in STARTING status received a broadcasted recover request
I0714 15:08:43.958762 27238 recover.cpp:188] Received a recover response from a replica in STARTING status
I0714 15:08:43.958945 27238 recover.cpp:542] Updating replica status to VOTING
I0714 15:08:43.975685 27238 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 16.646136ms
I0714 15:08:43.976367 27238 replica.cpp:320] Persisted replica status to VOTING
I0714 15:08:43.976824 27241 recover.cpp:556] Successfully joined the Paxos group
I0714 15:08:43.977072 27242 recover.cpp:440] Recover process terminated
I0714 15:08:43.980590 27236 log.cpp:656] Attempting to start the writer
I0714 15:08:43.981385 27236 replica.cpp:474] Replica received implicit promise request with proposal 1
I0714 15:08:43.999141 27236 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 17.705787ms
I0714 15:08:43.999222 27236 replica.cpp:342] Persisted promised to 1
I0714 15:08:44.004451 27240 coordinator.cpp:230] Coordinator attemping to fill missing position
I0714 15:08:44.004914 27240 replica.cpp:375] Replica received explicit promise request for position 0 with proposal 2
I0714 15:08:44.021456 27240 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 16.499775ms
I0714 15:08:44.021533 27240 replica.cpp:676] Persisted action at 0
I0714 15:08:44.022006 27240 replica.cpp:508] Replica received write request for position 0
I0714 15:08:44.022043 27240 leveldb.cpp:438] Reading position from leveldb took 21376ns
I0714 15:08:44.035969 27240 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 13.885907ms
I0714 15:08:44.036365 27240 replica.cpp:676] Persisted action at 0
I0714 15:08:44.040156 27238 replica.cpp:655] Replica received learned notice for position 0
I0714 15:08:44.058082 27238 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 17.860707ms
I0714 15:08:44.058161 27238 replica.cpp:676] Persisted action at 0
I0714 15:08:44.058176 27238 replica.cpp:661] Replica learned NOP action at position 0
I0714 15:08:44.058526 27238 log.cpp:672] Writer started with ending position 0
I0714 15:08:44.058872 27238 leveldb.cpp:438] Reading position from leveldb took 25660ns
I0714 15:08:44.060556 27238 registrar.cpp:346] Successfully fetched the registry (0B)
I0714 15:08:44.060845 27238 registrar.cpp:422] Attempting to update the 'registry'
I0714 15:08:44.062304 27238 log.cpp:680] Attempting to append 120 bytes to the log
I0714 15:08:44.062866 27236 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I0714 15:08:44.063154 27236 replica.cpp:508] Replica received write request for position 1
I0714 15:08:44.082813 27236 leveldb.cpp:343] Persisting action (137 bytes) to leveldb took 19.61683ms
I0714 15:08:44.082890 27236 replica.cpp:676] Persisted action at 1
I0714 15:08:44.083256 27236 replica.cpp:655] Replica received learned notice for position 1
I0714 15:08:44.097398 27236 leveldb.cpp:343] Persisting action (139 bytes) to leveldb took 14.104796ms
I0714 15:08:44.097475 27236 replica.cpp:676] Persisted action at 1
I0714 15:08:44.097488 27236 replica.cpp:661] Replica learned APPEND action at position 1
I0714 15:08:44.098569 27236 registrar.cpp:479] Successfully updated 'registry'
I0714 15:08:44.098906 27240 log.cpp:699] Attempting to truncate the log to 1
I0714 15:08:44.099608 27240 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2
I0714 15:08:44.100005 27240 replica.cpp:508] Replica received write request for position 2
I0714 15:08:44.100566 27236 registrar.cpp:372] Successfully recovered registrar
I0714 15:08:44.101227 27239 master.cpp:986] Recovered 0 slaves from the Registry (84B) ; allowing 10mins for slaves to re-register
I0714 15:08:44.118376 27240 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 18.329495ms
I0714 15:08:44.118455 27240 replica.cpp:676] Persisted action at 2
I0714 15:08:44.122258 27242 replica.cpp:655] Replica received learned notice for position 2
I0714 15:08:44.137336 27242 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 15.023553ms
I0714 15:08:44.137460 27242 leveldb.cpp:401] Deleting ~1 keys from leveldb took 55049ns
I0714 15:08:44.137480 27242 replica.cpp:676] Persisted action at 2
I0714 15:08:44.137492 27242 replica.cpp:661] Replica learned TRUNCATE action at position 2
I0714 15:08:44.143729 27216 containerizer.cpp:124] Using isolation: posix/cpu,posix/mem
I0714 15:08:44.145934 27242 slave.cpp:168] Slave started on 43)@127.0.1.1:55850
I0714 15:08:44.145953 27242 credentials.hpp:84] Loading credential for authentication from '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/credential'
I0714 15:08:44.146040 27242 slave.cpp:266] Slave using credential for: test-principal
I0714 15:08:44.146136 27242 slave.cpp:279] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0714 15:08:44.146198 27242 slave.cpp:324] Slave hostname: quantal
I0714 15:08:44.146209 27242 slave.cpp:325] Slave checkpoint: true
I0714 15:08:44.146708 27242 state.cpp:33] Recovering state from '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta'
I0714 15:08:44.146824 27242 status_update_manager.cpp:193] Recovering status update manager
I0714 15:08:44.146901 27242 containerizer.cpp:287] Recovering containerizer
I0714 15:08:44.147228 27242 slave.cpp:3126] Finished recovery
I0714 15:08:44.147531 27242 slave.cpp:599] New master detected at master@127.0.1.1:55850
I0714 15:08:44.147562 27242 slave.cpp:675] Authenticating with master master@127.0.1.1:55850
I0714 15:08:44.147614 27242 slave.cpp:648] Detecting new master
I0714 15:08:44.147652 27242 status_update_manager.cpp:167] New master detected at master@127.0.1.1:55850
I0714 15:08:44.147691 27242 authenticatee.hpp:128] Creating new client SASL connection
I0714 15:08:44.148533 27235 master.cpp:3507] Authenticating slave(43)@127.0.1.1:55850
I0714 15:08:44.148666 27235 authenticator.hpp:156] Creating new server SASL connection
I0714 15:08:44.149054 27242 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5
I0714 15:08:44.149447 27242 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5'
I0714 15:08:44.149917 27236 authenticator.hpp:262] Received SASL authentication start
I0714 15:08:44.149974 27236 authenticator.hpp:384] Authentication requires more steps
I0714 15:08:44.150208 27242 authenticatee.hpp:265] Received SASL authentication step
I0714 15:08:44.150720 27239 authenticator.hpp:290] Received SASL authentication step
I0714 15:08:44.150749 27239 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'quantal' server FQDN: 'quantal' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0714 15:08:44.150758 27239 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I0714 15:08:44.150771 27239 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0714 15:08:44.150781 27239 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'quantal' server FQDN: 'quantal' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0714 15:08:44.150787 27239 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0714 15:08:44.150792 27239 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0714 15:08:44.150804 27239 authenticator.hpp:376] Authentication success
I0714 15:08:44.150848 27239 master.cpp:3547] Successfully authenticated principal 'test-principal' at slave(43)@127.0.1.1:55850
I0714 15:08:44.157696 27242 authenticatee.hpp:305] Authentication success
I0714 15:08:44.158855 27242 slave.cpp:732] Successfully authenticated with master master@127.0.1.1:55850
I0714 15:08:44.158936 27242 slave.cpp:970] Will retry registration in 10.352612ms if necessary
I0714 15:08:44.161813 27216 sched.cpp:139] Version: 0.20.0
I0714 15:08:44.162608 27236 sched.cpp:235] New master detected at master@127.0.1.1:55850
I0714 15:08:44.162637 27236 sched.cpp:285] Authenticating with master master@127.0.1.1:55850
I0714 15:08:44.162747 27236 authenticatee.hpp:128] Creating new client SASL connection
I0714 15:08:44.163506 27239 master.cpp:2789] Registering slave at slave(43)@127.0.1.1:55850 (quantal) with id 20140714-150843-16842879-55850-27216-0
I0714 15:08:44.164086 27238 registrar.cpp:422] Attempting to update the 'registry'
I0714 15:08:44.165694 27238 log.cpp:680] Attempting to append 295 bytes to the log
I0714 15:08:44.166231 27240 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3
I0714 15:08:44.166517 27240 replica.cpp:508] Replica received write request for position 3
I0714 15:08:44.167199 27239 master.cpp:3507] Authenticating scheduler-225679c4-a9fd-4119-9deb-c7712eba37e1@127.0.1.1:55850
I0714 15:08:44.167867 27241 authenticator.hpp:156] Creating new server SASL connection
I0714 15:08:44.168058 27241 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5
I0714 15:08:44.168081 27241 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5'
I0714 15:08:44.168107 27241 authenticator.hpp:262] Received SASL authentication start
I0714 15:08:44.168149 27241 authenticator.hpp:384] Authentication requires more steps
I0714 15:08:44.168176 27241 authenticatee.hpp:265] Received SASL authentication step
I0714 15:08:44.168215 27241 authenticator.hpp:290] Received SASL authentication step
I0714 15:08:44.168233 27241 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'quantal' server FQDN: 'quantal' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0714 15:08:44.168793 27241 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I0714 15:08:44.168820 27241 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0714 15:08:44.168834 27241 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'quantal' server FQDN: 'quantal' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0714 15:08:44.168840 27241 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0714 15:08:44.168845 27241 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0714 15:08:44.168858 27241 authenticator.hpp:376] Authentication success
I0714 15:08:44.168895 27241 authenticatee.hpp:305] Authentication success
I0714 15:08:44.168970 27241 sched.cpp:359] Successfully authenticated with master master@127.0.1.1:55850
I0714 15:08:44.168987 27241 sched.cpp:478] Sending registration request to master@127.0.1.1:55850
I0714 15:08:44.169426 27239 master.cpp:1239] Queuing up registration request from scheduler-225679c4-a9fd-4119-9deb-c7712eba37e1@127.0.1.1:55850 because authentication is still in progress
I0714 15:08:44.169958 27239 master.cpp:3547] Successfully authenticated principal 'test-principal' at scheduler-225679c4-a9fd-4119-9deb-c7712eba37e1@127.0.1.1:55850
I0714 15:08:44.170440 27241 slave.cpp:970] Will retry registration in 8.76707ms if necessary
I0714 15:08:44.175359 27239 master.cpp:2777] Ignoring register slave message from slave(43)@127.0.1.1:55850 (quantal) as admission is already in progress
I0714 15:08:44.175916 27239 master.cpp:1247] Received registration request from scheduler-225679c4-a9fd-4119-9deb-c7712eba37e1@127.0.1.1:55850
I0714 15:08:44.176298 27239 master.cpp:1207] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0714 15:08:44.176858 27239 master.cpp:1306] Registering framework 20140714-150843-16842879-55850-27216-0000 at scheduler-225679c4-a9fd-4119-9deb-c7712eba37e1@127.0.1.1:55850
I0714 15:08:44.177408 27236 sched.cpp:409] Framework registered with 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.177443 27236 sched.cpp:423] Scheduler::registered took 12527ns
I0714 15:08:44.177727 27241 hierarchical_allocator_process.hpp:331] Added framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.177747 27241 hierarchical_allocator_process.hpp:724] No resources available to allocate!
I0714 15:08:44.177753 27241 hierarchical_allocator_process.hpp:686] Performed allocation for 0 slaves in 8120ns
I0714 15:08:44.179908 27241 slave.cpp:970] Will retry registration in 66.781028ms if necessary
I0714 15:08:44.180007 27241 master.cpp:2777] Ignoring register slave message from slave(43)@127.0.1.1:55850 (quantal) as admission is already in progress
I0714 15:08:44.183082 27240 leveldb.cpp:343] Persisting action (314 bytes) to leveldb took 16.533189ms
I0714 15:08:44.183125 27240 replica.cpp:676] Persisted action at 3
I0714 15:08:44.183465 27240 replica.cpp:655] Replica received learned notice for position 3
I0714 15:08:44.203276 27240 leveldb.cpp:343] Persisting action (316 bytes) to leveldb took 19.768951ms
I0714 15:08:44.203376 27240 replica.cpp:676] Persisted action at 3
I0714 15:08:44.203392 27240 replica.cpp:661] Replica learned APPEND action at position 3
I0714 15:08:44.204033 27240 registrar.cpp:479] Successfully updated 'registry'
I0714 15:08:44.204138 27240 log.cpp:699] Attempting to truncate the log to 3
I0714 15:08:44.204221 27240 master.cpp:2829] Registered slave 20140714-150843-16842879-55850-27216-0 at slave(43)@127.0.1.1:55850 (quantal)
I0714 15:08:44.204241 27240 master.cpp:3975] Adding slave 20140714-150843-16842879-55850-27216-0 at slave(43)@127.0.1.1:55850 (quantal) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0714 15:08:44.204387 27240 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4
I0714 15:08:44.204489 27240 slave.cpp:766] Registered with master master@127.0.1.1:55850; given slave ID 20140714-150843-16842879-55850-27216-0
I0714 15:08:44.204745 27240 slave.cpp:779] Checkpointing SlaveInfo to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta/slaves/20140714-150843-16842879-55850-27216-0/slave.info'
I0714 15:08:44.204954 27240 hierarchical_allocator_process.hpp:444] Added slave 20140714-150843-16842879-55850-27216-0 (quantal) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] available)
I0714 15:08:44.205023 27240 hierarchical_allocator_process.hpp:750] Offering cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140714-150843-16842879-55850-27216-0 to framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.205122 27240 hierarchical_allocator_process.hpp:706] Performed allocation for slave 20140714-150843-16842879-55850-27216-0 in 131192ns
I0714 15:08:44.205189 27240 slave.cpp:2323] Received ping from slave-observer(32)@127.0.1.1:55850
I0714 15:08:44.205258 27240 master.hpp:801] Adding offer 20140714-150843-16842879-55850-27216-0 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140714-150843-16842879-55850-27216-0 (quantal)
I0714 15:08:44.205303 27240 master.cpp:3454] Sending 1 offers to framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.205469 27240 sched.cpp:546] Scheduler::resourceOffers took 23591ns
I0714 15:08:44.206351 27241 replica.cpp:508] Replica received write request for position 4
I0714 15:08:44.208353 27237 master.hpp:811] Removing offer 20140714-150843-16842879-55850-27216-0 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140714-150843-16842879-55850-27216-0 (quantal)
I0714 15:08:44.208436 27237 master.cpp:2133] Processing reply for offers: [ 20140714-150843-16842879-55850-27216-0 ] on slave 20140714-150843-16842879-55850-27216-0 at slave(43)@127.0.1.1:55850 (quantal) for framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.208472 27237 master.cpp:2219] Authorizing framework principal 'test-principal' to launch task 4a6783aa-8d07-46e3-8399-2a5d047f0021 as user 'jenkins'
I0714 15:08:44.208909 27237 master.hpp:773] Adding task 4a6783aa-8d07-46e3-8399-2a5d047f0021 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140714-150843-16842879-55850-27216-0 (quantal)
I0714 15:08:44.208947 27237 master.cpp:2285] Launching task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140714-150843-16842879-55850-27216-0 at slave(43)@127.0.1.1:55850 (quantal)
I0714 15:08:44.209090 27237 slave.cpp:1001] Got assigned task 4a6783aa-8d07-46e3-8399-2a5d047f0021 for framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.209190 27237 slave.cpp:3398] Checkpointing FrameworkInfo to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/framework.info'
I0714 15:08:44.209413 27237 slave.cpp:3405] Checkpointing framework pid 'scheduler-225679c4-a9fd-4119-9deb-c7712eba37e1@127.0.1.1:55850' to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/framework.pid'
I0714 15:08:44.209710 27237 slave.cpp:1111] Launching task 4a6783aa-8d07-46e3-8399-2a5d047f0021 for framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.210978 27237 slave.cpp:3720] Checkpointing ExecutorInfo to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/executors/4a6783aa-8d07-46e3-8399-2a5d047f0021/executor.info'
I0714 15:08:44.211520 27237 slave.cpp:3835] Checkpointing TaskInfo to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/executors/4a6783aa-8d07-46e3-8399-2a5d047f0021/runs/19c466f8-bb5a-4842-a152-f585ff88762a/tasks/4a6783aa-8d07-46e3-8399-2a5d047f0021/task.info'
I0714 15:08:44.211714 27237 slave.cpp:1221] Queuing task '4a6783aa-8d07-46e3-8399-2a5d047f0021' for executor 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework '20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.211937 27236 containerizer.cpp:427] Starting container '19c466f8-bb5a-4842-a152-f585ff88762a' for executor '4a6783aa-8d07-46e3-8399-2a5d047f0021' of framework '20140714-150843-16842879-55850-27216-0000'
I0714 15:08:44.212242 27236 slave.cpp:560] Successfully attached file '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/executors/4a6783aa-8d07-46e3-8399-2a5d047f0021/runs/19c466f8-bb5a-4842-a152-f585ff88762a'
I0714 15:08:44.216187 27236 launcher.cpp:137] Forked child with pid '28451' for container '19c466f8-bb5a-4842-a152-f585ff88762a'
I0714 15:08:44.217281 27236 containerizer.cpp:705] Checkpointing executor's forked pid 28451 to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/executors/4a6783aa-8d07-46e3-8399-2a5d047f0021/runs/19c466f8-bb5a-4842-a152-f585ff88762a/pids/forked.pid'
I0714 15:08:44.219408 27236 containerizer.cpp:537] Fetching URIs for container '19c466f8-bb5a-4842-a152-f585ff88762a' using command '/var/jenkins/workspace/mesos-ubuntu-12.10-gcc/src/mesos-fetcher'
I0714 15:08:44.223963 27241 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 17.554461ms
I0714 15:08:44.224501 27241 replica.cpp:676] Persisted action at 4
I0714 15:08:44.225051 27241 replica.cpp:655] Replica received learned notice for position 4
I0714 15:08:44.242923 27241 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 17.806547ms
I0714 15:08:44.243057 27241 leveldb.cpp:401] Deleting ~2 keys from leveldb took 57154ns
I0714 15:08:44.243078 27241 replica.cpp:676] Persisted action at 4
I0714 15:08:44.243096 27241 replica.cpp:661] Replica learned TRUNCATE action at position 4
I0714 15:08:44.401140 27241 slave.cpp:2468] Monitoring executor '4a6783aa-8d07-46e3-8399-2a5d047f0021' of framework '20140714-150843-16842879-55850-27216-0000' in container '19c466f8-bb5a-4842-a152-f585ff88762a'
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0714 15:08:44.434221 28486 process.cpp:1671] libprocess is initialized on 127.0.1.1:34669 for 8 cpus
I0714 15:08:44.436146 28486 exec.cpp:131] Version: 0.20.0
I0714 15:08:44.438555 28500 exec.cpp:181] Executor started at: executor(1)@127.0.1.1:34669 with pid 28486
I0714 15:08:44.440846 27241 slave.cpp:1732] Got registration for executor '4a6783aa-8d07-46e3-8399-2a5d047f0021' of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.440917 27241 slave.cpp:1817] Checkpointing executor pid 'executor(1)@127.0.1.1:34669' to '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/executors/4a6783aa-8d07-46e3-8399-2a5d047f0021/runs/19c466f8-bb5a-4842-a152-f585ff88762a/pids/libprocess.pid'
I0714 15:08:44.442373 27243 process.cpp:1098] Socket closed while receiving
I0714 15:08:44.442790 27241 slave.cpp:1851] Flushing queued task 4a6783aa-8d07-46e3-8399-2a5d047f0021 for executor '4a6783aa-8d07-46e3-8399-2a5d047f0021' of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.443192 27243 process.cpp:1098] Socket closed while receiving
I0714 15:08:44.443994 28508 process.cpp:1037] Socket closed while receiving
I0714 15:08:44.444144 28508 process.cpp:1037] Socket closed while receiving
I0714 15:08:44.444741 28500 exec.cpp:205] Executor registered on slave 20140714-150843-16842879-55850-27216-0
Registered executor on quantal
I0714 15:08:44.446338 28500 exec.cpp:217] Executor::registered took 534236ns
I0714 15:08:44.446715 28500 exec.cpp:292] Executor asked to run task '4a6783aa-8d07-46e3-8399-2a5d047f0021'
Starting task 4a6783aa-8d07-46e3-8399-2a5d047f0021
I0714 15:08:44.447548 28500 exec.cpp:301] Executor::launchTask took 584306ns
sh -c 'sleep 1000'
Forked command at 28509
I0714 15:08:44.451202 28506 exec.cpp:524] Executor sending status update TASK_RUNNING (UUID: 323fc20a-b5b8-475d-8752-b1f853797f55) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.452327 27239 slave.cpp:2086] Handling status update TASK_RUNNING (UUID: 323fc20a-b5b8-475d-8752-b1f853797f55) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 from executor(1)@127.0.1.1:34669
I0714 15:08:44.452503 27239 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 323fc20a-b5b8-475d-8752-b1f853797f55) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.452520 27239 status_update_manager.cpp:499] Creating StatusUpdate stream for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.452775 27239 status_update_manager.hpp:342] Checkpointing UPDATE for status update TASK_RUNNING (UUID: 323fc20a-b5b8-475d-8752-b1f853797f55) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.472384 27239 status_update_manager.cpp:373] Forwarding status update TASK_RUNNING (UUID: 323fc20a-b5b8-475d-8752-b1f853797f55) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 to master@127.0.1.1:55850
I0714 15:08:44.472764 27237 master.cpp:3115] Status update TASK_RUNNING (UUID: 323fc20a-b5b8-475d-8752-b1f853797f55) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 from slave 20140714-150843-16842879-55850-27216-0 at slave(43)@127.0.1.1:55850 (quantal)
I0714 15:08:44.472854 27237 sched.cpp:637] Scheduler::statusUpdate took 17656ns
I0714 15:08:44.472920 27237 master.cpp:2639] Forwarding status update acknowledgement 323fc20a-b5b8-475d-8752-b1f853797f55 for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 to slave 20140714-150843-16842879-55850-27216-0 at slave(43)@127.0.1.1:55850 (quantal)
I0714 15:08:44.473122 27239 status_update_manager.cpp:398] Received status update acknowledgement (UUID: 323fc20a-b5b8-475d-8752-b1f853797f55) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.473146 27239 status_update_manager.hpp:342] Checkpointing ACK for status update TASK_RUNNING (UUID: 323fc20a-b5b8-475d-8752-b1f853797f55) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.473244 27237 slave.cpp:2244] Status update manager successfully handled status update TASK_RUNNING (UUID: 323fc20a-b5b8-475d-8752-b1f853797f55) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.473258 27237 slave.cpp:2250] Sending acknowledgement for status update TASK_RUNNING (UUID: 323fc20a-b5b8-475d-8752-b1f853797f55) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 to executor(1)@127.0.1.1:34669
I0714 15:08:44.473567 27243 process.cpp:1098] Socket closed while receiving
I0714 15:08:44.474095 28508 process.cpp:1037] Socket closed while receiving
I0714 15:08:44.474676 28502 exec.cpp:338] Executor received status update acknowledgement 323fc20a-b5b8-475d-8752-b1f853797f55 for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.491111 27239 slave.cpp:1672] Status update manager successfully handled status update acknowledgement (UUID: 323fc20a-b5b8-475d-8752-b1f853797f55) for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.491761 27216 slave.cpp:484] Slave terminating
I0714 15:08:44.492559 27216 containerizer.cpp:124] Using isolation: posix/cpu,posix/mem
I0714 15:08:44.494635 27237 master.cpp:766] Slave 20140714-150843-16842879-55850-27216-0 at slave(43)@127.0.1.1:55850 (quantal) disconnected
I0714 15:08:44.494663 27237 master.cpp:1608] Disconnecting slave 20140714-150843-16842879-55850-27216-0
I0714 15:08:44.495120 27237 slave.cpp:168] Slave started on 44)@127.0.1.1:55850
I0714 15:08:44.495133 27237 credentials.hpp:84] Loading credential for authentication from '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/credential'
I0714 15:08:44.495226 27237 slave.cpp:266] Slave using credential for: test-principal
I0714 15:08:44.495322 27237 slave.cpp:279] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0714 15:08:44.495407 27237 slave.cpp:324] Slave hostname: quantal
I0714 15:08:44.495419 27237 slave.cpp:325] Slave checkpoint: true
I0714 15:08:44.495939 27242 master.cpp:2469] Asked to kill task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.496207 27238 state.cpp:33] Recovering state from '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/meta'
I0714 15:08:44.498291 27240 slave.cpp:3194] Recovering framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.498325 27240 slave.cpp:3570] Recovering executor '4a6783aa-8d07-46e3-8399-2a5d047f0021' of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.498940 27240 status_update_manager.cpp:193] Recovering status update manager
I0714 15:08:44.498956 27240 status_update_manager.cpp:201] Recovering executor '4a6783aa-8d07-46e3-8399-2a5d047f0021' of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.498975 27240 status_update_manager.cpp:499] Creating StatusUpdate stream for task 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.499092 27240 status_update_manager.hpp:306] Replaying status update stream for task 4a6783aa-8d07-46e3-8399-2a5d047f0021
I0714 15:08:44.499241 27240 slave.cpp:560] Successfully attached file '/tmp/SlaveRecoveryTest_0_ReconcileKillTask_Zl9DUt/slaves/20140714-150843-16842879-55850-27216-0/frameworks/20140714-150843-16842879-55850-27216-0000/executors/4a6783aa-8d07-46e3-8399-2a5d047f0021/runs/19c466f8-bb5a-4842-a152-f585ff88762a'
I0714 15:08:44.499433 27240 containerizer.cpp:287] Recovering containerizer
I0714 15:08:44.499457 27240 containerizer.cpp:329] Recovering container '19c466f8-bb5a-4842-a152-f585ff88762a' for executor '4a6783aa-8d07-46e3-8399-2a5d047f0021' of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.495811 27237 hierarchical_allocator_process.hpp:483] Slave 20140714-150843-16842879-55850-27216-0 disconnected
I0714 15:08:44.501255 27240 slave.cpp:3067] Sending reconnect request to executor 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000 at executor(1)@127.0.1.1:34669
I0714 15:08:44.502030 28501 exec.cpp:251] Received reconnect request from slave 20140714-150843-16842879-55850-27216-0
I0714 15:08:44.502627 27243 process.cpp:1098] Socket closed while receiving
I0714 15:08:44.502681 28508 process.cpp:1037] Socket closed while receiving
I0714 15:08:44.503211 27240 slave.cpp:1911] Re-registering executor 4a6783aa-8d07-46e3-8399-2a5d047f0021 of framework 20140714-150843-16842879-55850-27216-0000
I0714 15:08:44.504238 28501 exec.cpp:228] Executor re-registered on slave 20140714-150843-16842879-55850-27216-0
I0714 15:08:44.505033 28501 exec.cpp:240] Executor::reregistered took 45053ns
Re-registered executor on quantal
I0714 15:08:44.505507 27243 process.cpp:1098] Socket closed while receiving
I0714 15:08:44.505558 28508 process.cpp:1037] Socket closed while receiving
I0714 15:08:44.948043 27241 hierarchical_allocator_process.hpp:686] Performed allocation for 1 slaves in 124255ns
I0714 15:08:45.948671 27237 hierarchical_allocator_process.hpp:686] Performed allocation for 1 slaves in 61521ns
I0714 15:08:46.503978 27238 slave.cpp:2035] Cleaning up un-reregistered executors
I0714 15:08:46.504050 27238 slave.cpp:3126] Finished recovery
I0",1.0,0.20.0,0.5,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.8846153846153846,0.9878787878787878,0.9878787878787878,0.1834862385321101
Improvement,Allow LoadGeneratorFramework to read password from a file,It currently just reads the flag as the value of the password.,1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15789473684210525,0.15,0.20512820512820512,0.13939393939393938,0.13939393939393938,0.0
Improvement,"Isolate system directories, e.g., per-container /tmp","Ideally, tasks should not write outside their sandbox (executor work directory) but pragmatically they may need to write to /tmp, /var/tmp, or some other directory.

1) We should include any such files in disk usage and quota.
2) We should make these ""shared"" directories private, i.e., each container has their own.
3) We should make the lifetime of any such files the same as the executor work directory.",3.0,0.20.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.10526315789473684,0.16428571428571428,0.10256410256410256,0.2121212121212121,0.2121212121212121,0.1834862385321101
Improvement,Add logging of the user uid when receiving SIGTERM.,"We currently do not log the user id when receiving a SIGTERM, this makes debugging a bit difficult. It's easy to get this information through sigaction.",1.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.014285714285714285,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.0
Bug,SlaveRecoveryTest/0.MultipleFrameworks is flaky,"{code}
[ RUN      ] SlaveRecoveryTest/0.MultipleFrameworks
Using temporary directory '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_6dJqxr'
I0626 00:04:39.557339  5450 leveldb.cpp:176] Opened db in 179.857593ms
I0626 00:04:39.565433  5450 leveldb.cpp:183] Compacted db in 8.071041ms
I0626 00:04:39.565457  5450 leveldb.cpp:198] Created db iterator in 4065ns
I0626 00:04:39.565466  5450 leveldb.cpp:204] Seeked to beginning of db in 596ns
I0626 00:04:39.565474  5450 leveldb.cpp:273] Iterated through 0 keys in the db in 396ns
I0626 00:04:39.565490  5450 replica.cpp:741] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0626 00:04:39.565827  5476 recover.cpp:425] Starting replica recovery
I0626 00:04:39.566033  5474 recover.cpp:451] Replica is in EMPTY status
I0626 00:04:39.566504  5474 replica.cpp:638] Replica in EMPTY status received a broadcasted recover request
I0626 00:04:39.566686  5477 recover.cpp:188] Received a recover response from a replica in EMPTY status
I0626 00:04:39.566905  5472 recover.cpp:542] Updating replica status to STARTING
I0626 00:04:39.568307  5471 master.cpp:288] Master 20140626-000439-1032504131-55423-5450 (juno.apache.org) started on 67.195.138.61:55423
I0626 00:04:39.568332  5471 master.cpp:325] Master only allowing authenticated frameworks to register
I0626 00:04:39.568339  5471 master.cpp:330] Master only allowing authenticated slaves to register
I0626 00:04:39.568348  5471 credentials.hpp:35] Loading credentials for authentication from '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_6dJqxr/credentials'
I0626 00:04:39.568461  5471 master.cpp:356] Authorization enabled
I0626 00:04:39.568739  5478 master.cpp:122] No whitelist given. Advertising offers for all slaves
I0626 00:04:39.568814  5475 hierarchical_allocator_process.hpp:301] Initializing hierarchical allocator process with master : master@67.195.138.61:55423
I0626 00:04:39.569206  5478 master.cpp:1122] The newly elected leader is master@67.195.138.61:55423 with id 20140626-000439-1032504131-55423-5450
I0626 00:04:39.569223  5478 master.cpp:1135] Elected as the leading master!
I0626 00:04:39.569231  5478 master.cpp:953] Recovering from registrar
I0626 00:04:39.569286  5475 registrar.cpp:313] Recovering registrar
I0626 00:04:39.600639  5477 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 33.682136ms
I0626 00:04:39.600661  5477 replica.cpp:320] Persisted replica status to STARTING
I0626 00:04:39.600790  5476 recover.cpp:451] Replica is in STARTING status
I0626 00:04:39.601184  5474 replica.cpp:638] Replica in STARTING status received a broadcasted recover request
I0626 00:04:39.601274  5477 recover.cpp:188] Received a recover response from a replica in STARTING status
I0626 00:04:39.601465  5471 recover.cpp:542] Updating replica status to VOTING
I0626 00:04:39.610605  5471 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 9.076262ms
I0626 00:04:39.610638  5471 replica.cpp:320] Persisted replica status to VOTING
I0626 00:04:39.610683  5471 recover.cpp:556] Successfully joined the Paxos group
I0626 00:04:39.610780  5471 recover.cpp:440] Recover process terminated
I0626 00:04:39.610946  5474 log.cpp:656] Attempting to start the writer
I0626 00:04:39.611486  5475 replica.cpp:474] Replica received implicit promise request with proposal 1
I0626 00:04:39.618924  5475 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 7.418789ms
I0626 00:04:39.618942  5475 replica.cpp:342] Persisted promised to 1
I0626 00:04:39.619220  5476 coordinator.cpp:230] Coordinator attemping to fill missing position
I0626 00:04:39.619763  5476 replica.cpp:375] Replica received explicit promise request for position 0 with proposal 2
I0626 00:04:39.627267  5476 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 7.485492ms
I0626 00:04:39.627295  5476 replica.cpp:676] Persisted action at 0
I0626 00:04:39.627822  5473 replica.cpp:508] Replica received write request for position 0
I0626 00:04:39.627861  5473 leveldb.cpp:438] Reading position from leveldb took 17132ns
I0626 00:04:39.635592  5473 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 7.714322ms
I0626 00:04:39.635612  5473 replica.cpp:676] Persisted action at 0
I0626 00:04:39.635797  5473 replica.cpp:655] Replica received learned notice for position 0
I0626 00:04:39.643941  5473 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 8.129347ms
I0626 00:04:39.643960  5473 replica.cpp:676] Persisted action at 0
I0626 00:04:39.643970  5473 replica.cpp:661] Replica learned NOP action at position 0
I0626 00:04:39.644207  5473 log.cpp:672] Writer started with ending position 0
I0626 00:04:39.644625  5471 leveldb.cpp:438] Reading position from leveldb took 9128ns
I0626 00:04:39.646010  5476 registrar.cpp:346] Successfully fetched the registry (0B)
I0626 00:04:39.646044  5476 registrar.cpp:422] Attempting to update the 'registry'
I0626 00:04:39.647274  5471 log.cpp:680] Attempting to append 136 bytes to the log
I0626 00:04:39.647337  5471 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I0626 00:04:39.647687  5476 replica.cpp:508] Replica received write request for position 1
I0626 00:04:39.655206  5476 leveldb.cpp:343] Persisting action (155 bytes) to leveldb took 7.499736ms
I0626 00:04:39.655225  5476 replica.cpp:676] Persisted action at 1
I0626 00:04:39.655467  5476 replica.cpp:655] Replica received learned notice for position 1
I0626 00:04:39.663534  5476 leveldb.cpp:343] Persisting action (157 bytes) to leveldb took 8.054929ms
I0626 00:04:39.663554  5476 replica.cpp:676] Persisted action at 1
I0626 00:04:39.663563  5476 replica.cpp:661] Replica learned APPEND action at position 1
I0626 00:04:39.663890  5478 registrar.cpp:479] Successfully updated 'registry'
I0626 00:04:39.663947  5478 registrar.cpp:372] Successfully recovered registrar
I0626 00:04:39.663969  5476 log.cpp:699] Attempting to truncate the log to 1
I0626 00:04:39.664044  5478 master.cpp:980] Recovered 0 slaves from the Registry (98B) ; allowing 10mins for slaves to re-register
I0626 00:04:39.664057  5476 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2
I0626 00:04:39.664341  5476 replica.cpp:508] Replica received write request for position 2
I0626 00:04:39.664681  5450 containerizer.cpp:124] Using isolation: posix/cpu,posix/mem
I0626 00:04:39.666721  5471 slave.cpp:168] Slave started on 173)@67.195.138.61:55423
I0626 00:04:39.666741  5471 credentials.hpp:35] Loading credentials for authentication from '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/credential'
I0626 00:04:39.666806  5471 slave.cpp:268] Slave using credential for: test-principal
I0626 00:04:39.666936  5471 slave.cpp:281] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0626 00:04:39.667000  5471 slave.cpp:326] Slave hostname: juno.apache.org
I0626 00:04:39.667009  5471 slave.cpp:327] Slave checkpoint: true
I0626 00:04:39.667572  5478 state.cpp:33] Recovering state from '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/meta'
I0626 00:04:39.667703  5475 status_update_manager.cpp:193] Recovering status update manager
I0626 00:04:39.667840  5475 containerizer.cpp:287] Recovering containerizer
I0626 00:04:39.668478  5471 slave.cpp:3128] Finished recovery
I0626 00:04:39.668712  5471 slave.cpp:601] New master detected at master@67.195.138.61:55423
I0626 00:04:39.668738  5471 slave.cpp:677] Authenticating with master master@67.195.138.61:55423
I0626 00:04:39.668802  5471 slave.cpp:650] Detecting new master
I0626 00:04:39.668861  5471 status_update_manager.cpp:167] New master detected at master@67.195.138.61:55423
I0626 00:04:39.668916  5471 authenticatee.hpp:128] Creating new client SASL connection
I0626 00:04:39.669087  5471 master.cpp:3499] Authenticating slave(173)@67.195.138.61:55423
I0626 00:04:39.669203  5471 authenticator.hpp:156] Creating new server SASL connection
I0626 00:04:39.669340  5471 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5
I0626 00:04:39.669359  5471 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5'
I0626 00:04:39.669386  5471 authenticator.hpp:262] Received SASL authentication start
I0626 00:04:39.669414  5471 authenticator.hpp:384] Authentication requires more steps
I0626 00:04:39.669457  5471 authenticatee.hpp:265] Received SASL authentication step
I0626 00:04:39.669514  5471 authenticator.hpp:290] Received SASL authentication step
I0626 00:04:39.669534  5471 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'juno.apache.org' server FQDN: 'juno.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0626 00:04:39.669543  5471 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I0626 00:04:39.669567  5471 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0626 00:04:39.669580  5471 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'juno.apache.org' server FQDN: 'juno.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0626 00:04:39.669589  5471 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0626 00:04:39.669594  5471 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0626 00:04:39.669606  5471 authenticator.hpp:376] Authentication success
I0626 00:04:39.669641  5471 authenticatee.hpp:305] Authentication success
I0626 00:04:39.669669  5471 master.cpp:3539] Successfully authenticated principal 'test-principal' at slave(173)@67.195.138.61:55423
I0626 00:04:39.669761  5450 sched.cpp:139] Version: 0.20.0
I0626 00:04:39.669764  5478 slave.cpp:734] Successfully authenticated with master master@67.195.138.61:55423
I0626 00:04:39.669826  5478 slave.cpp:972] Will retry registration in 3.190666ms if necessary
I0626 00:04:39.669950  5471 master.cpp:2781] Registering slave at slave(173)@67.195.138.61:55423 (juno.apache.org) with id 20140626-000439-1032504131-55423-5450-0
I0626 00:04:39.669960  5475 sched.cpp:235] New master detected at master@67.195.138.61:55423
I0626 00:04:39.669977  5475 sched.cpp:285] Authenticating with master master@67.195.138.61:55423
I0626 00:04:39.670073  5471 registrar.cpp:422] Attempting to update the 'registry'
I0626 00:04:39.670114  5475 authenticatee.hpp:128] Creating new client SASL connection
I0626 00:04:39.670263  5475 master.cpp:3499] Authenticating scheduler-e66c50d2-2790-4d20-bc77-a57af0e1780b@67.195.138.61:55423
I0626 00:04:39.670361  5474 authenticator.hpp:156] Creating new server SASL connection
I0626 00:04:39.670506  5475 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5
I0626 00:04:39.670526  5475 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5'
I0626 00:04:39.670559  5475 authenticator.hpp:262] Received SASL authentication start
I0626 00:04:39.670590  5475 authenticator.hpp:384] Authentication requires more steps
I0626 00:04:39.670619  5475 authenticatee.hpp:265] Received SASL authentication step
I0626 00:04:39.670650  5475 authenticator.hpp:290] Received SASL authentication step
I0626 00:04:39.670670  5475 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'juno.apache.org' server FQDN: 'juno.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0626 00:04:39.670677  5475 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I0626 00:04:39.670687  5475 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0626 00:04:39.670697  5475 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'juno.apache.org' server FQDN: 'juno.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0626 00:04:39.670706  5475 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0626 00:04:39.670712  5475 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0626 00:04:39.670723  5475 authenticator.hpp:376] Authentication success
I0626 00:04:39.670749  5475 authenticatee.hpp:305] Authentication success
I0626 00:04:39.670773  5475 master.cpp:3539] Successfully authenticated principal 'test-principal' at scheduler-e66c50d2-2790-4d20-bc77-a57af0e1780b@67.195.138.61:55423
I0626 00:04:39.670845  5475 sched.cpp:359] Successfully authenticated with master master@67.195.138.61:55423
I0626 00:04:39.670858  5475 sched.cpp:478] Sending registration request to master@67.195.138.61:55423
I0626 00:04:39.670899  5475 master.cpp:1241] Received registration request from scheduler-e66c50d2-2790-4d20-bc77-a57af0e1780b@67.195.138.61:55423
I0626 00:04:39.670922  5475 master.cpp:1201] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0626 00:04:39.671052  5475 master.cpp:1300] Registering framework 20140626-000439-1032504131-55423-5450-0000 at scheduler-e66c50d2-2790-4d20-bc77-a57af0e1780b@67.195.138.61:55423
I0626 00:04:39.671159  5474 sched.cpp:409] Framework registered with 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:39.671185  5474 sched.cpp:423] Scheduler::registered took 10223ns
I0626 00:04:39.671226  5474 hierarchical_allocator_process.hpp:331] Added framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:39.671241  5474 hierarchical_allocator_process.hpp:724] No resources available to allocate!
I0626 00:04:39.671247  5474 hierarchical_allocator_process.hpp:686] Performed allocation for 0 slaves in 8574ns
I0626 00:04:39.671879  5476 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 7.48781ms
I0626 00:04:39.671900  5476 replica.cpp:676] Persisted action at 2
I0626 00:04:39.672164  5471 replica.cpp:655] Replica received learned notice for position 2
I0626 00:04:39.674092  5472 slave.cpp:972] Will retry registration in 25.467893ms if necessary
I0626 00:04:39.674108  5476 master.cpp:2769] Ignoring register slave message from slave(173)@67.195.138.61:55423 (juno.apache.org) as admission is already in progress
I0626 00:04:39.680193  5471 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 8.01285ms
I0626 00:04:39.680223  5471 leveldb.cpp:401] Deleting ~1 keys from leveldb took 11393ns
I0626 00:04:39.680234  5471 replica.cpp:676] Persisted action at 2
I0626 00:04:39.680245  5471 replica.cpp:661] Replica learned TRUNCATE action at position 2
I0626 00:04:39.680585  5472 log.cpp:680] Attempting to append 326 bytes to the log
I0626 00:04:39.680670  5477 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3
I0626 00:04:39.680953  5474 replica.cpp:508] Replica received write request for position 3
I0626 00:04:39.688521  5474 leveldb.cpp:343] Persisting action (345 bytes) to leveldb took 7.548316ms
I0626 00:04:39.688542  5474 replica.cpp:676] Persisted action at 3
I0626 00:04:39.688750  5474 replica.cpp:655] Replica received learned notice for position 3
I0626 00:04:39.696851  5474 leveldb.cpp:343] Persisting action (347 bytes) to leveldb took 8.088289ms
I0626 00:04:39.696869  5474 replica.cpp:676] Persisted action at 3
I0626 00:04:39.696878  5474 replica.cpp:661] Replica learned APPEND action at position 3
I0626 00:04:39.697268  5474 registrar.cpp:479] Successfully updated 'registry'
I0626 00:04:39.697350  5474 log.cpp:699] Attempting to truncate the log to 3
I0626 00:04:39.697412  5474 master.cpp:2821] Registered slave 20140626-000439-1032504131-55423-5450-0 at slave(173)@67.195.138.61:55423 (juno.apache.org)
I0626 00:04:39.697423  5474 master.cpp:3967] Adding slave 20140626-000439-1032504131-55423-5450-0 at slave(173)@67.195.138.61:55423 (juno.apache.org) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0626 00:04:39.697535  5474 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4
I0626 00:04:39.697618  5474 slave.cpp:768] Registered with master master@67.195.138.61:55423; given slave ID 20140626-000439-1032504131-55423-5450-0
I0626 00:04:39.697754  5474 slave.cpp:781] Checkpointing SlaveInfo to '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/meta/slaves/20140626-000439-1032504131-55423-5450-0/slave.info'
I0626 00:04:39.697762  5471 hierarchical_allocator_process.hpp:444] Added slave 20140626-000439-1032504131-55423-5450-0 (juno.apache.org) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] available)
I0626 00:04:39.697845  5471 hierarchical_allocator_process.hpp:750] Offering cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140626-000439-1032504131-55423-5450-0 to framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:39.697854  5474 slave.cpp:2325] Received ping from slave-observer(142)@67.195.138.61:55423
I0626 00:04:39.698040  5471 hierarchical_allocator_process.hpp:706] Performed allocation for slave 20140626-000439-1032504131-55423-5450-0 in 231333ns
I0626 00:04:39.698051  5474 replica.cpp:508] Replica received write request for position 4
I0626 00:04:39.698118  5471 master.hpp:794] Adding offer 20140626-000439-1032504131-55423-5450-0 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140626-000439-1032504131-55423-5450-0 (juno.apache.org)
I0626 00:04:39.698170  5471 master.cpp:3446] Sending 1 offers to framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:39.698318  5471 sched.cpp:546] Scheduler::resourceOffers took 24371ns
I0626 00:04:39.699718  5477 master.hpp:804] Removing offer 20140626-000439-1032504131-55423-5450-0 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140626-000439-1032504131-55423-5450-0 (juno.apache.org)
I0626 00:04:39.699787  5477 master.cpp:2125] Processing reply for offers: [ 20140626-000439-1032504131-55423-5450-0 ] on slave 20140626-000439-1032504131-55423-5450-0 at slave(173)@67.195.138.61:55423 (juno.apache.org) for framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:39.699812  5477 master.cpp:2211] Authorizing framework principal 'test-principal' to launch task 897522cc-4ec5-4904-aed0-00b6b8c41028 as user 'jenkins'
I0626 00:04:39.700160  5477 master.hpp:766] Adding task 897522cc-4ec5-4904-aed0-00b6b8c41028 with resources cpus(*):1; mem(*):512 on slave 20140626-000439-1032504131-55423-5450-0 (juno.apache.org)
I0626 00:04:39.700188  5477 master.cpp:2277] Launching task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000 with resources cpus(*):1; mem(*):512 on slave 20140626-000439-1032504131-55423-5450-0 at slave(173)@67.195.138.61:55423 (juno.apache.org)
I0626 00:04:39.700392  5471 slave.cpp:1003] Got assigned task 897522cc-4ec5-4904-aed0-00b6b8c41028 for framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:39.700479  5477 hierarchical_allocator_process.hpp:546] Framework 20140626-000439-1032504131-55423-5450-0000 left cpus(*):1; mem(*):512; disk(*):1024; ports(*):[31000-32000] unused on slave 20140626-000439-1032504131-55423-5450-0
I0626 00:04:39.700505  5471 slave.cpp:3400] Checkpointing FrameworkInfo to '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/meta/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0000/framework.info'
I0626 00:04:39.700597  5477 hierarchical_allocator_process.hpp:588] Framework 20140626-000439-1032504131-55423-5450-0000 filtered slave 20140626-000439-1032504131-55423-5450-0 for 5secs
I0626 00:04:39.700686  5471 slave.cpp:3407] Checkpointing framework pid 'scheduler-e66c50d2-2790-4d20-bc77-a57af0e1780b@67.195.138.61:55423' to '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/meta/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0000/framework.pid'
I0626 00:04:39.700960  5471 slave.cpp:1113] Launching task 897522cc-4ec5-4904-aed0-00b6b8c41028 for framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:39.702287  5471 slave.cpp:3722] Checkpointing ExecutorInfo to '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/meta/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0000/executors/897522cc-4ec5-4904-aed0-00b6b8c41028/executor.info'
I0626 00:04:39.702738  5471 slave.cpp:3837] Checkpointing TaskInfo to '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/meta/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0000/executors/897522cc-4ec5-4904-aed0-00b6b8c41028/runs/9ad3a5ac-3587-47df-96c2-df76ea09328c/tasks/897522cc-4ec5-4904-aed0-00b6b8c41028/task.info'
I0626 00:04:39.702744  5476 containerizer.cpp:427] Starting container '9ad3a5ac-3587-47df-96c2-df76ea09328c' for executor '897522cc-4ec5-4904-aed0-00b6b8c41028' of framework '20140626-000439-1032504131-55423-5450-0000'
I0626 00:04:39.702987  5471 slave.cpp:1223] Queuing task '897522cc-4ec5-4904-aed0-00b6b8c41028' for executor 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework '20140626-000439-1032504131-55423-5450-0000
I0626 00:04:39.703039  5471 slave.cpp:562] Successfully attached file '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0000/executors/897522cc-4ec5-4904-aed0-00b6b8c41028/runs/9ad3a5ac-3587-47df-96c2-df76ea09328c'
I0626 00:04:39.704654  5477 launcher.cpp:137] Forked child with pid '7596' for container '9ad3a5ac-3587-47df-96c2-df76ea09328c'
I0626 00:04:39.704891  5477 containerizer.cpp:705] Checkpointing executor's forked pid 7596 to '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/meta/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0000/executors/897522cc-4ec5-4904-aed0-00b6b8c41028/runs/9ad3a5ac-3587-47df-96c2-df76ea09328c/pids/forked.pid'
I0626 00:04:39.705301  5474 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 7.183865ms
I0626 00:04:39.705343  5474 replica.cpp:676] Persisted action at 4
I0626 00:04:39.705912  5476 containerizer.cpp:537] Fetching URIs for container '9ad3a5ac-3587-47df-96c2-df76ea09328c' using command '/home/jenkins/jenkins-slave/workspace/Mesos-Trunk-Ubuntu-Build-Out-Of-Src-Disable-Java-Disable-Python-Disable-Webui/build/src/mesos-fetcher'
I0626 00:04:39.706073  5471 replica.cpp:655] Replica received learned notice for position 4
I0626 00:04:39.713664  5471 leveldb.cpp:343] Persisting action (18 bytes) to leveldb took 6.238172ms
I0626 00:04:39.713762  5471 leveldb.cpp:401] Deleting ~2 keys from leveldb took 42244ns
I0626 00:04:39.713788  5471 replica.cpp:676] Persisted action at 4
I0626 00:04:39.713810  5471 replica.cpp:661] Replica learned TRUNCATE action at position 4
I0626 00:04:40.378677  5475 slave.cpp:2470] Monitoring executor '897522cc-4ec5-4904-aed0-00b6b8c41028' of framework '20140626-000439-1032504131-55423-5450-0000' in container '9ad3a5ac-3587-47df-96c2-df76ea09328c'
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0626 00:04:40.413177  7631 process.cpp:1671] libprocess is initialized on 67.195.138.61:40619 for 8 cpus
I0626 00:04:40.414454  7631 exec.cpp:131] Version: 0.20.0
I0626 00:04:40.415856  7649 exec.cpp:181] Executor started at: executor(1)@67.195.138.61:40619 with pid 7631
I0626 00:04:40.416453  5471 slave.cpp:1734] Got registration for executor '897522cc-4ec5-4904-aed0-00b6b8c41028' of framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:40.416527  5471 slave.cpp:1819] Checkpointing executor pid 'executor(1)@67.195.138.61:40619' to '/tmp/SlaveRecoveryTest_0_MultipleFrameworks_G6ObtK/meta/slaves/20140626-000439-1032504131-55423-5450-0/frameworks/20140626-000439-1032504131-55423-5450-0000/executors/897522cc-4ec5-4904-aed0-00b6b8c41028/runs/9ad3a5ac-3587-47df-96c2-df76ea09328c/pids/libprocess.pid'
I0626 00:04:40.416998  5471 slave.cpp:1853] Flushing queued task 897522cc-4ec5-4904-aed0-00b6b8c41028 for executor '897522cc-4ec5-4904-aed0-00b6b8c41028' of framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:40.417186  5479 process.cpp:1098] Socket closed while receiving
I0626 00:04:40.417322  7648 exec.cpp:205] Executor registered on slave 20140626-000439-1032504131-55423-5450-0
I0626 00:04:40.417368  7653 process.cpp:1037] Socket closed while receiving
I0626 00:04:40.418385  7648 exec.cpp:217] Executor::registered took 115121ns
Registered executor on juno.apache.org
I0626 00:04:40.418544  7648 exec.cpp:292] Executor asked to run task '897522cc-4ec5-4904-aed0-00b6b8c41028'
Starting task 897522cc-4ec5-4904-aed0-00b6b8c41028
I0626 00:04:40.418609  7648 exec.cpp:301] Executor::launchTask took 35936ns
Forked command at 7654
sh -c 'sleep 1000'
I0626 00:04:40.420611  7650 exec.cpp:524] Executor sending status update TASK_RUNNING (UUID: 6d952e6d-b7d7-4f40-9f44-f7c3f81757af) for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:40.420953  5473 slave.cpp:2088] Handling status update TASK_RUNNING (UUID: 6d952e6d-b7d7-4f40-9f44-f7c3f81757af) for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000 from executor(1)@67.195.138.61:40619
I0626 00:04:40.421188  5474 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: 6d952e6d-b7d7-4f40-9f44-f7c3f81757af) for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:40.421206  5474 status_update_manager.cpp:499] Creating StatusUpdate stream for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:40.421469  5474 status_update_manager.hpp:342] Checkpointing UPDATE for status update TASK_RUNNING (UUID: 6d952e6d-b7d7-4f40-9f44-f7c3f81757af) for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:40.525890  5474 status_update_manager.cpp:373] Forwarding status update TASK_RUNNING (UUID: 6d952e6d-b7d7-4f40-9f44-f7c3f81757af) for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000 to master@67.195.138.61:55423
I0626 00:04:40.526053  5474 master.cpp:3107] Status update TASK_RUNNING (UUID: 6d952e6d-b7d7-4f40-9f44-f7c3f81757af) for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000 from slave 20140626-000439-1032504131-55423-5450-0 at slave(173)@67.195.138.61:55423 (juno.apache.org)
I0626 00:04:40.526087  5474 slave.cpp:2246] Status update manager successfully handled status update TASK_RUNNING (UUID: 6d952e6d-b7d7-4f40-9f44-f7c3f81757af) for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:40.526100  5474 slave.cpp:2252] Sending acknowledgement for status update TASK_RUNNING (UUID: 6d952e6d-b7d7-4f40-9f44-f7c3f81757af) for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000 to executor(1)@67.195.138.61:40619
I0626 00:04:40.526252  5474 sched.cpp:637] Scheduler::statusUpdate took 17393ns
I0626 00:04:40.526294  5474 master.cpp:2631] Forwarding status update acknowledgement 6d952e6d-b7d7-4f40-9f44-f7c3f81757af for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000 to slave 20140626-000439-1032504131-55423-5450-0 at slave(173)@67.195.138.61:55423 (juno.apache.org)
I0626 00:04:40.526371  5474 status_update_manager.cpp:398] Received status update acknowledgement (UUID: 6d952e6d-b7d7-4f40-9f44-f7c3f81757af) for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:40.526384  5474 status_update_manager.hpp:342] Checkpointing ACK for status update TASK_RUNNING (UUID: 6d952e6d-b7d7-4f40-9f44-f7c3f81757af) for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:40.526468  5479 process.cpp:1098] Socket closed while receiving
I0626 00:04:40.526574  7651 exec.cpp:338] Executor received status update acknowledgement 6d952e6d-b7d7-4f40-9f44-f7c3f81757af for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:40.526679  7653 process.cpp:1037] Socket closed while receiving
I0626 00:04:40.569715  5473 hierarchical_allocator_process.hpp:833] Filtered cpus(*):1; mem(*):512; disk(*):1024; ports(*):[31000-32000] on slave 20140626-000439-1032504131-55423-5450-0 for framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:40.569749  5473 hierarchical_allocator_process.hpp:686] Performed allocation for 1 slaves in 105698ns
I0626 00:04:40.576212  5477 slave.cpp:1674] Status update manager successfully handled status update acknowledgement (UUID: 6d952e6d-b7d7-4f40-9f44-f7c3f81757af) for task 897522cc-4ec5-4904-aed0-00b6b8c41028 of framework 20140626-000439-1032504131-55423-5450-0000
I0626 00:04:40.578642  5450 sched.cpp:139] Version: 0.20.0
I0626 00:04:40.578886  5475 sched.cpp:235] New master detected at master@67.195.138.61:55423
I0626 00:04:40.578902  5475 sched.cpp:285] Authenticating with master master@67.195.138.61:55423
I0626 00:04:40.579040  5475 authenticatee.hpp:128] Creating new client SASL connection
I0626 00:04:40.579202  5475 master.cpp:3499] Authenticating scheduler-bb54dd52-95dc-4ed9-b69c-7a65f1661180@67.195.138.61:55423
I0626 00:04:40.579313  5475 authenticator.hpp:156] Creating new server SASL connection
I0626 00:04:40.579414  5475 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5
I0626 00:04:40.579430  5475 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5'
I0626 00:04:40.579457  5475 authenticator.hpp:262] Received SASL authentication start
I0626 00:04:40.579488  5475 authenticator.hpp:384] Authentication requires more steps
I0626 00:04:40.579514  5475 authenticatee.hpp:265] Received SASL authentication step
I0626 00:04:40.579551  5475 authenticator.hpp:290] Received SASL authentication step
I0626 00:04:40.579573  5475 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'juno.apache.org' server FQDN: 'juno.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false 
I0626 00:04:40.579586  5475 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I0626 00:04:40.579601  5475 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0626 00:04:40.579612  5475 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'juno.apache.org' server FQDN: 'juno.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true 
I0626 00:04:40.579619  5475 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0626 00:04:40.579624  5475 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0626 00:04:40.579638  5475 authenticator.hpp:376] Authentication success
I0626 00:04:40.579664  5475 authenticatee.hpp:305] Authentication success
I0626 00:04:40.579687  5475 master.cpp:3539] Successfully authenticated principal 'test-principal' at scheduler-bb54dd52-95dc-4ed9-b69c-7a65f1661180@67.195.138.61:55423
I0626 00:04:40.579768  5475 sched.cpp:359] Successfully authenticated with master master@67.195.138.61:55423
I0626 00:04:40.579781  5475 sched.cpp:478] Sending registration request to master@67.195.138.61:55423
I0626 00:04:40.579825  5475 master.cpp:1241] Received registration request from scheduler-bb54dd52-95dc-4ed9-b69c-7a65f1661180@67.195.138.61:55423
I0626 00:04:40.579845  5475 master.cpp:1201] Authorizing framework principal 'test-principal' to receive offers for role '*'
I0626 00:04:40.579984  5475 master.cpp:1300] Registering framework 20140626-000439-1032504131-55423-5450-0001 at scheduler-bb54dd52-95dc-4ed9-b69c-7a65f1661180@67.195.138.61:55423
I0626 00:04:40.580056  5475 sched.cpp:409] Framework registered with 20140626-000439-1032504131-55423-5450-0001
I0626 00:04:40.580075  5475 sched.cpp:423] Scheduler::registered took 8994ns
I0626 00:04:40.580117  5475 hierarchical_allocator_process.hpp:331] Added framework 20140626-000439-1032504131-55423-5450-0001
I0626 00:04:40.580173  5475 hierarchical_allocator_process.hpp:750] Offering cpus(*):1; mem(*):512; disk(*):1024; ports(*):[31000-32000] on slave 20140626-000439-1032504131-55423-5450-0 to framework 20140626-000439-1032504131-55423-5450-0001
I0626 00:04:40.580366  5475 hierarchical_allocator_process.hpp:833] F",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.8846153846153846,0.9878787878787878,0.9878787878787878,0.0
Improvement,Update Rate Limiting Design doc to reflect the latest changes,"- Usage
- Design
- Implementation Notes",2.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15789473684210525,0.15,0.20512820512820512,0.13939393939393938,0.13939393939393938,0.0
Bug,Improve child exit if slave dies during executor launch in MC,"When restarting many slaves there's a reasonable chance that a slave will be restarted between the fork and exec stages of launching an executor in the MesosContainerizer.

The forked child correctly detects this however rather than abort it should safely log and then exit non-zero cleanly.",1.0,0.19.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.10526315789473684,0.16428571428571428,0.10256410256410256,0.2121212121212121,0.2121212121212121,0.17431192660550457
Bug,Build failure: Ubuntu 13.10/clang due to missing virtual destructor,"In file included from launcher/main.cpp:19:
In file included from ./launcher/launcher.hpp:24:
In file included from ../3rdparty/libprocess/include/process/future.hpp:23:
../3rdparty/libprocess/include/process/owned.hpp:188:5: error: delete called on 'mesos::internal::launcher::Operation' that is abstract but has non-virtual destructor [-Werror,-Wdelete-non-virtual-dtor]
    delete t;
    ^
/usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/bits/shared_ptr_base.h:456:8: note: in instantiation of member function 'process::Owned<mesos::internal::launcher::Operation>::Data::~Data' requested here
              delete __p;
              ^
/usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/bits/shared_ptr_base.h:768:24: note: in instantiation of function template specialization 'std::__shared_count<2>::__shared_count<process::Owned<mesos::internal::launcher::Operation>::Data *>' requested here
        : _M_ptr(__p), _M_refcount(__p)
                       ^
/usr/bin/../lib/gcc/x86_64-linux-gnu/4.8/../../../../include/c++/4.8/bits/shared_ptr_base.h:919:4: note: in instantiation of function template specialization 'std::__shared_ptr<process::Owned<mesos::internal::launcher::Operation>::Data, 2>::__shared_ptr<process::Owned<mesos::internal::launcher::Operation>::Data>' requested here
          __shared_ptr(__p).swap(*this);
          ^
../3rdparty/libprocess/include/process/owned.hpp:68:10: note: in instantiation of function template specialization 'std::__shared_ptr<process::Owned<mesos::internal::launcher::Operation>::Data, 2>::reset<process::Owned<mesos::internal::launcher::Operation>::Data>' requested here
    data.reset(new Data(t));
         ^
./launcher/launcher.hpp:101:7: note: in instantiation of member function 'process::Owned<mesos::internal::launcher::Operation>::Owned' requested here
  add(process::Owned<Operation>(new T()));
      ^
launcher/main.cpp:26:3: note: in instantiation of function template specialization 'mesos::internal::launcher::add<mesos::internal::launcher::ShellOperation>' requested here
  launcher::add<launcher::ShellOperation>();
  ^
1 error generated.",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05263157894736842,0.12142857142857143,0.07692307692307693,0.13333333333333333,0.13333333333333333,0.0
Bug,LogZooKeeperTest.WriteRead test is flaky,"{code}
[ RUN      ] LogZooKeeperTest.WriteRead
I0527 23:23:48.286031  1352 zookeeper_test_server.cpp:158] Started ZooKeeperTestServer on port 39446
I0527 23:23:48.293916  1352 log_tests.cpp:1945] Using temporary directory '/tmp/LogZooKeeperTest_WriteRead_Vyty8g'
I0527 23:23:48.296430  1352 leveldb.cpp:176] Opened db in 2.459713ms
I0527 23:23:48.296740  1352 leveldb.cpp:183] Compacted db in 286843ns
I0527 23:23:48.296761  1352 leveldb.cpp:198] Created db iterator in 3083ns
I0527 23:23:48.296772  1352 leveldb.cpp:204] Seeked to beginning of db in 4541ns
I0527 23:23:48.296777  1352 leveldb.cpp:273] Iterated through 0 keys in the db in 87ns
I0527 23:23:48.296788  1352 replica.cpp:741] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0527 23:23:48.297499  1383 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 505340ns
I0527 23:23:48.297513  1383 replica.cpp:320] Persisted replica status to VOTING
I0527 23:23:48.299492  1352 leveldb.cpp:176] Opened db in 1.73582ms
I0527 23:23:48.299773  1352 leveldb.cpp:183] Compacted db in 263937ns
I0527 23:23:48.299793  1352 leveldb.cpp:198] Created db iterator in 7494ns
I0527 23:23:48.299806  1352 leveldb.cpp:204] Seeked to beginning of db in 235ns
I0527 23:23:48.299813  1352 leveldb.cpp:273] Iterated through 0 keys in the db in 93ns
I0527 23:23:48.299821  1352 replica.cpp:741] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0527 23:23:48.300503  1380 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 492309ns
I0527 23:23:48.300516  1380 replica.cpp:320] Persisted replica status to VOTING
I0527 23:23:48.302500  1352 leveldb.cpp:176] Opened db in 1.793829ms
I0527 23:23:48.303642  1352 leveldb.cpp:183] Compacted db in 1.123929ms
I0527 23:23:48.303669  1352 leveldb.cpp:198] Created db iterator in 5865ns
I0527 23:23:48.303689  1352 leveldb.cpp:204] Seeked to beginning of db in 8811ns
I0527 23:23:48.303705  1352 leveldb.cpp:273] Iterated through 1 keys in the db in 9545ns
I0527 23:23:48.303715  1352 replica.cpp:741] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
2014-05-27 23:23:48,303:1352(0x2b1173a29700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5
2014-05-27 23:23:48,303:1352(0x2b1173a29700):ZOO_INFO@log_env@716: Client environment:host.name=minerva
2014-05-27 23:23:48,303:1352(0x2b1173a29700):ZOO_INFO@log_env@723: Client environment:os.name=Linux
2014-05-27 23:23:48,303:1352(0x2b1173a29700):ZOO_INFO@log_env@724: Client environment:os.arch=3.2.0-57-generic
2014-05-27 23:23:48,303:1352(0x2b1173a29700):ZOO_INFO@log_env@725: Client environment:os.version=#87-Ubuntu SMP Tue Nov 12 21:35:10 UTC 2013
2014-05-27 23:23:48,303:1352(0x2b1173e2b700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5
2014-05-27 23:23:48,304:1352(0x2b1173e2b700):ZOO_INFO@log_env@716: Client environment:host.name=minerva
2014-05-27 23:23:48,304:1352(0x2b1173e2b700):ZOO_INFO@log_env@723: Client environment:os.name=Linux
2014-05-27 23:23:48,304:1352(0x2b1173e2b700):ZOO_INFO@log_env@724: Client environment:os.arch=3.2.0-57-generic
2014-05-27 23:23:48,304:1352(0x2b1173e2b700):ZOO_INFO@log_env@725: Client environment:os.version=#87-Ubuntu SMP Tue Nov 12 21:35:10 UTC 2013
2014-05-27 23:23:48,304:1352(0x2b1173a29700):ZOO_INFO@log_env@733: Client environment:user.name=(null)
I0527 23:23:48.303988  1380 log.cpp:238] Attempting to join replica to ZooKeeper group
2014-05-27 23:23:48,304:1352(0x2b1173e2b700):ZOO_INFO@log_env@733: Client environment:user.name=(null)
2014-05-27 23:23:48,304:1352(0x2b1173a29700):ZOO_INFO@log_env@741: Client environment:user.home=/home/jenkins
I0527 23:23:48.304198  1385 recover.cpp:425] Starting replica recovery
2014-05-27 23:23:48,304:1352(0x2b1173e2b700):ZOO_INFO@log_env@741: Client environment:user.home=/home/jenkins
2014-05-27 23:23:48,304:1352(0x2b1173a29700):ZOO_INFO@log_env@753: Client environment:user.dir=/tmp/LogZooKeeperTest_WriteRead_Vyty8g
2014-05-27 23:23:48,304:1352(0x2b1173a29700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=127.0.0.1:39446 sessionTimeout=5000 watcher=0x2b11708e98d0 sessionId=0 sessionPasswd=<null> context=0x2b118002f4e0 flags=0
2014-05-27 23:23:48,304:1352(0x2b1173e2b700):ZOO_INFO@log_env@753: Client environment:user.dir=/tmp/LogZooKeeperTest_WriteRead_Vyty8g
I0527 23:23:48.304352  1385 recover.cpp:451] Replica is in VOTING status
2014-05-27 23:23:48,304:1352(0x2b1173e2b700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=127.0.0.1:39446 sessionTimeout=5000 watcher=0x2b11708e98d0 sessionId=0 sessionPasswd=<null> context=0x2b1198015ca0 flags=0
I0527 23:23:48.304417  1385 recover.cpp:440] Recover process terminated
2014-05-27 23:23:48,304:1352(0x2b12897b8700):ZOO_INFO@check_events@1703: initiated connection to server [127.0.0.1:39446]
2014-05-27 23:23:48,304:1352(0x2b12891b5700):ZOO_INFO@check_events@1703: initiated connection to server [127.0.0.1:39446]
I0527 23:23:48.311262  1352 leveldb.cpp:176] Opened db in 7.261703ms
2014-05-27 23:23:48,311:1352(0x2b12897b8700):ZOO_INFO@check_events@1750: session establishment complete on server [127.0.0.1:39446], sessionId=0x1463fff34bd0000, negotiated timeout=6000
I0527 23:23:48.312379  1381 group.cpp:310] Group process ((614)@67.195.138.8:35151) connected to ZooKeeper
I0527 23:23:48.312407  1381 group.cpp:784] Syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0)
I0527 23:23:48.312417  1381 group.cpp:382] Trying to create path '/log' in ZooKeeper
I0527 23:23:48.312422  1352 leveldb.cpp:183] Compacted db in 1.119843ms
I0527 23:23:48.312505  1352 leveldb.cpp:198] Created db iterator in 3901ns
I0527 23:23:48.312526  1352 leveldb.cpp:204] Seeked to beginning of db in 7398ns
I0527 23:23:48.312541  1352 leveldb.cpp:273] Iterated through 1 keys in the db in 6345ns
I0527 23:23:48.312553  1352 replica.cpp:741] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
2014-05-27 23:23:48,312:1352(0x2b1173627700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5
2014-05-27 23:23:48,312:1352(0x2b1173627700):ZOO_INFO@log_env@716: Client environment:host.name=minerva
2014-05-27 23:23:48,312:1352(0x2b1173627700):ZOO_INFO@log_env@723: Client environment:os.name=Linux
2014-05-27 23:23:48,312:1352(0x2b1173627700):ZOO_INFO@log_env@724: Client environment:os.arch=3.2.0-57-generic
2014-05-27 23:23:48,312:1352(0x2b1173627700):ZOO_INFO@log_env@725: Client environment:os.version=#87-Ubuntu SMP Tue Nov 12 21:35:10 UTC 2013
2014-05-27 23:23:48,312:1352(0x2b1173627700):ZOO_INFO@log_env@733: Client environment:user.name=(null)
2014-05-27 23:23:48,312:1352(0x2b12891b5700):ZOO_INFO@check_events@1750: session establishment complete on server [127.0.0.1:39446], sessionId=0x1463fff34bd0001, negotiated timeout=6000
2014-05-27 23:23:48,313:1352(0x2b1173627700):ZOO_INFO@log_env@741: Client environment:user.home=/home/jenkins
2014-05-27 23:23:48,313:1352(0x2b1173627700):ZOO_INFO@log_env@753: Client environment:user.dir=/tmp/LogZooKeeperTest_WriteRead_Vyty8g
2014-05-27 23:23:48,313:1352(0x2b1173627700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=127.0.0.1:39446 sessionTimeout=5000 watcher=0x2b11708e98d0 sessionId=0 sessionPasswd=<null> context=0x2b119001fd20 flags=0
I0527 23:23:48.313247  1380 group.cpp:310] Group process ((616)@67.195.138.8:35151) connected to ZooKeeper
I0527 23:23:48.313266  1380 group.cpp:784] Syncing group operations: queue size (joins, cancels, datas) = (1, 0, 0)
I0527 23:23:48.313273  1380 group.cpp:382] Trying to create path '/log' in ZooKeeper
2014-05-27 23:23:48,313:1352(0x2b12889b0700):ZOO_INFO@check_events@1703: initiated connection to server [127.0.0.1:39446]
2014-05-27 23:23:48,313:1352(0x2b1173828700):ZOO_INFO@log_env@712: Client environment:zookeeper.version=zookeeper C client 3.4.5
2014-05-27 23:23:48,313:1352(0x2b1173828700):ZOO_INFO@log_env@716: Client environment:host.name=minerva
2014-05-27 23:23:48,313:1352(0x2b1173828700):ZOO_INFO@log_env@723: Client environment:os.name=Linux
2014-05-27 23:23:48,313:1352(0x2b1173828700):ZOO_INFO@log_env@724: Client environment:os.arch=3.2.0-57-generic
2014-05-27 23:23:48,313:1352(0x2b1173828700):ZOO_INFO@log_env@725: Client environment:os.version=#87-Ubuntu SMP Tue Nov 12 21:35:10 UTC 2013
I0527 23:23:48.313436  1387 log.cpp:238] Attempting to join replica to ZooKeeper group
2014-05-27 23:23:48,313:1352(0x2b1173828700):ZOO_INFO@log_env@733: Client environment:user.name=(null)
2014-05-27 23:23:48,313:1352(0x2b1173828700):ZOO_INFO@log_env@741: Client environment:user.home=/home/jenkins
2014-05-27 23:23:48,313:1352(0x2b1173828700):ZOO_INFO@log_env@753: Client environment:user.dir=/tmp/LogZooKeeperTest_WriteRead_Vyty8g
2014-05-27 23:23:48,313:1352(0x2b1173828700):ZOO_INFO@zookeeper_init@786: Initiating client connection, host=127.0.0.1:39446 sessionTimeout=5000 watcher=0x2b11708e98d0 sessionId=0 sessionPasswd=<null> context=0x2b1190011ea0 flags=0
I0527 23:23:48.313601  1387 recover.cpp:425] Starting replica recovery
I0527 23:23:48.313721  1382 recover.cpp:451] Replica is in VOTING status
I0527 23:23:48.313794  1382 recover.cpp:440] Recover process terminated
2014-05-27 23:23:48,313:1352(0x2b1288bb1700):ZOO_INFO@check_events@1703: initiated connection to server [127.0.0.1:39446]
I0527 23:23:48.313973  1383 log.cpp:656] Attempting to start the writer
2014-05-27 23:23:48,315:1352(0x2b12889b0700):ZOO_INFO@check_events@1750: session establishment complete on server [127.0.0.1:39446], sessionId=0x1463fff34bd0002, negotiated timeout=6000
I0527 23:23:48.315682  1387 group.cpp:310] Group process ((619)@67.195.138.8:35151) connected to ZooKeeper
2014-05-27 23:23:48,315:1352(0x2b1288bb1700):ZOO_INFO@check_events@1750: session establishment complete on server [127.0.0.1:39446], sessionId=0x1463fff34bd0003, negotiated timeout=6000
I0527 23:23:48.315709  1387 group.cpp:784] Syncing group operations: queue size (joins, cancels, datas) = (0, 0, 0)
I0527 23:23:48.315738  1387 group.cpp:382] Trying to create path '/log' in ZooKeeper
I0527 23:23:48.315964  1386 group.cpp:310] Group process ((621)@67.195.138.8:35151) connected to ZooKeeper
I0527 23:23:48.315981  1386 group.cpp:784] Syncing group operations: queue size (joins, cancels, datas) = (1, 0, 0)
I0527 23:23:48.315989  1386 group.cpp:382] Trying to create path '/log' in ZooKeeper
I0527 23:23:48.317881  1385 network.hpp:423] ZooKeeper group memberships changed
I0527 23:23:48.317937  1381 group.cpp:655] Trying to get '/log/0000000000' in ZooKeeper
I0527 23:23:48.318205  1382 network.hpp:423] ZooKeeper group memberships changed
I0527 23:23:48.318317  1383 group.cpp:655] Trying to get '/log/0000000000' in ZooKeeper
I0527 23:23:48.319154  1382 network.hpp:461] ZooKeeper group PIDs: { log-replica(22)@67.195.138.8:35151 }
I0527 23:23:48.319541  1386 network.hpp:461] ZooKeeper group PIDs: { log-replica(22)@67.195.138.8:35151 }
I0527 23:23:48.319851  1381 replica.cpp:474] Replica received implicit promise request with proposal 1
I0527 23:23:48.319905  1387 replica.cpp:474] Replica received implicit promise request with proposal 1
I0527 23:23:48.319907  1384 network.hpp:423] ZooKeeper group memberships changed
I0527 23:23:48.320091  1385 group.cpp:655] Trying to get '/log/0000000000' in ZooKeeper
I0527 23:23:48.320384  1383 network.hpp:423] ZooKeeper group memberships changed
I0527 23:23:48.320441  1381 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 568396ns
I0527 23:23:48.320456  1384 group.cpp:655] Trying to get '/log/0000000000' in ZooKeeper
I0527 23:23:48.320461  1381 replica.cpp:342] Persisted promised to 1
I0527 23:23:48.320446  1387 leveldb.cpp:306] Persisting metadata (8 bytes) to leveldb took 516015ns
I0527 23:23:48.320497  1387 replica.cpp:342] Persisted promised to 1
I0527 23:23:48.320814  1383 coordinator.cpp:230] Coordinator attemping to fill missing position
I0527 23:23:48.321050  1384 group.cpp:655] Trying to get '/log/0000000001' in ZooKeeper
I0527 23:23:48.321063  1385 group.cpp:655] Trying to get '/log/0000000001' in ZooKeeper
I0527 23:23:48.321341  1387 replica.cpp:375] Replica received explicit promise request for position 0 with proposal 2
I0527 23:23:48.321375  1381 replica.cpp:375] Replica received explicit promise request for position 0 with proposal 2
I0527 23:23:48.321506  1387 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 89us
I0527 23:23:48.321530  1387 replica.cpp:676] Persisted action at 0
I0527 23:23:48.321584  1381 leveldb.cpp:343] Persisting action (8 bytes) to leveldb took 122910ns
I0527 23:23:48.321602  1381 replica.cpp:676] Persisted action at 0
I0527 23:23:48.321775  1383 network.hpp:461] ZooKeeper group PIDs: { log-replica(22)@67.195.138.8:35151, log-replica(23)@67.195.138.8:35151 }
I0527 23:23:48.321961  1381 replica.cpp:508] Replica received write request for position 0
I0527 23:23:48.321984  1381 leveldb.cpp:438] Reading position from leveldb took 7813ns
I0527 23:23:48.322064  1380 network.hpp:461] ZooKeeper group PIDs: { log-replica(22)@67.195.138.8:35151, log-replica(23)@67.195.138.8:35151 }
I0527 23:23:48.322073  1381 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 78683ns
I0527 23:23:48.322077  1383 replica.cpp:508] Replica received write request for position 0
I0527 23:23:48.322084  1381 replica.cpp:676] Persisted action at 0
I0527 23:23:48.322111  1383 leveldb.cpp:438] Reading position from leveldb took 17416ns
I0527 23:23:48.322330  1383 leveldb.cpp:343] Persisting action (14 bytes) to leveldb took 157199ns
I0527 23:23:48.322345  1383 replica.cpp:676] Persisted action at 0
I0527 23:23:48.322522  1386 replica.cpp:655] Replica received learned notice for position 0
I0527 23:23:48.322523  1382 replica.cpp:655] Replica received learned notice for position 0
I0527 23:23:48.322638  1386 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 86907ns
I0527 23:23:48.322661  1386 replica.cpp:676] Persisted action at 0
I0527 23:23:48.322670  1386 replica.cpp:661] Replica learned NOP action at position 0
I0527 23:23:48.322682  1382 leveldb.cpp:343] Persisting action (16 bytes) to leveldb took 85031ns
I0527 23:23:48.322693  1382 replica.cpp:676] Persisted action at 0
I0527 23:23:48.322700  1382 replica.cpp:661] Replica learned NOP action at position 0
I0527 23:23:48.322790  1380 log.cpp:672] Writer started with ending position 0
I0527 23:23:48.322898  1380 log.cpp:680] Attempting to append 11 bytes to the log
I0527 23:23:48.322978  1383 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I0527 23:23:48.323122  1380 replica.cpp:508] Replica received write request for position 1
I0527 23:23:48.323158  1381 replica.cpp:508] Replica received write request for position 1
I0527 23:23:48.323202  1380 leveldb.cpp:343] Persisting action (27 bytes) to leveldb took 66527ns
I0527 23:23:48.323215  1380 replica.cpp:676] Persisted action at 1
I0527 23:23:48.323238  1381 leveldb.cpp:343] Persisting action (27 bytes) to leveldb took 67074ns
I0527 23:23:48.323252  1381 replica.cpp:676] Persisted action at 1
I0527 23:23:48.323354  1380 replica.cpp:655] Replica received learned notice for position 1
I0527 23:23:48.323362  1382 replica.cpp:655] Replica received learned notice for position 1
I0527 23:23:48.323443  1380 leveldb.cpp:343] Persisting action (29 bytes) to leveldb took 77398ns
I0527 23:23:48.323461  1380 replica.cpp:676] Persisted action at 1
I0527 23:23:48.323463  1382 leveldb.cpp:343] Persisting action (29 bytes) to leveldb took 90567ns
I0527 23:23:48.323467  1380 replica.cpp:661] Replica learned APPEND action at position 1
I0527 23:23:48.323477  1382 replica.cpp:676] Persisted action at 1
I0527 23:23:48.323484  1382 replica.cpp:661] Replica learned APPEND action at position 1
I0527 23:23:48.323729  1380 leveldb.cpp:438] Reading position from leveldb took 7224ns
2014-05-27 23:23:48,324:1352(0x2b1173c2a700):ZOO_INFO@zookeeper_close@2505: Closing zookeeper sessionId=0x1463fff34bd0003 to [127.0.0.1:39446]

2014-05-27 23:23:48,324:1352(0x2b117301ff80):ZOO_INFO@zookeeper_close@2505: Closing zookeeper sessionId=0x1463fff34bd0002 to [127.0.0.1:39446]

I0527 23:23:48.326591  1386 network.hpp:423] ZooKeeper group memberships changed
I0527 23:23:48.326690  1382 group.cpp:655] Trying to get '/log/0000000000' in ZooKeeper
I0527 23:23:48.327450  1384 network.hpp:461] ZooKeeper group PIDs: { log-replica(22)@67.195.138.8:35151 }
2014-05-27 23:23:48,446:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:23:51,782:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:23:55,118:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
I0527 23:23:57.002908  1381 network.hpp:423] ZooKeeper group memberships changed
I0527 23:23:57.003042  1381 network.hpp:461] ZooKeeper group PIDs: {  }
2014-05-27 23:23:58,455:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:24:01,791:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:24:05,127:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:24:08,464:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:24:11,800:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:24:15,136:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:24:18,473:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:24:21,809:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:24:25,146:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:24:28,482:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:24:31,818:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:24:35,155:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:24:38,491:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:24:41,827:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:24:45,164:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:24:48,500:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:24:51,834:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:24:55,171:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:24:58,507:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:25:01,844:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:25:05,180:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:25:08,516:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:25:11,853:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:25:15,186:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:25:18,523:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:25:21,859:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:25:25,195:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:25:28,530:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:25:31,866:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:25:35,203:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:25:38,539:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:25:41,875:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:25:45,212:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:25:48,548:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:25:51,885:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:25:55,221:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:25:58,557:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:26:01,894:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:26:05,230:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:26:08,567:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:26:11,903:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:26:15,239:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:26:18,576:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:26:21,912:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:26:25,248:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:26:28,585:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:26:31,921:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:26:35,257:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:26:38,594:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:26:41,930:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:26:45,267:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:26:48,603:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:26:51,939:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:26:55,276:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:26:58,612:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:27:01,948:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:27:05,285:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:27:08,621:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:27:11,958:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:27:15,294:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:27:18,630:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:27:21,967:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:27:25,303:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:27:28,639:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:27:31,976:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:27:35,312:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:27:38,649:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:27:41,985:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:27:45,321:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:27:48,658:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client
2014-05-27 23:27:51,994:1352(0x2b12bc401700):ZOO_ERROR@handle_socket_error_msg@1697: Socket [127.0.0.1:51020] zk retcode=-4, errno=111(Connection refused): server refused to accept the client

{code}",1.0,0.19.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.8846153846153846,0.9878787878787878,0.9878787878787878,0.17431192660550457
Improvement,Mesos tests should not rely on echo,"Triggered by MESOS-1413 I would like to propose changing our tests to not rely on {{echo}} but to use {{printf}} instead.

This seems to be useful as {{echo}} is introducing an extra linefeed after the supplied string whereas {{printf}} does not. The {{-n}} switch preventing that extra linefeed is unfortunately not portable - it is not supported by the builtin {{echo}} of the BSD / OSX {{/bin/sh}}.
",1.0,0,0.5,0.06334841628959276,0.0,0.0,0.0,0.0,0.0,0.2982456140350877,0.19999999999999998,0.3717948717948718,0.24848484848484845,0.24848484848484845,0.0
Improvement,Document perf isolator flags,"Document interval, duration and the event flags. Document event name normalization for the protobuf.",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.10526315789473684,0.16428571428571428,0.10256410256410256,0.2121212121212121,0.2121212121212121,0.0
Improvement,Introduce a PerfStatistics protobuf,"Field names from `perf list` normalized to convert hyphens to underscores and down-cased. Start with just the hardware and software events, not raw hardware, breakpoints or tracepoints, 

All fields should be optional. Include as an optional field to ResourceStatistics.",2.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.10526315789473684,0.16428571428571428,0.10256410256410256,0.2121212121212121,0.2121212121212121,0.0
Improvement,Test perf isolator for slave roll forward/roll back,"Test that changes to add/remove perf isolator will be handled through slave recovery, e.g., containers started without the perf isolator continue to report resource statistics and containers started with the perf isolator will include perf statistics.",2.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.10526315789473684,0.16428571428571428,0.10256410256410256,0.2121212121212121,0.2121212121212121,0.0
Improvement,Test different versions of perf,"Test across different kernel versions (at least 2.6.XX and 3.X) and across different distributions.

Test input flags and parsing output.",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.10526315789473684,0.16428571428571428,0.10256410256410256,0.2121212121212121,0.2121212121212121,0.0
Improvement,Write parser for perf output.,"1. Should support output from pid and cgroup targets.
2. Should support output for the same events from >= 1 cgroup
3. Should return as PerfStatistics protobuf.
",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.10526315789473684,0.16428571428571428,0.10256410256410256,0.2121212121212121,0.2121212121212121,0.0
Bug,Failure when znode is removed before we can read its contents.,"Looks like the following can occur when a znode goes away right before we can read it's contents:

{noformat: title=Slave exit}
I0520 16:33:45.721727 29155 group.cpp:382] Trying to create path '/home/mesos/test/master' in ZooKeeper
I0520 16:33:48.600837 29155 detector.cpp:134] Detected a new leader: (id='2617')
I0520 16:33:48.601428 29147 group.cpp:655] Trying to get '/home/mesos/test/master/info_0000002617' in ZooKeeper
Failed to detect a master: Failed to get data for ephemeral node '/home/mesos/test/master/info_0000002617' in ZooKeeper: no node
Slave Exit Status: 1
{noformat}",3.0,0.19.0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.15789473684210525,0.15,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.17431192660550457
Task,Keep track of the principals for authenticated pids in Master.,Need to add a 'principal' field to FrameworkInfo and verify if the Framework has the claimed principal during registration.,3.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.15789473684210525,0.15,0.0,0.006060606060606061,0.006060606060606061,0.0
Task,Expose libprocess queue length from scheduler driver to metrics endpoint,We expose the master's event queue length and we should do the same for the scheduler driver.,1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05263157894736842,0.12142857142857143,0.07692307692307693,0.13333333333333333,0.13333333333333333,0.0
Bug,SlaveRecoveryTest/0.MultipleFrameworks is flaky,"--gtest_repeat=-1 --gtest_shuffle --gtest_break_on_failure

{noformat}
[ RUN      ] SlaveRecoveryTest/0.MultipleFrameworks
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0513 15:42:05.931761  4320 exec.cpp:131] Version: 0.19.0
I0513 15:42:05.936698  4340 exec.cpp:205] Executor registered on slave 20140513-154204-16842879-51872-13062-0
Registered executor on artoo
Starting task 51991f97-f5fd-4905-ad0f-02668083af7c
Forked command at 4367
sh -c 'sleep 1000'
WARNING: Logging before InitGoogleLogging() is written to STDERR
I0513 15:42:06.915061  4408 exec.cpp:131] Version: 0.19.0
I0513 15:42:06.931149  4435 exec.cpp:205] Executor registered on slave 20140513-154204-16842879-51872-13062-0
Registered executor on artoo
Starting task eaf5d8d6-3a6c-4ee1-84c1-fae20fb1df83
sh -c 'sleep 1000'
Forked command at 4439
I0513 15:42:06.998332  4340 exec.cpp:251] Received reconnect request from slave 20140513-154204-16842879-51872-13062-0
I0513 15:42:06.998414  4436 exec.cpp:251] Received reconnect request from slave 20140513-154204-16842879-51872-13062-0
I0513 15:42:07.006350  4437 exec.cpp:228] Executor re-registered on slave 20140513-154204-16842879-51872-13062-0
Re-registered executor on artoo
I0513 15:42:07.027039  4337 exec.cpp:378] Executor asked to shutdown
Shutting down
Sending SIGTERM to process tree at pid 4367
Killing the following process trees:
[ 
-+- 4367 sh -c sleep 1000 
 \--- 4368 sleep 1000 
]
../../src/tests/slave_recovery_tests.cpp:2807: Failure
Value of: status1.get().state()
  Actual: TASK_FAILED
Expected: TASK_KILLED

Program received signal SIGSEGV, Segmentation fault.
testing::UnitTest::AddTestPartResult (this=0x154dac0 <testing::UnitTest::GetInstance()::instance>, result_type=testing::TestPartResult::kFatalFailure, file_name=0xeb6b6c ""../../src/tests/slave_recovery_tests.cpp"", line_number=2807, message=..., os_stack_trace=...) at gmock-1.6.0/gtest/src/gtest.cc:3795
3795          *static_cast<volatile int*>(NULL) = 1;
(gdb) bt
#0  testing::UnitTest::AddTestPartResult (this=0x154dac0 <testing::UnitTest::GetInstance()::instance>, result_type=testing::TestPartResult::kFatalFailure, file_name=0xeb6b6c ""../../src/tests/slave_recovery_tests.cpp"", line_number=2807, message=..., os_stack_trace=...) at gmock-1.6.0/gtest/src/gtest.cc:3795
#1  0x0000000000df98b9 in testing::internal::AssertHelper::operator= (this=0x7fffffffb860, message=...) at gmock-1.6.0/gtest/src/gtest.cc:356
#2  0x0000000000cdfa57 in SlaveRecoveryTest_MultipleFrameworks_Test<mesos::internal::slave::MesosContainerizer>::TestBody (this=0x1954db0) at ../../src/tests/slave_recovery_tests.cpp:2807
#3  0x0000000000e22583 in testing::internal::HandleSehExceptionsInMethodIfSupported<testing::Test, void> (object=0x1954db0, method=&virtual testing::Test::TestBody(), location=0xed0af0 ""the test body"") at gmock-1.6.0/gtest/src/gtest.cc:2090
#4  0x0000000000e12467 in testing::internal::HandleExceptionsInMethodIfSupported<testing::Test, void> (object=0x1954db0, method=&virtual testing::Test::TestBody(), location=0xed0af0 ""the test body"") at gmock-1.6.0/gtest/src/gtest.cc:2126
#5  0x0000000000e010d5 in testing::Test::Run (this=0x1954db0) at gmock-1.6.0/gtest/src/gtest.cc:2161
#6  0x0000000000e01ceb in testing::TestInfo::Run (this=0x158cf80) at gmock-1.6.0/gtest/src/gtest.cc:2338
#7  0x0000000000e02387 in testing::TestCase::Run (this=0x158a880) at gmock-1.6.0/gtest/src/gtest.cc:2445
#8  0x0000000000e079ed in testing::internal::UnitTestImpl::RunAllTests (this=0x1558b40) at gmock-1.6.0/gtest/src/gtest.cc:4237
#9  0x0000000000e1ec83 in testing::internal::HandleSehExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool> (object=0x1558b40, method=(bool (testing::internal::UnitTestImpl::*)(testing::internal::UnitTestImpl * const)) 0xe07700 <testing::internal::UnitTestImpl::RunAllTests()>, 
    location=0xed1219 ""auxiliary test code (environments or event listeners)"") at gmock-1.6.0/gtest/src/gtest.cc:2090
#10 0x0000000000e14217 in testing::internal::HandleExceptionsInMethodIfSupported<testing::internal::UnitTestImpl, bool> (object=0x1558b40, method=(bool (testing::internal::UnitTestImpl::*)(testing::internal::UnitTestImpl * const)) 0xe07700 <testing::internal::UnitTestImpl::RunAllTests()>, 
    location=0xed1219 ""auxiliary test code (environments or event listeners)"") at gmock-1.6.0/gtest/src/gtest.cc:2126
#11 0x0000000000e076d7 in testing::UnitTest::Run (this=0x154dac0 <testing::UnitTest::GetInstance()::instance>) at gmock-1.6.0/gtest/src/gtest.cc:3872
#12 0x0000000000b99887 in main (argc=1, argv=0x7fffffffd9f8) at ../../src/tests/main.cpp:107
(gdb) frame 2
#2  0x0000000000cdfa57 in SlaveRecoveryTest_MultipleFrameworks_Test<mesos::internal::slave::MesosContainerizer>::TestBody (this=0x1954db0) at ../../src/tests/slave_recovery_tests.cpp:2807
2807      ASSERT_EQ(TASK_KILLED, status1.get().state());
(gdb) p status1
$1 = {data = {<std::__shared_ptr<process::Future<mesos::TaskStatus>::Data, 2>> = {_M_ptr = 0x1963140, _M_refcount = {_M_pi = 0x198a620}}, <No data fields>}}
(gdb) p status1.get()
$2 = (const mesos::TaskStatus &) @0x7fffdc5bf5f0: {<google::protobuf::Message> = {<google::protobuf::MessageLite> = {_vptr$MessageLite = 0x7ffff74bc940 <vtable for mesos::TaskStatus+16>}, <No data fields>}, static kTaskIdFieldNumber = 1, static kStateFieldNumber = 2, static kMessageFieldNumber = 4, 
  static kDataFieldNumber = 3, static kSlaveIdFieldNumber = 5, static kTimestampFieldNumber = 6, _unknown_fields_ = {fields_ = 0x0}, task_id_ = 0x7fffdc5ce9a0, message_ = 0x7fffdc5f5880, data_ = 0x154b4b0 <google::protobuf::internal::kEmptyString>, slave_id_ = 0x7fffdc59c4f0, timestamp_ = 1429688582.046252, 
  state_ = 3, _cached_size_ = 0, _has_bits_ = {55}, static default_instance_ = 0x0}
(gdb) p status1.get().state()
$3 = mesos::TASK_FAILED
(gdb) list
2802      // Kill task 1.
2803      driver1.killTask(task1.task_id());
2804
2805      // Wait for TASK_KILLED update.
2806      AWAIT_READY(status1);
2807      ASSERT_EQ(TASK_KILLED, status1.get().state());
2808
2809      // Kill task 2.
2810      driver2.killTask(task2.task_id());
2811
{noformat}",1.0,0,0.5,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.07692307692307693,0.13333333333333333,0.13333333333333333,0.0
Bug,GarbageCollectorIntegrationTest.DiskUsage is flaky.,"From Jenkins:
https://builds.apache.org/job/Mesos-Ubuntu-distcheck/79/consoleFull

{noformat}
[ RUN      ] GarbageCollectorIntegrationTest.DiskUsage
Using temporary directory '/tmp/GarbageCollectorIntegrationTest_DiskUsage_pU3Ym7'
I0507 03:27:38.775058  5758 leveldb.cpp:174] Opened db in 44.343989ms
I0507 03:27:38.787498  5758 leveldb.cpp:181] Compacted db in 12.411065ms
I0507 03:27:38.787533  5758 leveldb.cpp:196] Created db iterator in 4008ns
I0507 03:27:38.787545  5758 leveldb.cpp:202] Seeked to beginning of db in 598ns
I0507 03:27:38.787552  5758 leveldb.cpp:271] Iterated through 0 keys in the db in 173ns
I0507 03:27:38.787564  5758 replica.cpp:741] Replica recovered with log positions 0 -> 0 with 1 holes and 0 unlearned
I0507 03:27:38.787858  5777 recover.cpp:425] Starting replica recovery
I0507 03:27:38.788352  5793 master.cpp:267] Master 20140507-032738-453759884-58462-5758 (hemera.apache.org) started on 140.211.11.27:58462
I0507 03:27:38.788377  5793 master.cpp:304] Master only allowing authenticated frameworks to register
I0507 03:27:38.788383  5793 master.cpp:309] Master only allowing authenticated slaves to register
I0507 03:27:38.788389  5793 credentials.hpp:35] Loading credentials for authentication
I0507 03:27:38.789064  5779 recover.cpp:451] Replica is in EMPTY status
W0507 03:27:38.789115  5793 credentials.hpp:48] Failed to stat credentials file 'file:///tmp/GarbageCollectorIntegrationTest_DiskUsage_pU3Ym7/credentials': No such file or directory
I0507 03:27:38.789489  5779 master.cpp:104] No whitelist given. Advertising offers for all slaves
I0507 03:27:38.789531  5778 hierarchical_allocator_process.hpp:301] Initializing hierarchical allocator process with master : master@140.211.11.27:58462
I0507 03:27:38.791007  5788 replica.cpp:638] Replica in EMPTY status received a broadcasted recover request
I0507 03:27:38.791177  5780 master.cpp:921] The newly elected leader is master@140.211.11.27:58462 with id 20140507-032738-453759884-58462-5758
I0507 03:27:38.791198  5780 master.cpp:931] Elected as the leading master!
I0507 03:27:38.791205  5780 master.cpp:752] Recovering from registrar
I0507 03:27:38.791251  5796 recover.cpp:188] Received a recover response from a replica in EMPTY status
I0507 03:27:38.791323  5797 registrar.cpp:313] Recovering registrar
I0507 03:27:38.792137  5795 recover.cpp:542] Updating replica status to STARTING
I0507 03:27:38.807531  5781 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 15.124092ms
I0507 03:27:38.807559  5781 replica.cpp:320] Persisted replica status to STARTING
I0507 03:27:38.807621  5781 recover.cpp:451] Replica is in STARTING status
I0507 03:27:38.809319  5799 replica.cpp:638] Replica in STARTING status received a broadcasted recover request
I0507 03:27:38.809983  5795 recover.cpp:188] Received a recover response from a replica in STARTING status
I0507 03:27:38.811204  5778 recover.cpp:542] Updating replica status to VOTING
I0507 03:27:38.827595  5795 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 16.011355ms
I0507 03:27:38.827627  5795 replica.cpp:320] Persisted replica status to VOTING
I0507 03:27:38.827683  5795 recover.cpp:556] Successfully joined the Paxos group
I0507 03:27:38.827775  5795 recover.cpp:440] Recover process terminated
I0507 03:27:38.828966  5780 log.cpp:656] Attempting to start the writer
I0507 03:27:38.831114  5782 replica.cpp:474] Replica received implicit promise request with proposal 1
I0507 03:27:38.847708  5782 leveldb.cpp:304] Persisting metadata (8 bytes) to leveldb took 16.573137ms
I0507 03:27:38.847739  5782 replica.cpp:342] Persisted promised to 1
I0507 03:27:38.848141  5797 coordinator.cpp:230] Coordinator attemping to fill missing position
I0507 03:27:38.849684  5790 replica.cpp:375] Replica received explicit promise request for position 0 with proposal 2
I0507 03:27:38.863777  5790 leveldb.cpp:341] Persisting action (8 bytes) to leveldb took 14.076775ms
I0507 03:27:38.863801  5790 replica.cpp:676] Persisted action at 0
I0507 03:27:38.864915  5798 replica.cpp:508] Replica received write request for position 0
I0507 03:27:38.864949  5798 leveldb.cpp:436] Reading position from leveldb took 11807ns
I0507 03:27:38.879945  5798 leveldb.cpp:341] Persisting action (14 bytes) to leveldb took 14.978446ms
I0507 03:27:38.879976  5798 replica.cpp:676] Persisted action at 0
I0507 03:27:38.880491  5797 replica.cpp:655] Replica received learned notice for position 0
I0507 03:27:38.895969  5797 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 15.459949ms
I0507 03:27:38.895992  5797 replica.cpp:676] Persisted action at 0
I0507 03:27:38.896003  5797 replica.cpp:661] Replica learned NOP action at position 0
I0507 03:27:38.896411  5783 log.cpp:672] Writer started with ending position 0
I0507 03:27:38.898058  5798 leveldb.cpp:436] Reading position from leveldb took 11910ns
I0507 03:27:38.899749  5777 registrar.cpp:346] Successfully fetched the registry (0B)
I0507 03:27:38.899766  5777 registrar.cpp:422] Attempting to update the 'registry'
I0507 03:27:38.901458  5791 log.cpp:680] Attempting to append 137 bytes to the log
I0507 03:27:38.901666  5780 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 1
I0507 03:27:38.902773  5783 replica.cpp:508] Replica received write request for position 1
I0507 03:27:38.916127  5783 leveldb.cpp:341] Persisting action (156 bytes) to leveldb took 13.225715ms
I0507 03:27:38.916152  5783 replica.cpp:676] Persisted action at 1
I0507 03:27:38.916534  5790 replica.cpp:655] Replica received learned notice for position 1
I0507 03:27:38.928203  5790 leveldb.cpp:341] Persisting action (158 bytes) to leveldb took 11.652434ms
I0507 03:27:38.928225  5790 replica.cpp:676] Persisted action at 1
I0507 03:27:38.928236  5790 replica.cpp:661] Replica learned APPEND action at position 1
I0507 03:27:38.928546  5790 registrar.cpp:479] Successfully updated 'registry'
I0507 03:27:38.928642  5790 registrar.cpp:372] Successfully recovered registrar
I0507 03:27:38.929044  5783 master.cpp:779] Recovered 0 slaves from the Registry (99B) ; allowing 10mins for slaves to re-register
I0507 03:27:38.929502  5799 log.cpp:699] Attempting to truncate the log to 1
I0507 03:27:38.929888  5797 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 2
I0507 03:27:38.930161  5781 replica.cpp:508] Replica received write request for position 2
I0507 03:27:38.932977  5789 slave.cpp:140] Slave started on 56)@140.211.11.27:58462
I0507 03:27:38.932991  5789 credentials.hpp:35] Loading credentials for authentication
W0507 03:27:38.933567  5789 credentials.hpp:48] Failed to stat credentials file 'file:///tmp/GarbageCollectorIntegrationTest_DiskUsage_A9Pxks/credential': No such file or directory
I0507 03:27:38.933585  5789 slave.cpp:230] Slave using credential for: test-principal
I0507 03:27:38.933765  5789 slave.cpp:243] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0507 03:27:38.933854  5789 slave.cpp:271] Slave hostname: hemera.apache.org
I0507 03:27:38.933863  5789 slave.cpp:272] Slave checkpoint: false
I0507 03:27:38.934239  5778 state.cpp:33] Recovering state from '/tmp/GarbageCollectorIntegrationTest_DiskUsage_A9Pxks/meta'
I0507 03:27:38.934960  5792 status_update_manager.cpp:193] Recovering status update manager
I0507 03:27:38.935123  5779 slave.cpp:2945] Finished recovery
I0507 03:27:38.936998  5779 slave.cpp:526] New master detected at master@140.211.11.27:58462
I0507 03:27:38.937021  5779 slave.cpp:586] Authenticating with master master@140.211.11.27:58462
I0507 03:27:38.937077  5798 status_update_manager.cpp:167] New master detected at master@140.211.11.27:58462
I0507 03:27:38.937306 5779 slave.cpp:559] Detecting new master
I0507 03:27:38.937335  5800 authenticatee.hpp:128] Creating new client SASL connection
I0507 03:27:38.938030  5778 master.cpp:2798] Authenticating slave(56)@140.211.11.27:58462
I0507 03:27:38.938742 5783 authenticator.hpp:148] Creating new server SASL connection
I0507 03:27:38.939312  5786 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5
I0507 03:27:38.939340  5786 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5'
I0507 03:27:38.939390  5786 authenticator.hpp:254] Received SASL authentication start
I0507 03:27:38.939553  5786 authenticator.hpp:342] Authentication requires more steps
I0507 03:27:38.939592  5786 authenticatee.hpp:265] Received SASL authentication step
I0507 03:27:38.939715  5786 authenticator.hpp:282] Received SASL authentication step
I0507 03:27:38.939803  5786 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'hemera.apache.org' server FQDN: 'hemera.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false
I0507 03:27:38.939821 5786 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I0507 03:27:38.939831  5786 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0507 03:27:38.939841  5786 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'hemera.apache.org' server FQDN: 'hemera.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true
I0507 03:27:38.939851 5786 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0507 03:27:38.939857  5786 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0507 03:27:38.939870  5786 authenticator.hpp:334] Authentication success
I0507 03:27:38.939937  5786 authenticatee.hpp:305] Authentication success
I0507 03:27:38.940016  5778 master.cpp:2838] Successfully authenticated slave(56)@140.211.11.27:58462
I0507 03:27:38.940449 5799 slave.cpp:643] Successfully authenticated with master master@140.211.11.27:58462
I0507 03:27:38.940513 5799 slave.cpp:872] Will retry registration in 5.176207635secs if necessary
I0507 03:27:38.940625  5794 master.cpp:2134] Registering slave at slave(56)@140.211.11.27:58462 (hemera.apache.org) with id 20140507-032738-453759884-58462-5758-0
I0507 03:27:38.940800 5796 registrar.cpp:422] Attempting to update the 'registry'
I0507 03:27:38.940850  5781 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 10.659152ms
I0507 03:27:38.940871  5781 replica.cpp:676] Persisted action at 2
I0507 03:27:38.941843  5788 replica.cpp:655] Replica received learned notice for position 2
I0507 03:27:38.953193  5788 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 11.291343ms
I0507 03:27:38.953258  5788 leveldb.cpp:399] Deleting ~1 keys from leveldb took 33725ns
I0507 03:27:38.953274  5788 replica.cpp:676] Persisted action at 2
I0507 03:27:38.953282  5788 replica.cpp:661] Replica learned TRUNCATE action at position 2
I0507 03:27:38.953541  5797 log.cpp:680] Attempting to append 330 bytes to the log
I0507 03:27:38.953614  5797 coordinator.cpp:340] Coordinator attempting to write APPEND action at position 3
I0507 03:27:38.954731  5789 replica.cpp:508] Replica received write request for position 3
I0507 03:27:38.965240  5789 leveldb.cpp:341] Persisting action (349 bytes) to leveldb took 10.489719ms
I0507 03:27:38.965261  5789 replica.cpp:676] Persisted action at 3
I0507 03:27:38.966253  5780 replica.cpp:655] Replica received learned notice for position 3
I0507 03:27:38.977375  5780 leveldb.cpp:341] Persisting action (351 bytes) to leveldb took 11.098798ms
I0507 03:27:38.977408  5780 replica.cpp:676] Persisted action at 3
I0507 03:27:38.977421  5780 replica.cpp:661] Replica learned APPEND action at position 3
I0507 03:27:38.977859  5792 registrar.cpp:479] Successfully updated 'registry'
I0507 03:27:38.977926  5780 log.cpp:699] Attempting to truncate the log to 3
I0507 03:27:38.978060  5792 master.cpp:2174] Registered slave 20140507-032738-453759884-58462-5758-0 at slave(56)@140.211.11.27:58462 (hemera.apache.org)
I0507 03:27:38.978112  5792 master.cpp:3283] Adding slave 20140507-032738-453759884-58462-5758-0 at slave(56)@140.211.11.27:58462 (hemera.apache.org) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0507 03:27:38.978134  5784 coordinator.cpp:340] Coordinator attempting to write TRUNCATE action at position 4
I0507 03:27:38.978508  5785 slave.cpp:676] Registered with master master@140.211.11.27:58462; given slave ID 20140507-032738-453759884-58462-5758-0
I0507 03:27:38.978631 5786 hierarchical_allocator_process.hpp:444] Added slave 20140507-032738-453759884-58462-5758-0 (hemera.apache.org) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] available)
I0507 03:27:38.978677  5786 hierarchical_allocator_process.hpp:707] Performed allocation for slave 20140507-032738-453759884-58462-5758-0 in 5421ns
I0507 03:27:38.979872  5796 replica.cpp:508] Replica received write request for position 4
I0507 03:27:38.982084  5758 sched.cpp:121] Version: 0.19.0
I0507 03:27:38.982213  5789 sched.cpp:217] New master detected at master@140.211.11.27:58462
I0507 03:27:38.982228  5789 sched.cpp:268] Authenticating with master master@140.211.11.27:58462
I0507 03:27:38.982347  5788 authenticatee.hpp:128] Creating new client SASL connection
I0507 03:27:38.982676  5788 master.cpp:2798] Authenticating scheduler(59)@140.211.11.27:58462
I0507 03:27:38.983100  5788 authenticator.hpp:148] Creating new server SASL connection
I0507 03:27:38.983294  5788 authenticatee.hpp:219] Received SASL authentication mechanisms: CRAM-MD5
I0507 03:27:38.983312  5788 authenticatee.hpp:245] Attempting to authenticate with mechanism 'CRAM-MD5'
I0507 03:27:38.983360  5788 authenticator.hpp:254] Received SASL authentication start
I0507 03:27:38.983505  5788 authenticator.hpp:342] Authentication requires more steps
I0507 03:27:38.984220  5782 authenticatee.hpp:265] Received SASL authentication step
I0507 03:27:38.984275  5782 authenticator.hpp:282] Received SASL authentication step
I0507 03:27:38.984315  5782 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'hemera.apache.org' server FQDN: 'hemera.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: false
I0507 03:27:38.984347 5782 auxprop.cpp:153] Looking up auxiliary property '*userPassword'
I0507 03:27:38.984359  5782 auxprop.cpp:153] Looking up auxiliary property '*cmusaslsecretCRAM-MD5'
I0507 03:27:38.984370  5782 auxprop.cpp:81] Request to lookup properties for user: 'test-principal' realm: 'hemera.apache.org' server FQDN: 'hemera.apache.org' SASL_AUXPROP_VERIFY_AGAINST_HASH: false SASL_AUXPROP_OVERRIDE: false SASL_AUXPROP_AUTHZID: true
I0507 03:27:38.984377 5782 auxprop.cpp:103] Skipping auxiliary property '*userPassword' since SASL_AUXPROP_AUTHZID == true
I0507 03:27:38.984383  5782 auxprop.cpp:103] Skipping auxiliary property '*cmusaslsecretCRAM-MD5' since SASL_AUXPROP_AUTHZID == true
I0507 03:27:38.984397  5782 authenticator.hpp:334] Authentication success
I0507 03:27:38.984429  5782 authenticatee.hpp:305] Authentication success
I0507 03:27:38.984469  5795 master.cpp:2838] Successfully authenticated scheduler(59)@140.211.11.27:58462
I0507 03:27:38.985110  5782 sched.cpp:342] Successfully authenticated with master master@140.211.11.27:58462
I0507 03:27:38.985133  5782 sched.cpp:461] Sending registration request to master@140.211.11.27:58462
I0507 03:27:38.985326 5795 master.cpp:980] Received registration request from scheduler(59)@140.211.11.27:58462
I0507 03:27:38.985357  5795 master.cpp:998] Registering framework 20140507-032738-453759884-58462-5758-0000 at scheduler(59)@140.211.11.27:58462
I0507 03:27:38.985424  5795 sched.cpp:392] Framework registered with 20140507-032738-453759884-58462-5758-0000
I0507 03:27:38.985471  5792 hierarchical_allocator_process.hpp:331] Added framework 20140507-032738-453759884-58462-5758-0000
I0507 03:27:38.985610  5795 sched.cpp:406] Scheduler::registered took 36702ns
I0507 03:27:38.985646  5792 hierarchical_allocator_process.hpp:751] Offering cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140507-032738-453759884-58462-5758-0 to framework 20140507-032738-453759884-58462-5758-0000
I0507 03:27:38.985954  5792 hierarchical_allocator_process.hpp:687] Performed allocation for 1 slaves in 330895ns
I0507 03:27:38.986001  5789 master.hpp:612] Adding offer 20140507-032738-453759884-58462-5758-0 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140507-032738-453759884-58462-5758-0 (hemera.apache.org)
I0507 03:27:38.986090  5789 master.cpp:2747] Sending 1 offers to framework 20140507-032738-453759884-58462-5758-0000
I0507 03:27:38.986548  5792 sched.cpp:529] Scheduler::resourceOffers took 162873ns
I0507 03:27:38.986721  5792 master.hpp:622] Removing offer 20140507-032738-453759884-58462-5758-0 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 20140507-032738-453759884-58462-5758-0 (hemera.apache.org)
I0507 03:27:38.986781  5792 master.cpp:1812] Processing reply for offers: [ 20140507-032738-453759884-58462-5758-0 ] on slave 20140507-032738-453759884-58462-5758-0 at slave(56)@140.211.11.27:58462 (hemera.apache.org) for framework 20140507-032738-453759884-58462-5758-0000
I0507 03:27:38.986843  5792 master.hpp:584] Adding task 0 with resources cpus(*):2; mem(*):1024 on slave 20140507-032738-453759884-58462-5758-0 (hemera.apache.org)
I0507 03:27:38.986876  5792 master.cpp:2922] Launching task 0 of framework 20140507-032738-453759884-58462-5758-0000 with resources cpus(*):2; mem(*):1024 on slave 20140507-032738-453759884-58462-5758-0 at slave(56)@140.211.11.27:58462 (hemera.apache.org)
I0507 03:27:38.986981  5795 slave.cpp:906] Got assigned task 0 for framework 20140507-032738-453759884-58462-5758-0000
I0507 03:27:38.987180  5795 slave.cpp:1016] Launching task 0 for framework 20140507-032738-453759884-58462-5758-0000
I0507 03:27:38.987203  5787 hierarchical_allocator_process.hpp:546] Framework 20140507-032738-453759884-58462-5758-0000 left disk(*):1024; ports(*):[31000-32000] unused on slave 20140507-032738-453759884-58462-5758-0
I0507 03:27:38.987287  5787 hierarchical_allocator_process.hpp:589] Framework 20140507-032738-453759884-58462-5758-0000 filtered slave 20140507-032738-453759884-58462-5758-0 for 5secs
I0507 03:27:38.991395  5795 exec.cpp:131] Version: 0.19.0
I0507 03:27:38.991497  5779 exec.cpp:181] Executor started at: executor(27)@140.211.11.27:58462 with pid 5758
I0507 03:27:38.991510  5795 slave.cpp:1126] Queuing task '0' for executor default of framework '20140507-032738-453759884-58462-5758-0000
I0507 03:27:38.991566  5795 slave.cpp:487] Successfully attached file '/tmp/GarbageCollectorIntegrationTest_DiskUsage_A9Pxks/slaves/20140507-032738-453759884-58462-5758-0/frameworks/20140507-032738-453759884-58462-5758-0000/executors/default/runs/de776bec-2822-4bbc-befc-eec40eb5f674'
I0507 03:27:38.991595  5795 slave.cpp:2283] Monitoring executor 'default' of framework '20140507-032738-453759884-58462-5758-0000' in container 'de776bec-2822-4bbc-befc-eec40eb5f674'
I0507 03:27:38.991778  5795 slave.cpp:1599] Got registration for executor 'default' of framework 20140507-032738-453759884-58462-5758-0000
I0507 03:27:38.991874  5795 slave.cpp:1718] Flushing queued task 0 for executor 'default' of framework 20140507-032738-453759884-58462-5758-0000
I0507 03:27:38.991935  5780 exec.cpp:205] Executor registered on slave 20140507-032738-453759884-58462-5758-0
I0507 03:27:38.993419  5796 leveldb.cpp:341] Persisting action (16 bytes) to leveldb took 13.489998ms
I0507 03:27:38.993449  5796 replica.cpp:676] Persisted action at 4
I0507 03:27:38.994510  5777 replica.cpp:655] Replica received learned notice for position 4
I0507 03:27:38.994753  5780 exec.cpp:217] Executor::registered took 14516ns
I0507 03:27:38.994818  5780 exec.cpp:292] Executor asked to run task '0'
I0507 03:27:38.994849  5780 exec.cpp:301] Executor::launchTask took 18872ns
I0507 03:27:38.996703  5780 exec.cpp:524] Executor sending status update TASK_RUNNING (UUID: be7346ad-e198-4b38-9252-421ff759fdee) for task 0 of framework 20140507-032738-453759884-58462-5758-0000
I0507 03:27:38.996793  5780 slave.cpp:1954] Handling status update TASK_RUNNING (UUID: be7346ad-e198-4b38-9252-421ff759fdee) for task 0 of framework 20140507-032738-453759884-58462-5758-0000 from executor(27)@140.211.11.27:58462
I0507 03:27:38.996888  5780 status_update_manager.cpp:320] Received status update TASK_RUNNING (UUID: be7346ad-e198-4b38-9252-421ff759fdee) for task 0 of framework 20140507-032738-453759884-58462-5758-0000
I0507 03:27:38.996920  5780 status_update_manager.cpp:499] Creating StatusUpdate stream for task 0 of framework 20140507-032738-453759884-58462-5758-0000
I0507 03:27:38.996968  5780 status_update_manager.cpp:373] Forwarding status update TASK_RUNNING (UUID: be7346ad-e198-4b38-9252-421ff759fdee) for task 0 of framework 20140507-032738-453759884-58462-5758-0000 to master@140.211.11.27:58462
I0507 03:27:38.997189  5790 master.cpp:2450] Status update TASK_RUNNING (UUID: be7346ad-e198-4b38-9252-421ff759fdee) for task 0 of framework 20140507-032738-453759884-58462-5758-0000 from slave 20140507-032738-453759884-58462-5758-0 at slave(56)@140.211.11.27:58462 (hemera.apache.org)
I0507 03:27:38.997268  5780 slave.cpp:2071] Status update manager successfully handled status update TASK_RUNNING (UUID: be7346ad-e198-4b38-9252-421ff759fdee) for task 0 of framework 20140507-032738-453759884-58462-5758-0000
I0507 03:27:38.997321  5797 sched.cpp:620] Scheduler::statusUpdate took 77906ns
I0507 03:27:38.997336  5780 slave.cpp:2077] Sending acknowledgement for status update TASK_RUNNING (UUID: be7346ad-e198-4b38-9252-421ff759fdee) for task 0 of framework 20140507-032738-453759884-58462-5758-0000 to executor(27)@140.211.11.27:58462
I0507 03:27:38.998700  5797 slave.cpp:2341] Executor 'default' of framework 20140507-032738-453759884-58462-5758-0000 has exited with status 0
I0507 03:27:38.998814  5793 exec.cpp:338] Executor received status update acknowledgement be7346ad-e198-4b38-9252-421ff759fdee for task 0 of framework 20140507-032738-453759884-58462-5758-0000
I0507 03:27:39.000041  5797 slave.cpp:1954] Handling status update TASK_LOST (UUID: 4c8e572c-3fa7-43f3-aaf8-f82e77a70c1b) for task 0 of framework 20140507-032738-453759884-58462-5758-0000 from @0.0.0.0:0
I0507 03:27:39.000063  5797 slave.cpp:3446] Terminating task 0
I0507 03:27:39.000190  5797 status_update_manager.cpp:320] Received status update TASK_LOST (UUID: 4c8e572c-3fa7-43f3-aaf8-f82e77a70c1b) for task 0 of framework 20140507-032738-453759884-58462-5758-0000
I0507 03:27:39.000229  5779 master.cpp:2523] Executor default of framework 20140507-032738-453759884-58462-5758-0000 on slave 20140507-032738-453759884-58462-5758-0 at slave(56)@140.211.11.27:58462 (hemera.apache.org) has exited with status 0
I0507 03:27:39.000341  5797 status_update_manager.cpp:398] Received status update acknowledgement (UUID: be7346ad-e198-4b38-9252-421ff759fdee) for task 0 of framework 20140507-032738-453759884-58462-5758-0000
I0507 03:27:39.000385  5797 status_update_manager.cpp:373] Forwarding status update TASK_LOST (UUID: 4c8e572c-3fa7-43f3-aaf8-f82e77a70c1b) for task 0 of framework 20140507-032738-453759884-58462-5758-0000 to master@140.211.11.27:58462
I0507 03:27:39.000516  5791 slave.cpp:2071] Status update manager successfully handled status update TASK_LOST (UUID: 4c8e572c-3fa7-43f3-aaf8-f82e77a70c1b) for task 0 of framework 20140507-032738-453759884-58462-5758-0000
I0507 03:27:39.000686  5791 slave.cpp:1539] Status update manager successfully handled status update acknowledgement (UUID: be7346ad-e198-4b38-9252-421ff759fdee) for task 0 of framework 20140507-032738-453759884-58462-5758-0000
I0507 03:27:39.000759  5795 master.cpp:2450] Status update TASK_LOST (UUID: 4c8e572c-3fa7-43f3-aaf8-f82e77a70c1b) for task 0 of framework 20140507-032738-453759884-58462-5758-0000 from slave 20140507-032738-453759884-58462-5758-0 at slave(56)@140.211.11.27:58462 (hemera.apache.org)
I0507 03:27:39.000841  5784 sched.cpp:620] Scheduler::statusUpdate took 11418ns
I0507 03:27:39.000849  5795 master.hpp:602] Removing task 0 with resources cpus(*):2; mem(*):1024 on slave 20140507-032738-453759884-58462-5758-0 (hemera.apache.org)
I0507 03:27:39.001313  5799 hierarchical_allocator_process.hpp:636] Recovered cpus(*):2; mem(*):1024 (total allocatable: disk(*):1024; ports(*):[31000-32000]; cpus(*):2; mem(*):1024) on slave 20140507-032738-453759884-58462-5758-0 from framework 20140507-032738-453759884-58462-5758-0000
I0507 03:27:39.002792  5778 status_update_manager.cpp:398] Received status update acknowledgement (UUID: 4c8e572c-3fa7-43f3-aaf8-f82e77a70c1b) for task 0 of framework 20140507-032738-453759884-58462-5758-0000
I0507 03:27:39.002831  5778 status_update_manager.cpp:530] Cleaning up status update stream for task 0 of framework 20140507-032738-453759884-58462-5758-0000
I0507 03:27:39.002903  5778 slave.cpp:1539] Status update manager successfully handled status update acknowledgement (UUID: 4c8e572c-3fa7-43f3-aaf8-f82e77a70c1b) for task 0 of framework 20140507-032738-453759884-58462-5758-0000
I0507 03:27:39.002976  5778 slave.cpp:3470] Completing task 0
I0507 03:27:39.002991  5778 slave.cpp:2480] Cleaning up executor 'default' of framework 20140507-032738-453759884-58462-5758-0000
I0507 03:27:39.006098  5778 slave.cpp:2555] Cleaning up framework 20140507-032738-453759884-58462-5758-0000
I0507 03:27:39.006105  5800 gc.cpp:56] Scheduling '/tmp/GarbageCollectorIntegrationTest_DiskUsage_A9Pxks/slaves/20140507-032738-453759884-58462-5758-0/frameworks/20140507-032738-453759884-58462-5758-0000/executors/default/runs/de776bec-2822-4bbc-befc-eec40eb5f674' for gc 1.00000000231788weeks in the future
I0507 03:27:39.006146  5800 gc.cpp:56] Scheduling '/tmp/GarbageCollectorIntegrationTest_DiskUsage_A9Pxks/slaves/20140507-032738-453759884-58462-5758-0/frameworks/20140507-032738-453759884-58462-5758-0000/executors/default' for gc 1.00000000231788weeks in the future
I0507 03:27:39.006211  5786 status_update_manager.cpp:282] Closing status update streams for framework 20140507-032738-453759884-58462-5758-0000
I0507 03:27:39.006299  5786 gc.cpp:56] Scheduling '/tmp/GarbageCollectorIntegrationTest_DiskUsage_A9Pxks/slaves/20140507-032738-453759884-58462-5758-0/frameworks/20140507-032738-453759884-58462-5758-0000' for gc 1.00000000231788weeks in the future
I0507 03:27:39.010058  5777 leveldb.cpp:341] Persisting action (18 bytes) to leveldb took 15.533184ms
I0507 03:27:39.010144  5777 leveldb.cpp:399] Deleting ~2 keys from leveldb took 64787ns
I0507 03:27:39.010154  5777 replica.cpp:676] Persisted action at 4
I0507 03:27:39.010160  5777 replica.cpp:661] Replica learned TRUNCATE action at position 4
I0507 03:27:39.029413  5789 slave.cpp:2801] Current usage 90.00%. Max allowed age: 0ns
../../src/tests/gc_tests.cpp:658: Failure
Value of: os::exists(executorDir)
  Actual: true
Expected: false
{noformat}",2.0,0.19.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.17431192660550457
Improvement,"Add ""per-framework-principal"" counters for all messages from a scheduler on Master",Framework::principal is used identify one or more frameworks. If multiple frameworks use the same principal they'll have one counter showing their combined message count.,3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.15789473684210525,0.15,0.20512820512820512,0.13939393939393938,0.13939393939393938,0.0
Improvement,Improve Master and Slave metric names,"As we move the metrics to a new endpoint, we should consider revisiting the names of some of the current metrics to make them clearer.

It may also be worth considering changing some existing counter-style metrics to gauges.
",3.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05263157894736842,0.12142857142857143,0.07692307692307693,0.13333333333333333,0.13333333333333333,0.0
Improvement,Implement decent unit test coverage for the mesos-fetcher tool,"There are current no tests that cover the {{mesos-fetcher}} tool itself, and hence bugs like MESOS-1313 have accidentally slipped though.",2.0,0,0.5,0.06184012066365008,0.0,0.0,0.0,0.0,0.2,0.10526315789473684,0.12857142857142856,0.01282051282051282,0.006060606060606061,0.006060606060606061,0.0
Bug,"ExamplesTest.{TestFramework, NoExecutorFramework} flaky","I'm having trouble reproducing this but I did observe it once on my OSX system:

{noformat}
[==========] Running 2 tests from 1 test case.
[----------] Global test environment set-up.
[----------] 2 tests from ExamplesTest
[ RUN      ] ExamplesTest.TestFramework
../../src/tests/script.cpp:81: Failure
Failed
test_framework_test.sh terminated with signal 'Abort trap: 6'
[  FAILED  ] ExamplesTest.TestFramework (953 ms)
[ RUN      ] ExamplesTest.NoExecutorFramework
[       OK ] ExamplesTest.NoExecutorFramework (10162 ms)
[----------] 2 tests from ExamplesTest (11115 ms total)

[----------] Global test environment tear-down
[==========] 2 tests from 1 test case ran. (11121 ms total)
[  PASSED  ] 1 test.
[  FAILED  ] 1 test, listed below:
[  FAILED  ] ExamplesTest.TestFramework
{noformat}

when investigating a failed make check for https://reviews.apache.org/r/20971/
{noformat}
[----------] 6 tests from ExamplesTest
[ RUN      ] ExamplesTest.TestFramework
[       OK ] ExamplesTest.TestFramework (8643 ms)
[ RUN      ] ExamplesTest.NoExecutorFramework
tests/script.cpp:81: Failure
Failed
no_executor_framework_test.sh terminated with signal 'Aborted'
[  FAILED  ] ExamplesTest.NoExecutorFramework (7220 ms)
[ RUN      ] ExamplesTest.JavaFramework
[       OK ] ExamplesTest.JavaFramework (11181 ms)
[ RUN      ] ExamplesTest.JavaException
[       OK ] ExamplesTest.JavaException (5624 ms)
[ RUN      ] ExamplesTest.JavaLog
[       OK ] ExamplesTest.JavaLog (6472 ms)
[ RUN      ] ExamplesTest.PythonFramework
[       OK ] ExamplesTest.PythonFramework (14467 ms)
[----------] 6 tests from ExamplesTest (53607 ms total)
{noformat}",1.0,0,0.5,0.3740573152337858,0.0,0.0,0.0,0.2,0.4,0.43859649122807015,0.2714285714285714,0.10256410256410256,0.2121212121212121,0.2121212121212121,0.0
Improvement,stout's os::ls should return a Try<>,stout's os::ls returns a list that can be empty - instead it should return a Try<list...> to be consistent.,2.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05263157894736842,0.12142857142857143,0.07692307692307693,0.13333333333333333,0.13333333333333333,0.0
Improvement,stout's os module uses a mix of Try<Nothing> and bool returns,stout's os module should use Try<Nothing> for return values throughout.,2.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.05263157894736842,0.12142857142857143,0.07692307692307693,0.13333333333333333,0.13333333333333333,0.0
Improvement,"Subprocess is ""slow"" -> gated by process::reap poll interval","Subprocess uses process::reap to wait on the subprocess pid and set the exit status. However, process::reap polls with a one second interval resulting in a delay up to the interval duration before the status future is set.

This means if you need to wait for the subprocess to complete you get hit with E(delay) = 0.5 seconds, independent of the execution time. For example, the MesosContainerizer uses mesos-fetcher in a Subprocess to fetch the executor during launch. At Twitter we fetch a local file, i.e., a very fast operation, but the launch is blocked until the mesos-fetcher pid is reaped -> adding 0 to 1 seconds for every launch!

The problem is even worse with a chain of short Subprocesses because after the first Subprocess completes you'll be synchronized with the reap interval and you'll see nearly the full interval before notification, i.e., 10 Subprocesses each of << 1 second duration with take ~10 seconds!

This has become particularly apparent in some new tests I'm working on where test durations are now greatly extended with each taking several seconds.",1.0,0.18.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.10526315789473684,0.16428571428571428,0.10256410256410256,0.2121212121212121,0.2121212121212121,0.16513761467889906
Bug,systemd.slice + cgroup enablement fails in multiple ways. ,"When attempting to configure mesos to use systemd slices on a 'rawhide/f21' machine, it fails creating the isolator: 

I0407 12:39:28.035354 14916 containerizer.cpp:180] Using isolation: cgroups/cpu,cgroups/mem
Failed to create a containerizer: Could not create isolator cgroups/cpu: Failed to create isolator: The cpu subsystem is co-mounted at /sys/fs/cgroup/cpu with other subsytems

------ details ------
/sys/fs/cgroup
total 0
drwxr-xr-x. 12 root root 280 Mar 18 08:47 .
drwxr-xr-x.  6 root root   0 Mar 18 08:47 ..
drwxr-xr-x.  2 root root   0 Mar 18 08:47 blkio
lrwxrwxrwx.  1 root root  11 Mar 18 08:47 cpu -> cpu,cpuacct
lrwxrwxrwx.  1 root root  11 Mar 18 08:47 cpuacct -> cpu,cpuacct
drwxr-xr-x.  2 root root   0 Mar 18 08:47 cpu,cpuacct
drwxr-xr-x.  2 root root   0 Mar 18 08:47 cpuset
drwxr-xr-x.  2 root root   0 Mar 18 08:47 devices
drwxr-xr-x.  2 root root   0 Mar 18 08:47 freezer
drwxr-xr-x.  2 root root   0 Mar 18 08:47 hugetlb
drwxr-xr-x.  3 root root   0 Apr  3 11:26 memory
drwxr-xr-x.  2 root root   0 Mar 18 08:47 net_cls
drwxr-xr-x.  2 root root   0 Mar 18 08:47 perf_event
drwxr-xr-x.  4 root root   0 Mar 18 08:47 systemd
",3.0,0.18.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.017543859649122806,0.007142857142857143,0.02564102564102564,0.006060606060606061,0.006060606060606061,0.16513761467889906
Improvement,Add a TASK_ERROR task status.,"During task validation we drop tasks that have errors and send TASK_LOST status updates. In most circumstances a framework will want to relaunch a task that has gone lost, and in the event the task is actually malformed (thus invalid) this will result in an infinite loop of sending a task and having it go lost.",2.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.0,0.05263157894736842,0.12142857142857143,0.02564102564102564,0.0303030303030303,0.01818181818181818,0.0
Bug,ExamplesTest.JavaLog is flaky,"The {{ExamplesTest.JavaLog}} test framework is flaky, possibly related to a race condition between mutexes.
{noformat}
[ RUN      ] ExamplesTest.JavaLog
Using temporary directory '/tmp/ExamplesTest_JavaLog_WBWEb9'
Feb 18, 2014 12:10:57 PM TestLog main
INFO: Starting a local ZooKeeper server
...
F0218 12:10:58.575036 17450 coordinator.cpp:394] Check failed: !missing Not expecting local replica to be missing position 3 after the writing is done
*** Check failure stack trace: ***
tests/script.cpp:81: Failure
Failed
java_log_test.sh terminated with signal 'Aborted'
[  FAILED  ] ExamplesTest.JavaLog (2166 ms)
{noformat}

Full logs attached.",2.0,0.19.0,0.5,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.8846153846153846,0.9878787878787878,0.9878787878787878,0.17431192660550457
Bug,Python extension build is broken if gflags-dev is installed,"In my environment mesos build from master results in broken python api module {{_mesos.so}}:
{noformat}
nekto0n@ya-darkstar ~/workspace/mesos/src/python $ PYTHONPATH=build/lib.linux-x86_64-2.7/ python -c ""import _mesos""
Traceback (most recent call last):
  File ""<string>"", line 1, in <module>
ImportError: /home/nekto0n/workspace/mesos/src/python/build/lib.linux-x86_64-2.7/_mesos.so: undefined symbol: _ZN6google14FlagRegistererC1EPKcS2_S2_S2_PvS3_
{noformat}
Unmangled version of symbol looks like this:
{noformat}
google::FlagRegisterer::FlagRegisterer(char const*, char const*, char const*, char const*, void*, void*)
{noformat}
During {{./configure}} step {{glog}} finds {{gflags}} development files and starts using them, thus *implicitly* adding dependency on {{libgflags.so}}. This breaks Python extensions module and perhaps can break other mesos subsystems when moved to hosts without {{gflags}} installed.

This task is done when the ExamplesTest.PythonFramework test will pass on a system with gflags installed.",3.0,0,0.5,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.01282051282051282,0.006060606060606061,0.006060606060606061,0.0
Bug,SlaveRecoveryTest/1.SchedulerFailover is flaky,"[==========] Running 1 test from 1 test case.
[----------] Global test environment set-up.
[----------] 1 test from SlaveRecoveryTest/1, where TypeParam = mesos::internal::slave::CgroupsIsolator
[ RUN      ] SlaveRecoveryTest/1.SchedulerFailover
I0206 20:18:31.525116 56447 master.cpp:239] Master ID: 2014-02-06-20:18:31-1740121354-55566-56447 Hostname: smfd-bkq-03-sr4.devel.twitter.com
I0206 20:18:31.525295 56481 master.cpp:321] Master started on 10.37.184.103:55566
I0206 20:18:31.525315 56481 master.cpp:324] Master only allowing authenticated frameworks to register!
I0206 20:18:31.527093 56481 master.cpp:756] The newly elected leader is master@10.37.184.103:55566
I0206 20:18:31.527122 56481 master.cpp:764] Elected as the leading master!
I0206 20:18:31.530642 56473 slave.cpp:112] Slave started on 9)@10.37.184.103:55566
I0206 20:18:31.530802 56473 slave.cpp:212] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0206 20:18:31.531203 56473 slave.cpp:240] Slave hostname: smfd-bkq-03-sr4.devel.twitter.com
I0206 20:18:31.531221 56473 slave.cpp:241] Slave checkpoint: true
I0206 20:18:31.531991 56482 cgroups_isolator.cpp:225] Using /tmp/mesos_test_cgroup as cgroups hierarchy root
I0206 20:18:31.532470 56478 state.cpp:33] Recovering state from '/tmp/SlaveRecoveryTest_1_SchedulerFailover_7dC2N1/meta'
I0206 20:18:31.532698 56469 status_update_manager.cpp:188] Recovering status update manager
I0206 20:18:31.533962 56472 sched.cpp:265] Authenticating with master master@10.37.184.103:55566
I0206 20:18:31.534102 56472 sched.cpp:234] Detecting new master
I0206 20:18:31.534124 56484 authenticatee.hpp:124] Creating new client SASL connection
I0206 20:18:31.534299 56473 master.cpp:2317] Authenticating framework at scheduler(9)@10.37.184.103:55566
I0206 20:18:31.534459 56461 authenticator.hpp:140] Creating new server SASL connection
I0206 20:18:31.534572 56466 authenticatee.hpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0206 20:18:31.534595 56466 authenticatee.hpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0206 20:18:31.534667 56474 authenticator.hpp:243] Received SASL authentication start
I0206 20:18:31.534732 56474 authenticator.hpp:325] Authentication requires more steps
I0206 20:18:31.534814 56468 authenticatee.hpp:258] Received SASL authentication step
I0206 20:18:31.534946 56466 authenticator.hpp:271] Received SASL authentication step
I0206 20:18:31.535007 56466 authenticator.hpp:317] Authentication success
I0206 20:18:31.535084 56471 authenticatee.hpp:298] Authentication success
I0206 20:18:31.535107 56461 master.cpp:2357] Successfully authenticated framework at scheduler(9)@10.37.184.103:55566
I0206 20:18:31.535392 56476 sched.cpp:339] Successfully authenticated with master master@10.37.184.103:55566
I0206 20:18:31.535512 56465 master.cpp:812] Received registration request from scheduler(9)@10.37.184.103:55566
I0206 20:18:31.535570 56465 master.cpp:830] Registering framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 at scheduler(9)@10.37.184.103:55566
I0206 20:18:31.535856 56465 hierarchical_allocator_process.hpp:332] Added framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
I0206 20:18:31.537802 56482 cgroups_isolator.cpp:840] Recovering isolator
I0206 20:18:31.538462 56472 slave.cpp:2760] Finished recovery
I0206 20:18:31.538910 56472 slave.cpp:508] New master detected at master@10.37.184.103:55566
I0206 20:18:31.539036 56478 status_update_manager.cpp:162] New master detected at master@10.37.184.103:55566
I0206 20:18:31.539223 56464 master.cpp:1834] Attempting to register slave on smfd-bkq-03-sr4.devel.twitter.com at slave(9)@10.37.184.103:55566
I0206 20:18:31.539271 56472 slave.cpp:533] Detecting new master
I0206 20:18:31.539330 56464 master.cpp:2804] Adding slave 2014-02-06-20:18:31-1740121354-55566-56447-0 at smfd-bkq-03-sr4.devel.twitter.com with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0206 20:18:31.539454 56472 slave.cpp:551] Registered with master master@10.37.184.103:55566; given slave ID 2014-02-06-20:18:31-1740121354-55566-56447-0
I0206 20:18:31.539620 56472 slave.cpp:564] Checkpointing SlaveInfo to '/tmp/SlaveRecoveryTest_1_SchedulerFailover_7dC2N1/meta/slaves/2014-02-06-20:18:31-1740121354-55566-56447-0/slave.info'
I0206 20:18:31.539834 56475 hierarchical_allocator_process.hpp:445] Added slave 2014-02-06-20:18:31-1740121354-55566-56447-0 (smfd-bkq-03-sr4.devel.twitter.com) with cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (and cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] available)
I0206 20:18:31.540341 56472 master.cpp:2272] Sending 1 offers to framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
I0206 20:18:31.543433 56472 master.cpp:1568] Processing reply for offers: [ 2014-02-06-20:18:31-1740121354-55566-56447-0 ] on slave 2014-02-06-20:18:31-1740121354-55566-56447-0 (smfd-bkq-03-sr4.devel.twitter.com) for framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
I0206 20:18:31.543642 56472 master.hpp:411] Adding task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 2014-02-06-20:18:31-1740121354-55566-56447-0 (smfd-bkq-03-sr4.devel.twitter.com)
I0206 20:18:31.543781 56472 master.cpp:2441] Launching task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 2014-02-06-20:18:31-1740121354-55566-56447-0 (smfd-bkq-03-sr4.devel.twitter.com)
I0206 20:18:31.544002 56484 slave.cpp:736] Got assigned task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 for framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
I0206 20:18:31.544097 56484 slave.cpp:2899] Checkpointing FrameworkInfo to '/tmp/SlaveRecoveryTest_1_SchedulerFailover_7dC2N1/meta/slaves/2014-02-06-20:18:31-1740121354-55566-56447-0/frameworks/2014-02-06-20:18:31-1740121354-55566-56447-0000/framework.info'
I0206 20:18:31.544272 56484 slave.cpp:2906] Checkpointing framework pid 'scheduler(9)@10.37.184.103:55566' to '/tmp/SlaveRecoveryTest_1_SchedulerFailover_7dC2N1/meta/slaves/2014-02-06-20:18:31-1740121354-55566-56447-0/frameworks/2014-02-06-20:18:31-1740121354-55566-56447-0000/framework.pid'
I0206 20:18:31.544617 56484 slave.cpp:845] Launching task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 for framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
I0206 20:18:31.546721 56484 slave.cpp:3169] Checkpointing ExecutorInfo to '/tmp/SlaveRecoveryTest_1_SchedulerFailover_7dC2N1/meta/slaves/2014-02-06-20:18:31-1740121354-55566-56447-0/frameworks/2014-02-06-20:18:31-1740121354-55566-56447-0000/executors/d045a0bd-2ed2-410a-bd1f-5bd9219896e3/executor.info'
I0206 20:18:31.547317 56484 slave.cpp:3257] Checkpointing TaskInfo to '/tmp/SlaveRecoveryTest_1_SchedulerFailover_7dC2N1/meta/slaves/2014-02-06-20:18:31-1740121354-55566-56447-0/frameworks/2014-02-06-20:18:31-1740121354-55566-56447-0000/executors/d045a0bd-2ed2-410a-bd1f-5bd9219896e3/runs/9adabe16-5d84-45c9-bc83-1a72a6d1c986/tasks/d045a0bd-2ed2-410a-bd1f-5bd9219896e3/task.info'
I0206 20:18:31.547514 56484 slave.cpp:955] Queuing task 'd045a0bd-2ed2-410a-bd1f-5bd9219896e3' for executor d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework '2014-02-06-20:18:31-1740121354-55566-56447-0000
I0206 20:18:31.547590 56481 cgroups_isolator.cpp:517] Launching d045a0bd-2ed2-410a-bd1f-5bd9219896e3 (/home/vinod/mesos/build/src/mesos-executor) in /tmp/SlaveRecoveryTest_1_SchedulerFailover_7dC2N1/slaves/2014-02-06-20:18:31-1740121354-55566-56447-0/frameworks/2014-02-06-20:18:31-1740121354-55566-56447-0000/executors/d045a0bd-2ed2-410a-bd1f-5bd9219896e3/runs/9adabe16-5d84-45c9-bc83-1a72a6d1c986 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] for framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 in cgroup mesos_test/framework_2014-02-06-20:18:31-1740121354-55566-56447-0000_executor_d045a0bd-2ed2-410a-bd1f-5bd9219896e3_tag_9adabe16-5d84-45c9-bc83-1a72a6d1c986
I0206 20:18:31.548408 56481 cgroups_isolator.cpp:717] Changing cgroup controls for executor d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0206 20:18:31.548833 56481 cgroups_isolator.cpp:1007] Updated 'cpu.shares' to 2048 for executor d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
I0206 20:18:31.549294 56481 cgroups_isolator.cpp:1117] Updated 'memory.soft_limit_in_bytes' to 1GB for executor d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
I0206 20:18:31.550107 56481 cgroups_isolator.cpp:1147] Updated 'memory.limit_in_bytes' to 1GB for executor d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
I0206 20:18:31.550571 56481 cgroups_isolator.cpp:1174] Started listening for OOM events for executor d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
I0206 20:18:31.551553 56481 cgroups_isolator.cpp:569] Forked executor at = 56671
Checkpointing executor's forked pid 56671 to '/tmp/SlaveRecoveryTest_1_SchedulerFailover_7dC2N1/meta/slaves/2014-02-06-20:18:31-1740121354-55566-56447-0/frameworks/2014-02-06-20:18:31-1740121354-55566-56447-0000/executors/d045a0bd-2ed2-410a-bd1f-5bd9219896e3/runs/9adabe16-5d84-45c9-bc83-1a72a6d1c986/pids/forked.pid'
I0206 20:18:31.552222 56472 slave.cpp:2098] Monitoring executor d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 forked at pid 56671
Fetching resources into '/tmp/SlaveRecoveryTest_1_SchedulerFailover_7dC2N1/slaves/2014-02-06-20:18:31-1740121354-55566-56447-0/frameworks/2014-02-06-20:18:31-1740121354-55566-56447-0000/executors/d045a0bd-2ed2-410a-bd1f-5bd9219896e3/runs/9adabe16-5d84-45c9-bc83-1a72a6d1c986'
I0206 20:18:31.604012 56472 slave.cpp:1431] Got registration for executor 'd045a0bd-2ed2-410a-bd1f-5bd9219896e3' of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
I0206 20:18:31.604167 56472 slave.cpp:1516] Checkpointing executor pid 'executor(1)@10.37.184.103:46181' to '/tmp/SlaveRecoveryTest_1_SchedulerFailover_7dC2N1/meta/slaves/2014-02-06-20:18:31-1740121354-55566-56447-0/frameworks/2014-02-06-20:18:31-1740121354-55566-56447-0000/executors/d045a0bd-2ed2-410a-bd1f-5bd9219896e3/runs/9adabe16-5d84-45c9-bc83-1a72a6d1c986/pids/libprocess.pid'
I0206 20:18:31.605183 56472 slave.cpp:1552] Flushing queued task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 for executor 'd045a0bd-2ed2-410a-bd1f-5bd9219896e3' of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
Registered executor on smfd-bkq-03-sr4.devel.twitter.com
Starting task d045a0bd-2ed2-410a-bd1f-5bd9219896e3
sh -c 'sleep 1000'
Forked command at 56712
I0206 20:18:31.613098 56481 slave.cpp:1765] Handling status update TASK_RUNNING (UUID: fc151a46-751b-4c4b-b048-1727752f34e3) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 from executor(1)@10.37.184.103:46181
I0206 20:18:31.613628 56469 status_update_manager.cpp:314] Received status update TASK_RUNNING (UUID: fc151a46-751b-4c4b-b048-1727752f34e3) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
I0206 20:18:31.614006 56469 status_update_manager.hpp:342] Checkpointing UPDATE for status update TASK_RUNNING (UUID: fc151a46-751b-4c4b-b048-1727752f34e3) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
I0206 20:18:31.795529 56469 status_update_manager.cpp:367] Forwarding status update TASK_RUNNING (UUID: fc151a46-751b-4c4b-b048-1727752f34e3) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 to master@10.37.184.103:55566
I0206 20:18:31.795992 56480 slave.cpp:1890] Sending acknowledgement for status update TASK_RUNNING (UUID: fc151a46-751b-4c4b-b048-1727752f34e3) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 to executor(1)@10.37.184.103:46181
I0206 20:18:31.796131 56471 master.cpp:2020] Status update TASK_RUNNING (UUID: fc151a46-751b-4c4b-b048-1727752f34e3) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 from slave(9)@10.37.184.103:55566
I0206 20:18:31.797099 56483 status_update_manager.cpp:392] Received status update acknowledgement (UUID: fc151a46-751b-4c4b-b048-1727752f34e3) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
I0206 20:18:31.797165 56483 status_update_manager.hpp:342] Checkpointing ACK for status update TASK_RUNNING (UUID: fc151a46-751b-4c4b-b048-1727752f34e3) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
I0206 20:18:31.882767 56481 slave.cpp:394] Slave terminating
I0206 20:18:31.883112 56481 master.cpp:641] Slave 2014-02-06-20:18:31-1740121354-55566-56447-0 (smfd-bkq-03-sr4.devel.twitter.com) disconnected
I0206 20:18:31.883200 56476 hierarchical_allocator_process.hpp:484] Slave 2014-02-06-20:18:31-1740121354-55566-56447-0 disconnected
I0206 20:18:31.888206 56473 sched.cpp:265] Authenticating with master master@10.37.184.103:55566
I0206 20:18:31.888473 56473 sched.cpp:234] Detecting new master
I0206 20:18:31.888556 56469 authenticatee.hpp:124] Creating new client SASL connection
I0206 20:18:31.888978 56484 master.cpp:2317] Authenticating framework at scheduler(10)@10.37.184.103:55566
I0206 20:18:31.889348 56469 authenticator.hpp:140] Creating new server SASL connection
I0206 20:18:31.889925 56469 authenticatee.hpp:212] Received SASL authentication mechanisms: CRAM-MD5
I0206 20:18:31.889989 56469 authenticatee.hpp:238] Attempting to authenticate with mechanism 'CRAM-MD5'
I0206 20:18:31.890059 56469 authenticator.hpp:243] Received SASL authentication start
I0206 20:18:31.890233 56469 authenticator.hpp:325] Authentication requires more steps
I0206 20:18:31.890399 56468 authenticatee.hpp:258] Received SASL authentication step
I0206 20:18:31.890554 56484 authenticator.hpp:271] Received SASL authentication step
I0206 20:18:31.890630 56484 authenticator.hpp:317] Authentication success
I0206 20:18:31.890728 56470 authenticatee.hpp:298] Authentication success
I0206 20:18:31.890748 56484 master.cpp:2357] Successfully authenticated framework at scheduler(10)@10.37.184.103:55566
I0206 20:18:31.892210 56469 sched.cpp:339] Successfully authenticated with master master@10.37.184.103:55566
I0206 20:18:31.892410 56473 master.cpp:900] Re-registering framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 at scheduler(10)@10.37.184.103:55566
I0206 20:18:31.892460 56473 master.cpp:926] Framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 failed over
W0206 20:18:31.892691 56465 master.cpp:1048] Ignoring deactivate framework message for framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 from 'scheduler(9)@10.37.184.103:55566' because it is not from the registered framework 'scheduler(10)@10.37.184.103:55566'
I0206 20:18:31.897049 56466 slave.cpp:112] Slave started on 10)@10.37.184.103:55566
I0206 20:18:31.897207 56466 slave.cpp:212] Slave resources: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0206 20:18:31.897536 56466 slave.cpp:240] Slave hostname: smfd-bkq-03-sr4.devel.twitter.com
I0206 20:18:31.897554 56466 slave.cpp:241] Slave checkpoint: true
I0206 20:18:31.898388 56463 cgroups_isolator.cpp:225] Using /tmp/mesos_test_cgroup as cgroups hierarchy root
I0206 20:18:31.898936 56472 state.cpp:33] Recovering state from '/tmp/SlaveRecoveryTest_1_SchedulerFailover_7dC2N1/meta'
I0206 20:18:31.901702 56465 slave.cpp:2828] Recovering framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
I0206 20:18:31.901759 56465 slave.cpp:3020] Recovering executor 'd045a0bd-2ed2-410a-bd1f-5bd9219896e3' of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
I0206 20:18:31.902716 56464 status_update_manager.cpp:188] Recovering status update manager
I0206 20:18:31.902884 56464 status_update_manager.cpp:196] Recovering executor 'd045a0bd-2ed2-410a-bd1f-5bd9219896e3' of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
I0206 20:18:34.475915 56463 cgroups_isolator.cpp:840] Recovering isolator
I0206 20:18:34.476066 56463 cgroups_isolator.cpp:847] Recovering executor 'd045a0bd-2ed2-410a-bd1f-5bd9219896e3' of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
I0206 20:18:34.477478 56463 cgroups_isolator.cpp:1174] Started listening for OOM events for executor d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
I0206 20:18:34.478728 56463 slave.cpp:2700] Sending reconnect request to executor d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 at executor(1)@10.37.184.103:46181
I0206 20:18:34.480114 56476 slave.cpp:1597] Re-registering executor d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
I0206 20:18:34.480566 56476 cgroups_isolator.cpp:717] Changing cgroup controls for executor d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]
I0206 20:18:34.481370 56476 cgroups_isolator.cpp:1007] Updated 'cpu.shares' to 2048 for executor d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
I0206 20:18:34.481827 56476 cgroups_isolator.cpp:1117] Updated 'memory.soft_limit_in_bytes' to 1GB for executor d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
Re-registered executor on smfd-bkq-03-sr4.devel.twitter.com
I0206 20:18:34.489497 56471 slave.cpp:1713] Cleaning up un-reregistered executors
I0206 20:18:34.489588 56471 slave.cpp:2760] Finished recovery
I0206 20:18:34.490048 56463 slave.cpp:508] New master detected at master@10.37.184.103:55566
I0206 20:18:34.490257 56475 status_update_manager.cpp:162] New master detected at master@10.37.184.103:55566
I0206 20:18:34.490357 56463 slave.cpp:533] Detecting new master
W0206 20:18:34.490603 56480 master.cpp:1878] Slave at slave(10)@10.37.184.103:55566 (smfd-bkq-03-sr4.devel.twitter.com) is being allowed to re-register with an already in use id (2014-02-06-20:18:31-1740121354-55566-56447-0)
I0206 20:18:34.490927 56479 slave.cpp:601] Re-registered with master master@10.37.184.103:55566
I0206 20:18:34.491322 56461 hierarchical_allocator_process.hpp:498] Slave 2014-02-06-20:18:31-1740121354-55566-56447-0 reconnected
I0206 20:18:34.491421 56468 slave.cpp:1312] Updating framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 pid to scheduler(10)@10.37.184.103:55566
I0206 20:18:34.491444 56480 master.cpp:1673] Asked to kill task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
I0206 20:18:34.491488 56468 slave.cpp:1320] Checkpointing framework pid 'scheduler(10)@10.37.184.103:55566' to '/tmp/SlaveRecoveryTest_1_SchedulerFailover_7dC2N1/meta/slaves/2014-02-06-20:18:31-1740121354-55566-56447-0/frameworks/2014-02-06-20:18:31-1740121354-55566-56447-0000/framework.pid'
I0206 20:18:34.491497 56480 master.cpp:1707] Telling slave 2014-02-06-20:18:31-1740121354-55566-56447-0 (smfd-bkq-03-sr4.devel.twitter.com) to kill task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
I0206 20:18:34.491657 56468 slave.cpp:1013] Asked to kill task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
Shutting down
Killing process tree at pid 56712
Killed the following process trees:
[ 
--- 56712 sleep 1000 
]
Command terminated with signal Killed (pid: 56712)
I0206 20:18:34.615216 56463 slave.cpp:1765] Handling status update TASK_KILLED (UUID: d9d37827-3002-4a67-8659-fa36f1986fc7) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 from executor(1)@10.37.184.103:46181
I0206 20:18:34.615556 56483 cgroups_isolator.cpp:717] Changing cgroup controls for executor d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 with resources 
I0206 20:18:34.615624 56476 status_update_manager.cpp:314] Received status update TASK_KILLED (UUID: d9d37827-3002-4a67-8659-fa36f1986fc7) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
I0206 20:18:34.615701 56476 status_update_manager.hpp:342] Checkpointing UPDATE for status update TASK_KILLED (UUID: d9d37827-3002-4a67-8659-fa36f1986fc7) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
I0206 20:18:34.706945 56476 status_update_manager.cpp:367] Forwarding status update TASK_KILLED (UUID: d9d37827-3002-4a67-8659-fa36f1986fc7) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 to master@10.37.184.103:55566
I0206 20:18:34.707263 56476 slave.cpp:1890] Sending acknowledgement for status update TASK_KILLED (UUID: d9d37827-3002-4a67-8659-fa36f1986fc7) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 to executor(1)@10.37.184.103:46181
I0206 20:18:34.707352 56469 master.cpp:2020] Status update TASK_KILLED (UUID: d9d37827-3002-4a67-8659-fa36f1986fc7) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000 from slave(10)@10.37.184.103:55566
I0206 20:18:34.707620 56469 master.hpp:429] Removing task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 with resources cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] on slave 2014-02-06-20:18:31-1740121354-55566-56447-0 (smfd-bkq-03-sr4.devel.twitter.com)
I0206 20:18:34.708348 56466 hierarchical_allocator_process.hpp:637] Recovered cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000] (total allocatable: cpus(*):2; mem(*):1024; disk(*):1024; ports(*):[31000-32000]) on slave 2014-02-06-20:18:31-1740121354-55566-56447-0 from framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
I0206 20:18:34.708673 56469 status_update_manager.cpp:392] Received status update acknowledgement (UUID: d9d37827-3002-4a67-8659-fa36f1986fc7) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
I0206 20:18:34.708749 56469 status_update_manager.hpp:342] Checkpointing ACK for status update TASK_KILLED (UUID: d9d37827-3002-4a67-8659-fa36f1986fc7) for task d045a0bd-2ed2-410a-bd1f-5bd9219896e3 of framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
I0206 20:18:34.709411 56470 master.cpp:2272] Sending 1 offers to framework 2014-02-06-20:18:31-1740121354-55566-56447-0000
I0206 20:18:34.809782 56447 master.cpp:583] Master terminating
I0206 20:18:34.810066 56447 master.cpp:246] Shutting down master
I0206 20:18:34.810134 56482 slave.cpp:1965] master@10.37.184.103:55566 exited
W0206 20:18:34.810184 56482 slave.cpp:1968] Master disconnected! Waiting for a new master to be elected
I0206 20:18:34.810652 56447 master.cpp:289] Removing slave 2014-02-06-20:18:31-1740121354-55566-56447-0 (smfd-bkq-03-sr4.devel.twitter.com)
I0206 20:18:34.813144 56447 slave.cpp:394] Slave terminating
I0206 20:18:34.821583 56467 cgroups.cpp:1209] Trying to freeze cgroup /tmp/mesos_test_cgroup/mesos_test
I0206 20:18:34.821652 56467 cgroups.cpp:1248] Successfully froze cgroup /tmp/mesos_test_cgroup/mesos_test after 1 attempts
I0206 20:18:34.823129 56471 cgroups.cpp:1224] Trying to thaw cgroup /tmp/mesos_test_cgroup/mesos_test
I0206 20:18:34.823247 56471 cgroups.cpp:1334] Successfully thawed /tmp/mesos_test_cgroup/mesos_test
I0206 20:18:34.923945 56470 cgroups.cpp:1209] Trying to freeze cgroup /tmp/mesos_test_cgroup/mesos_test/framework_2014-02-06-20:18:31-1740121354-55566-56447-0000_executor_d045a0bd-2ed2-410a-bd1f-5bd9219896e3_tag_9adabe16-5d84-45c9-bc83-1a72a6d1c986
I0206 20:18:34.924018 56470 cgroups.cpp:1248] Successfully froze cgroup /tmp/mesos_test_cgroup/mesos_test/framework_2014-02-06-20:18:31-1740121354-55566-56447-0000_executor_d045a0bd-2ed2-410a-bd1f-5bd9219896e3_tag_9adabe16-5d84-45c9-bc83-1a72a6d1c986 after 1 attempts
I0206 20:18:34.925506 56461 cgroups.cpp:1224] Trying to thaw cgroup /tmp/mesos_test_cgroup/mesos_test/framework_2014-02-06-20:18:31-1740121354-55566-56447-0000_executor_d045a0bd-2ed2-410a-bd1f-5bd9219896e3_tag_9adabe16-5d84-45c9-bc83-1a72a6d1c986
I0206 20:18:34.925580 56461 cgroups.cpp:1334] Successfully thawed /tmp/mesos_test_cgroup/mesos_test/framework_2014-02-06-20:18:31-1740121354-55566-56447-0000_executor_d045a0bd-2ed2-410a-bd1f-5bd9219896e3_tag_9adabe16-5d84-45c9-bc83-1a72a6d1c986
[       OK ] SlaveRecoveryTest/1.SchedulerFailover (3408 ms)
[----------] 1 test from SlaveRecoveryTest/1 (3409 ms total)

[----------] Global test environment tear-down
../../src/tests/environment.cpp:247: Failure
Failed
Tests completed with child processes remaining:
-+- 56447 /home/vinod/mesos/build/src/.libs/lt-mesos-tests --verbose --gtest_filter=*SlaveRecoveryTest/1.SchedulerFailover* --gtest_repeat=10 
 \--- 56671 ()
",1.0,0.18.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.8846153846153846,0.9878787878787878,0.9878787878787878,0.16513761467889906
Bug,'Logging and Debugging' document is out-of-date.,"The following is no longer correct:
http://mesos.apache.org/documentation/latest/logging-and-debugging/

We should either delete this document or re-write it entirely.",1.0,0,0.5,1.0,0.0,0.0,0.0,0.0,0.8,0.5438596491228069,0.39999999999999997,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.0
Improvement,Set GLOG_drop_log_memory=false in environment prior to logging initialization.,"We've observed issues where the masters are slow to respond. Two perf traces collected while the masters were slow to respond:

{noformat}
 25.84%  [kernel]                [k] default_send_IPI_mask_sequence_phys
 20.44%  [kernel]                [k] native_write_msr_safe
  4.54%  [kernel]                [k] _raw_spin_lock
  2.95%  libc-2.5.so             [.] _int_malloc
  1.82%  libc-2.5.so             [.] malloc
  1.55%  [kernel]                [k] apic_timer_interrupt
  1.36%  libc-2.5.so             [.] _int_free
{noformat}

{noformat}
 29.03%  [kernel]                [k] default_send_IPI_mask_sequence_phys
  9.64%  [kernel]                [k] _raw_spin_lock
  7.38%  [kernel]                [k] native_write_msr_safe
  2.43%  libc-2.5.so             [.] _int_malloc
  2.05%  libc-2.5.so             [.] _int_free
  1.67%  [kernel]                [k] apic_timer_interrupt
  1.58%  libc-2.5.so             [.] malloc
{noformat}

These have been found to be attributed to the posix_fadvise calls made by glog. We can disable these via the environment:

{noformat}
GLOG_DEFINE_bool(drop_log_memory, true, ""Drop in-memory buffers of log contents. ""
                 ""Logs can grow very quickly and they are rarely read before they ""
                 ""need to be evicted from memory. Instead, drop them from memory ""
                 ""as soon as they are flushed to disk."");

{noformat}

{code}
    if (FLAGS_drop_log_memory) {
      if (file_length_ >= logging::kPageSize) {
        // don't evict the most recent page
        uint32 len = file_length_ & ~(logging::kPageSize - 1);
        posix_fadvise(fileno(file_), 0, len, POSIX_FADV_DONTNEED);
      }
    }
{code}

We should set GLOG_drop_log_memory=false prior to making our call to google::InitGoogleLogging, to avoid others running into this issue.",2.0,"0.15.0,0.16.0",0.5,0.3453996983408748,0.0,0.0,0.0,0.0,0.2,0.38596491228070173,0.35714285714285715,0.6282051282051282,0.7515151515151515,0.7515151515151515,0.1422018348623853
Story,Add Kerberos Authentication support,"MESOS-704 added basic authentication support using CRAM-MD5 through SASL. Now we should integrate Kerberos authentication using GSS-API, which is already supported by SASL. Kerberos is a widely-used industry standard authentication service, and integration with Mesos will make it easier for customers to integrate their existing security process with Mesos.",2.0,0,0.5,0.3740573152337858,0.0,0.0,0.0,0.2,0.4,0.43859649122807015,0.2714285714285714,0.05128205128205128,0.1515151515151515,0.15757575757575756,0.0
Bug,SlaveRecoveryTest/0.ReconcileTasksMissingFromSlave test is flaky,"[ RUN      ] SlaveRecoveryTest/0.ReconcileTasksMissingFromSlave
Checkpointing executor's forked pid 32281 to '/tmp/SlaveRecoveryTest_0_ReconcileTasksMissingFromSlave_NT1btb/meta/slaves/201310151913-16777343-35153-31491-0/frameworks/201310151913-16777343-35153-31491-0000/executors/0514b52f-3c17-4ee5-ba16-635198701ca2/runs/97c9e2cc-ceea-40a8-a915-aed5fed1dcb3/pids/forked.pid'
Fetching resources into '/tmp/SlaveRecoveryTest_0_ReconcileTasksMissingFromSlave_NT1btb/slaves/201310151913-16777343-35153-31491-0/frameworks/201310151913-16777343-35153-31491-0000/executors/0514b52f-3c17-4ee5-ba16-635198701ca2/runs/97c9e2cc-ceea-40a8-a915-aed5fed1dcb3'
Registered executor on localhost.localdomain
Starting task 0514b52f-3c17-4ee5-ba16-635198701ca2
Forked command at 32317
sh -c 'sleep 10'
tests/slave_recovery_tests.cpp:1927: Failure
Mock function called more times than expected - returning directly.
    Function call: statusUpdate(0x7fffae636eb0, @0x7f1590027a00 64-byte object <F0-2F D0-A1 15-7F 00-00 00-00 00-00 00-00 00-00 40-E9 01-90 15-7F 00-00 20-6B 03-90 15-7F 00-00 48-91 C3-00 00-00 00-00 B0-3B 01-90 15-7F 00-00 05-00 00-00 00-00 00-00 17-00 00-00 00-00 00-00>)
         Expected: to be called once
           Actual: called twice - over-saturated and active
Command exited with status 0 (pid: 32317)
",1.0,0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.0,0.8846153846153846,0.9878787878787878,0.9878787878787878,0.0
Bug,Also check 'git diff --shortstat --staged' in post-reviews.py.,We current check if you have any changes before we run post-reviews.py but we don't check for staged changes which IIUC could get lost.,1.0,0,0.5,0.06334841628959276,0.0,0.0,0.0,0.0,0.0,0.2982456140350877,0.19999999999999998,0.02564102564102564,0.0303030303030303,0.01818181818181818,0.0
Improvement,Update Contribution Documentation,"Our contribution guide is currently fairly verbose, and it focuses on the ReviewBoard workflow for making code contributions. It would be helpful for new contributors to have a first-time contribution guide which focuses on using GitHub PRs to make small contributions, since that workflow has a smaller barrier to entry for new users.",3.0,0,0.5,0.11161387631975868,0.0,0.0,0.0,1.0,0.8,0.894736842105263,1.0,0.0,0.0,0.0,0.0
Improvement,Report executor terminations to framework schedulers.,"The Scheduler interface has a callback for executorLost, but currently it is never called.",2.0,0,0.5,0.0,0.0,0.0,0.0,0.0,0.4,0.07017543859649122,0.12142857142857143,0.0,0.0,0.0,0.0
